
@article{gentzkow_text_2019,
	title = {Text as {Data}},
	volume = {57},
	issn = {0022-0515},
	url = {https://www.aeaweb.org/articles?id=10.1257/jel.20181020},
	doi = {10.1257/jel.20181020},
	abstract = {An ever-increasing share of human interaction, communication, and culture is recorded as digital text. We provide an introduction to the use of text as an input to economic research. We discuss the features that make text different from other forms of data, offer a practical overview of relevant statistical methods, and survey a variety of applications.},
	language = {en},
	number = {3},
	urldate = {2021-12-02},
	journal = {Journal of Economic Literature},
	author = {Gentzkow, Matthew and Kelly, Bryan and Taddy, Matt},
	month = sep,
	year = {2019},
	keywords = {Cluster Analysis, Economic Anthropology, Factor Models, Large Data Sets: Modeling and Analysis, Entertainment, Language, Media, Economic Sociology, Multiple or Simultaneous Equation Models: Classification Methods, Principal Components, Social and Economic Stratification},
	pages = {535--574},
	file = {Snapshot:C\:\\Users\\PC HP 2020\\Zotero\\storage\\KTEFJ3Z7\\articles.html:text/html},
}

@article{siegel_text_2018,
	title = {Text {Mining} in {Economics}},
	url = {https://www.researchgate.net/publication/322797994_Text_Mining_in_Economics},
	abstract = {Annual business reports – containing consolidated financial statements and management reports, as well as other information - are statutory instruments of financial accounting in Germany. They are an important source of information for business analysts. The information in management reports is mostly unstructured text, and is therefore complex for an algorithm to analyze. For some analysis questions related to economic research, we have identified techniques from speech technology that can effectively support the analysis. We have implemented these techniques in a prototype. It became clear that an approach based on semantic analysis and ontological information is useful for this purpose. Natural Language Processing (NLP) techniques are used to help building an ontology database.},
	urldate = {2021-12-02},
	journal = {Semantic Applications},
	author = {Siegel, Mélanie},
	year = {2018},
	file = {(PDF) Text Mining in Economics:C\:\\Users\\PC HP 2020\\Zotero\\storage\\VBAVUJMF\\322797994_Text_Mining_in_Economics.html:text/html},
}

@techreport{farris_judicial_2016,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Judicial {Impact} of {Law} {School} {Faculties}},
	url = {https://papers.ssrn.com/abstract=2826048},
	abstract = {This study is a follow-up to our scholarly impact study published in 2015, “Scholarly Impact of Law School Faculties in 2015: Updating the Leiter Score Ranking for the Top Third.”  Looking at an expanded time period (2005-2014), we assessed the extent to which extensive citations in the legal literature translated into citations by courts. It is important to acknowledge that the judicial citation rates were very low, precluding extensive analysis and making it difficult to regard some of the results as reliable and robust. Our study indicates that a certain subset of scholars are both noticed and cited by the judiciary as well as their peers.    To identify the scholars who are most cited by courts, we leveraged the same data (roster of names and name variations) and used a similar methodology to our 2015 scholarly impact study. Using the school rosters from the scholarly impact survey, we identified scholars with two hundred and fifty or more citations in the legal literature. We then created a very broad search  that would include any academic article cited by courts, but would exclude amicus briefs, testimony, judging, and any other incidences of writing or impact. We studied three different court cohorts: citations to scholarly works by the Supreme Court, citations to scholarly works by the U.S. Courts of Appeals, and citations to scholarly works by all state high courts (whether formally called Supreme Courts or Courts of Appeal).},
	language = {en},
	number = {ID 2826048},
	urldate = {2021-12-02},
	institution = {Social Science Research Network},
	author = {Farris, Nick and Aggerbeck, Valerie and McNevin, Megan and Sisk, Gregory C.},
	month = aug,
	year = {2016},
	doi = {10.2139/ssrn.2826048},
	keywords = {Gregory C. Sisk, Judicial Impact of Law School Faculties, Megan McNevin, Nick Farris, SSRN, Valerie Aggerbeck},
}

@misc{leiter_top_2014,
	title = {Top {Ten} {Law} {Faculty} (by area) in {Scholarly} {Impact}, 2009-2013},
	url = {https://www.leiterrankings.com/faculty/2014_scholarlyimpact.shtml},
	urldate = {2021-12-02},
	author = {Leiter, Brian},
	year = {2014},
	file = {New Document:C\:\\Users\\PC HP 2020\\Zotero\\storage\\CBIZEVT5\\2014_scholarlyimpact.html:text/html},
}

@inproceedings{smelyakov_effectiveness_2021,
	title = {Effectiveness of {Modern} {Text} {Recognition} {Solutions} and {Tools} for {Common} {Data} {Sources}},
	abstract = {In the article features of functioning of the most common optical character recognition (OCR) tools EasyOCR and TesserOCR are considered and experimental analysis of results of work of these OCRs are given. In the article features of functioning of the most common optical character recognition (OCR) tools EasyOCR and TesserOCR are considered; experimental analysis of results of work of these OCR is given for the most widespread data sources, such as electronic text document, internet resource, and banner; based on analysis of the experiment results from the comparative analysis of considered OCRs by time and accuracy was made; effective algorithm of using an OCR and recommendations for their application was offered for not-distorted data, for slightly and highly distorted data.},
	booktitle = {{COLINS}},
	author = {Smelyakov, K. and Chupryna, A. and Darahan, Dmytro and Midina, Serhii},
	year = {2021},
}

@article{vijayarani_performance_2015,
	title = {Performance {Comparison} of {OCR} {Tools}},
	volume = {6},
	doi = {10.5121/iju.2015.6303},
	abstract = {Optical Character Recognition (OCR) is a technique, used to convert scanned image into editable text
format. Many different types of Optical Character Recognition (OCR) tools are commercially available
today; it is a useful and popular method for different types of applications. OCR can predict the accurate
result depends on text pre-processing and segmentation algorithms. Image quality is one of the most
important factors that improve quality of recognition in performing OCR tools. Images can be processed
independently (.png, .jpg, and .gif files) or in multi-page PDF documents (.pdf). The primary objective of
this work is to provide the overview of various Optical Character Recognition (OCR) tools and analyses of
their performance by applying the two factors of OCR tool performance i.e. accuracy and error rate},
	journal = {International Journal of UbiComp},
	author = {Vijayarani, S and Sakila, A},
	month = jul,
	year = {2015},
	pages = {19--30},
	file = {Texte intégral:C\:\\Users\\PC HP 2020\\Zotero\\storage\\HUVBU2GQ\\S et A - 2015 - Performance Comparison of OCR Tools.pdf:application/pdf},
}

@misc{shinyama_pdfminersix_2021,
	title = {pdfminer.six},
	copyright = {MIT},
	url = {https://github.com/pdfminer/pdfminer.six},
	abstract = {Community maintained fork of pdfminer - we fathom PDF},
	urldate = {2021-12-06},
	publisher = {pdfminer},
	author = {Shinyama, Yusuke},
	month = dec,
	year = {2021},
	note = {original-date: 2014-08-29T14:04:53Z},
	keywords = {parser, pdf},
}

@misc{korzen_improved_2019,
	title = {Improved {Dehyphenation} of {Line} {Breaks} for {PDF} {Text} {Extraction}},
	url = {https://www.semanticscholar.org/paper/Improved-Dehyphenation-of-Line-Breaks-for-PDF-Text-Korzen/754758fc0f8ff1f52a94288256ec947cabb50270},
	abstract = {Words in layout-based documents can contain hyphens that divide the word into two parts across two lines. PDF documents only store information about individual characters, which makes it difficult to extract text correctly. Words which are hyphenated at the end of a line are especially problematic because in English there are some words where the hyphen should be kept, while others need to be merged. For example, “high-quality” can be a compound word. If this word splits on the hyphen across two lines, then the hyphen should be retained in the extracted text. The correct dehyphenation of line breaks requires the recognition of either words or sequences of characters. In this thesis, vocabulary-based baseline algorithms, logistic regression on the word level and a bi-LSTM Language Model on the character level are used to solve this problem. On the ClueWeb12 Extract data set, the vocabulary-based algorithm achieved a balanced accuracy (bACC) of 66.87\%, the logistic regression 92.38\% and the bi-LSTM Language Model 90.14\%. Our investigations showed a trade-off effect between recognising words which naturally contain hyphens and predicting words without hyphens correctly.},
	language = {en},
	urldate = {2021-12-06},
	author = {Korzen, Claudius},
	year = {2019},
	file = {Snapshot:C\:\\Users\\PC HP 2020\\Zotero\\storage\\RE8H3GVY\\754758fc0f8ff1f52a94288256ec947cabb50270.html:text/html},
}

@inproceedings{berg_towards_2012,
	address = {Jeju Island, Korea},
	title = {Towards {High}-{Quality} {Text} {Stream} {Extraction} from {PDF}. {Technical} {Background} to the {ACL} 2012 {Contributed} {Task}},
	url = {https://aclanthology.org/W12-3211},
	urldate = {2021-12-06},
	booktitle = {Proceedings of the {ACL}-2012 {Special} {Workshop} on {Rediscovering} 50 {Years} of {Discoveries}},
	publisher = {Association for Computational Linguistics},
	author = {Berg, Øyvind Raddum and Oepen, Stephan and Read, Jonathon},
	month = jul,
	year = {2012},
	pages = {98--103},
	file = {Full Text PDF:C\:\\Users\\PC HP 2020\\Zotero\\storage\\JSXGID5W\\Berg et al. - 2012 - Towards High-Quality Text Stream Extraction from P.pdf:application/pdf},
}

@article{ramakrishnan_layout-aware_2012,
	title = {Layout-aware text extraction from full-text {PDF} of scientific articles},
	volume = {7},
	issn = {1751-0473},
	url = {https://doi.org/10.1186/1751-0473-7-7},
	doi = {10.1186/1751-0473-7-7},
	abstract = {The Portable Document Format (PDF) is the most commonly used file format for online scientific publications. The absence of effective means to extract text from these PDF files in a layout-aware manner presents a significant challenge for developers of biomedical text mining or biocuration informatics systems that use published literature as an information source. In this paper we introduce the ‘Layout-Aware PDF Text Extraction’ (LA-PDFText) system to facilitate accurate extraction of text from PDF files of research articles for use in text mining applications.},
	number = {1},
	urldate = {2021-12-06},
	journal = {Source Code for Biology and Medicine},
	author = {Ramakrishnan, Cartic and Patnia, Abhishek and Hovy, Eduard and Burns, Gully APC},
	month = may,
	year = {2012},
	keywords = {Block Detection, Optical Character Recognition, Portable Document Format, Text Block, Text Extraction},
	pages = {7},
	file = {Full Text PDF:C\:\\Users\\PC HP 2020\\Zotero\\storage\\VV4VTVYA\\Ramakrishnan et al. - 2012 - Layout-aware text extraction from full-text PDF of.pdf:application/pdf;Snapshot:C\:\\Users\\PC HP 2020\\Zotero\\storage\\FYXWJA7E\\1751-0473-7-7.html:text/html},
}

@article{tkaczyk_cermine_2015,
	title = {{CERMINE}: automatic extraction of structured metadata from scientific literature},
	volume = {18},
	issn = {1433-2825},
	shorttitle = {{CERMINE}},
	url = {https://doi.org/10.1007/s10032-015-0249-8},
	doi = {10.1007/s10032-015-0249-8},
	abstract = {CERMINE is a comprehensive open-source system for extracting structured metadata from scientific articles in a born-digital form. The system is based on a modular workflow, whose loosely coupled architecture allows for individual component evaluation and adjustment, enables effortless improvements and replacements of independent parts of the algorithm and facilitates future architecture expanding. The implementations of most steps are based on supervised and unsupervised machine learning techniques, which simplifies the procedure of adapting the system to new document layouts and styles. The evaluation of the extraction workflow carried out with the use of a large dataset showed good performance for most metadata types, with the average F score of 77.5 \%. CERMINE system is available under an open-source licence and can be accessed at http://cermine.ceon.pl. In this paper, we outline the overall workflow architecture and provide details about individual steps implementations. We also thoroughly compare CERMINE to similar solutions, describe evaluation methodology and finally report its results.},
	language = {en},
	number = {4},
	urldate = {2021-12-06},
	journal = {International Journal on Document Analysis and Recognition (IJDAR)},
	author = {Tkaczyk, Dominika and Szostek, Paweł and Fedoryszak, Mateusz and Dendek, Piotr Jan and Bolikowski, Łukasz},
	month = dec,
	year = {2015},
	pages = {317--335},
	file = {Springer Full Text PDF:C\:\\Users\\PC HP 2020\\Zotero\\storage\\UC43I2I5\\Tkaczyk et al. - 2015 - CERMINE automatic extraction of structured metada.pdf:application/pdf},
}

@techreport{truc_forty_2021,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Forty {Years} of {Behavioral} {Economics}},
	url = {https://papers.ssrn.com/abstract=3762621},
	abstract = {The present article offers the first quantitative history of behavioral economics (BE) from the 1970s to the 2010s. We document the foundation of the field by Kahneman and Tversky in the 1980s and 1990s; the separation of experimental economics and BE in the 1990s; the decreasing importance of psychology in the 1990s onward; and the rise of European authors after the 2000s. Overall, we show that after the 1990s, BE transformed from a unified American research program with a clearly identifiable core, to a multipolar and international research program with relatively independent subspecialties. Despite claims that BE is mostly an empirical venture, we show that the field is heavily structured by theoretical contributions. A handful of seminal models capture most of the citations in the field and explain how the subspecialties in BE emerged, stabilized, and became more autonomous from the historical core.},
	language = {en},
	number = {ID 3762621},
	urldate = {2021-12-07},
	institution = {Social Science Research Network},
	author = {Truc, Alexandre},
	month = jan,
	year = {2021},
	doi = {10.2139/ssrn.3762621},
	keywords = {Behavioral Economics, Interdisciplinarity, Social Network Analysis},
}

@article{ambrosino_what_2018,
	title = {What topic modeling could reveal about the evolution of economics},
	volume = {25},
	doi = {10.1080/1350178X.2018.1529215},
	abstract = {The paper presents the topic modeling technique known as Latent Dirichlet Allocation (LDA), a form of text-mining aiming at discovering the hidden (latent) thematic structure in large archives of documents. By applying LDA to the full text of the economics articles stored in the JSTOR database, we show how to construct a map of the discipline over time, and illustrate the potentialities of the technique for the study of the shifting structure of economics in a time of (possible) fragmentation. © 2018},
	journal = {Journal of Economic Methodology},
	author = {Ambrosino, Angela and Cedrini, Mario and Davis, John and Fiori, Stefano and Guerzoni, Marco and Nuccio, Massimiliano},
	month = oct,
	year = {2018},
	pages = {1--20},
}

@book{hahn_proceedings_2018,
	address = {Melbourne, Australia},
	title = {Proceedings of the {First} {Workshop} on {Economics} and {Natural} {Language} {Processing}},
	url = {https://aclanthology.org/W18-3100},
	urldate = {2021-12-07},
	publisher = {Association for Computational Linguistics},
	editor = {Hahn, Udo and Hoste, Véronique and Tsai, Ming-Feng},
	month = jul,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\PC HP 2020\\Zotero\\storage\\RV8N3R6D\\Hahn et al. - 2018 - Proceedings of the First Workshop on Economics and.pdf:application/pdf},
}

@book{hahn_proceedings_2019,
	address = {Hong Kong},
	title = {Proceedings of the {Second} {Workshop} on {Economics} and {Natural} {Language} {Processing}},
	url = {https://aclanthology.org/D19-5100},
	urldate = {2021-12-07},
	publisher = {Association for Computational Linguistics},
	editor = {Hahn, Udo and Hoste, Véronique and Zhang, Zhu},
	month = nov,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\PC HP 2020\\Zotero\\storage\\LNKVX44C\\Hahn et al. - 2019 - Proceedings of the Second Workshop on Economics an.pdf:application/pdf},
}

@misc{shaham_beautiful-head_2020,
	title = {Beautiful-{Head}: {Tracing} the {Evolution} and {Associations} of {Name}-drops in {Blogs}},
	shorttitle = {Beautiful-{Head}},
	url = {https://towardsdatascience.com/beautiful-head-tracing-the-evolution-and-associations-of-name-drops-in-blogs-c6764b3504ee},
	abstract = {A Natural Language Processing \& Network Science approach to Visualize the Dynamics of Named Persons in Information Streams},
	language = {en},
	urldate = {2021-12-07},
	journal = {Medium},
	author = {Shaham, Farooq},
	month = feb,
	year = {2020},
	file = {Snapshot:C\:\\Users\\PC HP 2020\\Zotero\\storage\\IU8DS3LG\\beautiful-head-tracing-the-evolution-and-associations-of-name-drops-in-blogs-c6764b3504ee.html:text/html},
}

@misc{bandeh-ahmadi_case_2015,
	title = {The {Case} for {Natural} {Language} {Processing} in {Economics} – {LLRX}},
	url = {https://www.llrx.com/2015/06/the-case-for-natural-language-processing-in-economics/},
	language = {en-US},
	urldate = {2021-12-07},
	author = {Bandeh-Ahmadi, Ayeh},
	month = jun,
	year = {2015},
	file = {Snapshot:C\:\\Users\\PC HP 2020\\Zotero\\storage\\H9IW5F8J\\the-case-for-natural-language-processing-in-economics.html:text/html},
}

@inproceedings{blei_dynamic_2006,
	address = {New York, NY, USA},
	series = {{ICML} '06},
	title = {Dynamic topic models},
	isbn = {978-1-59593-383-6},
	url = {https://doi.org/10.1145/1143844.1143859},
	doi = {10.1145/1143844.1143859},
	abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.},
	urldate = {2021-12-08},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Blei, David M. and Lafferty, John D.},
	month = jun,
	year = {2006},
	pages = {113--120},
}

@article{murphy_mapping_2020,
	title = {Mapping the links between gender, status and genre in {Shakespeare}’s plays},
	volume = {29},
	issn = {0963-9470},
	url = {https://doi.org/10.1177/0963947020949438},
	doi = {10.1177/0963947020949438},
	abstract = {The Arts and Humanities Research Council-funded Encyclopedia of Shakespeare’s Language project has produced a resource allowing users to explore Shakespeare’s plays in a variety of (semi-automatic) ways, via a web-based corpus query processor interface hosted by Lancaster University. It enables users, for example, to interrogate a corpus of Shakespeare’s plays using queries restricted by dramatic genre, gender and/or social status of characters, and to target and explore the language of the plays not only at the word level but also at the grammatical and semantic levels (by querying part of speech or semantic categories). Using keyword techniques, we examine how female and male language varies in general, by social status (high or low) and by genre (comedy, history and tragedy). Among our findings, we note differences in the use of pronouns and references to male authority (female overuse of ‘I’ and ‘husband’ and male overuse of ‘we’ and ‘king’). We also observe that high-status males in comedies (as opposed to histories and tragedies) are characterised by polite requests (‘please you’) and sharp-minded ‘wit’. Despite many similarities between female and male usage of gendered forms of language (‘woman’), male characters alone use terms such as ‘womanish’ in a disparaging way.},
	language = {en},
	number = {3},
	urldate = {2021-12-08},
	journal = {Language and Literature},
	author = {Murphy, Sean and Archer, Dawn and Demmen, Jane},
	month = aug,
	year = {2020},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {Early Modern English, gender, genre, plays, rank, Shakespeare, status, web‐based corpus query processor interface},
	pages = {223--245},
	file = {Version acceptée:C\:\\Users\\PC HP 2020\\Zotero\\storage\\7V4UDGKW\\Murphy et al. - 2020 - Mapping the links between gender, status and genre.pdf:application/pdf},
}

@article{altszyler_comparative_2017,
	title = {Comparative study of {LSA} vs {Word2vec} embeddings in small corpora: a case study in dreams database},
	volume = {56},
	issn = {10538100},
	shorttitle = {Comparative study of {LSA} vs {Word2vec} embeddings in small corpora},
	url = {http://arxiv.org/abs/1610.01520},
	doi = {10.1016/j.concog.2017.09.004},
	abstract = {Word embeddings have been extensively studied in large text datasets. However, only a few studies analyze semantic representations of small corpora, particularly relevant in single-person text production studies. In the present paper, we compare Skip-gram and LSA capabilities in this scenario, and we test both techniques to extract relevant semantic patterns in single-series dreams reports. LSA showed better performance than Skip-gram in small size training corpus in two semantic tests. As a study case, we show that LSA can capture relevant words associations in dream reports series, even in cases of small number of dreams or low-frequency words. We propose that LSA can be used to explore words associations in dreams reports, which could bring new insight into this classic research area of psychology},
	urldate = {2021-12-08},
	journal = {Consciousness and Cognition},
	author = {Altszyler, Edgar and Sigman, Mariano and Ribeiro, Sidarta and Slezak, Diego Fernández},
	month = nov,
	year = {2017},
	note = {arXiv: 1610.01520},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	pages = {178--187},
	file = {arXiv Fulltext PDF:C\:\\Users\\PC HP 2020\\Zotero\\storage\\VWXT9DMQ\\Altszyler et al. - 2017 - Comparative study of LSA vs Word2vec embeddings in.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\PC HP 2020\\Zotero\\storage\\RZ8CAQJU\\1610.html:text/html},
}

@article{grimmer_bayesian_2010,
	title = {A {Bayesian} {Hierarchical} {Topic} {Model} for {Political} {Texts}: {Measuring} {Expressed} {Agendas} in {Senate} {Press} {Releases}},
	volume = {18},
	issn = {1047-1987, 1476-4989},
	shorttitle = {A {Bayesian} {Hierarchical} {Topic} {Model} for {Political} {Texts}},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/bayesian-hierarchical-topic-model-for-political-texts-measuring-expressed-agendas-in-senate-press-releases/74F30D05C220DB198F21FF5127EB7205#},
	doi = {10.1093/pan/mpp034},
	abstract = {Political scientists lack methods to efficiently measure the priorities political actors emphasize in statements. To address this limitation, I introduce a statistical model that attends to the structure of political rhetoric when measuring expressed priorities: statements are naturally organized by author. The expressed agenda model exploits this structure to simultaneously estimate the topics in the texts, as well as the attention political actors allocate to the estimated topics. I apply the method to a collection of over 24,000 press releases from senators from 2007, which I demonstrate is an ideal medium to measure how senators explain their work in Washington to constituents. A set of examples validates the estimated priorities and demonstrates their usefulness for testing theories of how members of Congress communicate with constituents. The statistical model and its extensions will be made available in a forthcoming free software package for the R computing language.},
	language = {en},
	number = {1},
	urldate = {2021-12-09},
	journal = {Political Analysis},
	author = {Grimmer, Justin},
	year = {2010},
	note = {Publisher: Cambridge University Press},
	pages = {1--35},
	file = {Full Text PDF:C\:\\Users\\PC HP 2020\\Zotero\\storage\\97SXWFUW\\Grimmer - 2010 - A Bayesian Hierarchical Topic Model for Political .pdf:application/pdf},
}

@article{piepenbrink_topic_2017,
	title = {Topic models as a novel approach to identify themes in content analysis},
	volume = {2017},
	issn = {0065-0668},
	url = {https://journals.aom.org/doi/abs/10.5465/AMBPP.2017.141},
	doi = {10.5465/AMBPP.2017.141},
	abstract = {In this paper, we demonstrate the usage of topic modeling as a computer aided content analytic tool in the larger context of methods for analyzing text data. We present some key features of topic modeling based on Latent Dirichlet Allocation (LDA), and demonstrated its application by analyzing the articles published in Organization Research Methods (ORM) since its inception. Our analysis, based on 421 ORM articles reveals 15 topics, which are quite similar to other, more human intensive review exercises. We also identified quantitative measures of relative importance of different topics, which could be used as a variable for further analysis in quantitative studies. To further demonstrate the usage of topic modeling, we identified how the emerged topics varied depending on the disciplinary background of the authors. We conclude by providing some examples of the usage of topic modeling in management research.},
	number = {1},
	urldate = {2021-12-09},
	journal = {Academy of Management Proceedings},
	author = {Piepenbrink, Anke and Gaur, Ajai Singh},
	month = aug,
	year = {2017},
	note = {Publisher: Academy of Management},
	keywords = {content analysis, Latent Dirichlet Allocation (LDA), Topic models},
	pages = {11335},
}

@inproceedings{chang_reading_2009,
	title = {Reading {Tea} {Leaves}: {How} {Humans} {Interpret} {Topic} {Models}},
	volume = {22},
	shorttitle = {Reading {Tea} {Leaves}},
	url = {https://papers.nips.cc/paper/2009/hash/f92586a25bb3145facd64ab20fd554ff-Abstract.html},
	urldate = {2021-12-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Boyd-graber, Jordan and Blei, David},
	year = {2009},
	file = {Full Text PDF:C\:\\Users\\PC HP 2020\\Zotero\\storage\\RTC4DAKP\\Chang et al. - 2009 - Reading Tea Leaves How Humans Interpret Topic Mod.pdf:application/pdf},
}

@inproceedings{griffiths_prediction_2003,
	title = {Prediction and {Semantic} {Association}},
	volume = {15},
	url = {https://papers.nips.cc/paper/2002/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html},
	urldate = {2021-12-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Griffiths, Thomas and Steyvers, Mark},
	year = {2003},
	file = {Full Text PDF:C\:\\Users\\PC HP 2020\\Zotero\\storage\\LTEJPYAZ\\Griffiths et Steyvers - 2003 - Prediction and Semantic Association.pdf:application/pdf},
}

@inproceedings{sievert_ldavis_2014,
	address = {Baltimore, Maryland, USA},
	title = {{LDAvis}: {A} method for visualizing and interpreting topics},
	shorttitle = {{LDAvis}},
	url = {https://aclanthology.org/W14-3110},
	doi = {10.3115/v1/W14-3110},
	urldate = {2021-12-09},
	booktitle = {Proceedings of the {Workshop} on {Interactive} {Language} {Learning}, {Visualization}, and {Interfaces}},
	publisher = {Association for Computational Linguistics},
	author = {Sievert, Carson and Shirley, Kenneth},
	month = jun,
	year = {2014},
	pages = {63--70},
	file = {Full Text PDF:C\:\\Users\\PC HP 2020\\Zotero\\storage\\YG6NA5S4\\Sievert et Shirley - 2014 - LDAvis A method for visualizing and interpreting .pdf:application/pdf},
}

@inproceedings{jelveh_detecting_2014,
	address = {Doha, Qatar},
	title = {Detecting {Latent} {Ideology} in {Expert} {Text}: {Evidence} {From} {Academic} {Papers} in {Economics}},
	shorttitle = {Detecting {Latent} {Ideology} in {Expert} {Text}},
	url = {https://aclanthology.org/D14-1191},
	doi = {10.3115/v1/D14-1191},
	urldate = {2021-12-10},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Jelveh, Zubin and Kogut, Bruce and Naidu, Suresh},
	month = oct,
	year = {2014},
	pages = {1804--1809},
	file = {Full Text PDF:C\:\\Users\\PC HP 2020\\Zotero\\storage\\3HIS8IW7\\Jelveh et al. - 2014 - Detecting Latent Ideology in Expert Text Evidence.pdf:application/pdf},
}

@techreport{jelveh_political_2018,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Political {Language} in {Economics}},
	url = {https://papers.ssrn.com/abstract=2535453},
	abstract = {Do empirical estimates in economics reflect the political orientation of economists? We show that policy-relevant parameters are correlated with economist partisanship as predicted from the text of published academic papers. Specifically, we predict observed political behavior of a subset of economists using the phrases from their academic articles, obtain good out-of-sample fit, and then predict partisanship for all economists. We show considerable sorting of economists into fields of research by predicted partisanship, and yet can detect differences in partisanship among economists even within a field, even across those estimating the same theoretical parameter. Using policy-relevant parameters collected from previous meta-analyses we then show that imputed partisanship is correlated with estimated parameters, such that the implied policy prescription is consistent with partisan leaning. For example, we find that going from the most left-wing authored estimate of the taxable top income elasticity to the most right-wing authored estimate decreases the optimal tax rate from 84\% to 58\%.},
	language = {en},
	number = {ID 2535453},
	urldate = {2021-12-10},
	institution = {Social Science Research Network},
	author = {Jelveh, Zubin and Kogut, Bruce and Naidu, Suresh},
	month = sep,
	year = {2018},
	doi = {10.2139/ssrn.2535453},
	keywords = {Bruce Kogut, Political Language in Economics, SSRN, Suresh Naidu, Zubin Jelveh},
	file = {Full Text PDF:C\:\\Users\\PC HP 2020\\Zotero\\storage\\7N8TMB99\\Jelveh et al. - 2018 - Political Language in Economics.pdf:application/pdf;Snapshot:C\:\\Users\\PC HP 2020\\Zotero\\storage\\J3ADF9KP\\papers.html:text/html},
}
