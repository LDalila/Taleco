Follow this and additional works at: https://chicagounbound.uchicago.edu/law_and_economics Recommended Citation Cass R. Sunstein, "Administrative Law Goes to War" ( John M. Olin Program in Law and Economics Working Paper No. 248, 2005). 
ADMINISTRATIVE LAW GOES TO WAR  
Cass R. Sunstein                  This paper can be downloaded without charge at the Public Law and Legal Theory Working Paper  Series:  http://www.law.uchicago.edu/academics/publiclaw/index.html and  
The Social Science Research Network Electronic Paper Collection:  http://ssrn.com/abstract_id=730025   
T H E   L A W   S C H O O L   T H E   U N I V E R S I T Y   O F   C H I C A G O   
May 2005  
What are the President’s war-making powers? This essay, a brief reply to an article by Curtis Bradley and Jack Goldsmith, contends that the answer lies in administrative law, at least in the first instance. The President’s authority often depends on what Congress has said, and under established principles, the President has a great deal of power to interpret ambiguities in congressional enactments – in war no less than in peace. The principal qualifications involve interpretive principles, also found in administrative law, that call for a narrow construction of presidential authority to invade constitutionally sensitive interests. The relevant arguments are illustrated throughout with reference to the 2001 authorization for the use of military force in response to the attacks of September 11; the authorization may or may not include the power to make war on Iraq and Afghanistan, to use force against those suspected of giving financial aid to terrorist organizations, and to detain American citizens. 
In the 2001 Authorization for the Use of Military Force (AUMF), Congress authorized the President to use all necessary and appropriate force against those nations, organizations, or persons he determines planned, authorized, committed, or aided the terrorist attacks that occurred on September 11, 2001, or harbored such organizations or persons, in order to prevent any future acts * Karl N. Llewellyn Distinguished Service Professor, Law School and Department of Political Science, University of Chicago. I am grateful to the Herbert Fried Fund for financial support. Thanks to Patrick Gudridge, Eric Posner, David Strauss, and Adrian Vermeule for valuable comments; special thanks to Vermeule for extended discussions. 
of international terrorism against the United States by such nations, organizations or persons.1 
who are brought to the United States and imprisoned because they knowingly provided significant financial assistance to organizations that supported Al Qaeda in 2000.2 11, 2001.” 
I suggest that there is, and that it can be found in a single area: administrative law. Most obviously, presidential action under the 2001 AUMF, or any imaginable AUMF, would appear subject to the principles that have emerged in the wake of the Supreme Court’s extraordinarily influential decision in Chevron USA v Natural Resources Defense 
assertion of authority to detain a “little old lady in Switzerland” who unknowingly writes a check to a front to finance al-Qaeda activities). 
to our understanding of presidential power amidst war.5 But I believe that their analysis would be clearer, simpler, and more straightforward if they focused more systematically on administrative law principles and if the international laws of war played a more subordinate role. 6 As we shall see, a special advantage of this approach is that it imposes the right incentives on all those involved, including Congress itself. 
discretion in interpreting the AUMF, and that any ambiguities are for him to resolve, subject to a general constraint of reasonableness. The principal qualification is that if the 
construed narrowly, whatever the President says. Under this framework, the President plainly has the authority to act in cases (1), (2), and (4) above. He lacks that authority in case (6). For reasons to be explored, cases (3) and (5) are extremely difficult. 
justiciability constraints7) and by members of the executive branch advising the President about the legality of proposed courses of action. Indeed, this framework furnishes the appropriate principles not only for understanding any authorization for the use of force, 3467 US 837 (1984). As a sign of Chevron’s influence, consider the fact that the decision was cited 2414 times in its first decade (between 1984 and January 1, 1994), 2,584 times in its next six years (between January 1, 1994 and January 1, 2000), and 2,235 times in its next five years (between January 1, 2000 and January 28, 2005). 
Rev 649 (2000). 5 Curtis Bradley and Jack Goldsmith, Congressional Authorization and the War on Terror, 118 Harv L Rev 2047 (2005). 6 There are difficult questions in the background. Why, exactly, do Bradley and Goldsmith place such emphasis on the laws of war? One possibility is that they believe that Congress legislates against the background that they set, and ought to be taken to be aware of them. This view seems to me artificial. A more promising possibility is that Bradley and Goldsmith believe that the laws of war provide an interpretive resource whether or not Congress is aware of them – that they furnish a set of principles, vindicated by tradition, against which authorizations for the use of force should be understood. On this view, the laws of war are invoked because their use serves to discipline and improve interpretation of any authorization to use force. This second view seems plausible and to justify attention, in hard cases, to the laws of war; but it is best to start with statutory text and with more familiar administrative law principles. 
See Webster v. Doe, 486 US 592 (1988); Dickson v. Secretary of Defense, 68 F.3d 1396 (DC Cir 1995). but also for evaluating all exercises of presidential power when Congress has authorized the President to protect the nation’s security.8 
directly decided the precise question at issue. The second is whether the agency’s interpretation is reasonable. In the aftermath of Chevron, the Court has emphasized the need to ask another question, one that precedes application of the Chevron framework: 
A. 
Chevron Step Zero11 
deference to executive interpretations of law into two categories: Skidmore13 cases and 
judicially, not by the executive; but the court will pay attention to what the executive has said, granting its interpretation “respect according to its persuasiveness.”14 In Chevron cases, the agency’s interpretation is binding unless it violates either of Chevron’s two steps.15 
executive’s action “to carry the force of law.”16 Of course Congress does not usually say, 
Dames & Moore v. Regan, 453 US 654 (1981). 9 467 US at 842-44. 10 US v. Mead Corporation, 533 US 218 (2001) 11 See Thomas W. Merrill & Kristin Hickman, Chevron’s Domain, 89 Geo L J 833, 836 (2001); Cass R. Sunstein, Chevron Step One (unpublished manuscript, May 2005). 12 Mead, supra. 13 Skidmore v. Swift & Co., 323 US 134 (1944). 14 Mead, 531 US at 221. 15 It does matter whether an exercise of authority falls under Skidmore or Chevron; but the difference should not be overstated. It is one of degree, not one of kind, and under Skidmore, courts are likely to accept reasonable agency interpretations. Skidmore itself is good evidence here. 323 US at 140. 16 Mead, 533 US at 221. with anything like clarity, whether it so intends.17 In the ordinary cases, courts infer a delegation of law-making power from “the agency’s power to engage in adjudication or notice-and-comment rulemaking, “ or (and this phrase will turn out to be critical) by “some other indication of comparable congressional intent.”18 It is clear that Chevron deference might be appropriate even if an agency’s decision does not follow from formal procedures of any kind.19 
been given neither adjudicatory authority nor the authority to engage in notice-andcomment rulemaking—and hence that he is not authorized to do anything, under the 
absent. This argument might be supported with an analogy. The executive branch is not entitled to Chevron deference insofar as it is enforcing the criminal law.20 The reason is straightforward: For the Department of Justice, the power of prosecution is not plausibly taken to confer law-interpreting authority.21 Perhaps the same can be said when the 
same relationship to the AUMF that the Department of Justice has to the statutes under which it brings prosecutions. In any case, the President is not an “agency” within the meaning of the Administrative Procedure Act,22 and perhaps this point renders Chevron inapplicable. 
central question is whether Congress should be understood to have conferred on the 
President will be construing any authorization to use force, and it has every incentive to limit his discretion if it wishes to do so. In ordinary Chevron cases, a delegation of lawinterpreting power is inferred from the authority to produce rules or orders with the force 17 Id. at 230: “It is fair to assume generally that Congress contemplates administrative action with the force of law when it provides for a relatively formal administrative procedure. . . . “ 18 Id. at 227. 19 See Barnhart v. Walton, 535 US 212, 222 (2002). 20 See Crandon v. US, 494 US 152, 158 (1990). 21 Note that for administrative agencies, Congress has a choice: It can grant rulemaking and adjudicatory power, or it can refuse to do so. Chevron and Mead deem that choice to be crucial to the decision whether the relevant agency is entitled to Chevron deference. 22 See Franklin v. Massachusetts, 505 U.S. 788, 800-801 (1992). of law.23 But with an authorization to use force, what is authorized is the use of force; there is no grant of rulemaking or adjudicatory authority, and hence the grant or denial of such authority is irrelevant. By its very nature, any AUMF is best taken as an implicit delegation to the President to resolve ambiguities as he (reasonably) sees fit. This position does not only track Congress’ likely expectations, to the extent that they exist; it has the additional advantage of imposing exactly the right incentives on Congress, by requiring it to limit the President’s authority through plain text if that is what it seeks to do. 
attempted reconstruction of congressional will.24 Where Congress has not spoken, interpretations must depend, at least in part, on assessments of the consequences of one or another approach; agencies are in a comparatively good position to make such assessments. And where questions of value are at stake, agencies, subject as they are to presidential control, should resolve those questions as they see fit.25 And if these are the foundations for Chevron, the President should be taken to have the authority to interpret ambiguities as he chooses. Interpretation of an authorization to use force—at least as much as any delegation of authority to agencies, and possibly more—calls for appreciation of consequences and for complex judgments of value. 
B. 
to the precise question at issue.”26 As Bradley and Goldsmith emphasize, the President 23 This point makes clear that Chevron stems from an understanding of organic statutes, not from the APA. 24 See 467 US at 865-66; Cass R. Sunstein, Law and Administration After Chevron, 90 Colum L Rev 2071, 2086-87 (1990) 25 There is a connection here between Chevron and Ronald Dworkin’s view on interpretation, as set out in Law’s Empire (1985). Dworkin contends that interpretation requires a judgment about “fit” with existing materials and also about “justification” of those materials; his conception of law as integrity requires judges to put existing materials in their “best constructive light.” In modern government, courts are often less capable of accomplishing this task than are agencies, precisely because of the comparatively greater expertise and accountability. If interpretation of the AUMF is an interpretive exercise in Dworkin’s sense, as I believe that it is, then the argument for deference to the President is overwhelming. 26 467 US at 842. could not use force against nations that cannot plausibly be connected with the attacks of 
prevent any future acts of international terrorism against the United States by such nations, organizations or persons.28 But that goal is to be pursued through a particular means, which is the use of force against those connected with the attacks of September arguments on behalf of the exercise of force.29 
permissible under the AUMF in 2003, assuming that the President “determined,” on the basis of evidence at the time, that Iraq assisted Al Qaeda before the September 11 attacks. 
about the meaning of relevant terms, such as “aided” or “harbored,” the President has a great deal of discretion to understand them as he sees fit. Those who provide financial assistance to Al Qaeda, certainly with the intention of doing so, appear to be subject to presidential exercises of force under step 1; hence presidential action is authorized for case (4). 
Under step 2, the question is whether the executive’s “answer is based on a permissible construction of the statute,” which requires a judgment about the reasonableness of that construction.30 Turn in this light to case (6). The President is supposed to use “all necessary and appropriate force,” and an execution of someone who can be detained instead is gratuitous; it is neither “necessary” nor “appropriate.” Or suppose that citizens of Switzerland, or the United States, gave small sums of money to 27 Cf. In re Guantanamo Detainee Cases, 355 F. Supp.2d at 475, discussing the detention of two people who were not connected with the attacks of September 11. One “was ‘associated’ with an Islamic missionary group,” who “planned to travel to Pakistan with an individual who later engaged in a suicide bombing,” and who “accepted free food, lodging, and schooling in Pakistan from an organization known to support terrorist acts.” The other had been indicted by a Spanish National High Court Judge “for membership in a terrorist organization.” These detentions are not authorized by the text of the AUMF. 28 Authorization for Use of Military Force, S.J. Res. 23, 107th Cong., 115 Stat. 224 (2001). 29 There are hard questions about whether those who assist Al Qaeda can be considered accessories afterthe-fact or (as Bradley and Goldsmith argue) as co-belligerents. It is not clear that the analysis of cobelligerents carries over to the analysis of those who aid a terrorist organization after the acts that are the predicate for the use of force. 30 See, e .g, Household Credit Services v. MBNA American Bank, 541 US 232 (2004); Republican National Comm. V. FEC, 76 F3d 400, 47 (DC Cir 1996). an umbrella organization, not knowing that some of its funds were going to Al Qaeda.31 It would be unreasonable to interpret the AUMF to authorize the President to use force against those citizens. 
permitted to interpret statutes so as to apply beyond the territorial boundaries of the 
Nor are they allowed to interpret ambiguous statutes to apply retroactively. 33 An agency cannot construe an ambiguous statute so as to raise serious constitutional doubts.34 In these and other contexts, courts have insisted on a series of nondelegation canons, which require legislative rather than merely executive deliberation on the issue in question.35 By their very nature, the nondelegation canons defeat Chevron deference. The reason is that they are specifically designed to require the nation’s lawmaker to make the relevant decisions explicitly. 
A. 
The Presumption of Liberty defining part of American law involving the relationship between liberty and security in wartime.36 Consider in this regard Ex Parte Endo,37 in which the Court struck down the detention of concededly loyal Japanese-Americans on the West Coast. The Court said that in “interpreting a wartime measure we must assume that [Congress’] purpose was to 31 See In re Guantanamo Detainee Cases, supra note, at 61, in which government asserted such authority, and also the authority to detain someone “who teaches English to the son of an al Qaeda member, and a journalist who knows the location of Osama Bin Laden but refuses to disclose it to protect her source.” Id. At least in the cases of unknowing financial assistance and mere instruction in English to a family member of someone in al Qaeda, the interpretation of the AUMF is unreasonable. 32EEOC v Arabian American Oil Co, 499 US 244, 248 (1991). 33Bowen v Georgetown University Hospital, 488 US 204, 208 (1988). 34See Bowen v Georgetown University Hospital, 488 US 204, 208–09 (1988). 35 For general discussion, see Cass R. Sunstein, Nondelegation Canons, 67 U Chi L Rev. 315 (2000). 36 See Richard Pildes and Samuel Issacharoff, Between Civil Libertarianism and Executive Unilateralism: An Institutional Process Approach to Right During Wartime, 5 Theoretical Inquiries in Law (Online Edition) No 1, Article 1 (Jan 2004), online at http://www.bepress.com/til/default/vol5/iss1/art1 (visited Dec 1, 2004); Cass R. Sunstein, Minimalism At War, Supreme Court Review (forthcoming). 37 320 US 81 (1943). allow for the greatest possible accommodation between those liberties and the exigencies of war.”38 The Court emphasized that even in the midst of war, the President would have to identify clear authorization: “if there is to be the greatest possible accommodation of the liberties of the citizen with this war measure, any such implied power [of the 
program.”39 
construction of the Act required the legislature, and not the executive alone, to focus specifically on the question whether national security justified an abridgement of liberty. 
in the decision of the court of appeals in the Padilla.43 At issue was the legality of the detention of an American citizen held as an enemy combatant after having been seized on 
appropriate force” to respond to the September 11 attacks should be understood in light of Endo, which required a specific congressional statement to support an intrusion into the domain of liberty. No such statement could be found.44 
congressional resolution of the competing claims.”45 Not having found any such 38 Id at 300. 39 Id at 302. 40 See Masses Publishing Co v Patten, 244 F 535 (SDNY 1917). 41See, e.g., United States v. Yates, 354 US 298 (1957); Sunstein, supra note. 42 Bradley and Smith argue that the clear statement principles have been applied only in cases involving “presidential actions, unsupported by historical practice, which undermined the constitutional rights of U.S. citizen non-combatants.” Bradley and Smith at 2105. It seems, however, that in at least some circumstances such principles should be applied in cases involving U.S. citizen combatants or foreigners within the terroritorial boundaries of the United States. If, for example, the President attempted to interfere with the religious practices of either citizen combatants or foreigners, a clear congressional statement should be required. 43 Padilla v Rumsfeld, 352 F3d 695 (2d Cir 2003), reversed on other grounds, Rumsfeld v Padilla, 124 S Ct 2711 (2004). 44 Id at 723. 45 Id. resolution, he concluded that Hamdi’s detention was unlawful. The Hamdi plurality disagreed, but it did not question Justice Souter’s claim that a clear statement was required. It concluded instead that the AUMF provided that statement, because the detention of “enemy combatants,” at least for the duration of the conflict in which the capture occurred, “is so fundamental and accepted an incident to war as to be” an authorized exercise of “necessary and appropriate force.”46 
me correct, and it is consistent with what I am emphasizing here: a requirement of legislative clarity for any interference with constitutionally sensitive interests. In case (5), which is Padilla itself, the question is whether the AUMF contains the requisite clarity: I tend to think so, but the point is reasonably disputed.47 B. 
Commander in Chief of the Armed Forces. Perhaps the President has considerable authority to protect the nation when its security is threatened; perhaps this is a central part of “executive” authority.48 If so, then the AUMF should be construed broadly, and in a way that is highly respectful of presidential prerogatives. On this approach, also conventional in administrative law, statutory enactments involving core executive authority should be construed hospitably to the president, so as to avoid the constitutional difficulties that would come from a narrow construction.49 
In recent years, this view can be found most explicitly in the dissenting opinion of Justice Clarence Thomas in the Hamdi case.50 Justice Thomas emphasized that the Constitution accords to the President the “primary responsibility . . . to protect the national security and to conduct the nation’s foreign relations.”51 In support, Justice 46 Id. at 2640. 47As Bradley and Goldsmith emphasize, the constitutionality of any procedures for detention raise separate issues from the question of authorization to detain. 48 See Hamdi, 124 S Ct 2633 (Thomas, J., dissenting). 49 Department of Navy v. Egan, 484 US 518 (1988); US v. Johnson, 481 US 681 (1987); Cartlucci v. Doe, 488 US 939, 99 (1988). 50 Hamdi, 124 S Ct 2633, 2674 (2004). 51 Id. Thomas might well have cited the Court’s decision in Ex Parte Quirin,52 where the Court upheld the use of military commissions to try German saboteurs captured during World War II. In that case, the President asked the Court to hold that as Commander in Chief, the President had inherent authority to create and to use military tribunals. The Court refused to accept this argument: “It is unnecessary for present purposes to determine to what extent the President as Commander in Chief has constitutional power to create military commissions without the support of Congressional legislation. For here Congress has authorized trial of offenses against the law of war before such commissions.”53 But where had Congress done so? The Court relied on Article 15 of the Articles of War54; but Article 15 did not specifically authorize such commissions. Hence the Court’s ruling is best seen as motivated by a desire to avoid ruling on the President’s broad claims about his constitutional authority as Commander in Chief. 
powers of the President, it should be interpreted generously so to permit the President to do as he sees fit. In this domain, the President receives the kind of super-strong deference that comes from the combination of Chevron with what are plausibly taken to be his constitutional responsibilities.55 C. 
Some of the most difficult cases will arise when the relevant canons point in opposing directions. Suppose, for example, that the President makes a reasonable claim of inherent authority to engage in actions that threaten constitutionally sensitive interests.56 The question is whether it is possible to develop rules of priority and harmonization to sort out the relevant conflicts. 52 317 US 1 (1942). 53 Id at 39. 54 See Quirin, 317 U.S. at 27. 55 Note that we are not assuming that the President has clear constitutional power to do as he proposes. Under that assumption, the AUMF is irrelevant. The question here is how the AUMF should be construed when there is a plausible claim – not a holding – that the President has the constitutional power to act. 56 Of course the likelihood of such conflicts depends on judgments about the merits – about the substance of the underlying constitutional principles. If the President has inherent authority to act in the relevant domains, then no such conflicts will arise, simply because clear statements principles will not be required. 
In my view, the answer is straightforward: Constitutionally sensitive rights have a kind of interpretive priority, so that the President needs explicit legislative permission to invade them even if he claims, plausibly, that he is operating in the general domain of his constitutional authority. Consider the constitutional analogue. Even if the President is acting in accordance with his inherent power, he remains subject to the constraints established by the rights-protecting provisions of the Constitution. It follows that for the interpretation of an authorization to use force, liberty should always receive the benefit of the doubt. This point strengthens the conclusion that the President cannot act in case (6), and it helps explain why case (5) is so difficult; it bears on many other issues as well. 
organized and disciplined if it is undertaken with close reference to standard principles of administrative law. It follows that the President has a great deal of discretion to interpret congressional authorizations for the use of force, subject only to the limits of reasonableness. I am suggesting, in short, that Chevron has imperialistic aspirations. Its broad coverage includes the President’s statutory authority in the war on terror. 
are constrained by principles that require explicit congressional deliberation on the question at hand. From the standpoint of national security, this conclusion might seem to give the President less than he needs. But if national security is genuinely at risk, clear congressional authorization will almost certainly be forthcoming. 
Readers with comments should address them to: So too if the Constitution’s various safeguards of liberty rarely apply in the contexts in which the AUMF is properly invoked. But let us imagine that on the correct view, ambiguous provisions must sometimes be construed in the face of canons pointing in opposite directions.   1.  2.  37.  71.  216.  217.  218.  221.  222.  223.  224.  225.  226.  227.  228.  229.  230.  231.  232.  233.  234.  235.  237.  238.  240.  241.  242.  243.  244.  245.  246.   
Luis Garicano and Thomas N. Hubbard, Specialization, Firms, and Markets: The Division of Labor  within and between Law Firms (April 2004)  Luis Garicano and Thomas N. Hubbard, Hierarchies, Specialization, and the Utilization of  Knowledge: Theory and Evidence from the Legal Services Industry (April 2004)  James C. Spindler, Conflict or Credibility: Analyst Conflicts of Interest and the Market for  Underwriting Business (July 2004)  Alan O. Sykes, The Economics of Public International Law (July 2004)  Douglas Lichtman and Eric Posner, Holding Internet Service Providers Accountable (July 2004)  Shlomo Benartzi, Richard H. Thaler, Stephen P. Utkus, and Cass R. Sunstein, Company Stock,  Market Rationality, and Legal Reform (July 2004)  Cass R. Sunstein, Group Judgments: Deliberation, Statistical Means, and Information Markets  (August 2004, revised October 2004)  Cass R. Sunstein, Precautions against What? The Availability Heuristic and Cross‐Cultural Risk  Perceptions (August 2004)  M. Todd Henderson and James C. Spindler, Corporate Herroin: A Defense of Perks (August 2004)  Eric A. Posner and Cass R. Sunstein, Dollars and Death (August 2004)  Randal C. Picker, Cyber Security: Of Heterogenity and Autarky (August 2004)  Randal C. Picker, Unbundling Scope‐of‐Permission Goods: When Should We Invest in Reducing  Entry Barriers? (September 2004)  Christine Jolls and Cass R. Sunstein, Debiasing through Law (September 2004)  Richard A. Posner, An Economic Analysis of the Use of Citations in the Law (2000)  Cass R. Sunstein, Cost‐Benefit Analysis and the Environment (October 2004)  Kenneth W. Dam, Cordell Hull, the Reciprocal Trade Agreement Act, and the WTO (October 2004)  Richard A. Posner, The Law and Economics of Contract Interpretation (November 2004)  Lior Jacob Strahilevitz, A Social Networks Theory of Privacy (December 2004)  Cass R. Sunstein, Minimalism at War (December 2004)  Douglas Lichtman, How the Law Responds to Self‐Help (December 2004)  Eric A. Posner, The Decline of the International Court of Justice (December 2004)  Eric A. Posner, Is the International Court of Justice Biased? (December 2004)  Alan O. Sykes, Public vs. Private Enforcement of International Economic Law: Of Standing and  Remedy (February 2005)  Douglas G. Baird and Edward R. Morrison, Serial Entrepreneurs and Small Business Bankruptcies  (March 2005)  Eric A. Posner, There Are No Penalty Default Rules in Contract Law (March 2005)  Randal C. Picker, Copyright and the DMCA: Market Locks and Technological Contracts (March  2005)  Cass R. Sunstein and Adrian Vermeule, Is Capital Punishment Morally Required? The Relevance of  Life‐Life Tradeoffs (March 2005)  Alan O. Sykes, Trade Remedy Laws (March 2005)  Randal C. Picker, Rewinding Sony: The Evolving Product, Phoning Home, and the Duty of  Ongoing Design (March 2005)  Cass R. Sunstein, Irreversible and Catastrophic (April 2005)   James C. Spindler, IPO Liability and Entrepreneurial Response (May 2005)  Douglas Lichtman, Substitutes for the Doctrine of Equivalents: A Response to Meurer and Nard  (May 2005)  Cass R. Sunstein, A New Progressivism (May 2005)  Douglas G. Baird, Property, Natual Monopoly, and the Uneasy Legacy of INS v. AP (May 2005)  Douglas G. Baird and Robert K. Rasmussen, Private Debt and the Missing Lever of Corporate Governance (May 2005) Cass R. Sunstein, Administrative Law Goes to War (May 2005) 
Follow this and additional works at: https://chicagounbound.uchicago.edu/law_and_economics Part of the Law Commons Recommended Citation Cass R. Sunstein, "A New Progressivism" ( John M. Olin Program in Law and Economics Working Paper No. 245, 2005). 
Cass R. Sunstein 
T H E L A W S C H O O L T H E U N I V E R S I T Y O F C H I C A G O 
May 2005 
Based on an address for a conference on Law and Transformation in South Africa, this paper explores problems with two twentieth-century approaches to government: the way of markets and the way of planning. It urge that the New Progressivism simultaneously offers (1) a distinctive conception of government’s appropriate means, an outgrowth of the late-twentieth-century critique of economic planning, and (2) a distinctive understanding of government’s appropriate ends, an outgrowth of evident failures with market arrangements and largely a product of the mid-twentieth-century critique of laissez faire. It emphasizes the need to replace bans and commands with appropriate incentives, and to attend to social norms and social meanings in leading human behavior in welfare-promoting directions. The ultimate goal is to promote some of the goals associated with America’s New Deal and Europe’s social democracy, but without using the crude, inflexible, and often counterproductive methods associated with those approaches. Some attention is devoted to the effects of globalization, the AIDS crisis, crime prevention, and the role of economic growth. 
The German psychologist Dietrich Dorner has done some fascinating experiments designed to see whether people can engage in successful social engineering.1 Dorner’s experiments are run via computer. Participants are asked to solve problems faced by the inhabitants of some region of the world; the problems may involve poverty, poor medical care, inadequate fertilization of crops, sick cattle, insufficient water, or excessive hunting and fishing. Through the magic of the computer, many policy initiatives are available (improved care of cattle, childhood immunization, drilling more wells), and participants can choose among them. Once particular initiatives are chosen, the computer projects, over short periods and then over decades, what is likely to happen in the region. 
In these experiments, success is entirely possible; some initiatives will actually make for effective and enduring improvements. But most of the participants—even the most educated and professional ones—produce calamities. They do so because * This paper is based on a keynote address on a conference on Law and Transformation in South Africa; readers are asked to make allowances for a paper originally intended for oral presentation. I am grateful to Lesley Wexler for superb research assistance and to Martha Nussbaum and Theunis Roux for many helpful comments and discussions. 1 Dietrich Dorner, The Logic of Failure (1997). they do not see the complex, system-wide effects of particular interventions. For example, they may see the importance of increasing the number of cattle, but once they do that, they create a serious risk of overgrazing, and they fail to anticipate that problem. They may understand full well the value of drilling more wells to provide water, but they do not anticipate the energy and environmental effects of the drilling, which then endangers the food supply. Only the rare participant is able to see a number of steps down the road—to understand the multiple effects of one-shot interventions into the system. 
Since the 1980s, many nations have been concerned to make three transitions. The first involves the shift from an authoritarian system to some kind of democracy. The second involves the creation of a secure system of individual rights. The third involves the introduction of reforms that will promote economic growth. The relationships among the three transitions are complex and contested. Are rights necessary for democratic self-government, or antithetical to it? Might democracy undermine economic growth, at least if people are demanding measures that will make prosperity less likely? Will a stable system of rights weaken, or strengthen, the prospects for economic development? 
My goal in this essay is to elaborate an understanding of government’s role that promises, at once, to strengthen individual rights, to promote democratic self-rule, and to increase economic growth. With some trepidation, I will use describe this understanding as a New Progressivism. New Progressivists attempt to combine an appreciation of a great lesson of the first half of the twentieth century, the failure of markets, with an appreciation of the great lesson of the second half of the twentieth century, the failure of planning. New Progressivists offer a certain conception of both rights and democracy. They understand rights to include freedom from desperate conditions; they understand democracy to require a certain measure of deliberation and reflection, rather than automatic responses to what the public currently wants. Because freedom from desperate conditions is a right, they are concerned that total reliance on is connected with a failure to protect individual rights. They also think that a deliberative democracy will supplement markets, and hence that a system of market ordering alone does not allow a proper domain for democratic self-rule. The failure of planning is simultaneously a failure in terms of economics, democracy, and rights. 
I urge that the New Progressivism simultaneously offers (1) a distinctive conception of government’s appropriate means, an outgrowth of the late-twentiethcentury critique of economic planning, and (2) a distinctive understanding of government’s appropriate ends, an outgrowth of evident failures with market arrangements and largely a product of the mid-twentieth-century critique of laissez faire. For this reason the New Progressivism should not be seen as a compromise between right and left, or as an effort to seek some midpoint between those who believe in markets and those who reject them. Far from being a compromise or a midpoint, the New Progressivism offers both means and ends of its own. 
With respect to means, the New Progressivism rejects approaches prominently associated with both social democracy and the New Deal, on the ground that they are frequently ineffective or even counterproductive, especially from the economic point of view. To the New Progressivists, social democrats are too often like participants in Dorner’s experiments, compromising economic and other goals without much sense of what they are doing. Those who endorse the New Progressivism are insistently focused on consequences, and they know that initiatives designed to help people who need help might backfire in practice—and that good intentions are no excuse for bad consequences. Above all, those who endorse the New Progressivism have learned from the past fifty years of experience with markets and with efforts to discipline and constrain markets. They are alert to side effects and unintended harmful consequences. Wherever possible, they attempt to use market-oriented strategies, enlisting markets on behalf of human interests—not because those strategies are morally superior, and not because markets have special moral claims (they do not), but because such strategies are likely to work. New Progressivists believe that if economic growth is a goal, markets should be enlisted far more often than they now are. 
It follows that New Progressivists are alert to the central role of civil society, and especially to the importance of social norms, which often drive private behaviour and which can change, for better or worse, over time. They are also skeptical, on both economic and democratic grounds, of command-and-control regulation and of aggressive interference with the labour market. They want to supplement markets, not to displace them. They favour such initiatives as the Earned Income Tax Credit (EITC) and housing subsidies for the poor, and they are cautious about a high minimum wage and rent control legislation. They believe that environmental problems should be handled through economic incentives, not through centralized mandates, which invite protectionism and interest-group maneuvering. Thus, for example, the New Progressivism • attempts to control problems associated with social norms, such as crime, unsafe sex, and other risk-taking behavior, through democratic efforts at norm management, often involving public-private partnerships; • places the highest possible premium on education and training; • rejects economic protectionism; • favors incentives rather than centralized governmental commands; • attempts to ensure flexibility in the labour market if and on the ground that it helps low-income workers as well as others; • sees economic growth as a central (though far from exclusive) part of antipoverty policy. 
So much for means—what of ends? Those who endorse the New Progressivism seek initially to achieve an incompletely theorized agreement2—an agreement on various practices and initiatives capable of attracting support from a wide range of theoretical perspectives. Utilitarians, Kantians, those who begin from diverse theological positions, and numerous others can support the approach urged here. But to the extent that theoretical depth is required, New Progressivists will insist that markets should be identified with neither justice nor liberty. They do not think that markets will inevitably protect individual rights, especially because they believe that the right to be free from discrimination counts as a right, one that markets can promote. They see markets as operating against a background that includes considerable injustice, and also limited liberty. They think that to a large degree, market ordering will merely build on that unacceptable background. 
At the same time, New Progressivists have considerable sympathy for some of the rights emphasized by social democrats in Europe and New Dealers in the United States: decent life prospects for all, a social safety net, a safe environment, genuine equality of opportunity. But they believe that these rights have often been quite murky and ill defined—and also that they have been confused, too often, with a kind of freestanding egalitarianism, concerned to ensure equal economic outcomes as such. Those who believe in a New Progressivism insist on an acceptable floor for everyone. But they are not much concerned with large disparities in wealth, not because these are fair, but because the much more important goal is to ensure decent outcomes for all, and because allowing such disparities may well be necessary to provide appropriate incentives. 
To those who believe in a New Progressivism, what is most necessary is to ensure that basic human capabilities do not fall below a certain, reasonably generous threshold.3 To this New Progressivists add a distinctive conception of equality, one that forbids second-class citizenship, or lower caste status, for members of any group. This anticaste principle makes sex equality a singularly high priority, as a means for economic development and an end in itself. The rights embodied in the anticaste principle are indispensable to growth. They promote democracy as well. For these reasons, the New Progressivists hope to offer a conception of government that simultaneously promote democratic and economic goals, and that does so without compromising individual rights, properly conceived. Of course these various ideas can be specified in ways that will lead to intense conflicts among them. New Progressivists attempt a specification that will reduce the conflicts, seeing rights, for example, as a precondition for democracy and growth, and seeing growth as an (imperfect) ally of rights. 
There has of course been a great deal of recent discussion, both supportive and critical, of what a ‘Third Way’ might involve. What I am calling the New Progressivism has an obvious relationship to the idea of a Third Way, especially insofar as those who endorse Third Way thinking tend to be receptive to markets and growth at the same time that they are committed to improving the rights of disadvantaged people. I do not intend here to summarize that discussion, or to endorse 2 See Cass R. Sunstein, Legal Reasoning and Political Conflict (1996). 3 See Amartya Sen, Development As Freedom (1999); Martha Nussbaum, Women and Human Development (1999); Anthony Giddens, The Third Way and its Critics (2000). the Third Way programme.4 What I mean to do instead is to develop a freestanding account of what a New Progressivism might be understood to entail. 
The American law professor Karl Llewellyn is said to have said, ‘Technique without morals is a menace; but morals without technique is a mess.’ This is a fitting criticism of some of the experiments in social democracy of the last several decades; it is a shorthand description of the failures of many participants in Dorner’s experiments, failures with parallels in the economic and democratic disasters brought about by twentieth-century ‘planners’ of various stripes. But Llewellyn is also said to have said: ‘Morals without technique is a mess; but technique without morals is a menace.’ This is a fitting criticism of many twentieth-century experiments in social engineering, but it is a criticism too of excessive reliance on free markets. The real task, for those interested in a New 
approaches and methods that are neither menace nor mess. 
Recent debates, including debates over the Third Way, have drawn attention to two possible strategies for dealing with markets: to leave them alone or to displace them. But the dichotomy is much too simple; in fact it is damagingly simple. It is too simple, first, because the idea of ‘displacing’ markets conceals a range of options, from nationalizing industries, to blocking certain deals, to limiting waivers, to providing information. It is too simple, second, because it is possible to complement rather than to displace markets—to provide institutions that do what markets do not do, and to help people who are failed by markets, an emphatically human institution. As Amartya Sen has written, very much in the spirit of the New Progressivism, ‘it is possible to argue at the same time both (1) for more market institutions, and (2) for going more beyond the market.’5 
With respect to methods and strategies, that is what I will be suggesting here. Throughout I will paint with an extremely broad brush, discussing many issues that could easily be treated in a short book, or even a long one. My hope is that the brisk and sometimes reckless treatment of many issues might make up for its otherwise unpardonable neglect of the trees, by providing a decent perspective on the forest, a 4 Two prominent discussions are Anthony Giddens, The Third Way (1998), and Giddens, ibid. In my view, Giddens offers too large a collection of ideas, a pastiche in fact, and too much of his discussion is platitudinous. Amartya Sen and Jean Dreze, India (1997), does not bill itself in these terms, but it seems to me the best presentation of what a New Progressivism might actually look like. Cass R. Sunstein, Free Markets and Social Justice (1997), is also intended to set out ideas in this direction. 5 Sen and Dreze, ibid. at 28. perspective that perhaps continues to be absent from existing treatments of possible twenty-first century ‘ways’. 
The plan of the essay is as follows. The second part offers a brief discussion of two familiar ‘ways’ and of what is wrong with them. Here globalization is seen to be a process that intensifies problems with both markets and planning, in a way that jeopardizes economic growth and also certain individual rights. The third part deals with social norms, especially in the context of crime prevention, HIV/AIDS, and related problems. The fourth part deals with employment policy and poverty. The fifth part explores command-and-control regulation and some alternatives, especially in the context of environmental protection. The sixth part investigates the importance and limits of economic growth, with particular stress on the need to attend to human capabilities and sex equality. The seventh part is a brief conclusion. 
If there is a New Progressivism, what is it opposing? There are two candidates. Let us understand the first to be a version of the Reagan-Thatcher programme, routed above all in the work of Nobel Prize winner Friedrich Hayek. Call this ‘the way of markets’. 
On this view, free markets are indispensable to liberty, rights, and economic growth. The role of the state is to create the preconditions for well-functioning markets, by establishing rights of private property and freedom of contract, by ensuring competition rather than monopoly, and by preventing force and fraud. Perhaps the government should also provide a social safety net, though a relatively weak one (to ensure proper work incentives). But according to those who endorse the way of markets, government should not interfere with the labour market through, for example, high minimum wage requirements or through special protections for labour unions. And those who endorse the way of markets place the highest possible premium on economic growth, on a particular, market-based conception of liberty, and on social ordering through market ordering, which they see as essential to growth and liberty alike. Of course there is a spectrum of possible approaches here, from those who reject a social safety net to those willing to accept an ample set of protections for those at the bottom. What I mean to emphasize, sometimes captured in the idea of ‘neo-liberalism’, or ‘liberalization’, is the idea of placing primary emphasis on free markets and economic productivity. 
Like the way of markets, the way of planning is actually a spectrum of ‘ways’, ranging from Soviet-style centralized planning to those forms of social democracy (not excluding America’s) that are comfortable, in some areas, with nationalized industries, aggressive regulation of markets, price and wage controls, and most generally ‘planning’ of various kinds. As random examples, consider the following: • laws that make it difficult for employers to discharge employees; • laws that make it hard for landlords to evict tenants; • environmental regulation that specifies the technology that must be used by industry; • high minimum wage requirements; • tariffs; • public ownership of industry; • ceilings and floors on prices as a whole. 
Of course it is possible for a nation to adopt narrow or broad plans, or to be a planner in only a few small domains. A government that is generally skeptical of planning might conclude, for example, that it makes sense to have tariffs in some areas, or that agriculture should be protected with price supports, or that workers should be protected from discharge, or that technological requirements are properly placed on new cars, to reduce levels of air pollution. 
Let us now briefly explore some of the problems in both of these ‘ways’. For those who seek the way of markets, the initial difficulty is that markets should not be identified with liberty or with rights properly conceived. Markets operate against the backdrop set by existing distributions of resources, opportunities, and talents. When an employee is able to attract only a small amount of money for an hour’s work, it is surely wrong to say that liberty is respected if we simply respect the deal. To the extent that existing distributions are a product of a lack of liberty, market ordering is a problem, not the solution. Whether the deal should be disrupted is another question. But what markets generate does not come, from the standpoint of liberty, with a stamp of approval. 
An equally fundamental problem is that the consequences of market ordering may not be so wonderful for many people, who will be left with bad opportunities and few resources. It should not be necessary to belabour this point. In any large nation, respect for market ordering will predictably produce a situation in which millions of people end up with low wages, long hours, and bad working conditions—if they end up with work at all. Of course a social safety net can help. But even if it is generous, it is not going to do all of what must be done. It will not, for example, protect people against unsafe working conditions, sexual harassment, pollution, or unfair discrimination. To the extent that the category of rights includes protection against some or all of these injuries, markets will compromise rights. Indeed we have every reason to believe that in some circumstances, markets will promote discrimination— as, for example, when customers or coworkers would prefer someone of a certain race or gender. 
At the same time, democratic values are compromised by any effort to treat “the market” as sacrosanct. Put to one side the evident point that markets must be created by government, willing and able to protect private property and freedom of contract. A real problem is that a democratic citizenry will often want to supplement or displace markets, simply to protect the interests of some, many, or all. A commitment to market ordering will sharply diminish the terrain by which people make choices about the kind of society in which they will live. 
But planning faces difficulties of its own. One problem is that it is exceedingly likely to be vulnerable to pressures from self-interested private groups with stakes in a particular outcome. If so, democracy is jeopardized. It is very common, for example, to observe environmental regulation being turned into a mechanism for the distribution of benefits to groups whose interests have precious little to do with environmental protection.6 An even more fundamental problem has to do with the unintended bad consequences of the most well-motivated plans. ‘Planners’ are constantly surprised. To take just two examples, a law that makes it hard for employers to discharge employees is likely to make employers reluctant to hire people 6 In the United States, a classic discussion is Bruce Ackerman and William Hassler, Clean Coal/Dirty Air (1983). in the first instance; a high minimum wage is likely to decrease employment. The problem is pervasive, and it can lead to unfortunate consequences, especially for the disadvantaged. We can understand the problem a bit better in light of Dorner’s work, showing the unfortunate systemic effects of one-shot interventions. Economic and social orders are systems, and the difficulty with plans is that their architects are infrequently able to foresee the consequences of their actions. 
Planners frequently refer to the unfairness of markets, and they are often right to do so. But a reference to unfairness should not be allowed to support laws that actually do no good. In Sen’s words, “The rhetoric of ‘equity’ has often been invoked to justify governmental interventions without any scrutinized political assessment of how these powers will be exercised and what actual effects they will have. In practice, these ill-directed regulations have not only interfered often enough with the efficiency of economic operations (especially of modern industries), they have also failed fairly comprehensively to promote any kind of real equity in distributional matters.”7 The point does not show that all plans are bad. But an overriding problem with the way of planning is simply that the consequences of plans are often bad from the perspective of well-meaning planners themselves. Plans may well violate rights, as has often been seen in Communist nations. But plans are often ineffectual too, and the result can be economic disaster. A low GDP is no ally of individual rights, especially if we understand poverty as a rights violation. 
Thus far I have said nothing about globalization. (By the term I mean to refer, very simply, to the increasing mobility of persons, information, and products from one area of the globe to another.) What is the effect of globalization on the way of planning and the way of markets? 
The most important point is that with respect to markets and planning, global pressures produce a difference not of kind but of degree. We best think of a situation of globalization as ‘like ordinary domestic markets—only more so’. The major effect of global pressures is to intensify competition, so that stringent national regulation might well leave companies within the nation at a disadvantage both abroad and at home. The resulting disadvantage might be reduced through protectionist measures, but these create familiar problems of their own, because they hurt local consumers (by increasing prices) and are likely, generally speaking, to harm the national economy. 
In fact globalization increases the problems associated with both of the two ‘ways’. Insofar as markets produce rights violations of certain kinds, and bad results for millions of people, global pressures increase the difficulty. (Of course such markets also carry many benefits as well, for poor people as well as everyone else, especially insofar as the consequence is to cut prices.) If workers within the nation have to compete with everyone in the world, or in any case with a larger class of people, those without training and skills are going to be left further behind. At the same time, global pressures will inevitably confound ill-considered ‘plans’. Because markets are frequently global, the effects of plans will often be very different from what was sought and anticipated. This is a particular problem if, for example, an effect of stringent regulation of the labor market is to reduce the demand for domestic labour, thus hurting workers themselves. 
This is hardly a claim that states are incapable of governing in a global era. There is a great deal that they can do. Evidence simply does not support the claim that the increased mobility of capital has disabled national regulation. But it is a claim that the problems with the two ways are simultaneously compounded by the existence of global markets. 
THE ROLE OF COSTS: THE INEVITABLY CLOSE RELATIONSHIP BETWEEN RIGHTS AND MONEY Those who favour markets and those who favour planning often make a sharp distinction between ‘negative rights’ and ‘positive rights’. The former are said to be barriers to government action, and to that extent costless. The second are said to be entitlements to government protection, and to that extent costly. Market enthusiasts argue for the former and against the latter. Planners tend to argue for both. 
From the standpoint of the New Progressivism (as well as from the standpoint of the Constitutional Court of South Africa8), both views suffer from a problem that is both conceptual and practical: along the relevant dimension, there is no difference between negative and positive rights. Both consist of entitlements to government action, and to that extent, both are costly. In a poor nation, both sorts of rights are at risk. Compare, for example, the right to free speech with the right to a minimum income. In a state of effective anarchy, or in a state without funds, neither right can exist. Of course the minimum income guarantee will lack taxpayer support. But the same is true for the right to free speech. Without a judiciary willing to protect people from public (as well as private) intrusions on freedom of speech, that freedom cannot exist. Without a state willing and able to prevent public marauders from silencing opposition, free speech is a chimera. Without public protection against private acts of violence against controversial statements, free speech is plainly absent. 
What is true for free speech is true for the whole universe of negative rights.9 Consider, for example, two of the most quintessential liberal rights: the right to private property and the right to freedom of contract. Both of these depend not incidentally but essentially on government protection. Private property does not exist in the state of nature. It is a taxpayer-subsidized right, justified on the ground that it benefits individuals and society as a whole. People can make contracts without a strong state; but unless a legal system, subsidized by taxpayers, stands ready and able to ensure that contracts are enforced, what force are mere words likely to have? 
Those who believe in a New Progressivism know that all rights are costly and that a poor state cannot protect rights. One of the reasons why they favour a strong economy, and economic growth, is that these are important preconditions for ample rights protection. And in a rich or poor state, it follows that a central task for democratic self-government is to obtain the necessary funds and to ensure that they are allocated in a way that reflects sensible priority setting. The field of public finance is not separable from the field of democratic theory. 8 See Ex parte Chairperson of the Constitutional Assembly: in re Certification of the Constitution of the Republic of South Africa, 1996 1996 (4) S.A. 744 (CC) para 77. 9 For detailed discussion, see Stephen Holmes and Cass R. Sunstein, The Cost of Rights (1998). 
Those who endorse the way of markets have little to say about the relationship between government and civil society. Civil society is defined as equivalent to economic society. Thus market enthusiasts tend to rely on existing norms and preferences; this is part of their conception of laissez faire. Those who endorse the way of planning also have little to say about civil society, sometimes treating it as a domain that must be enlisted in the interest of social goals. But there is a better approach, one that promises to do what governments often seek to do, but at lower cost. Such an approach would emphasize several points: (1) the crucial importance of social norms in producing both desirable and undesirable behaviour; (2) the inevitable role of government in helping to constitute such norms; (3) the dependence of social norms on current information; (4) the often-rapid change in social norms over time; (5) the extent to which highly visible, or cognitively ‘available,’ examples and events can alter norms and behaviour; and (5) the possible use of government power to move norms in desirable directions. 
As we will see, these points help suggest such promising possibilities for controlling many social problems, including crime, HIV/AIDS, discrimination, and environmental protection. What I will be emphasizing here is the significance of social cascades, including norm cascades, in which social interactions can lead behaviour in dramatic directions. Sometimes such cascades are induced by new beliefs; sometimes they are induced by new understandings of the meaning of certain actions. Since people take their cues from the actions of others, and since they care about their reputations, certain policies can backfire, and certain others can have large and desirable effects. 
PROBLEMS WITH BANS, ECONOMIC AND OTHERWISE When government seeks to discourage certain conduct, what should it do? Those committed to planning generally have a simple answer: Forbid it. Those committed to markets provide the same answer, though they are reluctant to ask government to discourage private behaviour falling outside the basic categories of force and fraud. Those who believe in a New Progressivism think that the shared answer—‘forbid it’—is much too simple, and in important ways misleading and even damaging, both to rights and to the economy. I am speaking here, then, of a promising but often overlooked means for promoting a range of social goals. 
New Progressivists do believe that incentives are important, and for anyone who believes that, it seems natural to think that if conduct is banned, there will be less of it. But bans create problems of their own. In some circumstances they can be selfdefeating, producing more of the behaviour that they seek to reduce. The reason is that behaviour is often driven by social norms, and hence by the signal that the behaviour carries; a ban can amplify the signal and increase the conduct. Suppose, for example, that people are engaging in certain harmful conduct precisely because the conduct is a way of defying authority and offering certain signals to relevant people. You might smoke, for example, precisely because smoking is, in some places, a dissident act; so too with a decision to engage in unsafe sex; so too for a decision to commit a crime. In short, actions have meanings, and the social meaning of action is an important determinant of what people will do and when they will do it. (In the United States, some increases in risky sex have come from deliberate efforts at defying efforts to convince people to use condoms.) 
Many people will engage in conduct that they do not otherwise enjoy or value, because of the social meaning of engaging in it; many people will fail to engage in activity that they would enjoy or value, for exactly the same reason. If the social meaning of harm-avoiding activity is cowardice or capitulation, people are likely to refuse harm-avoiding activity. If the social meaning of harm-producing activity is boldness and independence, people are more likely to impose harms. In these circumstances any governmental effort to ‘forbid’ certain conduct might be futile or even counterproductive. If what is driving behaviour is its social meaning, the effect of a ban could be to increase it, by increasing the strength of the signal that is the motivation after all. Compare the finding that when people are paid to engage in certain desirable activity (cleaning up, for example, or arriving on time to pick up children from school), they will sometimes actually engage in that behaviour less rather than more, because the payment reduces the effect of the norm, which would otherwise have the effect of the payment. Those who seek a New Progressivism are especially interested in enlisting an understanding of social norms and social meanings in the service of improved policies, involving both rights and growth. They emphasize that in many domains, people frequently think and do what they think and do because of what they think (relevant) others think and do. Thus, for example, employees are more likely to file suit if members of the same workgroup have also done so10; teenage girls who see that other teenagers are having babies are more likely to become pregnant themselves11; littering and non-littering behavior appears to be contagious12; the same is true of violent crime13; those who know other people who are on welfare are more likely to go on welfare themselves14; the behaviour of proximate others affects the decision whether to recycle15; a good way to increase the incidence of tax compliance is to inform people of high levels of voluntary tax compliance16; and students are less likely to engage in binge drinking if they think that most of their fellow students do not engage in binge drinking, so much so that disclosure of this fact is one of the few successful methods of reducing binge drinking on university campuses in the United States.17 
Social influences affect behaviour via two different mechanisms. The first is informational. If many other people support a particular candidate, or refuse to use drugs, or carry guns, observers, and particularly observers within a common group, are given a signal about what it makes sense to do. The second mechanism is reputational, as group members impose sanctions on perceived deviants, and would10 See Harold H. Gardner, Nathan L. Kleinman, and Richard J. Butler, ‘Workers’ Compensation and Family and Medical Leave Act Claim Contagion’, (2000) 20 Journal of Risk and Uncertainty 89, 11 See, e.g., George A. Akerlof, Janet L. Yellen and Michael L. Katz, ‘An Analysis of Out-of-Wedlock Childbearing in the United States’, (1996) 111 Quarterly Journal of Economics 277. 12 See R. Cialdini et al., ‘A Focus Theory of Normative Conduct: Recycling the Concept of Norms to Reduce Littering in Public Places’, (1990) 58 Journal of Personality and Social Psychology 1015. 13 See Washington Post (December 1999). [Exact date? Query author] 14 See Marianne Bertrand, Erzo F.P. Luttmer and Sendhil Millainathan, Network Effects and Welfare Cultures (unpublished manuscript, April 9, 1998). 15 See Ardith Spence, Wants for Waste (unpublished Ph.D. dissertation, University of Chicago, 1999). 16 See Stephen Coleman, The Minnesota Income Tax Compliance Experiment State Tax Results (Minnesota Department of Revenue, April 1996). 17 See H. Wesley Perkins, ‘College Student Misperceptions of Alcohol and Other Drug Norms Among Peers’, in Designing Alcohol and Other Drug Prevention Programs in Higher Education (US Department of Education ed., 1997) 177-206; Timur Kuran and Cass R. Sunstein, ‘Availability Cascades and Risk Regulation’, (1999) 51 Stanford Law Review 683, 767. A good outline of contagion effects can be found in Gardner, Kleinman and Butler, n. 9 above, at 91-4. be deviants anticipate the sanctions in advance.18 Even when people do not believe that what other people do provides information about what actually should be done, they may think that the actions of others provide information about what other people think should be done. People care about their reputations, and hence they have an incentive to do what (they think) other group members think they should do. Reputational considerations may, for example, lead people to obey or not to obey the law, urge a certain view in group discussions, drive while drunk, help others, or talk about political issues in a certain way. 
The central question is how an understanding of these points might lead policies in better directions. What is especially promising is the possibility of achieving a ‘tipping point’, in which large numbers of people end up moving in novel directions.19 People typically have different ‘thresholds’ for choosing to believe or do something new or different. As those with low thresholds come to a certain belief or action, people with somewhat higher thresholds join them, possibly to a point where a critical mass is reached, making groups, possibly even nations ‘tip’.20 The result of this process can be to produce snowball or cascade effects, as small or even large groups of people end up believing something—even if that something is false— simply because other people seem to believe that it is true. Real world phenomena also seem to have a great deal to do with cascade effects.21 Consider, for example, smoking, participating in protests, striking, recycling, using birth control, rioting, choosing what to put on television,22 even leaving bad dinner parties.23 We can understand certain people, in the private and public sectors, as ‘norm entrepreneurs’, seeking to give certain signals to many people, and in the process helping to shift norms in a desirable direction. 
With respect to policy tools, of special note is the availability heuristic: people tend to think that an event is more likely if an incident of its occurrence can readily be 18 See George Akerlof, ‘A Theory of Social Custom, of which Unemployment May Be One Consequence’, in George Akerlof, An Economic Theorist’s Book of Tales (1984) 69. 19 See Malcolm Gladwell, The Tipping Point (2000), for a popular overview. 20 See Mark Granovetter, ‘Threshold Models of Collective Behavior’, (1978) 83 American Journal of Sociology 1420; for a recent popular treatment, see ibid. at 5-22. 21 See Sushil Bikhchandani et al., ‘A Theory of Fads, Fashion, Custom, and Cultural Change as Informational Cascades’, (1992) 100 Journal of Political Economy 992; Kuran and Sunstein, n. 16 above, at 715-35. 22 See Kennedy, supra note. [Query author] 23 Several of these examples are discussed in Kuran and Sunstein, n. 16 above, at 725-35, and in Granovetter, n. 19 above, at 1422-24, brought to mind. It is for this reason that a single, highly publicized event can have important behavioual consequences, as when disclosure that a famous athlete or actor is HIV-positive can make thousands or even millions of people change their behavior. The single incident makes the risk seem both higher and more salient. It can even change the meaning of action. ‘Availability entrepreneurs’ can draw public attention to individual persons and cases, with the self-conscious goal of producing ‘availability cascades’, with which perceptions, by millions of people, can simultaneously shift. In the United States, this has happened with fear of abandoned hazardous waste dumps and nuclear power, as well as HIV/AIDS; Thailand used the strategy successfully in the later context.24 
There is a closely related phenomenon: group polarization. It has been shown that any group of like-minded people, after deliberating with one another, tends to end up believing a more extreme version of what they thought before they spoke together. Consider some examples of the basic phenomenon, known as group polarization, which has been found in over a dozen nations.25 (a) A group of moderately profeminist women will become more strongly profeminist after discussion.26 (b) After discussion, citizens of France become more critical of the United States and its intentions with respect to economic aid.27 (c) After discussion, whites predisposed to show racial prejudice offer more negative responses to the question whether white racism is responsible for conditions faced by African-Americans in American cities.28 (d) After discussion, whites predisposed not to show racial prejudice offer more positive responses to the same question.29 
As statistical regularities, it should follow, for example, that those moderately critical of an ongoing war effort will, after discussion, sharply oppose the war; that those who believe that global warming is a serious problem are likely, after 24 See Kuran and Sunstein, n. 16 above. 25 See Cass R. Sunstein, ‘Deliberative Trouble? Why Groups Go To Extremes’, 110 Yale Law Journal 171 (2000). These include the United States, Canada, New Zealand, Germany, and France. See, e.g., Johannes Zuber et al., ‘Choice Shift and Group Polarization’, (1992) 62 Journal of Personality and Social Psychology 50 (Germany); Dominic Abrams et al., ‘Knowing What To Think By Knowing Who You Are’, (1990) 29 British Journal of Social Psychology 97, 112 New Zealand). Of course it is possible that some cultures would show a greater or lesser tendency toward polarization; this would be an extremely interesting area for empirical study. 26 See D.G. Myers, ‘Discussion-Induced Attitude Polarization’, (1975) 28 Human Relations 699. 27 Brown, supra note, at 224. [Query author] 28 D.G. Myers and G.D. Bishop, ‘The Enhancement of Dominant Attitudes in Group Discussion’, (1976) 20 Journal of Personality and Social Psychology 286. 29 See ibid. discussion, to hold that belief with considerable confidence; that people tending to believe in the inferiority of a certain racial group will become more entrenched in this belief as a result of discussion. For present purposes, group polarization is important because it shows how groups tend to move. If some members of a group can be convinced to shift their position, the group’s middle will shift as well, and quite large changes can be anticipated. 
All this is quite abstract. The central question remains: How might government, seeking to promote its goals at low cost, induce tipping, or social cascades? 
Suppose that people are engaging in activity that involves harm to self or to others; for simplicity, assume that the activity is not itself criminal. From the standpoint of the New Progressivism, hoping not to intrude on rights and to respect democratic ideals, the first prescription is simple: Inform people. With respect to cigarette smoking and unsafe sex, for example, a great deal of risky behaviour comes simply from a lack of information. Evidence shows that by itself, information, if it can be made salient and vivid, produces substantial changes in behaviour. In Thailand, a revelation that 44 per cent of sex workers in Chiang Mai were infected with HIV appears to have contributed to a substantial increase in the use of condoms. Growth in condom use in the United States in the 1980s was driven largely by information. Public information campaigns, use of the mass media, and face-to-face education and training programmes are all able to help.30 
This should hardly be surprising. But there is a somewhat more subtle point. It is possible to produce information-induced norm cascades, in which social norms, and social meanings, change dramatically as a result of changes in beliefs. In the United States, this has happened with large-scale shifts in judgments about cigarette smoking and, in the early 1990s, with large-scale shifts in judgments about both sexual 30 See World Bank, Efficient and Effective Strategies for Preventing HIV/HIV/AIDS, available at www.worldbank.org/HIV/AIDS-econ/confront/confrontfull/chapter3/chap3.html. 
Thus far I have been discussing what the New Progressivism opposes. What does it support instead? The answer is not reliance on markets alone. New Progressivists firmly believe that simple and even effective redistribution, such as cash grants to the poor from taxpayer funds, are justified in principle. They believe in a social safety net; in this way they accept the South African approach, which understands the category of rights to include gross deprivation of basic needs. But they do not believe that planners offer promising responses to poverty, unemployment, and general deprivation. Instead New Progressivists emphasize approaches that will give people an ability to help themselves, through education, training, and job opportunities, fuelled by economic incentives in the form (for example) of tax credits to employers and employees alike. They seek to supplement rather than to displace markets. 
In case there was any doubt, a recent study of South Africa, based on 1996 data, shows the enormous importance of education to employment prospects.34 The study shows that high ratios of teachers to students dramatically increase the likelihood of employment—with a statistically significant effect for males, and a very large effect for females. The authors conclude that the apartheid ‘system continues to profoundly influence the life chances of many Black Africans, through its long lasting effects on the country’s education system. Many Black Africans currently in the labor force attended schools with inadequately trained teachers, insufficient textbooks, and pupil-teacher ratios above 80 children per class.’ The result of all this is to decrease both years in school and employment prospects. Better education and training is the most direct and effective way of helping people to obtain jobs and decent wages. 
For many people, of course, other steps are required. A central New Progressivist approach is to develop systems to supplement low wages. A key here is to generate a strategy that will neither (a) discourage employers from hiring people nor (b) discourage prospective workers from seeking employment. Many countries have experimented with some variation on the Earned Income Tax Credit (EITC). Under this approach, working people (the programme may or may not be limited to people with children) who earn low wages obtain a tax credit from the government, sufficient to raise their total compensation to a decent level. The tax credit is phased out once workers receive money above a certain level. In the United States, for example, someone who earns an income of about $10,000 will receive about $3800 at tax time; a mother and father who earn $25,000 will receive about $1200 at tax time. In the United States alone, the EITC has been a terrific success, lifting millions of people from poverty. In 1996 alone, the effect of the EITC was to take 4.6 million people, including 2.4 million children, out of poverty.35 
Compared to planning-type initiatives, there are three large advantages to the EITC approach. The first is that unlike the minimum wage, it does not make labour more costly for employers, and thus does not decrease employers’ desire to hire people. The second is that it increases people’s incentives to work, by making work more remunerative. The third is that unlike the minimum wage, the EITC is genuinely and specifically targeted at people who are poor. In the United States, for example, nearly three-quarters of those who benefit from the EITC are poor or near-poor.36 The EITC is the model of a New Progressivism antipoverty programme. 
Creative policymakers could easily build on this model—and use it as an alternative to well-motivated but crude plans. Planners interested in protecting tenants might, for example, impose ceilings on rent increases. Those interested in protecting those dependent on certain important commodities might impose price ceilings. If the government is interested in ensuring that tenants cannot be evicted for nonpayment of rent, it might impose large procedural hurdles to eviction. If decent housing is unavailable, government might build housing on its own. These strategies are all associated with planning, and they might do some good. But New Progressivists would look for alternatives. They might seek, for example, to provide housing vouchers to poor people, or to provide food stamps to those who are unable to buy enough to eat. 
34 Anne Case and Motohiro Yogo, Does School Quality Matter? Returns to Education and Characteristics of Schools in South Africa [1999 draft, available on the Social Science Research Network (SSRN)]. 35 Benjamin Aldrich-Moodie, ‘The Earned Income Tax Credit’, Issue Brief No. 1., The Century Foundation (September 1999). 36 Shaviro, n. 31 above, at 435. businesses a tax credit, or direct subsidies, to the extent that they are willing to locate in areas that need help, and to employ people who need jobs. These are marketoriented strategies that do not make a fetish of market ordering. They deploy markets in the interest of human goals. Learning from the failures and half-successes of planning, those who use such strategies attempt to be like the successful participants in Dorner’s experiments, whose interventions are alert to the potentially harmful sideeffects of intrusions into systems. 
In controlling pollution and other social harms, a number of states, including some of the most proudly capitalist, have engaged in a modest form of ‘planning’, through command-and-control regulation. Such regulation involves centralized regulatory requirements imposed on dozens, hundreds, thousands, or even millions of producers. As examples, consider air pollution control requirements, imposed on new cars and trucks; ‘best available technology’ (BAT) requirements, imposed on polluting firms; and specified levels of mandatory pollution reductions, imposed on companies of various kinds. Similar approaches can be found in the area of occupational safety, where national authorities sometimes specify the safety technologies and approaches that must be used by all or most firms. 
It is important to acknowledge that in many states, command-and-control regulation has accomplished significant good. Planning of this form has hardly been a complete failure. In the United States, it has helped produce dramatic reductions in levels of air and water pollution. Reliance on markets alone would have been far worse. In this domain, the way of planning is far better than the way of markets. But for several reasons, the New Progressivism rejects the basic approach. They are concerned that command-and-control approaches will compromise economic development and also undermine democracy. 
The first problem is that it is usually unnecessarily expensive, even wasteful for government to prescribe the means for achieving social objectives. At least as a general rule, it is especially inefficient for government to dictate technology. One of the many problems with BAT strategies, for example, is that they ignore the enormous differences among plants and industries and among geographical areas. It is not sensible to impose the same technology on industries in diverse areas—regardless of whether they are polluted or clean, populated or empty, or expensive or cheap to clean up. BAT strategies also require all new industries to adopt costly technology, and allow more lenient standards to be imposed on existing plants and industries. Through this route BAT strategies actually penalize new products, thus discouraging investment and perpetuating old, dirty technology. The result is inefficiency in investment strategies, in innovation, and even in environmental protection. Such strategies also fail to encourage new pollution control technology and indeed serve to discourage it by requiring its adoption for no financial gain. In general, governmental specification of the ‘means’ of achieving desired ends is a good way of producing unnecessarily high costs. Instead of permitting industry and consumers to choose the ‘means’—and thus to impose a form of market discipline on that question— government often selects the means in advance. The governmentally-prescribed means are often the most inefficient. 
More fundamentally, command-and-control approaches are severely deficient from the standpoint of democracy. The focus on the question of ‘means’ tends to increase the power of well-organized private groups, by allowing them to press environmental and regulatory law in the service of their own parochial ends. The BAT approach, for example, ensures that citizens and representatives will be focusing their attention not on what levels of reduction are appropriate, but instead on the largely incidental and nearly impenetrable question of what technologies are now available. Because of its sheer complexity, this issue is not easily subject to democratic resolution. Moreover, the issue is not the relevant one for democratic politics, which is the appropriate degree and nature of environmental protection—an issue to which the BAT question is only incidental. 
The approach favoured by the New Progressivism is much simpler: (1) begin by informing people about risks, including environmental risks, and (2) where this is inadequate, impose a tax on harmful behaviour, and let market forces determine the response to the increased cost. Thus governments might adopt a simple, two-step reform policy in the area of social risks and social harms. First, those who impose harm must pay for it—by purchasing permission to do so, perhaps through a licensing procedure. Second, those who obtain the resulting permission should be able to trade their ‘licenses’ with other people. In the pollution context, this would mean that people who reduce their pollution below a specified level could trade their ‘pollution rights’ for cash. In one stroke, such a system would create market-based disincentives to pollute and market-based incentives for pollution control. Such a system would also reward rather than punish technological innovation in pollution control, and do so with the aid of private markets. An idea of this kind might be made part and parcel of a system of ‘green taxes’. With such a system, nations might impose taxes on people who impose harms on others—users of dirty automobiles, farmers who employ pesticides, coal-fired power plants, gasoline that produces air pollution, products that contribute to destruction of the ozone layer or the greenhouse effect. 
Economic incentives could be applied in other areas as well. Workers' compensation plans, for example, operate as effective guarantees of workplace safety. According to a careful study, ‘[i]f the safety incentives of workers' compensation were removed, fatality rates in the United States economy would increase by almost 30 percent. Over 1200 more workers would die from job injuries every year in the absence of the safety incentives provided by workers' compensation.’37 This contrasts with a mere 2-4 per cent reduction in injuries from the Occupational Safety and Health Act (OSHA), an amount that links up well with the fact that annual workers' compensation premiums are more than 1000 times as large as total annual OSHA penalties. The tax system could be used to provide better incentives to employers who furnish dangerous workplaces. The consumer product safety commission might experiment with a system in which producers of harm-producing products must pay a fee into the federal treasury. Ultimately, we might hope for a coordinated system of risk regulation, one that imposed specified fees for harm-producing activities. 
For democracy, such an approach would have significant consequences, and these consequences would be extremely beneficial. The large question—how much environmental protection at what cost—does not easily permit legislators to favor a well-organized, narrow group, such as the agricultural lobby, or the coal lobby. Special favours cannot readily be provided through a system of economic incentives. The very generality of the question will work against narrow favouritism. 
The international movement toward economic incentives, a central part of the New Progressivism, is preliminary but real. Thus far, it has occurred mostly in the environmental area. An important series of administrative initiatives have brought about ‘emissions trading’, especially under the American Clean Air Act.38 Under the Environmental Protection Agency’s policy, a firm that reduces its emissions below legal requirements may obtain ‘credits’ that can be used against higher emissions elsewhere. Overall, the programme has produced savings of between $525 million and $12 billion.39 By any measure, this is an enormous gain. On balance, moreover, the environmental consequences have been beneficial. Offsets must, by definition, produce environmental gains. The preliminary evidence shows favourable effects from bubbles as well. There may be modest beneficial effects from banking and modest adverse effects from netting. The overall environmental effect is therefore good, cost entirely to one side. 
As part of the process for eliminating lead from gasoline—a decision that was, not incidentally, strongly supported by a cost-benefit study—the EPA also permitted emissions trading. Under this policy, a refinery that produced gasoline with lower than required lead levels could earn credits. These could be traded with other refineries or banked for future use. Until the termination of the programme in 1987, when the phase-down of lead ended, emissions credits for lead were widely traded. EPA concluded that there had been cost savings of about 20 per cent over alternative systems, marking total savings in the hundreds of million of dollars. There have been similar efforts with water pollution and ozone depletion.40 
Perhaps the most dramatic programme of economic incentives can be found in the 1990 amendments to the Clean Air Act. The Act now explicitly creates an emissions trading system for the control of acid deposition. In these amendments, Congress has made an explicit decision about aggregate emission levels for a pollutant. Whether the particular decision is the correct one may be disputed. But surely there are large democratic benefits from ensuring that public attention is focused on that issue. There are palpable economic benefits as well. The system for controlling acid deposition is billions of dollars less expensive than the commandand-control alternatives. 37 W. Kip Viscusi, Reforming Products Liability, (1991) 178. 38See Environment Protection Agency, ‘Emissions Trading Policy Statement: General Principles for Creation, Banking, and Use of Emission Reduction Credits’, (1986) 51 Federal Regulations 43,814. 39 See Hahn & Hester, ‘Marketable Permits: Lessons for Theory and Practice’, (1989) 16 Ecology Law Quarterly 361, 374 table 2. [Query author – names or initials?] 40On ozone depletion, see 53 Federal Regulations 30,566 (August 11, 1988); on water pollution, see Hahn & Stavns, ‘Incentive-Based Environmental Regulation: A New Era From An Old Idea’, (1991) 18 Ecology Law Quarterly 1, 18-19. [Query author – names or initials?] 
The idea can be adapted to many domains. Consider, for example, the African Elephant Conservation Act, which created a two-prong strategy to combat species loss. These included cooperative conservation projects and strict import controls for countries with endangered elephant populations. The United States would not import from any such country; but once a nation was able to stabilize its elephant population and to receive a new listing, it would be permitted to engage in the trade of ivory. In a way this policy might seem worse than a flat ban on ivory trade. But in practice, it has worked far better, because it creates an incentive to have sizeable herds. In many cases, elephants were ‘owned’ and protected by the relevant communities, increasing efforts to quash poaching in the knowledge that they would be economically better off if they were able to be listed as having stabilized their elephant population. The Act has been extremely successful, and served as the model for the subsequent Rhinoceros and Tiger Act and the Asian Elephant Act. Nations interested in preserving their own environmental goods, including endangered species, might well build on this idea. 
Thus far I have dealt with New Progressivist means—stressing the need to attend to both incentives and norms, and to avoid the crudeness associated with many plans. It is now time to turn to the question of ends. That question cannot be answered without saying something about the relationship between economic growth, social welfare, and individual rights. 
For the last decades, many people have evaluated national well-being in terms of economic growth alone. Indeed, this approach has been characteristic of those who emphasize the way of markets; and much of the time, it has been adopted by those who attempt the way of planning as well. On this view, a nation’s performance is assessed by asking about Gross Domestic Product, and by seeing its movement over time. 
Certainly GDP is a useful figure, for it bears a relationship to important social goals. If we think of income as an all-purpose means—as something that people want regardless of what else they want—we might well attend to GDP. Though its importance is merely instrumental, growth does matter for a wide range of important goals. Thus a recent study of anti-poverty measures in the United States, by someone well-disposed to government programmes and unenthusiastic about the way of markets, announces as Lesson 1, ‘A strong macroeconomy matters more than anything else.’41 Well-protected property rights and freedom of contract, protected by state institutions, are quite central to economic growth.42 They are at least as important in poor countries as in rich ones. 
There are, however, a number of problems with relying on GDP as a measure of well-being. 
The most obvious problem involves distributional considerations. If income is unequally distributed, a high GDP may disguise the fact that many people are living bad or even desperate lives. For example, the United States has the highest per capita real GDP in the world. But it also has a higher rate of children living in poverty— about one in five—than does any other wealthy country in the world. The rate of children living in poverty is double that of the industrialized nations taken as a whole and four times that of Western Europe. Nearly half of all black children in the United States live in poverty. While white Americans, if taken separately, would rank first on the United Nations Development Programme’s Human Development Index, African-Americans, if taken separately, would rank somewhere in the thirties. These crucial economic facts are undisclosed by GDP. And if deprivation is seen as a rights violation, economic growth can conceal substantial rights violations. 
GDP sometimes seems to be a general placeholder for a number of diverse indicators of social and economic well-being. But in fact, it may not be closely correlated with some important indicators. Consider two major social goals: reduction of poverty and reduction of unemployment. Of course GDP growth can be an important factor in counteracting both unemployment and poverty. But it is also possible for GDP increases to be accompanied by increases in unemployment and hence poverty (which is closely correlated with unemployment); indeed, this phenomenon has often occurred. Or consider the likelihood of subjection to violent crime. Physical security is surely an important ingredient in well-being, but it is at best indirectly reflected in GDP. Consider also the fact that there is no inevitable 41 Rebecca Black, ‘Fighting Poverty: Lessons from Recent US History’, (2000) 14 Journal of Economic Perspectives 3, 6. connection between GDP and life expectancy. Some countries have a relatively low GDP but long life expectancy and low rates of infant mortality. Many countries have a high GDP but do poorly in promoting longevity. Education is an important part of a good life, whether or not educated people accumulate wealth; but the association between education and GDP, while real, is extremely crude. 
GDP also fails to account for goods and services that are free, including some that are closely connected with economic well-being. For example, unpaid domestic labour is not a part of GDP. Many environmental amenities, such as clean air and water, are not reflected in GDP. The GDP figure thus fails adequately to measure either the benefits of a healthy environment or the costs of its degradation. There are other gaps in what GDP measures. It does not, for example, reflect changes in leisure time; but it is clear that any increase in leisure is a gain even in economic terms, since leisure is something for which people are willing to pay, sometimes a great deal. Most generally, a serious problem with GDP is that the figure excludes all social costs and benefits that do not have prices. 
The most general point is that GDP does not adequately capture what a good society is concerned to ensure. Responding to these points, many people have attempted to come up with other measures of national well-being. Of these the UNDP’s effort is the best known. The UNDP uses an index of well-being based on per capita income, educational attainment, and longevity. In 1999, South Africa was ranked 101 on the list, below China, Turkey, Georgia, and Albania. Some of the key numbers include a life expectancy at birth of 54.7 years, an adult literacy rate of 84 per cent, and real GDP per capita of $7,380. The most striking number here is the low life expectancy number—below, for example, that of Bangladesh, Nepal, India, and Pakistan. 
Of course any ‘index’ is bound to have a degree of arbitrariness. What is important is that a nation with a low GDP can also give its citizens long lives and a good education—and that a nation with a high GDP can give its citizens a poor education and relatively low life expectancies. (On the latter point, the United States is an unfortunate case, with an extremely high per capita income but one of the lowest life expectancies among comparable countries.) The theoretical roots of these efforts to go beyond GDP lie in the ‘capabilities approach’, which attempts to assess ‘what 42 See Mancur Olson, Power and Prosperity (1999). people are actually able to do and to be’.43 (We can view the capabilities approach as an effort, within both economics and philosophy, to give some concrete detail to the aspects of human rights and ‘basic needs’.) 
The central idea, rooted in a notion of human dignity, is that a nation should ensure not ‘satisfaction of preferences’, and not a certain level of resources, but instead an ability to choose what kind of life to have. Of course preferences and beliefs may well adapt to existing deprivation—a common phenomenon, and one that creates serious problems for the way of markets insofar as that way makes ‘preferences’ determinative of social policy. Nor should it be contentious to urge that certain capabilities are necessary to a decent human life. These include the ability to live a decently long life, to have good health, to participate in political choices that affect one’s life, to be educated, to hold property.44 When people lack one or more of these abilities, they suffer from ‘capability failure’, and this is the problem to which government should respond. The minimal responsibility of a New Progressivist government is to ensure that all citizens are able to come above a certain threshold. The less minimal obligation is to ensure that all citizens have more than minimal capabilities on all of these dimensions. 
This general idea cannot without arbitrariness be turned into a formula or index. But insofar as the Human Development Index, and related efforts, go beyond GDP to provide a simple, salient, easily understood figure by which to facilitate international comparisons and changes over time, they are a good place to start. They also provide a great deal of help with priority-setting. In the United States, Finland, Botswana and South Africa, for example, it is clear that a large premium should be placed on improving longevity; the citizens of these nations can expect relatively short lives, undoubtedly in part as a result of high levels of crime and low levels of health care. By contrast, Sweden, New Zealand, Cuba and Georgia should concentrate on increasing economic growth; all of these nations provide decent education, and relatively long lives, but keep their citizens relatively poor. 
43 See Nussbaum, n. 3 above, at 5. 44 See ibid. at 78-80. 
The capabilities approach gives a sense of the New Progressivist account of liberty; but it says little about equality. For New Progressivists, I have suggested it is important to ensure that everyone has decent life prospects, and also that everyone comes over a ‘capability threshold’. But a separate and equally central equality goal is the attack on any situation in which some people, defined in terms of a morally irrelevant characteristic, are treated as second-class citizens, or turned into members of a lower caste. Thus an anticaste principle, undergirding a constitutional equality norm, plays a large role in New Progressivist thinking. 
The anticaste principle is not a thoroughly egalitarian goal. It is compatible with large disparities in wealth and resources. But it insists that morally irrelevant characteristics, such as race, religion, and gender, should not be turned into a basis for second-class citizenship. Thus New Progressivists see one of the most serious inequality problems of the twentieth century in the pervasive practice of seizing on a characteristic lacking moral relevance, and using it as the basis for the systematic subordination of members of certain social groups. This conception of equality is opposed to several others, including the ‘formal equality’ principle, which does not focus attention on systematic subordination, and indeed which sees efforts to counteract such subordination as a form of discrimination. Commitment to the anticaste principle hardly exhausts the domain of rights. But it is an important component of a rights-respecting society. 
SEX EQUALITY AS A CENTRAL GOAL, AS A MEANS AND AN END For developing and wealthy countries alike, an end to sexual hierarchy is an especially high priority—as both a means and an end. With respect to many problems—HIV/AIDS, crime, economic growth and overpopulation—there are few higher priorities. Indeed any generalized attack on poverty must be combined with an attack on sexual hierarchy. The two are closely intermingled problems. 
Note first that sex equality is an important means to human development, as Amartya Sen and others have shown, with their particular emphasis on ‘women’s agency’.45 A nation in which girls and women have a chance to do and to be what they want to do and to be is much likelier to develop and to develop well. A nation with sexual hierarchy is far more likely to suffer economically and in other ways. In fact a system of equality on the basis of sex is likely to redress many problems not normally thought to be associated with it, such as overpopulation and the individual and social drains that come from unwanted children. When women have a range of opportunities, and when their choices are not a product of deprivation, these problems are sharply diminished. If women are able to decide whether to have children, unwanted children are less likely, and children who are unwanted are far more likely to suffer and to make society suffer in response, sometimes through criminal behaviour. If women have agency, many crimes will be reduced as a result, emphatically including sex crimes. If women have agency, the economy will do better, simply because there will be more people willing and able to do good work. If women have agency, there will be an immediate and substantial reduction in the AIDS crisis, simply because women will be able to engage in sexual relations only when they want, and on the terms that they choose. This is a potentially enormous benefit for men and women alike, and for adults and children alike as well. 
This idea has many implications. It means that education and training for girls are crucial. It also means that by both law and norms, girls and women should be allowed to decide when and whether to become mothers—which means, among other things, that sexual relations should always be a choice, not a requirement. Here there is an evident link to the earlier discussion of norms and norm cascades, and a particular link to the problems of crime and HIV/AIDS. In fact a social policy directed against sexual subordination in multiple spheres is highly likely to combat both problems. 
I have attempted in this essay to clarify the relationships among rights, democracy, and economic growth, endorsing in the process a New Progressivism. Of course it is 45 See Sen and Dreze, n. 4 above. possible to conceive of rights, and of democracy, in a way that leads to great tension with economic growth. But we can also understand democracy in deliberative terms, in a way that reduces the tension with both rights and growth. We can also understand rights in a way that makes economic growth their alley rather than their adversary. Rights in general depend on a certain degree of economy development, simply because rights cost money. And if rights include freedom from desperate conditions, economic growth is extremely important. 
But it is not sufficient. Markets must be supplemented by additional efforts, supported by a democratic society. Too much of the time, markets do little to help people who face circumstances of deprivation. From the standpoint of freedom and justice, they are nothing to celebrate. From the standpoint of economics, this was the great lesson of the first half of the twentieth century. We have seen that markets are emphatically human products, not natural but conventional, created by law and to be evaluated in terms of their consequences. The question is what they do to and for the people who are subject to them. 
At the same time, economic planning, in its many diverse forms, is often futile and counterproductive, partly because of the difficulty of foreseeing the systemic effects of one-shot interventions. In its various guises, planning often invites interest group manoeuvering, thus damaging democracy, and produces unintended harmful consequences, notwithstanding the good intentions of many planners. This has been the great lesson of the second half of the twentieth century. 
In these circumstances, any New Progressivism, to be worthy of use, should not be understood as a compromise measure, or as steering ‘between’ two poles. It is insistently and unapologetically committed to a certain understanding of the goals associated with social democracy in Europe and the New Deal in the United States. These do not include egalitarianism, understood as equal or roughly equal economic outcomes. But they do include decent life prospects for all, a social safety net in the form of adequate floors, political equality, and an anticaste principle, in the form of opposition to second-class citizenship for members of any social group. Attempting to understand social democratic goals in a coherent way, those who believe in a New Progressivism insist on a self-conscious effort to promote the fundamental capabilities of all citizens, by ensuring that everyone falls above a certain generous threshold. Those who endorse a New Progressivism therefore reject the idea that markets should be identified with liberty at the same time that they seek to go beyond the sometimes sloppy and diffuse set of goals associated with the way of planning. Those who endorse a New Progressivism also put the highest premium on the achievement of an end to sexual hierarchy. 
With respect to means, New Progressivists seek a government that acts as catalyst; the basic goal is to create incentives that will move behaviour in desirable directions. New Progressivists see much planning as analogous to the hapless participants in Dietrich Dorner’s experiments—as ill-conceived interventions into systems that confound the goals of the interveners. To avoid the problem of ‘mess’, New Progressivism programmes are based on a presumption against blocked exchanges and command-and-control regulation. There is an effort to complement rather than to displace markets. Antipoverty programmes take the form of incentives to work, designed to ensure that everyone who works is able to live decently, that everyone has a chance to work, and that those who choose not to work have a good reason for not doing so. Regulatory programmes are based on a presumption in favour of informing people and providing economic incentives for desirable conduct. 
Finally, New Progressivists insist on the importance of civil society and above all of the norms that often drive behaviour. Alert to the possibility of social cascades, those who seek a New Progressivism insist on the provision of information, and also on seemingly small steps that can, under favorable conditions, have large effects on behaviour. 
There is no policy blueprint here. Of courlicy on a more secure sense of gose no country is exactly like any other. No one can design a set of policy initiatives that will work well, or at all, in every nation on the globe. But the problems with the way of markets and the way of planning are pervasive. In many nations, an alternative that attempts to promote human capabilities, in a way that is alert to the nature of incentives and the role of social norms, promises to draw on both of the great lessons of the twentieth century, and to build povernment’s appropriate ends, and of the means that are most likely to promote those ends.             2.  Chicago Working Papers in Law and Economics  
(Second Series)  William M. Landes, Copyright Protection of Letters, Diaries and Other Unpublished Works: An  Economic Approach (July 1991)  Richard A. Epstein, The Path to The T. J. Hooper: The Theory and History of Custom in the Law  of Tort (August 1991)  Cass R. Sunstein, On Property and Constitutionalism (September 1991)  Richard A. Posner, Blackmail, Privacy, and Freedom of Contract (February 1992)  Randal C. Picker, Security Interests, Misbehavior, and Common Pools (February 1992)  Tomas J. Philipson & Richard A. Posner, Optimal Regulation of AIDS (April 1992)  Douglas G. Baird, Revisiting Auctions in Chapter 11 (April 1992)  William M. Landes, Sequential versus Unitary Trials: An Economic Analysis (July 1992)  William M. Landes & Richard A. Posner, The Influence of Economics on Law: A Quantitative  Study (August 1992)  Alan O. Sykes, The Welfare Economics of Immigration Law: A Theoretical Survey With An  Analysis of U.S. Policy (September 1992)  Douglas G. Baird, 1992 Katz Lecture: Reconstructing Contracts (November 1992)  Gary S. Becker, The Economic Way of Looking at Life (January 1993)  J. Mark Ramseyer, Credibly Committing to Efficiency Wages: Cotton Spinning Cartels in  Imperial Japan (March 1993)  Cass R. Sunstein, Endogenous Preferences, Environmental Law (April 1993)  Richard A. Posner, What Do Judges and Justices Maximize? (The Same Thing Everyone Else  Does) (April 1993)  Lucian Arye Bebchuk and Randal C. Picker, Bankruptcy Rules, Managerial Entrenchment, and  Firm‐Specific Human Capital (August 1993)  J. Mark Ramseyer, Explicit Reasons for Implicit Contracts: The Legal Logic to the Japanese  Main Bank System (August 1993)  William M. Landes and Richard A. Posner, The Economics of Anticipatory Adjudication  (September 1993)  Kenneth W. Dam, The Economic Underpinnings of Patent Law (September 1993)  Alan O. Sykes, An Introduction to Regression Analysis (October 1993)  Richard A. Epstein, The Ubiquity of the Benefit Principle (March 1994)  Randal C. Picker, An Introduction to Game Theory and the Law (June 1994)  William M. Landes, Counterclaims: An Economic Analysis (June 1994)  J. Mark Ramseyer, The Market for Children: Evidence from Early Modern Japan (August 1994)  Robert H. Gertner and Geoffrey P. Miller, Settlement Escrows (August 1994)  Kenneth W. Dam, Some Economic Considerations in the Intellectual Property Protection of  Software (August 1994)  Cass R. Sunstein, Rules and Rulelessness, (October 1994)  David Friedman, More Justice for Less Money: A Step Beyond Cimino (December 1994)  Daniel Shaviro, Budget Deficits and the Intergenerational Distribution of Lifetime  Consumption (January 1995)  Douglas G. Baird, The Law and Economics of Contract Damages (February 1995)  Daniel Kessler, Thomas Meites, and Geoffrey P. Miller, Explaining Deviations from the Fifty  Percent Rule: A Multimodal Approach to the Selection of Cases for Litigation (March 1995)  Geoffrey P. Miller, Das Kapital: Solvency Regulation of the American Business Enterprise  (April 1995)  Richard Craswell, Freedom of Contract (August 1995)  J. Mark Ramseyer, Public Choice (November 1995)  Kenneth W. Dam, Intellectual Property in an Age of Software and Biotechnology (November  1995)  Cass R. Sunstein, Social Norms and Social Roles (January 1996)  J. Mark Ramseyer and Eric B. Rasmusen, Judicial Independence in Civil Law Regimes:  Econometrics from Japan (January 1996)  73.  107.  108.  143.  144.  179.  222.  223.  224.  
Follow this and additional works at: https://chicagounbound.uchicago.edu/law_and_economics Part of the Law Commons Recommended Citation Cass R. Sunstein, "Beyond Marbury: The Executive's Poer to Say What the Law Is" ( John M. Olin Program in Law and Economics Working Paper No. 268, 2005). 
                    Beyond Marbury: The Executive’s Power to Say What the Law Is  
C a s s   R .   S u n s t e i n   
This paper can be downloaded without charge at:  The Chicago Working Paper Series Index: http://www.law.uchicago.edu/Lawecon/index.html  and at the Social Science Research Network Electronic Paper Collection:   http://ssrn.com/abstract_id=839444   T H E   U N I V E R S I T Y   O F   C H I C A G O   
November 2005  
This paper can be downloaded free of charge from the Social Science Research Network at: http://ssrn.com/abstract=839444 
* The author is Karl N. Llewellyn Distinguished Service Professor, Law School and Department of Political Science, University of Chicago. For valuable help, he is grateful to Adrian Vermeule for many discussions, to Robert Hahn for valuable comments, and to participants in superb legal theory workshops at New York University, the University of Chicago Law School, and Yale Law School. Excellent research assistance was provided by Blake Roberts, Andres Sawicki, Cristina I. Miller-Ojeda, Anne Pogue, and Ken Merber. In order to promote public understanding of the impact of regulations on consumers, business, and government, the American Enterprise Institute and the Brookings Institution established the AEI-Brookings Joint Center for Regulatory Studies. The Joint Center’s primary purpose is to hold lawmakers and regulators more accountable by providing thoughtful, objective analysis of relevant laws and regulations. Over the past three decades, AEI and Brookings have generated an impressive body of research on regulation. The Joint Center builds on this solid foundation, evaluating the economic impact of laws and regulations and offering constructive suggestions for reforms to enhance productivity and welfare. The views expressed in Joint Center publications are those of the authors and do not necessarily reflect the views of the Joint Center. 
Executive Director 
COUNCIL OF ACADEMIC ADVISERS 
University of Maryland 
All AEI-Brookings Joint Center publications can be found at www.aei-brookings.org © 2005 by the author. All rights reserved. 
Executive Summary 
Under Marbury v. Madison, it is “emphatically the province and duty of the judicial department to say what the law is.” But as a matter of actual practice, statements about “what the law is” are often made by the executive department, not the judiciary. In the last quarter-century, the Supreme Court has legitimated the executive’s power of interpretation, above all in Chevron v. Natural Resources Defense Council, the most-cited case in modern public law. Chevron reflects a salutary appreciation of the fact that the executive is in the best position to make the judgments of policy and principle on which resolution of statutory ambiguities often depends. But the theory that underlies Chevron remains poorly understood, and in the last two decades, significant efforts have been made to limit the executive’s interpretive authority. In general, these efforts should be resisted. The principal qualification involves certain sensitive issues, most importantly those involving constitutional rights. When such matters are involved, Congress should be required to speak unambiguously; executive interpretation of statutory ambiguities is not sufficient. Beyond Marbury: The Executive's Power To Say What the Law Is 

Consider the following cases: a. Under the administration of President Jimmy Carter, the Department of the Interior adopted a broad definition of what it meant to “harm” a member of an endangered species.1 The governing statute made it unlawful to “take” a member of an endangered species, and it defined “take” to include “harass, harm, pursue, hunt, shoot, wound, kill, trap, capture, or collect.”2 The Interior Department interpreted “harm” to include “significant habitat modification or degradation where it actually kills or injures wildlife by significantly impairing essential behavioral functions, including breeding, feeding, or sheltering.”3 The interpretation was challenged as inconsistent with the statute. A majority of the Supreme Court rejected a challenge to the Carter-era regulation,4 over a dissenting opinion by Justice Scalia, joined by Chief Justice Rehnquist and Justice Thomas.5 b. Under the administration of President George W. Bush, the Environmental Protection Agency (EPA) rejected a petition to issue regulations to control the emission of greenhouse gases from motor vehicles.6 The underlying statute required the agency to regulate emissions of air pollutants from motor vehicles if, in his judgment, those emissions “may reasonably be anticipated to endanger public health or welfare.”7 The EPA concluded that it lacked statutory authority over greenhouse gases and that even if it had such authority, it would use its discretion and refuse to exercise it. Environmental 1 See Babbitt v. Sweet Home Chapter of Communities for a Great Oregon, 515 U.S. 687 (1995). 2 16 U.S.C. § 1532 (1992). 3 50 C.F.R. § 17.3 (1994). 4 515 U.S. at 708. 5 515 U.S. at 714. 6 Massachusetts v. EPA, 415 F.3d 50 (DC Cir 2005). 7 42 U.S.C. § 7521(a)(1) (2000) groups and others challenged the EPA’s decision as inconsistent with the underlying statute. The D.C Circuit rejected the challenge. Judge Randolph wrote the opinion for the court, refusing to resolve the question of statutory authority, but concluding that the EPA had properly exercised its discretion. Judge Tatel dissented, emphasizing that the EPA had taken an unlawfully “constricted view” of its statutory authority8 and that the agency had exercised its discretion unlawfully.9 c. Under the administration of President Bill Clinton, the Food and Drug Administration (FDA) asserted authority over tobacco and tobacco products.10 The governing statute allows the FDA to regulate “drugs” and “devices,” and it defines “drugs” to include “articles (other than food) intended to affect the structure or any function of the human body.”11 In the FDA’s view, nicotine qualifies as a “drug.” Tobacco companies challenged the FDA’s assertion of jurisdiction. The Supreme Court agreed, ruling that Congress had not authorized the FDA to control tobacco and tobacco products and that the agency’s initiative under President Clinton was therefore unlawful.12 Justice Breyer wrote a dissenting opinion, joined by Justices Stevens, Souter, and Ginsburg.13 
My major goal here is to vindicate the law-interpreting authority of the executive branch. This authority, I suggest, is indispensable to the healthy operation of modern government. Indeed, the executive’s law-interpreting authority is a natural and proper outgrowth of one of the most important legal developments of the twentieth century: the shift from regulation through common law courts to regulation through administrative agencies. In the modern era, statutory interpretation must often be undertaken, at least in the first instance, by the President, the Department of Defense, the Environmental Protection Agency, the Federal Communications Commission, the Securities and Exchange Commission, the Federal Trade Commission, the National Labor Relations Board, the Department of Homeland Security, and countless other institutions within the executive branch.14 Because the resolution of statutory ambiguities often calls for assessments of both policy and principle, the key judgments are legitimately made by executive officers, not courts. 
In the three cases above, the relevant statutes were ambiguous, and statutory interpretation was inevitably driven by some combination of political values and assessments of disputed facts. In the first two, federal courts were correct to defer to the executive; the Supreme Court should have deferred in the third as well. It should be no surprise that when federal judges disagreed with one another in all three cases, the disagreement operated along unmistakably political lines — splitting the stereotypically liberal judges from the stereotypically conservative ones. There is no reason to believe that in cases of this kind, the meaning of federal law should be settled by the inclinations and predispositions of federal judges. The outcome should instead depend on the commitments and beliefs of those who operate under the President. 
The recognition of the executive’s interpretive power fits well with the institutional judgments that are embodied in the post-New Deal willingness to embrace presidential authority, including the countless forms of administrative power that are exercised under the President. I shall suggest that recognition of the executive’s interpretive power has the same relationship to the last half of the twentieth century what Erie Railroad Co. v. Tompkins15 had to the first: an institutional shift in interpretive power brought about by a realistic understanding of what interpretation involves. In short, Chevron is our Erie. When courts resolve genuine ambiguities, they cannot appeal to any 14 Throughout I shall treat the so-called independent agencies (such as the Federal Trade Commission, the Federal Communications Commission, and the National Labor Relations Board) as within the executive branch, even though the heads of such agencies are not at-will employees of the President. See Elena Kagan, Presidential Administration, 114 HARV. L. REV. 2245. The key point is that the independent agencies are subject to a range of presidential controls, so that their own judgments line up fairly well with those of the President. See Peter L. Strauss, The Place of Agencies in Government: Separation of Powers and the Fourth Branch, 84 COLUM. L. REV. 573 (1984). It is controversial to see independent agencies as part of the executive branch, see id., but I believe that the analysis would be qualified, rather than fundamentally different, if independent agencies were not so seen. Thus I shall use the terms “agencies” and “executive branch” interchangeably, though readers should be aware that some agencies are not always thought to be within that branch. 15 304 U.S. 64 (1938). “brooding omnipresence in the sky”16; often they must rely on policy judgments of their own. The meaning of statutory law should not depend on the policymaking discretion of the judiciary. 
The grant of interpretive authority to the executive stems above all from a belief that when statutory interpretation calls for political judgments, those judgments should be made by those with a high degree of political accountability. If Congress has not settled a particular question, the settlement should be made by responsive officials, rather than by judges. The executive’s interpretive authority also rests on an understanding that the resolution of ambiguities often requires specialized competence; here too the executive should be preferred to the judiciary. Of course these claims must be qualified in various ways.17 But they provide the right foundation for the allocation of interpretive authority. To see why, it is necessary to explore Chevron itself. 
What Chevron Said 
The Administrative Procedure Act18 (APA), the basic charter governing administrative agencies, was enacted in 1946. The governing provision of the APA says that the “reviewing court shall decide all relevant questions of law, [and] interpret statutory provisions.”19 
Marbury v. Madison,20 that questions of statutory interpretation must be resolved by courts, not the executive.21 Many post-APA decisions seemed to embrace this understanding.22 But there were important contrary indications, in 16 Southern Pac. Co. v. Jensen, 244 U.S. 205, 222 (1917) (Holmes, J., dissenting). 17 See the discussion of nondelegation canons, below. 18 5 U.S.C. § § 551-559, 701-706 (2000 & Supp. II 2002) 19 Id. § 706 (2000). 20 5 U.S. 137 (1803). 21 See John F. Duffy, Administrative Common Law in Judicial Review, 77 TEX. L. REV. 113 (1998). 22 See, e.g., NLRB v. Hearst Publications, 322 U.S. 111 (1944); FEC v. Democratic Senatorial Campaign Comm., 454 U.S. 27, 32 (1981); NLRB v. Insurance Agents, 361 U.S. 477, 499 (1960); NLRB v. Highland Park Mfg. Co., 341 U.S. 322 (1951); Office Employees Intl. Union v. NLRB, 353 U.S. 313 (1957). For recognition of the ambiguity of the cases, see Pittston Stevedoring Corp. v. Dellaventura, 544 F.2d 35 (2d Cir. 1976). which courts suggested that agency interpretations would be upheld so long as they were rational.23 
The law remained complex and confused until 1984, when the Court decided Chevron.24 The Court’s ruling is difficult to understand without a sense of the context. The case involved an ambitious effort by the EPA to increase private flexibility under the Clean Air Act,25 in a way that presaged the substitution of economic incentives for command-and-control regulation.26 
The initiative was part of the 
Administration’s general effort to reduce regulatory burdens on the private sector.27 More particularly, the EPA redefined “stationary source”28 under the Act so as to include an entire factory, rather than each pollution-emitting unit within the plant. Responding to a lower court invalidation of the new definition,29 the Supreme Court created a novel twostep inquiry for assessing agency decisions. The first inquiry is whether Congress has directly decided the precise question at issue.30 If not, the second inquiry is whether the agency’s decision is “permissible,” which is to say reasonable.31 In the court’s view, Congress had not forbidden a plant-wide definition of “source,” and hence the EPA could supply whatever (reasonable) definition it chose. Thus an inquiry into the two relevant “steps” validated the EPA’s decision.32 
Strikingly, the Court did not discuss the language or history of the APA. But it did note that Congress sometimes explicitly delegates law-interpreting power to agencies.33 In the face of an explicit delegation of that power, courts would certainly defer. No one doubts that Congress has the constitutional power to say that some statutory terms (“source,” for example, or “take”34) may be defined by the executive.35 But the Court 23 See Gray v. Powell, 314 U.S. 402, 411 (1941); Udall v. Tallman, 380 U.S. 1, 16 (1965). Ford Motor Co. v. NLRB, 441 U.S. 488 (1979); Ford Motor Credit Co. v. Milhollin, 440 U.S. 555, 565 (1980). 24 467 U.S. 837 (1984). 25 42 U.S.C. § 7401 et seq (2000). 26 See A, DENNY ELLERMAN ET AL., MARKETS FOR CLEAN AIR (2000). 27 See, e.g., Executive Order 12291. 28 42 U.S.C. § 7502(c). 29 NRDC v. Gorsuch, 685 F.2d 718 (D.C. Cir. 1982). 30 467 U.S. at 842–43. 31 Id. 32 Id. 33 467 U.S. at 844. 34 Babbitt v. Sweet Home Chapter of Communities for a Great Oregon, 515 U.S. 687 (1995). could not, and did not, contend that the relevant provision of the Clean Air Act contained any explicit delegation. Hence the Court added that “sometimes the legislative delegation to an agency on a particular question is implicit rather than explicit.”36 An implicit delegation would give rise to deference as well. The Clean Air Act does give the EPA the power to issue regulations; in granting that power, perhaps the Act is best taken to say that the agency is implicitly entrusted with the interpretation of statutory terms. The Court referred to this possibility, noting that Congress might have wanted the agency to strike the relevant balance with the belief “that those with great expertise and charged with responsibility for administering the provision would be in a better position to do so.”37 But lacking any evidence on the question, the Court did not insist that Congress in fact so thought. It did not say that the power to issue regulations is best taken to signal a delegation of law-interpreting power. On the contrary, it said that Congress’s particular intention “matters not.”38 
Instead the Court referred to two pragmatic points: judges lack expertise and they are not politically accountable. In interpreting law, the agency may “properly rely upon the incumbent administration’s views of wise policy to inform its judgments. While agencies are not directly accountable to the people, the Chief Executive is.”39 The Court was alert to the fact that it was reviewing a decision made by the Reagan Administration, altering an interpretation by the Carter Administration; and to say the least, the Reagan Administration had a self-conscious program for reorienting the administrative state. Some of that program would inevitably be undertaken through fresh interpretations of statutory terms. In the Court’s view, that was not objectionable. It would be appropriate for agencies operating under the Chief Executive, rather than judges, to resolve “competing interests which Congress itself either inadvertently did not resolve, or intentionally left to be resolved in light of everyday realities.”40 35 In extreme cases, the power to interpret statutory terms might be taken to raise nondelegation problems; but those problems are not an issue in the ordinary cases of explicit delegation to define terms. On nondelegation issues and Chevron, see infra. 36 Chevron, 467 U.S. at 844. 37 467 U.S. at 865. 38 Id. 39 Id. 40 Id. 
What is most striking about this passage is the suggestion that resolution of statutory ambiguities requires a judgment about resolving “competing interests.” This is a candid recognition that assessments of policy are sometimes indispensable to statutory interpretation. Of course we can imagine cases in which courts resolve ambiguities through the standard sources — by, for example, using dictionaries, consulting statutory structure, deploying canons of construction, or relying on legislative history if that technique is thought to be legitimate.41 Under Chevron Step One, the executive will lose if the standard sources show that the agency is wrong.42 But sometimes those sources will leave gaps; Chevron itself is such a case, and there are many others. If the Court’s analysis is accepted on this point, its deference principle seems inescapable. Chevron’s Fiction: Delegation, Realism, and Institutional Competence Fictions 
In the years since Chevron, a consensus has developed on an important proposition, one that now provides the foundation for Chevron itself: The executive’s law-interpreting power turns on congressional will.43 If Congress wanted to repudiate Chevron, it could do precisely that. Before Chevron, some courts appeared to understand that the deference question was for congressional resolution; they approached the deference question on a statute-by-statute basis, asking whether the relevant statute should be taken to include an implicit delegation.44 In Chevron, the Court replaced that case-by-case inquiry with a simple rule, to the effect that delegations of rulemaking power implicitly include the power to interpret ambiguities.45 But as Justices Breyer and Scalia have independently emphasized,46 this is a legal fiction; usually the legislature has not expressly conferred that power at all. The view that the executive may “say what the law is” results not from any reading of any statutory text, but from a heavily pragmatic 41 See Stephen Breyer, On the Uses of Legislative History, 65 SO. CAL. L. REV. 845 (1992). 42 See, e.g., MCI Telecom. Corp. v. AT&T Co., 512 U.S. 218 (1994). 43 See, e.g., United States v. Mead Corp., 533 U.S. 218, 227–231 (2001). 44 Antonin Scalia, Judicial Deference to Agency Interpretations of Law, 1989 DUKE L.J. 511. 45 Id. 46 Id.; Stephen Breyer, Judicial Review of Questions of Law and Policy, 38 ADMIN. L. REV. 363 (1986). construction of (nonexistent) congressional instructions. Any judgment about those instructions is inevitably an ascription; it is not a matter of finding something. 
In terms of the standard sources of law, Chevron’s fiction is not at all easy to defend. As noted, the text of the APA appears to contemplate independent review of judgments of law. The history supports the text. For example, Representative Walter, Chairman of the House Subcommittee on Administrative Law and author of the House Committee Report on the bill, plainly said that the provision "requires courts to determine independently all relevant questions of law, including the interpretation of constitutional or statutory provisions."47 Hence the only possible justification for deference is that certain grants of authority, in organic statutes, implicitly contain interpretive power as well.48 But this argument also runs into difficulty. At the time the APA was enacted, the bulk of important agency business was done via adjudication,49 and if Congress wanted courts to defer to interpretations that were produced through adjudication, someone would almost certainly have said so at some point in the extensive debates.50 The claim that agency adjudicators (or rulemakers) have interpretive authority is certainly weakened by the absence of any contemporaneous suggestions to that effect in Congress itself. 
Perhaps subsequent grants of adjudicative or rulemaking power, as for example in the Clean Air Act or the Endangered Species Act, are best taken to confer interpretive power on the executive. But if this is so, the question must be explored on a case-by-case basis, and it is likely that courts will be unable to find any clear expression of congressional will — and hence we are back in the world of fictions. 
To say that Chevron rests on a fiction, and one that does not clearly track congressional instructions, is to acknowledge that the judicial judgment on the deference question involves judicial policymaking — subject to legislative override, to be sure, but not rooted in actual legislative judgments. I suggest that the Court’s allocation of interpretive power to the executive should be seen as an outgrowth of two closely related 47 92 Cong. Rec. 5654 (1946) (statement of Rep. Walter), reprinted in APA Legislative History, (Sen. Doc). 48 See Scalia, supra note. 49 See Nathanson; STEPHEN BREYER ET AL., ADMINISTRATIVE LAW AND REGULATORY POLICY (5th ed. 2001). 50 For relevant discussion, see Duffy, supra note. Note also the Attorney General’s Manual, relied on by Justice Scalia, supra note, is supportive of the deference principle. But in this context in particular, the Attorney General’s Manual is unreliable, stating as it does the views of the executive branch, which would naturally be inclined in favor of deference to its own views. See Duffy, supra note. developments. The first is the legal realist attack on the autonomy of legal reasoning. The second is the twentieth century shift from regulation through common law courts to regulation through executive agencies. 
Realists and realism 
The legal realists saw the interpretation of statutory ambiguities as necessarily involving judgments of policy and principle.51 They insisted that when courts understand statutes to mean one thing rather than another, they are inevitably using judgments of their own, at least in genuinely hard cases. In the realist view, judicial judgments often hide behind standard interpretive devices, such as legislative intent and canons of construction, and these did not in fact motivate courts. 
In a famous article, for example, Max Radin attacked the standard tools as largely unhelpful. Canons of construction often have little “foundation in logic and in ordinary habits of speech.”52 In his view, “A legislative intent, undiscoverable in fact, irrelevant if it were discovered, . . . is a queerly amorphous piece of slag. Are we really reduced to such shifts that we must fashion monsters and endow them with imaginations in order to understand statutes?” 53 Nor is it helpful to rely on purpose, which can be characterized in multiple different ways.54 In the end, a judge must be “impelled to make his selection . . . by those psychical elements which him the kind of person that he is. That this is pure subjectivism and therefore an unfortunate situation is beside the point.”55 Radin said that a key question was inevitably: “Will the inclusion of this particular determinate in the statutory determinable lead to a desirable result? What is desirable will be what is just, what is proper, what satisfies the social emotions of the judge, what fits into the ideal scheme of society which he entertains.”56 
Radin’s argument was characteristic of the general period in which courts were being displaced by regulatory agencies. A specialist in administrative law, Ernst Freund saw that for some statutes, “executive interpretation is an important factor.”57 Freund noted, with evident concern, that “in view of the inevitable ambiguities of language, a power of interpretation is a controlling factor in the effect of legislative instruments, and makes the courts that exercise it a rival with the legislature in the development of written law.” 58 After surveying the various sources of interpretation, Freund said that “in cases of genuine ambiguity courts should use the power of interpretation consciously and deliberately to promote sound law and sound principles of legislation.”59 Freund added: “That object is far more important than a painstaking fidelity to the supposed legislative intent. This intent is in reality often a fiction, and the legislature is fully aware that any but the most explicit language is subject to the judicial power of interpretation. That power might, therefore, as well be frankly and vigorously used as a legitimate instrument of legal development and of balancing legislative inadvertence by judicial deliberation.”60 
For his part, Karl Llewellyn contended that the standard sources of interpretation, above all the canons of construction, were a fraud, masking judgments that were really based on other grounds.61 He asked courts to “strive to make sense as a whole out of our law as a whole.”62 In his view, statutory meaning should be derived from "the good sense of the situation and a simple construction of the available language to achieve that sense, by tenable means, out of the statutory language."63 
Radin, Llewellyn, and Freund undoubtedly overstated their arguments. Canons of construction, for example, can discipline judicial (or executive) interpretation,64 and it may well be better to rely on them than on a judge’s general sense of what is best. But 57 Id. at 211. 58 Ernst Freund, Interpretation of Statutes, 65 U. PA L. REV. 207, 208 (1917). 59 Id. at 231. 60 Id. 61 Karl N. Llewellyn, Remarks on the Theory of Appellate Decision and the Rules of Canons about How Statutes Are to be Construed, 3 VAND. L. REV. 395, 399 (1950) (emphasis in original). 62 Id. at 399 (emphasis in original). 63 Id. at 401. 64 See ANTONIN SCALIA, A MATTER OF INTERPRETATION (1997). suppose that the realists were broadly right to suggest that in the face of genuine ambiguity, courts are often making judgments of policy,65 and that they should candidly acknowledge that fact and try to “promote sound law and sound principles of legislation.” Suppose that in hard cases, the search for “legislative intent” is often a fraud, and that when courts speak for that intent, they are often speaking for their own preferred views.66 (I will offer evidence for that proposition below.67) If Radin, Llewellyn, and Freund are indeed right, then there seems to be little reason to think that courts, rather the executive, should be making those judgments. The President himself should be in a better position to 
make the relevant judgments, simply because of his comparatively greater accountability. And if specialized knowledge is required, executive agencies have large advantages over generalist judges. Return to the question in Chevron itself. In the abstract, it is difficult to know whether a plantwide definition of source will help or hurt the environment; and the economic gains, if any, from the increased flexibility are not easily assessed by federal judges. Consider also strong evidence that for hard statutory questions within the Supreme Court, policy arguments of one or another sort often play a central role, even in a period in which “textualism” has seemed on the ascendancy.68 
We can even bring the realist view of interpretation in close contact with Ronald Dworkin’s account of law as “integrity.”69 Of course Dworkin is no realist; his own view of adjudication places a large emphasis on the constraints imposed by the existing legal materials.70 Nonetheless, his account strongly supports the argument for executive interpretation. Dworkin contends that interpretation, including statutory interpretation,71 requires a judgment about “fit” with existing materials and also about “justification” of those materials; his conception of law as integrity requires judges to put existing materials in their “best constructive light.”72 Where “fit” leaves more than one possibility, 65 See Jane Schacter, The Confounding Common Law Originalism in Recent Supreme Court Statutory Interpretation: Implications for the Legislative History Debate and Beyond, 51 STAN. L. REV. 1 (1998). 66 Not incidentally, the question of deference to executive interpretations itself seems to fall in this category; it is hard to tease out, from the existing legal materials, an authoritative legislative judgment on that question, and hence it is necessary, as we have seen, to speak in terms of legal fictions. 67 See infra Part IV. 68 SeeSchacter, supra note 65. 69 See RONALD DWORKIN, LAW’S EMPIRE (1985). 70 See id. 71 See id. 72 Id. judges have a degree of discretion. Everyone should agree that the executive, no less than the judiciary, has a duty of “fit”; many of the hard cases arise when the key question is which interpretation puts the law in its best constructive light. 
But – and here is a question Dworkin does not ask -- why should courts be entrusted with the duty to carry out that task? In modern government, courts are often less capable on that count than is the executive, precisely because of its comparatively greater expertise and accountability. In deciding how to understand the Endangered Species Act, the Food and Drug Act, and the Clean Air Act, it would be puzzling to suggest that courts are in a particularly good position to identify the “best constructive light.” The New Deal and beyond 
These points are easily linked with the post-New Deal transfer of effective lawmaking power from common law courts to federal bureaucracies. For much of the nation’s history, the basic rules of regulation were elaborated by common law courts, using the principles of tort, contract, and property to set out the ground rules for social and economic relationships. In the early part of the twentieth century, some of those rules were taken to have constitutional status, so as to forbid legislative adjustments.73 In a wholesale attack on the adequacy of the common law,74 the New Deal saw the rise and legitimation of a vast array of new agencies, including the National Labor Relations Board, the Securities and Exchange Commission, the Social Security Administration, the Federal Deposit Insurance Corporation, an expanded Federal Trade Commission, an expanded Food and Drug Administration, and more.75 Many of the agencies were necessarily in the business of interpreting ambiguous statutory provisions; indeed, interpretation was a central part of their job. And there is a close link between the realists’ emphasis on the policy-driven nature of interpretation and the New Deal’s enthusiasm for expert administrators.76 
The Marbury principle, calling for independent judicial judgments about law, came under intense pressure as a result. After Roosevelt’s triumph in the Supreme Court 73 See Cass R. Sunstein, Lochner’s Legacy, 87 COLUM. L. REV. 873 (1987). 74 See Cass R. Sunstein, Constitutionalism After the New Deal, 101 HARV. L. REV. 421 (1987). 75 See BREYER, supra note, at 29. 76 See Llewellyn, supra note. in the late 1930s, courts started to signal that the executive would have considerable lawinterpreting power. A strong and representative statement came in 1941, when the Court upheld a controversial interpretation by the Department of the Interior: “Although we have here no dispute as to the evidentiary facts, that does not permit a court to substitute its judgment for that of the Director. It is not the province of a court to absorb the administrative functions to such an extent that the executive or legislative agencies become mere factfinding bodies deprived of the advantages of prompt and definite action.”77 Two features of this passage are especially noteworthy. The first is the suggestion that “administrative functions” include judgments of law. The second is the emphasis on the need for “prompt and definite action” — an emphasis that is understandable on the heels of Roosevelt’s effort to take bold action in the face of the Great Depression.78 
Or consider this passage, written in the same year, from the Attorney General's Committee on Administrative Procedure79: “Even on questions of law [independent judicial] judgment seems not to be compelled. The question of statutory interpretation might be approached by the court de novo and given the answer which the court thinks to be the ‘right interpretation.’ Or the court might approach it, somewhat as a question of fact, to ascertain, not the ‘right interpretation,’ but only whether the administrative interpretation has substantial support. Certain standards of interpretation guide in that direction. Thus, where the statute is reasonably susceptible of more than one interpretation, the court may accept that of the administrative body. Again, the administrative interpretation is to be given weight — not merely as the opinion of some men or even of a lower tribunal, but as the opinion of the body especially familiar with the problems dealt with by the statute and burdened with the duty of enforcing it. This may be legislation that deals with complex matters calling for expert knowledge and judgment.” 
It is in this light that a recognition of the executive’s law-interpreting power can be understood as a natural outgrowth of the twentieth-century shift from judicial to executive branch lawmaking.80 The replacement has been spurred by dual commitments to specialized competence and democratic accountability — and also by an understanding of the need for frequent shifts in policy over time, with new understandings of fact and with new values as well. For banking, telecommunications, foreign relations, and environmental protection — among many other areas — changing circumstances often require agencies to adapt old provisions to unanticipated problems. And if interpretation of unclear terms cannot operate without some judgments of the interpreter’s own, then the argument for executive interpretation seems overwhelming. 
Vacillations and counterarguments 
The period between 1940 and 1984 offered a mixed picture on the deference question. In a number of cases, the Court seemed to indicate that it would take a firm hand.81 The rise of the “hard look doctrine” in the 1970s, spurred by judicial distrust of agency discretion, could not easily coexist with deference to agency interpretations of law.82 A key development was the new administration of President Reagan, which in relevant ways replicated that of President Roosevelt.83 In both cases, the executive branch was attempting to reorient the law in significant respects, with large-scale rethinking of the approach offered by its predecessor. It should come as no surprise that in the same period that the President was attempting such rethinking, the Supreme Court firmly 80 An illuminating study is PRICE FISHBACK AND SHAWN KANTOR, A PRELUDE TO THE WELFARE STATE: THE ORIGINS OF WORKERS’ COMPENSATION (1999). See also JERRY MASHAW, DUE PROCESS IN THE ADMINISTRATIVE STATE (1983), for a valuable discussion in the context of social security disability determinations. 81 The most important of these cases is Citizens to Preserve Overton Park v. Volpe, 401 U.S. 402 (1971). 82 See Harold Leventhal, Environmental Decisionmaking and the Role of the Courts, 122 U. PA. L. REV. 509 (1974). 83 See Matthew SPITZER & LINDA COHEN, SOLVING THE CHEVRON PUZZLE, 57 LAW AND CONTEMP. PROBS. 65 (1994). endorsed the law-interpreting power of the executive branch. At the time the Court itself may have had limited ambitions for its decision,84 but Chevron was soon viewed as a kind of revolution, a counter-Marbury for the modern era85 — or even as a McCulloch v Maryland,86 granting the executive broad discretion to choose its own preferred means to promote statutory ends. 
We can now summarize the discussion in order to venture a simple account of Chevron’s understanding of (implicit, fictional) legislative instructions. 
First, interpretation of statutes often calls for technical expertise, and here the executive has conspicuous advantages over the courts. Second, interpretation of statutes often calls for political accountability, and the executive has conspicuous advantages on that count as well. Third, the executive administers laws that apply over extended periods and across heterogeneous contexts. Changes in both facts and values argue strongly for considerable executive power in interpretation. Courts are far too cumbersome and too decentralized to do enough “updating,” or to adapt statutes to diverse domains. Fourth, it is often important to permit the modern state to act promptly and decisively. Deference to executive interpretations promotes that goal far better than a strong judicial role, and for two different reasons. It reduces the likelihood that judicial disagreement will result in time-consuming remands to the agency for further proceedings.87 More subtly, it combats the risk that different lower courts will disagree about the appropriate interpretation of statutes — and thus works to counteract the balkinization of federal law.88 These ideas help to account for Chevron’s legal fiction, which is that Congress has delegated lawinterpreting power to the executive. 
understood in the terms I have sketched, are intensely pragmatic, and a challenge might be mounted on pragmatic grounds. Suppose that we believe that executive agencies do 84 See Robert Percival, Environmental Law in the Supreme Court: Highlights from the Marshall Papers, 23 ENVTL. L. REP. 10,606, 10,613 (1993). 85 See, e.g., Kenneth Starr, Judicial Review in the Post-Chevron Era, 3 YALE J. REG. 283 (1986); Richard Pierce, Chevron and its Aftermath: Judicial Review of Agency Interpretations of Statutory Provisions, 41 VAND. L. REV. 301 ( 86 17 U.S. 316 (1819); see Duffy, supra note. 87 See JERRY MASHAW & DAVID HARFST, THE STRUGGLE FOR AUTO SAFETY (1990). 88 See Peter Strauss, One Hundred Fifty Cases Per Year: Some Implications of the Supreme Court’s Limited Resources for Judicial Review of Agency Action, 87 COLUM. L. REV. 1093, 1105–16 (1987). not usually deploy technical expertise in a way that is properly disciplined by political accountability. Suppose we think that such agencies are largely controlled by wellorganized private groups hoping to redistribute wealth or opportunities in their favor.89 If claims of agency “capture” are valid, deference to the executive might seem perverse. And if agencies are thought to be systematically biased, then the argument for independent judicial judgments on questions of law will seem much stronger.90 
We can easily imagine a parallel world, perhaps not so very different from our own, in which there is a high risk of unreliable or biased interpretations from the executive branch; perhaps courts can be trusted by comparison. In that parallel world, Chevron would be written very differently, and independent judicial interpretation would be the norm. And if some agencies are different from others, perhaps a single deference rule makes no sense. It might be tempting to distinguish between those decisions, and those agencies, that are attributable to the views of high-level officials or those with technical expertise, and those decisions that involve low-visibility judgments that do not require, or do not benefit from, such expertise. Perhaps the rule of deference should, and does, reflect an understanding of contextual differences. 
There is a further point. Political accountability and technical expertise are both important, but they might not march hand-in-hand. Perhaps politically accountable actors are not much interested in technical expertise; often they have agendas of their own.91 If the displacement of common law courts by regulatory agencies is seen as an effort to ensure that judgments are made by specialists rather than generalists, then a strong judicial hand might, on occasion, be necessary to vindicate specialization against politics. 
Few institutional judgments can be defended in the abstract. If agencies are systematically biased, independent judicial review of legal judgments is certainly easier to defend. In practice, Chevron is not uniformly applied, and courts trust some agencies more than others; but it would be extremely difficult to alter the formal doctrine in a way that recognized such differences. Notwithstanding the counterarguments, the general argument for judicial deference to executive interpretations of law remains sound; it rests 89 See, e.g., Sam Peltzman, Toward a More General Theory of Regulation, 19 J Law and Econ 211 (1976); for a good collection, see Chicago Studies in Regulation (George Stigler ed. 1988). 90 Cf. Cynthia R. Farina, Statutory Interpretation and the Balance of Power in the Administrative State, 89 COLUM. L. REV. 452 (1989). 91 For a controversial account, see Chris Mooney, The Republican War on Science (2005). on the undeniable claim that specialized competence is often highly relevant and that political accountability plays a legitimate role in the choice of one or another approach. If the executive’s judgment ignores relevant facts, then the proper approach is not to abandon Chevron, but to invalidate that judgment under Chevron Step 2 or as unlawfully arbitrary.92 A central goal of Chevron is to ensure that within the realm of reasonableness, the key judgments are made by policymaking officials, not by those with strictly legal competence. Of course those officials are fallible; but in a democratic society, that goal is worth pursuing. 
Does Chevron matter? At first glance, we would predict, with some confidence, that the decision would produce a significant increase in judicial validations of regulations. Giving the benefit of the doubt to agencies, courts would be expected to uphold agency action that would be struck down if courts were interpreting the law on their own. But a skeptic, or a dedicated realist, might believe that the judicial rules governing deference would not have much of an effect, and that judicial policy preferences would in the end turn out to be determinative. 
Schuck and Elliott 
An early study attempts to measure the effect of Chevron by examining affirmance rates in different periods. Schuck and Elliott find a statistically significant increase in validation rates in the immediate aftermath of Chevron. In particular, they find an increase in affirmance rates from 71% in the pre-Chevron year of 1984 to 81% in the post-Chevron year of 1985.93 They also find a dramatic decrease in judicial remands on the ground that agencies erred on the law.94 The combination of a higher rate of affirmance with a lower rate of remands for errors of law strongly suggests that Chevron had a significant impact.95 92 See Motor Vehicle Manufacturers’ Assn. V. State Farm, 463 US 29 (1983). 93 Peter Schuck & E. Donald Elliott, To the Chevron Station: An Empirical Study of Federal Administrative Law, 42 DUKE L.J. 984 (1989). 94 Id. at 1032-33. 95 Id. at 1034. 
We must be careful, however, with findings of this sort. First, it might well be expected that panel composition would matter as much as or more than Chevron itself. We might hypothesize, for example, that a panel of three Republican appointees would uphold agency action under President Reagan, whatever the formal doctrine — and that a panel of three Democratic appointees would be far less likely to do so, even with a strong deference signal from the Supreme Court.96 A realist perspective on the application of Chevron would speculate that the formal deference rules could be dwarfed by political inclinations. A shift in the direction of greater deference might suggest, not that the deference rule matters, but that Republican appointees are deferring to a Republican president. This point matters because Schuck and Elliott did not control for panel composition. Perhaps the higher deference rates, in 1985, were a product of a greater percentage of Republican appointees, rather than Chevron itself. I will return to this point below. 
Second, litigants should be expected to adjust their behavior to a post-Chevron world. Suppose that Chevron does make it more difficult to convince a court that an agency violated the law. If this is so, then litigants will not bring the cases they would have brought, and their success rate will change accordingly. This possibility suggests a hypothesis: The rate of judicial validations of agency interpretations of law should remain fairly constant over time, as litigants adjust their claims to the prevailing deference principles. But there is a countervailing factor: After Chevron, agencies might be willing to defend interpretations that they would not have made in a pre-Chevron world. As a result of this factor too, it might be expected that the rate of validation will remain constant. The general point is that because the mix of cases will shift, the world cannot be held constant for a test of Chevron’s effect. Even without an increase in deference rates, Chevron might have had a large effect, simply because different cases are being litigated; the margin along which challenges occur might have shifted. A comparison between 1984 and 1985 helpfully concentrates attention on the year immediately after Chevron, when litigants may not have fully adjusted to the new regime. But it is hard to draw largescale inferences from the limited data. 
Schuck and Elliott are entirely aware of this general point, and hence they attempted to study federal appeals decisions for a two-month period in March and April hard to know how to evaluate the relevant numbers, reflecting as they might a shift in the nature of the cases that were reaching the courts of appeals. 
Merrill decisions. 
Thomas Merrill offers an interesting before-and-after picture of Supreme Court decisions involving deference to executive agencies.98 In the three-year period before Chevron, the Court decided forty-five cases on the deference question, accepting the agency’s view 75% of the time. In the seven-year period after Chevron, the Court decided ninety cases on that question, accepting the agency’s view 70% of the time. Merrill concludes that Chevron did not produce an increase in the level of deference to agency 
That conclusion does not, however, follow from his data.99 Here as well, litigants on both sides might have adjusted their behavior in accordance with Chevron, and hence the world cannot be held constant between 1981 and 1990. Other variables might also account for the shift, including changes in the substantive areas with which the Supreme Court was confronted. 
Point Estimates v. Policy Space: A Glimpse from the Trenches 
E. Donald Elliott, a former General Counsel of the EPA, has offered an informal but illuminating account of the impact of Chevron, one that strongly supports the argument I have sketched on behalf of deference to the executive.100 Elliott reports that 97 Schuck & Elliott, supra note, at 1040. 98 Thomas W. Merrill, Judicial Deference to Executive Precedent, 101 Yale L.J. 969 (1992). 99 See COHEN & SPITZER, LAW AND CONTEMP PROBLEMS, at 92. 100 E. Donald Elliott, Chevron Matters: How the Chevron Doctrine Redefined the Roles of Congress, Courts and Agencies in Environmental Law, 16 VILL. J. ENV. L. 1 (2005). Chevron “change[d] the way that we did business.”101 Before Chevron, the Office of Legal Counsel (OLC) within EPA usually assumed that a statute was “a prescriptive text having a single meaning, discoverable by specialized legal training and tools.”102 In Elliott’s view, the single meaning approach created a special role for lawyers, one that “led to a great deal of implicit policy-making.”103 But post-Chevron, lawyers within the EPA offered no single point estimate. Instead they “attempt[ed] to describe a permissible range of agency policy-making discretion that arises out of a statutory ambiguity.”104 The result is not a single meaning but a "policy space,"105 containing a range of permissible interpretive discretion. It follows that the “agency's policy-makers, not its lawyers, should decide which of several different but legally defensible interpretations to adopt.”106 
In Elliott’s account, “Chevron opened up and validated a policy-making dialogue within agencies about what interpretation the agency should adopt for policy reasons, rather than what interpretation the agency must adopt for legal reasons.”107 The result has been to “increase[] the weight given to the views of air pollution experts in the air program office relative to the lawyers.”108 At the same time, there has been a shift from an emphasis on legal texts to an emphasis on consequences. “Chevron moved the debate from a sterile, backward-looking conversation about Congress' nebulous and fictive intent to a forward-looking, instrumental dialogue about what future effects the proposed policy is likely to have.”109 In short, “Chevron is significant for reducing the relative power of lawyers within EPA and other agencies and for increasing the power of other professionals.”110 
This account strongly supports the general justification that I am attempting to offer here. The precise point of Chevron is to acknowledge that in the face of ambiguity, the key questions are not for those with legal training, but instead for other professionals. We do not know enough to know whether the shift that Elliott describes has also occurred 101 Id. at 11. 102 Id. 103 Id. 104 Id. 105 Id. at 12. 106 Id. 107 Id. 108 Id. 109 Id. at 13. 110 Id. within other agencies. But if the FCC is deciding whether or how to engage in deregulation, if the President is deciding how to implement an authorization to use force in response to the attacks of September 11,111 and if the Department of the Interior is deciding on the reach of the Endangered Species Act,112 there is every reason to think that the job of lawyers, and of reviewing courts, is to identify policy spaces, not to insist on point estimates. 
Republican and Democratic appointees, and beyond 
Let us return to data and explore the results of a study, conducted by Thomas Miles and me, of judicial behavior in Chevron cases.113 The central goal of the study is to see whether political affiliation, or political convictions, play a role in judicial review of agency interpretations of law. 
On the lower courts, the study involves all published court of appeals decisions between 1990 and the present, reviewing interpretations of law by the Environmental Protection Agency, the Federal Communications Commission, and the National Labor Relations Board. Decisions are generally coded as "liberal" if the agency decision is upheld against industry attack; decisions are also generally coded as liberal if the agency decision is invalidated as a result of an attack by a public interest group. Here are the principal findings: 
than Democratic appointees. The former provide “liberal” votes 49% of the time; the latter provide such votes 60% of the time.114 
Democratic appointees sit only with Democratic appointees, the gap grows – to 28%. Republican appointees show far more conservative voting patterns (43% liberal votes) 111 See Cass R. Sunstein, Administrative Law Goes to War, 118 HARV. L. REV. 2663 (2005). 112 See Babbitt v. Sweet Home Chapter of Communities for a Great Oregon, 515 U.S. 687 (1995). 113 See Cass R. Sunstein and Thomas Miles, Do Judges Make Regulatory Policy? An Empirical Investigation of Chevron, U Chi L Rev (forthcoming 2006). 114 Id. when sitting only with other Republican appointees; the same is even more true for Democratic appointees on the liberal side (70% liberal votes).115 
Republican presidents than those of Democratic presidents. Democratic appointees are more likely to uphold the interpretations of Democratic presidents than those of Republican presidents.116 
What about the Supreme Court? Here the study investigates the votes of the individual justices between 1990 and the present in all clear Chevron cases -- that is, in all cases in which the Court applies the Chevron framework. These are the main results: rates under the Bush Administration than under the Clinton Administration. Justices Souter, Stevens, Breyer, and Ginsburg show higher deference rates under the Clinton Administration than under the Bush Administration.117 (The change in the latter group is 13%, smaller than the 18% increase shown by the former group.) 
Rehnquist show the most conservative voting patterns in Chevron cases, while Justices Stevens, Souter, Breyer, and Ginsburg show the most liberal.118 This finding is noteworthy because Chevron requires courts to defer to agency interpretations of ambiguous statutes; for that reason, it might be anticipated to ensure that judicial votes, reviewing agency interpretations of law, do not fall along the expected ideological lines. 
the likelihood that the agency’s decision will be liberal? For the Souter, Stevens, Breyer, and Ginsburg group, the likelihood is 33%; for the Rehnquist, Scalia, and Thomas group, the likelihood is 62%.119 
These findings present many questions, and this is not the space to explore them in detail.120 The most general conclusion is that even under Chevron, the political commitments of reviewing judges continue to play a significant role in the decision whether to uphold decisions by the executive branch. For courts of appeals, the difference between Republican and Democratic appointees is smaller than it is on the Supreme Court, which decides the hardest cases; but the difference remains substantial. At first glance, this evidence fortifies the argument for a strong reading of Chevron. There is no reason to think that where statutes are ambiguous, their meaning should depend on the composition of the panel that litigants draw. 
Since 1984, there have been serious attacks on the idea that the executive has the power to say what the law is. Many observers have feared that this idea ultimately compromises the rule of law, by allowing a combination of executive and adjudicatory authority in a way that eliminates an independent judicial check.121 In the last twenty years, efforts to cabin the executive’s power have taken several forms. I outline the principal efforts here and explain why they should be rejected. The underlying point is that those who seek an independent judicial check would, in reality, increase the likelihood that judgments of policy would be made by federal judges, not by Congress. Pure Questions of Law 
In one of the most important pre-Chevron cases, the Court seemed to distinguish between purely legal questions, to be resolved by judges, and applications of law to fact, for which deference would be appropriate.122 
The key case was INS v. Cardoza-Fonseca.123 At issue was a statutory provision permitting the Attorney General to deny asylum to an alien who wishes to stay in the United States “because of . . . a well-founded fear of persecution” in his home country. The INS interpreted “well-founded fear” to require persecution to be “more likely than not.” On this view, a good-faith subjective belief, based on evidence, would not be enough; a 51% probability of persecution was necessary. The Court struck down the 121 See Farina, supra note; Breyer, supra note, is in the same spirit. 122 See NLRB v. Hearst Publications, 322 US 111 (1944). 123 480 US 421 (1987). agency’s interpretation, emphasizing the text, structure, and history of the statute. But the Court added that the issue in the case involves “a pure question of statutory construction for the courts to decide.”124 That question “is, of course, quite different from the question of interpretation that arises in each case in which the agency is required to apply” a standard “to a particular set of facts.”125 In applying law to fact through case-by-case decisions, agencies would receive judicial respect. But the interpretive issue in CardozaFonseca “is well within the province of the judiciary.”126 
Taken on its face, Cardoza-Fonseca seems to be an effort to restore the preChevron status quo, by asserting the primacy of the judiciary on any “pure question of statutory construction.” And in fact, Justice Scalia saw Justice Stevens’ opinion in exactly that way. In his concurrence, he said that the Court’s “discussion is flatly inconsistent” with the view that Chevron established “that courts must give effect to a reasonable agency interpretation of a statute unless that interpretation is inconsistent with a clearly expressed congressional intent.”127 On pure questions of law, no less than on mixed questions, Justice Scalia contended that deference was the appropriate approach. 
On this count Justice Scalia was clearly correct. Suppose that the statute in Cardoza-Fonseca was genuinely ambiguous – that an investigation of the standard legal sources did not say whether Congress meant to forbid the INS from adopting a “more probable than not” standard. In that event, judicial deference would be appropriate, however “pure” the legal question. By hypothesis, the ambiguity is genuine, and hence a judgment of policy is involved (in a particularly sensitive area). The key point is that even when purely legal questions are raised, purely legal competence may not be enough to resolve them. Chevron itself is an example. The definition of “source” did not involve the application of law to fact, and nonetheless the Court deferred to the EPA’s view. 
Justice Scalia’s concurrence has triumphed, in the sense that there is no separate category of cases involving purely legal questions. The distinction drawn in CardozaFonseca has failed to appear in subsequent cases. Jurisdiction 
Does Chevron apply to jurisdictional disputes? The Supreme Court has divided on the question,128 which remains unsettled in the lower courts.129 If courts are entitled to make independent judgments about jurisdictional issues, the executive would be deprived of law-interpreting power in many of the areas in which it would most like to have that power. It can be contested whether an exception for jurisdictional questions would arise in many cases; but the importance of such an exception would be incontestable. This, then, is a second route by which Chevron’s reach might be cabined. 
In the abstract, there are plausible arguments on both sides. Recall that Chevron is rooted in a theory of implied delegation, and it is reasonable to think that Congress should not be taken to have delegated to agencies the power to decide on the scope of their own authority. That question, it might be thought, ought to be answered by an independent institution, not by the agency itself. If foxes ought not to guard henhouses, then perhaps agencies should not be understood to have power to assess the reach of their own authority. For this reason, Justice Brennan argued that judgments about jurisdiction “have not been entrusted to the agency” and might well “conflict with the agency's institutional interests in expanding its own power.”130 
On the other hand, any exemption of jurisdictional questions is vulnerable on two grounds. First, the line between jurisdictional and nonjurisdictional questions is far from clear, and hence any exemption threatens to introduce much more complexity into the inquiry into the deference question. With this point in mind, Justice Scalia argued that “there is no discernible line between an agency's exceeding its authority and an agency's exceeding authorized application of its authority.”131 
more fundamentally, the considerations that underlie Chevron might well support, rather than 128 Miss. Power & Light Co. v. Mississippi, 487 U.S. 357 ( 129 See, e.g., Alaska v. Babbitt, 72 F.3d 698 (9th Cir. 1995) (deferring on jurisdictional issue involving definition of “public lands”); Cavert Acquisition Corp. v. NLRB, 83 F.3d 598 (3d Cir. 1996) (deferring on jurisdictional issue involving definition of “employee”); United Trans. Union v. Surface Transp. Bd., 183 F.3d 606 (7th Cir. 1999) (refusing to defer on jurisdictional issue). A recent discussion can be found in NRDC v. Abraham, 355 F.3d 179 (2d Cir. 2004), in which the court, after finding a Step One violation, adds that “it seems highly unlikely that a responsible Congress would implicitly delegate to an agency the power to define the scope of its own power” — and then suggested that Mead (!) provided the appropriate framework. See id. at 199-200. 130 487 U.S. at 387 (Brennan, J., dissenting). 131 Id. at 345 (Scalia, J., concurring in the judgment). undermine, its application to jurisdictional questions. If an agency is asserting or denying jurisdiction over some area, it is either because democratic forces are leading it to do so or because its own specialized competence justifies its jurisdictional decision. 
To be sure, an assertion of jurisdiction is an expansion of authority, but in the abstract, a refusal to exercise that authority may be troubling as well. In the face of ambiguity with respect to jurisdiction, the executive, and not the judiciary, should make the underlying judgments of policy and principle. 
Major Questions 
Although debate continues over the executive’s authority to decide jurisdictional questions, the Court has recently raised a closely related question: whether Chevron applies to “major” questions. This issue was initially signaled in 1986 in a short essay by then-Judge Breyer, who suggested that Chevron should be read not to establish a simple rule, but instead to provide the foundation for a more particularistic inquiry into Congress’ likely instructions on the deference issue.132 In his view, the inference would rest on an inquiry into “what a sensible legislator would have expected given the statutory circumstances.”133 The expectations of the sensible legislator would depend on an inquiry into institutional competence: “The less important the question of law, the more interstitial its character, the more closely related to the everyday administration of the statute and to the agency's (rather than the court's) administrative or substantive expertise, the less likely it is that Congress (would have) ‘wished’ or ‘expected’ the courts to remain indifferent to the agency's views. Conversely, the larger the question, the more its answer is likely to clarify or stabilize a broad area of law, the more likely Congress intended the courts to decide the question themselves.”134 
For present purposes, the key distinction is between “less important” and “more interstitial” questions on the one hand and “larger” questions on the other. Justice Breyer’s apparent suggestion is that for the latter, an independent judicial hand is desirable. 
The Court as a whole signaled a strong interest in this distinction in FDA v. Brown & Williamson,135 the tobacco case with which I began. Recall that the statutory language appeared ambiguous on the question, for it defined drug to include “articles (other than food) intended to affect the structure or any function of the body.”136 Under this language and with the assistance of Chevron, the FDA contended, with considerable force, that it could assert authority over tobacco. But the Court rejected its analysis through a complicated route. Much of its opinion emphasized the wide range of tobaccospecific legislation enacted by Congress in the last decades — legislation that, in the Court’s view, should “preclude an interpretation of the FDCA that grants the FDA authority to regulate tobacco products.”137 But the Court added a closing word. It said that its inquiry into the Chevron question “is shaped, at least in some measure, by the nature of the question presented.”138 Chevron, the Court noted, is based on “an implicit delegation,” but in “extraordinary cases,” courts should “hesitate before concluding that Congress has intended such an implicit delegation.”139 The Court cited no case for this key proposition, but instead referred to the 1986 essay by then-Judge Breyer, 140 encapsulating one of his central arguments, that there is a difference between “major questions,” on which “Congress is more likely to have focused,” and “interstitial matters.” At that point the Court added, “we are confident that Congress could not have intended to delegate a decision of such economic and political significance to an agency in so cryptic a fashion.”141 
How should this passage be read? It would be plausible to say that for decisions of great “economic and political significance,” an implicit delegation ought not to be 135 529 U.S. 120 (2000). 136 21 U.S.C. § 321(g)(1)(c). 137 529 U.S. at 157. 138 529 U.S. at 159. 139 Id. 140 See Breyer, supra note. 141 Id at 160. found. And if an exception exists for major questions, then the executive’s power of interpretation faces a large limitation. And indeed, the EPA has seized on Brown & Williamson in contending that it lacks the power to regulate greenhouse gases.142 The brief passage in Brown & Williamson could be invoked in many contexts, limiting Chevron to “interstitial” questions, as Justice Breyer would apparently prefer. The problem is that there is no good justification for the conclusion that major questions should be resolved judicially rather than administratively. To say the least, no simple line separates interstitial and major questions; and an insistence on that line would raise doubts about an array of decisions, including Chevron itself. In any case expertise and accountability, the linchpins of Chevron’s legal fiction, are highly relevant to the resolution of major questions. 
Assume, for example, that the statutes in Brown & Williamson were genuinely ambiguous — that the relevant sources of interpretation could plausibly be read to support or to forbid the agency action at issue. If so, the argument for judicial deference would be exceptionally strong. In Brown & Williamson, the FDA was taking action to reduce one of the nation’s most serious public health problems, in a judgment that had a high degree of public visibility and required immersion in the subject at hand. Perhaps Congress could not easily be taken to delegate the resolution of these questions to administrative agencies. But would it really be better to understand Congress to have delegated the resolution of those questions to federal courts? Which federal courts? Nominated by which president? 
A different version of the “major questions” exception would have greater appeal. Suppose it is thought that agencies should not be allowed to move the law in new directions without congressional approval. On this view, courts would not be displacing policy decisions by the executive branch. They would be attempting to block the executive from initiating massive changes without clear legislative authorization. Perhaps Brown & Williamson can be understood in these terms.143 This claim is on the right track 142 See Note, Trapped in the Greenhouse?, 54 DUKE L.J. 147 (2004); Note, Carbon Dioxide: A Pollutant in the Air, but Is the EPA Correct that It is Not an Air Pollutant?, 104 Colum. L. Rev. 1996 (2004). 143 And so too for MCI Telecommunications Corp. v. ATT, 512 US 218 (1994), which prohibited the FCC from adopting a large-scale deregulatory initiative, and which emphasized that initiative would amount to a “radical or fundamental change in the Act’s tariff-filing requirement. . . . It is highly unlikely that Congress insofar as it emphasizes the relevance of nondelegation concerns to application of the Chevron framework.144 But it runs into two problems. First, the distinction between “major” changes and less major ones remains ambiguous; consider Chevron itself. Second, it is legitimate for the executive to make “major” changes insofar as it is doing so through reasonable interpretation of genuinely ambiguous statutes. The best use of nondelegation concerns lies elsewhere, as we shall shortly see. 
Chevron Step Zero 
In recent years, the most active debates over the executive’s power to interpret the law have involved “Chevron Step Zero” – the threshold inquiry into whether the executive’s law-interpreting power exists at all.145 The Step Zero inquiry has produced a great deal of confusion and complexity, defying the hopes of those who hoped that Chevron would simplify the law.146 
The key case is United States v. Mead Corporation.147 The question was the legal status of a tariff clarification ruling by the United States Customs Service. Was such a ruling entitled to Chevron deference? The Court concluded that it was not, distinguishing between Chevron cases, subject to the two-step framework, and other kinds of cases, in which the agency’s decision would be consulted but would not receive deference at all.148 The Court’s central suggestion is that Chevron applies “when it appears that Congress has delegated authority to the agency generally to make rules carrying the force of law, and that the agency interpretation claiming deference was promulgated in the exercise of that authority.”149 An implicit delegation of interpretive authority would be apparent if Congress “would expect the agency to be able to speak with the force of law.”150 would leave the determination of whether an agency will be entirely, or even substantially, rate-regulated to agency discretion . . . .” 144 See infra. 145 See Cass R. Sunstein, Chevron Step Zero, Virginia L Rev (forthcoming 2006). 146 Lisa Schultz Bressman, How Mead Has Muddled Judicial Review of Agency Action, VALD. L. REV. (forthcoming 2005); Adrian Vermeule, Mead in the Trenches, 71 GEO. WASH. L. REV. 847 (2003). 147 533 U.S. 218 (2001). 148 These cases follow Skidmore v. Swift & Co., 323 U.S. 134 (1944), and hence it is now possible to distinguish between “Chevron deference” and “Skidmore deference.” 149 Id. at 226-27. 150 Id. 
In the Court’s view, a “very good indicator of delegation” is authorization “to engage in the process of rulemaking or adjudication that produces regulations or rulings for which deference is claimed.”151 If agencies have been given power to use relatively formal procedures, and if they have exercised that power, they are entitled to Chevron deference. Nonetheless, Chevron deference can be found, and has sometimes been found, “even when no such administrative formality was required and none was afforded.”152 Why, then, was the tariff ruling in Mead not entitled to deference? A relevant factor was that formal procedures were not involved. Another was that nearly fifty customs offices issue tariff classifications, producing 10,000 to 15,000 annually. “Any suggestion that rulings intended to have the force of law are being churned out at a rate of 10,000 a year at an agency’s 46 scattered offices is simply self-refuting.”153 
What is motivating the Court to restrict Chevron’s domain? The Court’s own rationale speaks of the absence of a congressional delegation of law-interpreting power. Perhaps there has been no delegation in cases in which Chevron has been held not to apply. But recall that we are speaking here of fictions, not of actual congressional instructions. Why is the refusal to defer to the executive the most sensible fiction? The Court must be thinking that if an agency is not operating pursuant to formal procedures, it is not entitled to deference, because the absence of such procedures signals a lack of accountability and a risk of arbitrariness. Perhaps formal procedures increase the likelihood that expertise will be properly applied; perhaps they also ensure political constraints on agency discretion. 
These suggestions are understandable, but there are two problems with the resulting state of affairs. The first involves the burdens of decision. To say the least, it is unfortunate if litigants and courts have to work extremely hard to know whether a decision by the executive is entitled to deference. The second problem involves institutional comparisons. Even when an agency’s decision is not preceded by formal procedures, there is no reason to think that courts are in a better position than agencies to resolve statutory ambiguities. For the future, Mead should not be taken to establish anything like a presumption against judicial deference when the agency has not 151 Id. 152 Id. at 231. 153 Id. at 233. proceeded through formal procedures. Instead it should be seen as an unusual case in an exceedingly unusual setting, in which low-level administrators were required to produce thousands of rulings. 
A narrow understanding of Mead would continue to allow deference to be applied to many agency decisions not preceded by formal procedures.154 Most important, that narrow understanding would suggest that the President himself is entitled to deference in his interpretations of law, even if he has not proceeded through formal procedures. When Congress delegates authority to the President, it ought to be presumed to have entitled him to construe ambiguities as he sees fit, subject to the general requirement of Canons Against Chevron 
My general argument has been in favor of an expansive view of the executive’s power to interpret the law. But there is one area in which that power is limited. The area involves interpretive principles that require Congress to decide certain issues explicitly. In that area, an exception to the Chevron principle, calling for invalidation of agency decisions at Step One, is entirely appropriate. 
It is familiar to hear the idea that Congress must speak with clarity, most obviously in connection with the nondelegation doctrine156; and in fact, my argument on behalf of judicial deference to executive interpretations of law might seem to be in tension with that doctrine. On one view, Article I forbids Congress from “delegating” its power to anyone else, and open-ended grants of authority are unconstitutional. While the Supreme Court has not used the nondelegation doctrine to invalidate a federal statute since in 1935, 157 the Court continues to pay lip service to the doctrine, and to hold it in 154 For more detailed discussion, see Sunstein, supra note. 155 See Acree v. Republic of Iraq, 370 F3d 41, 68 n. 2 (DC Cir 2004) (Roberts, J., concurring): “The applicability of Chevron to presidential interpretations is apparently unsettled, but it is interesting to note that this would be an easy case had the EWSAA provided that, say, the Secretary of State may exercise the authority conferred under Section 1503. It is puzzling why the case should be so much harder when the authority is given to the Secretary's boss” (citations omitted). 156 For general discussion and critique, see Eric Posner and Adrian Vermeule, Interring the Nondelegation Doctrine, 69 U Chi L Rev 1721 (2002). 157 Schechter Poultry Corp v US, 295 US 495 (1935). reserve for extreme cases.158 Why has the Court been so reluctant to use the doctrine to strike down statutes? One reason is that the idea of nondelegation is difficult to enforce, requiring as it does difficult judgments of degree.159 There are also questions about the constitutional pedigree of the doctrine, and about whether it would make American government work better or worse.160 
At most, the nondelegation doctrine now operates as a tool of statutory construction, suggesting a presumption in favor of narrow rather than open-ended grants of authority.161 Perhaps Chevron is objectionable on nondelegation grounds, because it grants the executive the authority to interpret the very statutes that limit its power.162 But there is a serious problem with this objection. If the executive is denied interpretive authority, that authority is given to the judiciary instead, and it is not clear that any nondelegation concern is reduced as a result. On the contrary, an allocation of policymaking authority to the executive seems to reduce that concern, precisely because the executive has a measure of accountability.163 
lawinterpreting authority on the ground that the key decisions must be explicitly made by the national lawmaker. The most important idea here is that the executive is not permitted to construe statutes so as to raise serious constitutional doubts.164 Note that this principle is far more ambitious than the modest claim that a statute will be construed so as to be constitutional rather than unconstitutional (thus forbidding the executive to adopt unconstitutional interpretations). Under the idea that I am describing, the executive is forbidden to adopt interpretations that are constitutionally sensitive, even if those interpretations might ultimately be upheld. The only limitations on the principle are that the constitutional doubts must be serious and substantial, and that the statute must be 158 Whitman v. ATA, 531 US 457 (2001). 159 See Richard B. Stewart, Beyond Delegation Docrtine, 36 Am U L Rev 323, 324-28 (1987) (discussing "the absence of judicially manageable and enforceable criteria to distinguish permissible from impermissible delegations"); Mistretta v US, 488 US 361, 415-16 (Scalia dissenting) (emphasizing problems with judicial enforcement of the conventional doctrine). 160 See Eric Posner and Adrian Vermeule, Interring the Nondelegation Doctrine, 69 U Chi L Rev 1721 (2002). 161 See The Benzene Case, 448 US 607 (1980). 162 See Farina, supra note. 163 See Mistretta v US, 488 US 361, 415-16 (Scalia, J., dissenting). 164See, e.g, Solid Waste Agency v. USACE, 121 S Ct 675, 683 (2001); DeBartolo Corp v, Fla East Coast, 485 US 568 ( fairly capable of an interpretation contrary to the agency’s own.165 So long as the statute question via explicit statement. 
Why does this idea overcome the executive’ s usual power of interpretation? The reason is that we are speaking of a kind of nondelegation canon – one that attempts to require Congress to make its instructions exceedingly clear, and that does not permit the executive to make constitutionally sensitive decisions on its own.166 For example, a court of appeals invalidated a Federal Election Commission rule that interpreted the governing statute so as to allow it to make a public release of the files of a completed investigation.167 The court acknowledged that the statute was ambiguous, but said the agency’s interpretation was unreasonable because it would create first amendment difficulties. 
Chevron as well. One of the most fundamental forbids the executive to apply statutes outside of the territorial borders of the United States.168 The central notion here is that extraterritorial application calls for extremely sensitive judgments involving international relations; such judgments must be made via the ordinary lawmaking process (in which the President of course participates). The executive may not make this decision on its own.169 Consider also the notion that unless Congress has spoken with clarity, the executive is not allowed to apply statutes retroactively, even if the relevant terms are quite unclear.170 Retroactivity is disfavored in the law,171 and Congress will not be taken to have delegated to the executive the authority to choose to apply statutes retroactively. 165See Rust v Sullivan, 500 US 173, 191 (1991). 166 I discuss this idea more generally in Cass R. Sunstein, Nondelegation Canons, 67 U Chi L Rev 315 (2000). 167 AFL-CIO v. FEC, 333 F3d 168 (DC Cir 2003). 168EEOC v Arabian American Oil Co., 499 US 244, 248 (1991). 169Of course the executive is permitted to make a large number of quite sensitive decisions involving foreign relations, partly because of express constitutional commitments, partly because of perceived contemporary necessities. And it would not be impossible to imagine a legal system in which the executive was permitted, in the event of ambiguity, to resolve the issue of extraterritoriality. Recall that my goal here is descriptive, not normative. The best defense of this particular nondelegation canon would be that the question whether the enacted law should be applied outside of the nation’s borders is a large and essentially legislative one, which cannot be made by the executive on its own. 170Bowen v Georgetown University Hospital, 488 US 204, 208 ( 171Id. 
One of the most general nondelegation canons is the rule of lenity, which says that in the face of ambiguity, criminal statutes will be construed favorably to criminal defendants.172 Criminal law must be a product of a clear judgment on Congress’s part. Where no clear judgment has been made, the statute will not apply merely because it is plausibly interpreted, by courts or enforcement authorities, to fit the case at hand. For broadly related reasons, the executive cannot interpret statutes and treaties unfavorably to Native Americans.173 Where statutory provisions are ambiguous, the government will not prevail. This idea is plainly an outgrowth of the complex history of relations between the United States and Native American tribes, which have semi-sovereign status; it is an effort to ensure that any unfavorable outcome will be a product of an explicit judgment from the national legislature. The institutional checks created by congressional structure must be navigated before an adverse decision may be made. There are many other examples.174 In many areas, ranging from broadcasting to the war on terror,175 the nondelegation canons operate as constraints on the interpretive discretion of the executive. 
What emerges is therefore a simple structure. In general, the executive is permitted to interpret ambiguous statutes as it sees fit, subject to the constraints of reasonableness. The only limitations can be found in the nondelegation canons. The resulting framework is admirably well-suited to the needs of modern government; it grants the executive the same degree of discretion that it deserves to have. 
Chevron is best taken as a vindication of the realist claim that resolution of statutory ambiguities often calls for judgments of policy and principle. Indeed, that claim 172But see Dan Kahan, Is Chevron Relevant to Federal Criminal Law, 110 Harv L Rev 469 (1996). 173See Ramah Navajo Chapter v Lujan, 112 F3d 1455, 1461–62 (10th Cir 1997) (grounding a canon of statutory construction favoring Native Americans in “the unique trust relationship between the United States and the Indians”); Williams v Babbitt, 115 F2d 657, 660 (9th Cir 1997) (noting in dicta that courts “are required to construe statutes favoring Native Americans liberally in their favor”); Tyonek Native Corp v Secretary of Interior, 836 F2d 1237, 1239 (9th Cir 1988) (noting in dicta that “statutes benefiting Native Americans should be construed liberally in their favor”). 174United States Department of Energy v Ohio, 503 US 607, 615 (1992); National Association of Regulatory Utility Commissioners v FCC, 880 F2d 422 (DC Cir 1989). 175 See Curtis Bradley and Jack Goldsmith, Congressional Authorization and the War on Terrorism, 118 Harv L Rev. 2047 (2005). 
Follow this and additional works at: https://chicagounbound.uchicago.edu/law_and_economics Part of the Law Commons Recommended Citation 
        Boundedly Rational Borrowing: A Consumer’s Guide  
Cass R. Sunstein  
T H E   L A W   S C H O O L   T H E   U N I V E R S I T Y  O F   C H I C A G O   
  July 2005  
  
This paper can be downloaded without charge at:  The Chicago Working Paper Series Index: http://www.law.uchicago.edu/Lawecon/index.html  and at the Social Science Research Network Electronic Paper Collection:   http://ssrn.com/abstract_id=772186    Draft 7/28/05 Forthcoming University of Chicago Law Review 
Excessive borrowing, no less than insufficient savings, might be a product of bounded rationality. Identifiable psychological mechanisms are likely to contribute to excessive borrowing; these include myopia, procrastination, optimism bias, “miswanting,” and what might be called cumulative cost neglect. Suppose that excessive borrowing is a significant problem for some or many; if so, how might the law respond? The first option involves weak paternalism , through debiasing and other strategies that leave people free to choose as they wish. Another option is strong paternalism, which forecloses choice. Because of private heterogeneity and the risk of government error, regulators should have a firm presumption against strong paternalism, and hence the initial line of defense against excessive borrowing consists of information campaigns, debiasing, and default rules. On imaginable empirical findings, however, there may be a plausible argument for strong paternalism in the form of restrictions on various practices, perhaps including “teaser rates” and late fees. The two larger themes, applicable in many contexts, involve the importance of an ex post perspective on the consequences of consumer choices and the virtues and limits of weak forms of paternalism, including debiasing and libertarian paternalism. 
The world contains three kinds of people: those who borrow the right amount, those who borrow too much, and those who borrow too little. 
The evaluation of whether borrowing is optimal might be made ex ante or ex post. Economists and economically oriented lawyers prefer the ex ante perspective. From that perspective, people might borrow too little if they lack adequate information about the high benefits or low costs of borrowing in a particular instance. Or they might borrow too little if some kind of emotion—unjustified fear of debt, for example—disables them from borrowing money in circumstances in which they would do so if not thus disabled. * Karl N. Llewellyn Distinguished Service Professor, Law School and Department of Political Science, University of Chicago. For comments on an earlier draft, I am grateful to Eric Posner, Richard Posner, Richard Thaler, Adrian Vermeule, and participants in a conference on consumer behavior at the University of Chicago Law School. 
Excessive borrowing could have a similar ex ante explanation. Borrowers might be insufficiently informed of the costs of credit, believing that those costs are far lower than they actually are. Alternatively, borrowers might suffer from a cognitive or motivational problem, such as impulsiveness, that leads them to borrow, and perhaps to face high interest rates, when fully rational people would not do so. 
At least as plausibly, the question of optimal borrowing should be investigated ex post, with close reference to the actual effects of borrowing behavior on people’s lives. If we are concerned about human welfare, there is much to be said in favor of the ex post perspective. Sometimes there is a gap between what people want and what people like; if so, the wanting-asking gap might be large enough to justify governmental concern.1 A central question is the effects of consumer choices, including borrowing, on consumers. We might find that insufficient or excessive borrowing ensures that people’s lives go significantly less well than they otherwise would. 
Suppose, for example, that people’s consumption choices lead them to purchase products that do not much improve their well-being, but that the resulting debt much impairs their well-being—by, for example, making current earnings go to debt repayment rather than food and medical care. Or perhaps consumer behavior leads to a battle for greater relative position, one that amounts to a kind of arms race in which people try to keep pace with one another.2 That arms race, involving competition over “positional goods,” almost certainly plays a role in recent increases in consumer debt. If borrowing means that people’s welfare is significantly reduced, then there is a real problem, one that might justify some kind of legal intervention. 
The risk of excessive borrowing is paralleled by the risk of insufficient borrowing. Perhaps people’s lives would go better if they were willing to incur debt at various stages. Perhaps many young people, and some older ones too, are unduly fearful of debt, and hence refuse to borrow money when it is very much in their interest to do so. It is fully imaginable that the problem of excessive borrowing is equaled or exceeded by the problem of insufficient borrowing. 
I do not seek in this essay to reach any final conclusions about the sources and magnitude of boundedly rational borrowing or about the appropriate legal response. My narrower goal is to provide a kind of regulator’s guide, or a conceptual map—a general outline of the reasons that boundedly rational borrowing might occur and the possible legal remedies. My hope is that the discussion will be applicable to a wide range of situations in which bounded rationality is a potential problem; hence much of the analysis could be applied to such behavior as smoking, drinking, eating, exercising, saving, vacationing, and working. Evaluation of the relevant mechanisms and remedies would require detailed empirical investigation, which I do not venture here. There are two larger themes. The first, already suggested, involves the importance of evaluating the effects of consumer behavior ex post rather than ex ante—of investigating the consequences of consumer choices for the lives of consumers. Too often, the ex ante perspective has ruled that evaluation off-limits. The second involves the uses and limits of weak paternalism. It is possible to imagine approaches to boundedly rational behavior that preserve freedom of choice while also steering people in directions that will promote their own well-being. As a presumption, regulators should favor weak rather than strong paternalism, simply because the preservation of choice is an important safeguard against government error. 
My focus here will be on excessive borrowing. A key question is why people might be excessive rather than optimal borrowers. The most obvious reason, suggested above, involves a lack of information: Borrowers might not be adequately informed of the costs and benefits of borrowing. They might not read the fine print; they might believe that short-term “teaser rates” are actually long-term, or at least neglect the fact that such rates will have only a small beneficial effect on their finances. To suggest this possibility, it is not necessary to observe that human beings are boundedly rational. But now assume that we are dealing with homo sapiens, not homo economicus.3 Five problems are likely to contribute to excessive borrowing. 3 An excellent discussion, from which I have learned a great deal, is Oren Bar-Gill, Seduction by Plastic, 98 Nw. L. Rev. 1373 (2004). hesitate before borrowing $20,000 at a high rate of interest. But if a long series of much smaller purchases has that same effect, the cost might well be less visible. A distinctive form of bounded rationally stems from neglect of the aggregate effect of large numbers of relatively small borrowing choices. Call this cumulative cost neglect.4 When borrowing is excessive, the reason often lies in that form of neglect. Addictive behavior is the most serious problem here, but cumulative cost neglect can be a problem even without addiction. interest rates and late charges. Timely payments will eliminate the problem. But some borrowers procrastinate, ensuring that some bills are paid late. As a result, significant charges can accumulate. It is apparently difficult for some people to overcome the costs of inertia even when transaction costs are minuscule; I speculate that the strikingly high economic level of late fees5 are, in nontrivial part, a result of procrastination. believing that they will be able to repay a debt when this is unlikely. Unrealistically optimistic borrowers will make welfare-reducing consumption choices, simply because they will fail to appreciate the problems associated with their borrowing. Most young smokers falsely believe that they will not be smoking in a few years.7 So too, many borrowers, I suggest, falsely believe that they will not have a serious debt problem as a result of their behavior. Unrealistic optimism affects consumers at the time when they are making large expenditures. 
4 See Paul Slovic, What Does It Mean to Know a Cumulative Risk? Adolescents’ Perceptions of ShortTerm and Long-Term Consequences of Smoking, 13 J Behav. Decision Making 259 (2000); Paul Slovic, Do Adolescent Smokers Know the Risks, 47 Duke LJ 1133 (1998). 5 See Thomas Brown, this issue. 6 See David Armor & Shelley E. Taylor, When Predictions Fail: The Dilemma of Unrealistic Optimism, in Heuristics and Biases: The Psychology of Intuitive Judgment 334 (Thomas Gilovich et al. eds. 2002). 7 See Slovic, supra note. the short-term at the expense of the future.8 Myopic borrowing might be seen as a taste for current well-being over future well-being, in a way that raises no concerns about bounded rationality; but if a day’s welfare produces long-term distress, bounded rationality is probably involved. Myopia contributes to selfcontrol problems, by which consumers make decisions that undermine their wellbeing over time. In this way, excessive borrowing belongs not only in the same general family with insufficient savings, but also with insufficient exercise, obesity, poor diet, and excessive smoking and drinking.9 a problem of “miswanting”;10 they want (and buy) things that do not promote their welfare, and they do not want things that would promote their welfare. When this is so, then the idea of consumer sovereignty loses some of its underlying justification; people’s decisions do not actually make their lives go better.11 Some borrowing behavior is undoubtedly a product of miswanting. A related problem arises when borrowing is a product of competition to achieve better relative position with respect to goods—a competition from which consumers do not benefit as a whole.12 
competition for better relative position, they are acting rationally, and they are not miswanting: If relative position matters—and it does—then people should try to maintain it. But to the extent that easy borrowing accelerates that competition, it is likely to produce a great deal of harm. 8 See George Loewenstein and Drazen Prelec, Anomalies in Intertemporal Choice, in Choices, Values, and Frames 592-93 (Daniel Kahneman and Amos Tversky eds. 2000). 9 Some problems generally assessed under the rubric of myopia and self-control problems might well be better handled under the framework presented in George Loewenstein, this volume; in particular, Loewenstein’s emphasis on the role of emotions and temptation has obvious applications to borrowing behavior as well as to the more familiar contexts of overeating, drinking, and smoking. 10.On “miswanting,” see Daniel T. Gilbert & Timothy D. Wilson, Miswanting, in FEELING AND THINKING: THE ROLE OF AFFECT IN SOCIAL COGNITION 178, 179 (Joseph P. Forgas ed., 2000). See generally Timothy D. Wilson & Daniel T. Gilbert, Affective Forecasting, in 35 ADVANCES IN EXPERIMENTAL SOCIAL PSYCHOLOGY 345 (Mark P. Zanna ed., 2003) (analyzing people’s inability to predict their own feelings). 11.. Daniel Kahneman et al., Back to Bentham? Explorations of Experienced Utility, 112 Q.J. ECON. 375, 379–88 (1997) (distinguishing betwee “experienced utility” and “decisional utility”). 12 See Frank, supra note. 
We do not know whether for society as a whole, borrowing is more properly described as insufficient or as excessive. Even for particular people, it is difficult to know whether a particular level of borrowing is optimal. And for behavior that unambiguously qualifies as excessive, it is hard to uncover the role of each of these variables. At least in theory, however, these forms of bounded rationality are likely to produce significant problems for some consumers. Optimistic observers might well contend that borrowing is generally rational, and perhaps they are right. But the fact that the average American household has an average credit card debt of $6500 is at least suggestive,13 and no one doubts that credit card debt is closely associated with bankruptcy filings by consumers.14 More generally, a great deal of evidence about the credit card market throws the optimistic view into some doubt.15 
It is certainly plausible to distinguish between excessive and optimal borrowing. It is even plausible to suggest that market pressures will lead companies to appeal to the human tendency, grounded in the factors just outlined, to borrow excessively.16 Suppose, purely for purposes of argument, that excessive borrowing occurs at a significant rate, and that it causes individual and social harm. What is the appropriate solution? Is there anything that law might do to help? 
Let us begin by distinguishing between strong and weak paternalism. As I understand it here, strong paternalism forecloses choice, typically on the ground that all or most people will choose unwisely. Mandatory seat belt laws and bans on the use of cocaine and heroin can be understood as strongly paternalistic (though third-party effects are relevant as well). In the context of excessive borrowing, an appreciation of bounded rationality might well spur proposals for strong paternalism, on the theory that people will make choices that undermine their own well-being.17 If borrowing is likely to be 13 See Bar-Gill, supra note, at 1384. 14 Id. at 1385-86. 15 See Bar-Gill, supra note. 16 Id. 17 Id. boundedly rational, some such proposals might seem to make a great deal of sense on certain empirical assumptions.18 
In general, there are three problems with such proposals. The first involves individual heterogeneity. Even if many borrowers suffer from bounded rationality, others do not; and it is unfortunate if government is punishing the latter group to help the former. Indeed, any governmental help for those who are boundedly rational may remove a desirable incentive to learn over time. 
In addition, what seems to be bounded rationality may simply involve idiosyncratic tastes. If some people are eating a great deal of ice cream, and gaining a lot of weight as a result, this may be because they greatly enjoy ice cream. No problem of bounded rationality need be involved. If some people are refusing to exercise, it may be because they really dislike exercise and because the health gains from exercise, even over a lifetime, do not justify the costs. Some apparently excessive borrowers may know that they will be able to pay back their loans as a result of growing income in future years; for them, it is worthwhile to pay high interest rates now in return for the option to consume a great deal immediately. Perhaps they are borrowing in order to provide good opportunities for their young children; perhaps they will be in a better position to eliminate their debt when their children are older. Perhaps they are borrowing in the reasonable expectation that their earnings will increase in the future, when they will have less time to enjoy themselves. 
The second problem involves the risk of government error. No less than ordinary people, government officials are subject to various forms of bounded rationality, including myopia, cumulative cost neglect, and unrealistic optimism.19 Worse still, public officials are subject to parochial pressures, including interest-group power, that can greatly distort their judgments. A government that indulges in strong paternalism might make erroneous decisions as a result of its own blunders and the efforts of groups with a strong state in the outcome. In the domain of borrowing behavior, it is easy to imagine 18 Id. 19 See the discussion of “behavioral bureaucrats” in Christine Jolls, Cass R. Sunstein & Richard Thaler, A Behavioral Approach to Law and Economics, 50 STAN. L. REV. 1471 (1998). apparently public-interested restrictions that are actually an effort to promote the interests of well-organized private groups. It is even plausible to suggest that an appreciation of behavioral issues raises more, rather than fewer, concerns about the risk of government error, simply because officials are human beings too. 
The third problem has to do with the corrective potential of individual choice. If government allows people to opt out of its preferred arrangement, it will create an immediate safeguard. That safeguard can protect against governmental error and work to protect against “one-size-fits-all” solutions. It also ensures that if circumstances change over time, and if markets and individual initiative create novel opportunities and arrangements, people can take advantage of them. In the context of borrowing, markets are rapidly changing,20 and strong paternalism might be responding to a problem that is diminishing. One of the advantages of weak paternalism is that it may be technologyforcing, in the sense that it can spur innovations that respond to individual needs in ways that government may be unable to imagine. Because strong paternalism forbids private choice, it may freeze a solution that, at best, works for most rather than all, and that might cease to work for most as time passes. 
For all of these reasons, there should be a firm presumption against strong paternalism. And in fact a great deal of recent attention has been focused on forms of paternalism that steer boundedly rational people in directions that will promote their own well-being. Three ideas have received particular attention. 
“asymmetrical paternalism,” in the form of interventions that promise to deliver significant benefits to those who do suffer from bounded rationality, without imposing significant costs on those who do not so suffer.21 A core example is a “cooling off” period. A waiting period for certain decisions can protect people facing self-control problems without much harming people who do not face those problems. If people are asked to wait for a certain period before buying encyclopedias, getting married, or 20 See Brown, supra note. 21 Colin Camerer, et al, Regulation for Conservatives: Behavioral Economics and the Case for “Asymmetric Paternalism,” 151 U Pa L Rev 1211 (2003). becoming divorced, many may be helped while few will be hurt, and those who are hurt are unlikely to be hurt much. So too with information disclosure, which should protect those who suffer from certain cognitive biases without injuring consumers who do not need any such information. The central goal of asymmetrical paternalism is to develop modest initiatives that serve to correct individual errors without adversely affecting those who do not err. 
in the form of approaches that steer people in welfare-promoting directions while also allowing them to do as they wish.22 An approach is both libertarian and paternalistic if it retains freedom of choice while also leading people to make decisions that will improve their well-being. Defining examples are private and public default rules, based on a sensible view about the proper course of action; consider automatic enrollment plans for savings, which lead to dramatic increases in savings rates.23 Or consider the Save More Tomorrow (SMarT) plan, by which some employers have provided their employees with a novel option: Allocate a portion of future wage increases to savings. Employees who choose this plan are free to opt out at any time. A large number of employees have agreed to try the plan, and only a few have opted out. The result has been significant increases in savings rates.24 
There is a large overlap between asymmetrical paternalism and libertarian paternalism. The reason is that interventions that are choice-preserving (and hence libertarian) are generally asymmetrical, because they are not likely to impose significant costs on people who do not suffer from bounded rationality. But the two concepts are not the same. It is possible to imagine a form of paternalism that is libertarian but not asymmetrical—as, for example, in a default rule that does not help people who are boundedly rational, but that leads to an outcome that would protect some segment of the population. (A default rule in which 0% of wages go to savings certainly qualifies as 22 See Cass R. Sunstein and Richard A. Thaler, Libertarian Paternalism Is Not An Oxymoron, 70 U Chi L Rev 1159 (2003). 23 See James J. Choi, et al, Defined Contribution Pensions: Plan Rules, Participant Choices, and the Path of Least Resistance, in James M. Poterba, ed, 16 Tax Policy and the Economy 67, 70 (MIT 2002); Brigitte C. Madrian and Dennis F. Shea, The Power of Suggestion: Inertia in 401(k) Participation and Savings Behavior, 116 Q J Econ 1149, 1149–50 (2001). 24 See Richard H. Thaler and Shlomo Benartzi, Save More Tomorrow: Using Behavioral Economics to Increase Employee Saving, 112 J Polit Econ S164 (2004). libertarian, and it steers employees in a particular direction, but it cannot qualify as a form of asymmetrical paternalism.) We could also imagine a form of asymmetrical paternalism that denies choice and hence does not count as libertarian; consider a ban on certain purchases that are almost never made by people who do not suffer from bounded rationality. 
to “debias people through law,”25 by taking advantage of empirical work on strategies operating to “debias” people from the effects of bounded rationality.26 Consider an approach that attempts to respond to unrealistic optimism on the part of consumers by harnessing the availability heuristic.27 With such strategies, excessive optimism is met by vivid narratives of possible harm, in a way that is meant to give people a more realistic appreciation of the risks at stake.28 Debiasing strategies are the weakest form of weak paternalism; the relevant steering operates directly on bounded rationality and allows people to act as they see fit. When debiasing strategies are used, consumers and others remain entirely free to choose. 
the ground that even if it is weak, paternalism involves a form of governmental manipulation of consumers, in a way that might violate their autonomy and produce welfare losses as well.29 Slippery slope problems might also seem especially severe in this domain. Suppose that government believes itself entitled to debias and that officials and relevant groups know exactly how to accomplish that task. If so, regulators might well be tempted to engage in a form of “mind control,” steering both preferences and choices in their preferred directions. As the history of government propaganda suggests, there can be no assurance that any debiasing will be exercised benignly. And if regulators 25 See Christine Jolls and Cass R. Sunstein, Debiasing Through Law, J. Legal Stud. (forthcoming 2006). 26 Baruch Fischhoff, Debiasing, in JUDGMENT UNDER UNCERTAINTY: HEURISTICS AND BIASES 422 (Daniel Kahneman et al. eds., 1982); Lawrence Sanna, Norbert Schwarz & Shavaun L. Stocker, When Debiasing Backfires: Accessible Content and Accessibility Experiences in Debiasing Hindsight, 28 J. EXPERIMENTAL PSYCHOL.: LEARNING, MEMORY, & COGNITION 497 (2002); and Neil D. Weinstein & William M. Klein, Resistance of Personal Risk Perceptions to Debiasing Interventions, in HEURISTICS AND BIASES: THE PSYCHOLOGY OF INTUITIVE JUDGMENT 313 (Thomas Gilovich et al. eds., 2002). 27 See Jolls and Sunstein, supra note. 28 FRANK A. SLOAN, DONALD H. TAYLOR & V. KERRY SMITH, THE SMOKING PUZZLE: INFORMATION, RISK PERCEPTION, AND CHOICE 122-23, 127, 161, 180-81 (2003). 29 See Edward Glaeser, this issue. and their allies know about the power of default rules, perhaps they will default people into options that serve parochial interests rather than the interests of those who are supposed to be benefited. It is easy to imagine, for example, that groups with a financial interest in increased savings might favor approaches that increase savings, whatever the welfare effects on workers. It is also possible to imagine interest-group maneuvering that would protect certain kinds of lenders at the expense of others. 
The simplest response to this objection is that some forms of weak paternalism are unavoidable. In many cases, weak paternalism, in the form of government steering, is inevitable, and it is utterly pointless to ask whether such steering is desirable.30 For example, any legal system must rely on default rules; “every policy must have a noaction default, and defaults impose physical, cognitive, and . . . emotional costs on those who must change their status.”31 Default rules specify, among other things, what happens when parties have not agreed on the time of performance, or when employers and employees have not specified whether employment is at will or for cause, or whether employees are entitled to vacation time and to be free from discrimination on the basis of age. In the world of borrowing, no less than in the world of employment, default terms are pervasive; they identify the background for bargaining by saying who must obtain what sort of agreement from whom. This background inevitably affects preferences and choices. This is the sense in which government steering, and in that sense weak paternalism, is unavoidable. 
Those who object to weak paternalism sometimes speak if government can be absent—as if the default terms that set the background come from nature or from the sky. This is a major confusion. To be sure, it is possible that the default terms that now apply in any particular context are generally best, in the sense that they promote the interests of the parties on net. But that view must be defended, not asserted. The central point is that whether or the default terms are desirable, they help to shape preferences and choices, and in that sense reflect a form of weak paternalism. 30 See Sunstein and Thaler, supra note. 31 Johnson and Goldstein, supra note, at 1339. Two qualifications are necessary here. First, it is possible to follow an approach of “coerced choosing,” by which no default rule is in place and people are forced to select a rule or policy. In many cases, however, coerced choosing is not feasible, and in many others it is not desirable. Second, the ellipses eliminate the words “in the case of [organ] donation,” where the emotional costs may be especially high; but emotional costs often attach to changes in default rules. 
Perhaps the committed antipaternalist could acknowledge this point and argue, more cautiously, that there should be a strong presumption against weak paternalism except to the extent that it is inevitable. And it is true that many forms of weak paternalism are far from inevitable. Regulators may or may not choose information campaigns, cooling off periods, and debiasing strategies. Should there be a presumption against those admittedly optional approaches? This question is hard to answer in the abstract. Suppose we believe that most of the time, boundedly rational behavior is not likely to be terribly damaging and that market forces, together with social influences and private learning, will provide a corrective. Suppose we believe that regulators are highly likely to confused or self-serving, and that their apparent efforts to correct bounded rationality will usually do more harm than good, even if the efforts at correction involve weak paternalism. If so, a presumption against weak paternalism would be justified. 
Nothing said here rules that presumption out of bounds. On imaginable empirical assumptions, it makes a great deal of sense. A recognition of bounded rationality does not by itself justify any particular form of governmental response; cognitive error on the private end might be marched or even exceeded by cognitive (and other) failure by public institutions. Perhaps this is so of boundedly rational borrowing; recall that I have not attempted to show that the problem of excessive borrowing is serious enough to justify a governmental response of any kind. But it is also plausible to think that in many contexts, bounded rationality causes significant harm, that the market does not provide an adequate corrective, and that modest regulatory interventions can make people much better off. Let us shift, then, from more general questions to some concrete possibilities. 
Those inclined to weak paternalism might consider three approaches to excessive borrowing. 
The first response would be purely informational—the kind of approach taken by the Truth in Lending Act. Here the goal would be to ensure that borrowers know what they are doing. If important facts seem hidden, buried in fine print, or unintelligible, regulators might take corrective steps. The most obvious example is to require clear disclosure of interest rates. Suppose market and regulatory pressures now work well, so that consumers generally know about rates and so that competition is actively working to lower rates.32 If so, perhaps it is time to consider informational approaches for late fees, a significant source of revenue and perhaps a product of bounded rationality for many consumers. Whatever the target of disclosure, the advantage of this approach is that it is unlikely to impose any real costs on those who seek to borrow, while at the same time producing real benefits to those who might borrow excessively. 
There are, however, serious problems with informational approaches. Most fundamentally, they do not adequately come to terms with bounded rationality. If borrowers are both myopic and excessively optimistic, there is a serious risk that purely informational responses will do little or nothing.33 If consumers are suffering from cumulative cost neglect, then most disclosure strategies will not work—unless, perhaps, they explicitly focus people on cumulative costs, in which case disclosure is sliding into a form of debiasing. The general lesson is that the strategy of “provide more information,” favored on standard economic grounds, should be helpful when people merely lack knowledge; but as a response to biases and self-control problems, it is most likely to be inadequate. 
In fact the problem is worse still. When government attempts to “provide more information,” it has to engage in some kind of framing. Because of bounded rationality, some frames will have more of an impact than others. For those who suffer from serious forms of bounded rationality, steps like those in the Truth in Lending Act may well do little good. Of course the key questions here are empirical ones; it would be extremely valuable to have a sense of the effects of the Truth in Lending Act. It is possible that the effects are small. Whether or not they are small, they might be sufficient. Perhaps more self-conscious efforts to inform consumers would have desirable effects. But unless the problem really is a simple lack of information, there is little reason for much confidence on that count. 32 See Brown, supra note. 33 See Bar-Gill, supra note. 
A second possibility would involve debiasing. Here government would be highly alert to the psychological mechanisms that create a risk of excessive borrowing, and it would take steps specifically designed to counteract those risks. Consider the parallel domain of smoking behavior, where a great deal of work has been devoted to debiasing.34 Of course the argument for debiasing is far stronger for smoking; borrowing is most unlikely to kill people. But for those who borrow excessively, the underlying mechanisms are not unrelated to those that account for excessive smoking.35 Optimism bias, cumulative cost neglect, and myopia play a role in both settings. It might therefore be productive to use the “smoking model” far more generally, drawing on that model in any effort to develop debiasing strategies to combat the risk of excessive borrowing. 
An obvious approach would enlist salience and availability in the debiasing effort. It is well-established that in thinking about risks, people rely on certain heuristics, or rules of thumb, which serve to simplify their inquiry.36 Heuristics typically work through a process of “attribute substitution,” in which people answer a hard question by substituting an easier one.37 When people use the availability heuristic, they assess the magnitude of risks by asking whether examples can readily come to mind.38 If people can easily think of such examples, they are far more likely to be affected than if they cannot. For example, “a class whose instances are easily retrieved will appear more numerous than a class of equal frequency whose instances are less retrievable.”39 
This is a point about how familiarity can affect the availability of instances. A risk that is familiar, like those associated with guns, will be seen as more serious than a risk that is less familiar, like those associated with sun-bathing.40 But salience is important as 34 See Frank A. Sloan, Donald H. Taylor, & V. Kerry Smith, The Smoking Puzzle: Information, Risk Perception, and Choice 122-23, 127, 161 (2003). 35 An obvious exception is that smoking can be physically addictive; for most excessive borrowers, the idea of addiction is only a metaphor. 36 See Daniel Kahneman, Paul Slovic, & Amos Tversky, Judgment Under Uncertainty: Heuristics and Biases (1982). 37 See Daniel Kahneman & Shane Frederick, Representativeness Revisited: Attribute Substitution in Intuitive Judgment 49, 53 in Heuristics and Biases: The Psychology of Intuitive Judgment, Thomas Gilovich, Dale Griffin, & Daniel Kahneman, eds. (Cambridge: Cambridge Univ. Press, 2002). 38 See Amos Tversky & Daniel Kahneman, Judgment Under Uncertainty: Heuristics and Biases, in Judgement under Uncertainty: Heuristics and Biases 3, 11-14 (Daniel Kahneman et al. eds 1982). 39 Id. at 11. 40 See Steven Levitt, Freakonomics 150-52 (2005). well. “For example, the impact of seeing a house burning on the subjective probability of such accidents is probably greater than the impact of reading about a fire in the local paper.”41 Salience is highly related to vividness. A vivid example often does much more than statistical information is ensuring that people attend to potential risks.42 
How might these points be used in a debiasing campaign to counteract excessive borrowing? We could imagine three possibilities. First, and best of all, credit card companies might attempt on their own to debias the most vulnerable borrowers, without any kind of governmental mandate. Either self-interested or public-interested companies might take steps in this direction. Such campaigns might involve vivid accounts, by real people, of problems created by excessive borrowing. To be especially effective, the accounts should involve brief, memorable narratives of typical behavior that has led people to serious harm. Second, government might engage in public education campaigns that are specifically designed to alert people to the risks. Such public education campaigns are likely to work best if they actually engage people’s emotions.43 A third and least possibility would involve a disclosure mandate imposed on credit card companies themselves. Such companies might be required to provide vivid warnings of the risks of excessive borrowing, perhaps accompanied by narratives of real lives that have been adversely affected by it. This is the least plausible solution, because the risks associated with borrowing have not been shown to be sufficiently large to justify a compulsory warning program on this kind. 
Would either of the first two steps be worthwhile? The question cannot be answered in an empirical vacuum. Everything depends on the magnitude of the problem and the effects of debiasing strategies in reducing it. All that can be said is that on the basis of what is now known, such strategies might well do some good and are most unlikely to produce significant harm. 
Libertarian paternalists are especially interested in the use of default rules to move behavior in welfare-promoting directions; the creative use of default rules is central to the 41 Id. 42 See Lee Ross and Richard Nisbett, Human Inference 43-53 (1980). 43 See Loewenstein, this issue. project of libertarian paternalism.44 Such rules can have exceptionally powerful effects on choices, behavior, and outcomes.45 I have already referred to the effects of automatic enrollment plans in increasing savings in the work place. Consider a few other examples. a. For insurance, a natural experiment demonstrated the potential stickiness of default rules.46 Pennsylvania offered a default program for drivers containing a full right to sue and a relatively high premium; purchasers could elect to switch to a new plan by “selling” the more ample right to sue and paying a lower premium. By contrast, New Jersey created a system in which the default plan included a relatively low premium and no right to sue; purchasers were allowed to deviate from the default program and to purchase the right to sue by choosing a program with that right and also a higher premium. In both cases, the default rule tended to stick. A strong majority accepted the default rule in both states, with only about 20 percent of New Jersey drivers acquiring the full right to sue, and 75 percent of Pennsylvanians retaining that right.47 There is no reason to think that there was a systematic difference between the preferences of citizens of the two states. b. In the United States, those who want their organs to be available for others must affirmatively say so, usually through an explicit notation on their drivers’ licenses. But in many other nations—Austria, Belgium, Denmark, Finland, France, Italy, Luxembourg, Norway, Singapore, Slovenia, and Spain—people are presumed to consent to allow their organs to be used, after death, for the benefit of others; they are permitted to overcome the presumption, usually through an explicit notation to that effect on their drivers’ licenses.48 Johnson and Goldstein find that with respect to organ donation, people lack stable preferences and their decisions are very much influenced by the default rule.49 Similarly, a controlled online experiment showed a substantial effect from the default rule: The opt-in system created a 42 percent consent rate, about half of the 82 percent rate for an 44 See Sunstein and Thaler, supra note. 45 See Ian Ayres, this issue. 46 See Camerer, Prospect Theory in the Wild, in Heuristics and Biases: The Psychology of Intuitive Judgment 294–95; Johnson, et al, Framing, Probability Distortions, Insurance Decisions, in id at 238. 47 See Johnson, et al, Framing, Probability Distortions, Insurance Decisions, supra note, at 238. 48 See http://www.presumedconsent.org/solutions.htm. 49 See Eric Johnson and Daniel Goldstein, Do Defaults Save Lives?, 302 Science 1338 (2004). opt-out system.50 The real-world evidence is even more dramatic. Presumed consent nations show consent rates ranging from a low of 85.9 percent (Sweden) to a high of 100 percent (Austria), with a median of 99 percent.51 The default also produces a significant, though less dramatic, increase in actual donations, meaning that many lives are saved as a result of the presumed consent system.52 c. A substantial effect from the legal default rule was found in an experimental study of law student reactions to different state law provisions governing vacation time from firms.53 The study involved two conditions. In the first, state law guaranteed two weeks of vacation time, and law students were asked to state their median willingness to pay (in reduced salary) for two extra weeks of vacation. In the second condition, state law provided a mandatory, non-waivable two-week vacation guarantee, but it also provided employees (including associates at law firms) with the right to two additional weeks of vacation, a right that could be “knowingly and voluntarily waived.” Students were asked how much employers would have to pay them to give up their right to the two extra weeks. All by itself, the switch in the default rule more than doubled the students’ responses, raising the median amount from $6000 to $13,000. 
There are several possible explanations of the effects of default rules in these and other cases. Inertia undoubtedly plays a role.54 Action is necessary to change a default rule, and inertia therefore favors such rules. The default rule also conveys information by virtue of its status as such.55 Perhaps the state, or an employer, has chosen a default rule for a reason, and if so, people should stick with it unless they have a reason of their own to depart. Finally, the default rule may create an endowment effect,56 leading people to favor the existing allocation of entitlements; for that reason the rule will tend to stick. 50 Id. 51 Id. 52 Id. Johnson and Goldstein estimate that switching to an opt-out system increases organs actually used by 16 percent, holding everything else constant. 53 See Cass R. Sunstein, Switching the Default Rule, 77 NYU L Rev 106, 113-14 (2002). 54 Brigitte C. Madrian and Dennis F. Shea, The Power of Suggestion: Inertia in 401(k) Participation and Savings Behavior, 116 Q J Econ 1149, 1149-50 (2001). 55 Id. at 1160-61. 56 See, e.g., Russell Korobkin, The Endowment Effect and Legal Analysis, 97 NW L Rev 1227 (2003). 
How might these ideas be enlisted to reduce the risk of excessive borrowing? Here is a simple inspiration for seriously investigating that question: Many people who are purchasing goods and services with credit cards would do much better if they used debit cards instead. With debit cards, they would avoid high interest rates and late fees. For many consumers, the use of credit cards is a product of habit and inertia, rather than of any kind of reflective choice against the use of debit cards. Significant charges result from simple procrastination. On this view, a narrow question is how to ensure optimal decisions, in terms of individual welfare, in the choice between credit cards and debit cards. Fortunately, there is evidence that many consumers are shifting from credit cards to debit cards,57 perhaps in order to avoid the effects of their own bounded rationality. But it is reasonable to think that the shifts have been occurring more rarely than they should if consumer welfare is our goal. 
Suppose that many people who are using credit cars would do better to use debit cards instead. If so, it would be valuable to make debit cards a kind of default option, to be used unless borrowers specifically decide otherwise. Credit card companies might take, or be encouraged to take, steps in this direction on their own. They might say, for example, that unless consumers specifically say otherwise, payments will be automatically deducted every month from a specified checking account. The “opt out” might done at the time of initial application for the card; perhaps better, it might be required on an annual, biannual, or monthly basis. Alternatively, regulators might encourage or require credit card companies to make automatic deduction an option for new credit card owners, saying that at the time of initial acquisition of the card, consumers must be given the opportunity to use automatic deduction, with a salient notation to this effect. Somewhat more aggressively, regulators might require companies to make automatic deduction the default option, subject to override either at the point of purchase or on an annual, biannual, or even monthly basis. 
Here is another possibility. We have seen that with the Save More Tomorrow plan, workers can be encouraged to produce significant increases in savings rates.58 It is 57 See Brown, supra note. 58 See Richard H. Thaler and Shlomo Benartzi, supra note. easy to imagine a parallel plan, Borrow Less Tomorrow (BLT).59 Private institutions could assist consumers who have borrowed excessively by helping them to pay down their debt, perhaps by encouraging them to enter into agreements to ensure that the level of debt would be lower each month than it was before. Credit card companies could themselves help certain customers in this way—not by mandating participation, but by encouraging people to participate if they choose. Perhaps governments could consider incentives to lead the private sector to provide this kind of assistance. In fact a BLT plan might be combined with the idea of automatic enrollment, ensuring that for expenditures above a specified limit, payments would come out of existing checking accounts, rather than being used to increase debt. 
Whether any kind of weak paternalism makes sense in this context cannot be decided in the abstract. It is necessary to know the magnitude of the problem of excessive borrowing and the likely effectiveness of any weakly paternalistic response. My own tentative judgment is that an information campaign, encouraging the use of automatic payment from existing accounts, would probably do some good and little harm. 
If boundedly rational borrowing is pervasive, and if consumer choices are frequently impairing consumers’ welfare, it is possible that more intrusive government responses are desirable. Suppose that myopia, procrastination, cumulative cost neglect, and excessive optimism are leading many people to borrow money on behalf of purchases that produce little short-term gain but significant long-term harm. If so, prohibitions on voluntary agreements might be justified, at least if the aggregate benefits exceed the aggregate costs. When such agreements can be shown to injure the very people who enter into them, the firm presumption against strong paternalism might be overcome; and bounded rationality is often the source of the relevant injury. In the context of the credit card market, there is an important supplemental point. The very structure of that market appears to lead many companies to appeal to bounded rationality, rather than to attempt to counteract it.60 As with state lotteries, where advertising 59 I am grateful to Matthew Rabin for the acronym. 60 Bar-Gill, supra note. campaigns appeal to unrealistic optimism and probability neglect, so too, plausibly, for borrowing: Some companies encourage people to obtain cards with the hope that many of them will procrastinate and pay significant late fees and interest charges. 
We could imagine a continuum of responses. The most modest would single out particular features of agreements that are peculiarly likely to reflect bounded rationality, and to operate to harm consumers ex post. A possible candidate is the “teaser rate,” by which consumers are given an opportunity to use credit cards at a low rate—sometimes a zero interest rate—until the expiration of the introductory period. The teaser rate operates to tempt consumers to begin to use credit cards, apparently with insufficient awareness of the effects, over time, on their welfare. Many consumers who are excited about teaser rates do not sufficiently appreciate the small size of the actual gain. As Oren Bar-Gill explains, “The teaser strategy works. Despite the fact that most borrowing is done at the high post-promotion rates, consumers appear to be extremely sensitive to teaser rates.”61 
Suppose that teaser rates can be shown to lead boundedly rational people to become credit card holders, in a way that produces aggregate harm. If so, the argument for banning such rates is not entirely implausible. Unfortunately, a ban would impose costs on many borrowers, above all those who rationally opt for the teaser rates (either because they will switch cards after the expiration of the relevant period or because they are net gainers from the package they receive). Indeed, a ban on teaser rates would be not only strongly paternalistic but also asymmetrical in an important and unattractive sense: It would harm the sophisticated consumers who take advantage of them. Here as elsewhere, weak paternalism should be the first line of defense. But if a ban on teaser rates would protect people against excessive borrowing while also imposing modest costs on those who benefit from them, it might be worth considering. I do not, however, believe that the argument for such a ban has yet been made out. 
A far more aggressive approach would be to enact usury laws. Such laws are price controls, and most of the time, the argument against price controls is devastatingly powerful: They create scarcity and typically harm the very people they are designed to help, by depriving them of access to a good that they want. In this context, an obvious danger with price controls is that they will make it impossible for (rational) people to 61 Id at 1393. obtain loans—perhaps injuring those people who most need those loans, and who are in the worst position to obtain needed resources through other routes. But the argument for usury laws is strengthened by identifiable features of the credit card market.62 It is exceptionally easy to obtain a credit card. Companies make much of their money from high interest rates and late charges. As Bar-Gill has explained, market pressures give companies a strong incentive to take advantage of bounded rationality.63 In the abstract, usury laws would appear to be a well-designed response. A ban on interest rates above a certain level would counteract the tendency to exploit bounded rationality by restructuring the system of pricing in the direction of annual fees—precisely the structure that would plausibly emerge if myopia, cumulative cost neglect, and unrealistic optimism were not involved. 
On the other hand, usury restrictions would impose serious costs. It is possible to imagine that many fully rational people benefit from a situation in which annual rates are low and in which interest rates are high. This is obviously so if they do not maintain debt and hence pay no interest. Such people use credit cards essentially as debit cards; they lose nothing by the current arrangement and benefit from the existence of a kind of subsidy by card users who pay significant debt and large late fees. To be sure, there is a serious problem with this system, because it involves a perverse system of redistribution, from relatively wealthy people who pay on time to less wealthy people who maintain debt. But it is also the case that some people, some of the time, are better off with low annual fees and high interest than with higher fees and lower interest—at least if they have a reasonable expectation of growing income over time. 
One question is the size of this population, and the extent to which its members are behaving rationally rather than in a way that is distorted by cognitive and motivational problems. This is an empirical question that cannot be resolved by abstractions. But it is reasonable to speculate that if usury laws provided a reallocation of consumer costs toward higher annual fees, the principal beneficiaries would be people who suffer from bounded rationality and hence from high consumer debt. But there is a strong response to the argument for usury laws: Recent evidence suggests that there is 62 Id. 63 See Bar-Gill, supra note, at 1422-23. intense competition over interest rates in the credit card market.64 Because such competition has been occurring, a governmental response does not appear to be necessary or even desirable. 
Another form of strong paternalism would target the particular payment provisions that most harm those with bounded rationality, such as late fees. It is plausible to think that such fees are incurred precisely by those who most suffer from the problems catalogued here, including procrastination, myopia, and excessive optimism. If this is so, it might be worthwhile to consider restrictions on the magnitude of late fees, at least if the consequence of such restrictions would be to shift from fee payments to higher upfront charges. Such a shift would have the desirable effect of ensuring that payment practices would not target those suffering most severely from bounded rationality. 
Many people borrow excessively, whether excessiveness is measured ex ante or ex post. I have emphasized here the importance of the ex post perspective, simply because the most serious problems arise when borrowing behavior produces serious economic distress. A number of psychological mechanisms are likely to contribute to excessive borrowing, including procrastination, myopia, cumulative cost neglect, unrealistic optimism, and miswanting. These mechanisms help to illuminate many areas in which bounded rationality leads people to decisions that impair their welfare. Hence an analysis of boundedly rational borrowing should apply to other contexts in which more serious problems result from bounded rationality. 
In exploring legal responses to problems of this sort, we should distinguish between weak and strong paternalism. Here as elsewhere, regulators ought to adopt a firm presumption in favor of weak paternalism and freedom of choice. Unfortunately, informational approaches are unlikely to provide a great deal of help, simply because they do not respond to bounded rationality. Debiasing is far more promising; if government enlists salience and availability in the interest of reducing excessive borrowing, it is unlikely to impose significant costs and it might produce real gains. It seems clear that many people who use credit cards would do better to use debit cards 64 See Brown, supra note. instead. One possibility is that private and public institutions should alter existing default rules so as to move closer to the preferred state of affairs. 
Strong paternalism—in the form of restrictions on particular practices or more general usury laws—is usually to be avoided, partly because it is likely to introduce inefficiencies, and partly because it is likely to hurt many of the people that it is intended to help. Notwithstanding the peculiar nature of the credit card market, I do not believe that existing evidence provides adequate grounds for strong paternalism. Here, as elsewhere, it is best to begin, and probably to end, with weak paternalism of the kind that I have outlined here. 
University of Chicago Law School 1111 East 60th Street Chicago, IL 60637 csunstei@uchicago.edu    1.  253.    
Celebrating God, Constitutionally CASS R. SUNSTEIN* 
In recent years, the Supreme Court has often been confronted with "ceremonial deism." As I understand the term here, ceremonial deism is categorized by the following features. First, it is not coercive; no one is being required to do or to say anything. Second, it involves public displays that refer generally to God, without choosing any particular conception of God. Third, it involves activities that are either specifically honored by tradition or essentially indistinguishable from activities that are specifically honored by tradition. 
Ceremonial deism has a core and a periphery.' The core might be said to include the use of the words "In God We Trust" on currency and as the national motto, legislative prayers, public oaths that refer to God and that use the Bible, and the use of the phrase "God Save This Honorable Court" to begin judicial proceedings. The periphery includes the words "under God" in the Pledge of Allegiance, prayers at public university ceremonies, and displays of religious symbols on public property. All I mean by ceremonial deism, then, is non-coercive public displays that refer to God in the way that is time honored and fits with our traditions. 
That's the topic. In these remarks, I have two goals. The first is to explore how the constitutional law might evaluate ceremonial deism. In the first part of these remarks, I will try to illuminate current constitutional debate by attempting to identify the approaches that are behind the current controversy. 
The second goal is to argue that federal courts, including the Supreme Court, should generally permit ceremonial deism. This means that the Supreme Court should get out of the business of attacking references to God, if they are in fact vindicated by tradition or if they are hard to distinguish, in principle, from those references that are vindicated by tradition. The underlying idea is that if a practice is longstanding and if it does not offend existing decisions, then courts should impose a heavy burden on those who strike down the practices. Here is an effort to free up other institutions, including state government, from the heavy hand of * Karl N. Llewellyn Distinguished Service Professor, Law School and Department of Political Science, University of Chicago. This essay is a lightly revised version of the transcript of the McElroy Lecture, delivered at the University of Detroit Mercy School of Law, on March 7, 2006. The author would like to thank members of the community of the law school for their extraordinary kindness and generosity on that occasion. Readers are asked to make allowances for an essay that draws on an oral presentation. 
COLUM. L. REv. 2083 (1996). constitutional law insofar as those institutions are engaging in ceremonial deism. The harm done by ceremonial deism, if there is any, is purely symbolic. It involves feelings rather than material effects. Note in this regard that we have a diverse country, most of whose citizens are religious believers. Public inferences to God should not be struck down by federal courts in light of the kind of diversity we have. 
That's the basic argument. What I'm going to do now is to begin by exploring theories of constitutional interpretation first. After that, I will explore ceremonial deism in particular. The theories are going to be a bit abstract and I will be exploring four of them. Each has been with us for about 200 years now, and they often have been in the background. We ought to bring them to the foreground.2 
The first approach to constitutional interpretation might be called bipartisan restraint. It is associated with Oliver Wendell Holmes, who practiced it, and to a large extent Chief Justice John Marshall, who usually practiced it also. There is no practitioner of bipartisan restraint on the current Supreme Court - though members of the Senate Judiciary Committee have shown some interest in bipartisan restraint at least for the last year. 
What bipartisan means is that the Court should strike laws down only if the violation of the Constitution is clear and unambiguous. People who believe in bipartisan restraint emphasize that the Court is composed not by sages but by lawyers. The Court consists of people who are educated in the law. Advocates of bipartisan restraint think that the Constitution is ambiguous, as it certainly is, and they insist that laws should not be struck down unless they're unambiguously in violation of the Constitution of the United States. 
At the turn of the century a law professor named James Bradley Thayer argued for bipartisan restraint. 3 Thayer argued that because the American Constitution is often ambiguous, those who decide on its meaning must inevitably exercise discretion. Laws that "will seem unconstitutional to one man, or body of men, may reasonably not seem so to another; ... the constitution often admits of different interpretations;... there is often a range of choice and judgment."4 In Thayer's view, "whatever choice is rational is constitutional. 5 Thayer's argument, in brief, was that courts should strike down laws only "when those who have the right to make laws have not merely made a mistake, but have made a RADICALS IN ROBES: WHY EXTREME RIGHT-WING JUDGES ARE WRONG FOR AMERICA (Basic Books 2005). 
ConstitutionalLaw, 7 HARV. L. REv. 129 (1893). 
very clear one-so clear that it is not open to rational question. ''6 The question for courts "is not one of the mere and simple preponderance of reasons for or agai7nst, but of what is very plain and clear, clear beyond a reasonable doubt.", 
I said that Oliver Wendell Holmes was a practitioner of bipartisan restraint. As he once said, "If my fellow citizens want to go to Hell, I'll help them. It's my job."8 Holmes has no followers on the current Court; there's no one who is consistently willing to uphold legislation, including in the area of separation of church and state, against constitutional attack. So that's one approach and some of my remarks will be an effort to cover it a little bit. 
The second approach that has got a lot of attention in recent years: originalism. Originalists believe, not that the Supreme Court should uphold every law that is not clearly unconstitutional, but instead that the Court should go in a time machine. The Court should recover the original understanding of the document. Hence originalists insist that the Constitution means what it meant at the time it was ratified. For evaluating the idea of ceremonial deism, or separation of church and State, the task is to recover the original understanding of those who gave us the Constitution of the United States. This might be said to be a "time travel" conception of the constitutional law. 
There's a lot to be said on behalf of originalism. It attempts to vindicate the rule of law by reducing the discretion of federal judges. It insists that, instead of allowing federal judges to make their own decisions about what religious freedom requires in a diverse society, judges ought to figure out what We the People thought was meant by the Establishment Clause at the time the Constitution was ratified. Here there is a democratic, as well as a rule of law reason, for originalism. 
But there are two large problems with the originalist project. The first is it may be somewhat contradictory. If the original understanding is to be binding, it must be because the original understanding was that the original understanding would be binding. If the original understanding was not that the original understanding would be binding, then the originalist project defeats itself. For the idea of this theory to work, it has to be a case that the people who gave us the free exercise clause and the establishment clause and other clauses of the Constitution specifically meant their specific understandings to bind prosperity. 

HOLMES-LASKI LETTERS: THE CORRESPONDENCE OF MR. JUSTICE HOLMES AND HAROLD J. LASKI, 1916-1935, 248-49 (Mark DeWolfe Howe ed., 1953). 
UNIVERSITY OFDETROIT MERCY LA W REVIEW 
Unfortunately, there's a lot of reason historically to think they did not mean to entrench their specific views. 9 In that case, the originalist approach defeats itself, because the approach was not that of those who originally ratified the Constitution. 
There is a further problem. The originalist approach for the Constitution would cause serious problems for our system of rights and with our present institutions. I'll give you just one example from the area I'm focusing on in this set of remarks. Justice Thomas has argued that the separation of church and state does not apply to state government.'0 It follows, from this view, that if people wanted to establish a state church for California or Massachusetts or Michigan that would be fine as far as the national Constitution is concerned. And if the state wants to favor a particular religion through funds or through any other approach, the state government runs into no constitutional problem because the Establishment Clause doesn't apply to state government-only to national government. 
The real problem with Thomas' approach, in my view, is he's probably right on history. It may well be a stretch to say that, as a matter of history, the establishment clause is incorporated in the Fourteenth Amendment according to the original understanding. He may well have the history right, I think. But if his approach were accepted, we would so alter our current understandings at the level of state government as to make our system essentially unrecognizable. If we were to unleash religious struggle on state legislature, unconstrained by the Establishment Clause, then we would turn the United States into a very different country than it's been for fifty years or more. 
So my suggestion with respect to originalism is, first, that it may embed a theory of interpretation that is not the theory of the original ratifiers of the Constitution. And second, it would transform our document in ways that would alter radically and for the worse the system of government by which we have long lived. We have now explored two theories of interpretation, bipartisan restraint by Justice Holmes on the one hand, originalism of Justice Thomas on the other. 
Now let me turn to the third approach which I'm going to call perfectionism. This is an approach that has played a dominant role in the area of religion in the last forty or fifty years. We might treat William Brennan as the leading perfectionist in the last half-century. The perfectionists believe that the Constitution sets out strong aspirations for freedom in general and for religious freedom in particular." Under their view, the particular meaning of these aspirations is transformed from one REv. 885 (1985). 
denied, 542 U.S. 961 (2004). 
(Belknap Press 2006). generation to another. Justice Breyer has defended a kind of perfectionist approach to the Constitution, saying the document is not static and it changes over time.12 Certainly Americans have learned more about how to live with one another, and as we do, perhaps our understanding of the requirements of religious liberty are being altered. 
Many of the dominant ideas for last thirty years or so in the area of religious liberty hold their source to this approach to constitutional interpretation. For example, the wall of separation between church and state is a metaphor that is the result of the perfectionist approach, trying to make our system of religious liberty as good as it possibly can be. (I do not mean to endorse the metaphor, which seems to me misleading, and damaging; I mean simply to note that it is a perfectionist idea.) 
Also perfectionist is the idea of "neutrality" between religion and nonreligion or neutrality across religious. That idea is less dangerous and damaging-indeed it has considerable appeal-but it has an unmistakable perfectionist feature. The ratifiers of the First Amendment did not believe in neutrality in a strong sense. The notion of neutrality between religion and nonreligion and across religions is more a development of the last fifty years of than in the founding period. 
In my view, there is a lot of nobility to the perfectionist approach to the Constitution, but let me outline some problems with it. The first is if our judges are trying to perfect our constitutional ideas by trying to figure out what free exercise means or respecting what establishment means, they may blunder. Judges have no special access to moral or political truth. If judges are trying to figure out how best to understand our constitutional aspirations, it is possible that they will make our constitutional system less perfect and not more so. If judges have no privileged access to political or moral truth, then to entrust them with the task of putting our aspirations in the best light is risky just because of the inevitable fallibility of judges. Even if judges were not fallible in this way, democracy and self government have their claims, do they not? Perfectionism threatens to unsettle our ability to rule ourselves. If we entrust our judges with the task of figuring out what free exercise of religion means or what respecting establishment of religion really means, we might compromise selfgovernment. If we entrust judges with the task of figuring out our deepest aspirations, we might be asking them to do something better done by our citizens and ordinary people, not just judges. 
The fourth approach, which is the final one, would be written in small letters. It's call minimalism. Minimalists believe in little constitutional law rather than in big heroic constitutional law. They believe that constitutional law should be little in the two different ways. First, suppose that the Supreme Court is confronting a problem like the use of the word, Under God, in the Pledge of Allegiance. Minimalists want judges to take a 
small step and focus on the use of the words "under God" in the Pledge of Allegiance, not a large step that resolves all sorts of questions of religious liberty and establishment. 
Hence minimalists like incremental rather than earth-shaking decisions. They prefer nudges over earthquakes. In addition, minimalists like shallowness rather than depth. That is very different from what you would want in a romantic relationship or in a teacher! In constitutional law, minimalists want shallowness for the following reason: They think that theoretical depth, in the constitutional domain, forces judges to take stands on issues that most deeply divide us. They want to avoid that. They like shallowness in the form of rulings and rationale that we can all agree on, notwithstanding our diversity and our plurality. So by shallowness, I mean an outcome and an argument that we can accept notwithstanding our disagreements on foundational issues. For example, people can agree right now in the United States that government cannot censor speech unless there is a clear and present danger. And we agree on that despite our disagreement on theological issues and political issues. Across many differences, we support freedom of speech against censorship. What minimalists want to do is to achieve a big goal for a diverse nation, which is to make it possible for people to agree where agreement is necessary, while also making it unnecessary for people to agree where agreement is not possible. 
Justice O'Connor, recent retiree from the Supreme Court, was a practitioner of m-inimalism.13 There are two objections to the minimalist approach that one might make. One is that it is unpredictable. Justice O'Connor could not easily be pigeon-holed; people didn't know how she would vote in hard cases. And so the narrowness that she prized might be a problem for those who want to know what the law is. Narrow decisions leave a great deal of uncertainty. Shallowness is also a problem if the deep theory would be right. If there is an excellent deep theory, then we should opt for depth. But often we can't find one. And even if some people think they have the right theory, some of our fellow citizens will not agree to it, and it is respectful for our fellow citizens not to insist on that theory if we can't get them to agree to it. 
I am done with these abstract remarks about constitutional theory. Now it is time to talk about ceremonial deism in particular. In terms of the approaches I have described, it's clear that if you believe in bipartisan restraint, as Oliver Wendell Holmes, then the answer is clear: There is absolutely nothing wrong with the public ceremony that refers to God, that mentions the Ten Commandments, or that uses the Bible. Indeed, there is nothing wrong with public displays of the Ten Commandments, or with the inclusion of the words "under God" in the Pledge of Allegiance. Those who believe in bipartisan restraint find ceremonial deism easy and fine. The Constitution does not clearly or unambiguously forbid references, in public, to God. For those who believe in bipartisan restraint, these cases do not present serious challenges; judges should simply back off. 
Something similar is probably true if you are an originalist and if you accept the originalist approach of Justices Scalia and Thomas. Under that approach, ceremonial deism is constitutionally acceptable. Justice Thomas' view, if you will recall, is that the Establishment Clause does not apply to states at all. If a university or state wants to recognize the existence of God, there is no problem. Justice Scalia does not take that view but he is able to point to a great deal of history suggesting that ceremonial deism is legitimate.' 4 For originalists, as for advocates of bipartisan restraint, these questions are easy. 
Many perfectionists reach a very different conclusion. They believe that the state must be neutral as between religion and non-religion, and also neutral as between some religions other religions.1 5 They ask: What is neutral in state recognition of the existence of God? After all, millions of Americans are agnostic or atheist, and many others do not accept monotheism. For perfectionists who insist on genuine neutrality, it is clear that ceremonial deism is in constitutional trouble, hence, for example, the view that the pledge of the allegiance cannot constitutionally contain the words "under God." Under this view, it is not possible to celebrate God constitutionally. 
The minimalists are split. Some of them, such as Justice Breyer, want to go case by case,16 making fine decisions amongst different kinds of religious displays. For such minimalists, it is important to pay close attention to context, which will make some forms of ceremonial deism constitutionally valid and other forms invalid. I want to identify here the intellectual parent of minimalism and explore how his views might bear on the problem. 
I mean to refer here to the English social theorist Edmund Burke, who believed that each of us has a private stock of wisdom.17 Each person in this room has such a private stock. But what each of us knows, as an individual, is pitiful really compared to what tradition knows. This is not because the individuals who constitute traditions were smarter or wiser than each of us; it is because traditions contain the knowledge of so many-not of dozens or hundreds or thousands but of millions of 
UNIVERSITY OF DETROITMERCY LAW REVIEW individual minds. Burke rejected the revolutionary temperament because of its theoretical ambition. 18 Burke's key claim is that the "science of constructing a commonwealth, or reforming it, is, like every other experimental science, not to be taught a priori."'19 To make this argument, Burke opposes theories and abstractions, developed by individual minds, to traditions, built up by many minds over long periods. In his most vivid passage, Burke writes: 
We wished at the period of the Revolution, and do now wish, to derive all we possess as an inheritancefrom ourforefathers.... The science of government being therefore so practical in itself, and intended for such practical purposes, a matter which requires experience, and even more experience than any person can gain in his whole life, however sagacious and observing he may be, it is with infinite caution that any man ought to venture upon pulling down an edifice which has answered in any tolerable degree, for ages the common purposes of society, or on building it up again, without having models and patterns of approved utility before his eyes. 20 
Thus Burke stresses the need to rely on experience, and in particular the experience of generations; and he objects to "pulling down an edifice," a metaphor capturing the understanding of social practices as reflecting the judgments of numerous people extending over time. It is for this reason that Burke describes the "spirit of innovation" as "the result of a selfish temper and confined views," 2 1 and offers the term "prejudice" as one of enthusiastic approval, noting that "instead of casting away all our old prejudices, we cherish them to a very considerable degree., 22 Emphasizing the critical importance of stability, Burke adds a reference to "the evils of inconstancy and versatility, ten tho23usand times worse than those of obstinacy and the blindest prejudice., 
In short, Burke thought that legal structures and individual rights should be built up with careful reference to longstanding practice. In the domain of religion, this was, in many ways, Chief Justice Rehnquist's approach to constitutional questions. Chief Justice Rehnquist analyzed hard questions by reference to our traditions. Strikingly, Chief Justice Rehnquist's defense of the use of the words "under God" in the Pledge of Allegiance is an almost entirely Burkean exercise, stressing practices rather EDMUND BURKE, 416-51 (Isaac Kramnick ed. 1999) (1790). 
than reasons for practices. 24 Indeed, Chief Justice Rehnquist's view of the Establishment Clause has a persistent Burkean feature, at least insofar as he would permit public recognition of God by reference not to theories or principle, but by reference to history alone.25 
At least for ceremonial deism, I think that Chief Justice Rehnquist was right. The Burkean approach that bases constitutional law on longstanding traditions, rather than on theories and abstractions, has a lot to recommend it. 
What I've just done is to suggest that minimalists who are inspired by Burke have no problem with ceremonial deism. So too for the originalists and for those who believe in bipartisan restraint. It is only the perfectionists who have trouble. I think the Burkean minimalists are right, and I want to clarify that judgment in the remarks that remain. 
One objection of ceremonial deism is that if we allow it, we are on a slippery slope to invocation specifically of religious values in a way that would undermine rather than support our constitutional tradition of respect for diversity. I want to address that objection by building up cases now with minimalist fashion. And I'm going to start with the cases in which the argument for striking down legislation is strongest. I will end with cases in which the argument for striking down legislation is weakest. And there is going to be a specific point in which we switch from the imposition of particular religious convictions, which is not legitimate, to ceremonial deism, which is legitimate. I hope that by virtue of the specificity of the cases, the slippery slope problem will dissipate. 
First, the strongest case for invalidating legislation involves mandatory school prayer. The prohibition of school prayer has been with us for a long, long time and it's very hard when you're dealing with children to speak of ceremonial deism rather than inculcation of the state's preferred religious beliefs. I do not mean to suggest that it is entirely clear that the Supreme Court was originally right to strike down school prayer; the issues were hard. But the Court's decisions are well engrained and my defense of ceremonial deism does not support mandatory school prayers. 
A somewhat harder case but also a good one for Supreme Court invalidation was a case from a few years ago when a state legislature said that the Ten Commandments had to be posted on a wall of every public school classroom. 26 That's not the easiest case in the world but it's not so far from school prayer. This case involved children, not adults. It's not purely ceremonial; it's the educational setting. To display the Ten Commandments on every public school classroom seems to be an effort to inculcate in children a set of religious convictions, and it is not the states' business to do that. 
The third case is a little harder for an aggressive Supreme Court role. The third case is also a case from a few years ago, in which the state legislature didn't mandate any prayers, but said one minute would be set aside for voluntary prayer or meditation.27 This is harder for the Supreme Court to strike down, because there is no reference to any particular set of religious conviction. But in my view, this case did not involve ceremonial deism, for it was an effect to enlist public schools in the process of religious practice, and the Supreme Court knew that. The Court was probably right to strike the legislation down. 
Thus far, I have given three cases in which the Supreme Court properly validated the legislation; but now we're going to ceremonial deism. Last year the Supreme Court was confronted with two cases in which the Ten Commandments were put on public property. If the argument thus far is correct, the Supreme Court was right, in one of those cases, to accept the display of the Ten Commandments on public property, as part of a general celebration of our history. But the Supreme Court was wrong to strike down the posting of the Ten Commandments on the courthouse. After all, the courthouse had many other historical influences displayed, including secular ones. This was not a case in which religious symbols were the only ones. I suggest that a display of the Ten Commandments, if it is part of a series of displays that are not solely religious, is part of legitimate ceremonial deism. The state is entitled to recognize that the Ten Commandments helped to give rise to the legal system in which we now live. To recognize that role is not to commit yourself, as the state did not, to a particular set of religious convictions. 
It follows that the Supreme Court was wrong to set its face against the Ten Commandments in the courthouses as part of the historical display. Now let's consider cases that are easier for the government, and whose facts make it harder and harder for the Supreme Court to play an aggressive role. In what seems on reflection to be a surprising decision, the Court struck down a public prayer at a graduation ceremony.2 8 Why is it invalid for graduation ceremonies to include prayers, if those ceremonies contain a great deal more than prayers? The relevant ceremony did not require anyone to do anything. It did not use public funds. It did not mandate or coerce any behavior. It simply recognized the existence of religious convictions in the relevant community. From the standpoint of Burkean minimalism, the Court's decision was a blunder. 

Wallace v. Jaffree, 472 U.S. 38 (1985). 
Lee v. Weisman, 505 U.S. 577 (1992). 
Let's now pause over the Pledge of Allegiance which we began. In my view, the ceremonial deism that is involved here is not illegitimate, and the Supreme Court ought to find the use of the words "under God" to be constitutional and not problematic. The Pledge of Allegiance, which many of us said in school, is not a religious ceremony. It is not itself a prayer. There are two words in the Pledge, and these two words are a recognition of what 90% or so of Americans believe, in a context in which no one is forced to say those words if they don't want to do so. The use of the words "under God" in the Pledge of Allegiance might offend some people, but it hardly establishes an official religion. In context, these two words are not the sort of thing the Supreme Court should concern itself with invalidating. 
Here is the last case: a Christmas display in which a badly divided Supreme Court rejected a constitutional attack.2 9 The Court said that the Christmas display served a secular purpose, which was to celebrate the holiday. At first glance, the Court's idea that a Christmas display has a secular purpose seems quite odd. It is Christmas, after all, not Holidayness. But on reflection, there is a lot to be said for the Court's reasoning. What the locality was essentially doing in this case was recognizing that a number of citizens celebrate Christmas; the locality was acknowledging that practice. This was not by the way a Christmas display to the exclusion of a Hanukkah display or other displays; they were there too. The locality was going beyond ceremonial deism, to be sure. It was not simply recognizing the existence of God. But its Christmas display was part of a general holiday celebration, not an effort to inculcate a particular set of religious convictions. 
By way of summary: I have had two goals here. The first is to try to sketch some theories of constitutional interpretation and to bring them to bear on religious liberty and the Establishment Clause in particular. The theories, to recapitulate, are bipartisan restraint, originalism, perfectionism, and minimalism. 
My second goal is to defend ceremonial deism. I have argued in favor of an approach that recognizes our history and respect for religious practices and beliefs in a way that doesn't force anyone to do anything, but that acknowledges widespread commitments. Such acknowledgement is not an offense to the Constitution of the United States. Those are my two goals but I actually have a broader goal that I haven't yet identified. In our society today, we have real issues of justice and liberty and equality that constitutional law might help to address. Some of those issues involve the war on terror-the uses and limits of presidential power in the context of national security and the place of civil liberties too. In addition, we have continuing free speech issues on university campuses and state legislatures. In the religious context, we have very serious constitutional issues emerging that have to do with the teaching of intelligent design in public 
schools. Mandatory teaching of intelligent design raises serious constitutional problems going well beyond what I've described here. 
The attack on ceremonial deism, I suggest, is a diversion from the serious issues. That attack insults religious believers and simultaneously insults American citizens more generally. The insult does not do a lot of good for nonbelievers and members of religious minorities. Ours is a secular Constitution; there is no doubt about that. But under that secular Constitution, it is nonetheless possible to celebrate God constitutionally. 
Follow this and additional works at: https://chicagounbound.uchicago.edu/law_and_economics Part of the Law Commons Recommended Citation Cass R. Sunstein, "Chevron Step Zero" ( John M. Olin Program in Law and Economics Working Paper No. 249, 2005). 
Chevron Step Zero  
Cass R. Sunstein  
T H E   L A W   S C H O O L   T H E   U N I V E R S I T Y  O F   C H I C A G O   
  May 2005  
  
This paper can be downloaded without charge at:  The Chicago Working Paper Series Index: http://www.law.uchicago.edu/Lawecon/index.html  and at the Social Science Research Network Electronic Paper Collection:  
 http://ssrn.com/abstract_id=739129   
The most famous case in administrative law, Chevron U.S.A. v. Natural Resources Defense Council, Inc., has come to be seen as a counter-Marbury, or even a McCulloch v. Maryland, for the administrative state. But in the last period, new debates have broken out over Chevron Step Zero—the initial inquiry into whether Chevron applies at all. These debates are the contemporary location of a longstanding dispute between Justice Scalia and Justice Breyer over whether Chevron is a revolutionary decision, establishing an across-the-board rule, or instead a mere synthesis of preexisting law, inviting a case-by-case inquiry into congressional instructions on the deference question. In the last decade, Justice Breyer’s case-by-case view has enjoyed significant victories. Two trilogies of cases—one explicitly directed to the Step Zero question, another implicitly so directed—suggest that the Chevron framework may not apply (a) to agency decisions not preceded by formal procedures and (b) to agency decisions that involve large-scale questions about agency authority. Both of these trilogies threaten to unsettle the Chevron framework, and to do so in a way that produces unnecessary complexity for judicial review and damaging results for regulatory law. These problems can be reduced through two steps. First, courts should adopt a broader understanding of Chevron’s scope. Second, courts should acknowledge that the argument for Chevron deference is strengthened, not weakened, when major questions of statutory structure are involved. 
Over twenty years after its birth, the Supreme Court’s decision in Chevron U.S.A. v. Natural Resources Defense Council, Inc.1 shows no sign of losing its influence. On the contrary, the decision has become foundational, even a quasi-constitutional text—the undisputed starting point for any assessment of the allocation of authority between federal courts and administrative agencies. Ironically, Justice Stevens, the author of Chevron, had no broad ambitions for the decision; the Court did not mean to do anything dramatic.2 But shortly after it appeared, Chevron was quickly taken to establish a new * Karl N. Llewellyn Distinguished Service Professor, Law School and Department of Political Science, University of Chicago. I am grateful to Douglas Lichtman, Richard Posner, Adrian Vermeule, and participants in the work-in-progress lunch at the University of Chicago Law School for valuable comments on a previous draft. Blake Roberts provided valuable research assistance. 1 467 U.S. 837 (1984). As a sign of Chevron’s influence, consider the fact that the decision was cited 2,414 times in its first decade (between 1984 and January 1, 1994), 2,584 times in its next six years (between January 1, 1994 and January 1, 2000), and 2,235 times in its next five years (between January 1, 2000 and January 28, 2005). LEXIS search, March, 2005. 2 See Robert Percival, Environmental Law in the Supreme Court: Highlights from the Marshall Papers, 23 ENVTL. L. REP. 10606, 10613 (1993). In fact it is possible, and fascinating, to trace a series of opinions in which Justice Stevens expressed reservations about the broad reading of Chevron, and attempted to domesticate the decision. See, e.g., Young v. Community Nutrition Institute, 476 U.S. 974, 985 (1986) (Stevens, J., dissenting); INS v. Cardoza-Fonseca, 480 U.S. 421 (1987); Babbitt v. Sweet Home Chapter of approach to judicial review of agency interpretations of law,3 going so far as to establish a kind of counter-Marbury for the administrative state. It seemed to declare that in the face of ambiguity, it is emphatically the province and duty of the administrative department to say what the law is.4 
Chevron also appeared to have imperialistic aspirations, cutting across countless areas of substantive law and the full range of procedures by which agencies might interpret statutory law. Some of those ambitions have been realized, for Chevron has had a fundamental impact on areas as disparate as taxation,5 labor law,6 environmental protection,7 immigration,8 foods and drugs,9 and highway safety.10 In all of these areas, and many more, Chevron has signaled a substantial increase in agency discretion to make policy through statutory interpretation. For this reason, Chevron might well be seen not only as a kind of counter-Marbury, but even more fundamentally as the administrative state’s very own McCulloch v. Maryland,11 permitting agencies to do as they wish so long as there is a reasonable connection between agency choices and congressional instructions. This grant of permission seemed to depend on a distinctive account of legal interpretation, one that sees resolution of statutory ambiguity as involving judgments of principle and policy, and insists that the executive, not the courts, should be making those judgments.12 Communities for a Great Oregon, 515 U.S. 687 (1995). See also Christensen v. Harris County, 529 U.S. 576, 595 n.2 (2000) (Stevens, J., dissenting) (endorsing “fully” Justice Breyer’s narrow reading of Chevron). 3 See, e.g., Kenneth Starr, Judicial Review in the Post-Chevron Era, 3 YALE J. REG. 283 (1986); Richard Pierce, Chevron and its Aftermath: Judicial Review of Agency Interpretations of Statutory Provisions, 41 VAND. L. REV. 301 (1988). On the real-world consequences of Chevron, see Peter Schuck and E. Donald Elliott, To the Chevron Station: An Empirical Study of Federal Administrative Law, 42 DUKE L.J. 984 (1989). Schuck and Elliott find a significant effect from Chevron, an increase in affirmance rates from 71% in the pre-Chevron year of 1984 to 81% in the post-Chevron year of 1985. Over more extended periods, studies are hard to conduct, because prospective litigants will adjust their mix of cases to the rules governing judicial review of agency action; when challenges are hard to sustain under doctrines of deference, fewer challenges will be brought. On the other hand, agencies and their lawyers may adjust their own practices to deference doctrines as well, and hence take legal risks that they would not assume if courts were less likely to defer. Relevant findings, exploring the importance of whether a panel is composed of Republican or Democratic appointees to the application of Chevron, can be found in Frank Cross and Emerson Tiller, Judicial Partisanship and Obedience to Legal Doctrine: Whistleblowing on the Federal Courts of Appeals, 107 YALE L.J. 2155 (1998) (finding that allRepublican panels are particularly willing to strike down agency action at the behest of an industry challenge, notwithstanding Chevron). 4 See Marbury v. Madison, 5 U.S. 137, 177 (1803) (“It is emphatically the province and duty of the judicial department to say what the laws is.”). 5 Atlantic Mutual Ins. Co. v. Commissioner, 523 U.S. 382 (1998); Tate & Lyle v. Commissioner, 87 F.3d 99 (3d Cir. 1996). 6 See NLRB v. United Food Workers Union, 484 U.S. 112 (1987); Cavert Acquisition Co. v. NLRB, 83 F.3d 598 (3d Cir 1996). 7 See Chemical Manufacturers Association v. NRDC, 470 U.S. 116 (1985). 8 See INS v. Cardoza-Fonseca, 480 U.S. 421 (1987). 9 Medtronic, Inc. v. Lohr, 518 U.S. 470 (1996). 10 Geier v. American Honda Motor Co., 529 U.S. 861 (2000). 11 17 U.S. 316 (1819). 12 See infra notes 40-44. 
In the last fifteen years, however, the simplest interpretations of Chevron have unraveled. Like a novel or even a poem, the decision has inspired fresh and occasionally even shocking readings. In some cases, the Court appears to have moved strongly in the direction of pre-Chevron law, in an evident attempt to reassert the primacy of the judiciary in statutory interpretation. At times, the effort to re-establish judicial supremacy has been quite explicit.13 But the result has not been a restoration of pre-Chevron principles; it has instead been the addition of several epicycles to the Chevron framework, producing not only a decrease in agency authority, but also a significant increase in uncertainty about the appropriate approach. More than at any time in recent years, a threshold question—the scope of judicial review—has become one of the most vexing in regulatory cases.14 
Chevron famously creates a two-step inquiry for courts to follow in reviewing agency interpretations of law.15 The first step asks whether Congress has “directly spoken to the precise question at issue,” an inquiry that requires an assessment of whether Congress’s intent “is clear” and “unambiguously expressed.”16 The second step asks whether the agency’s interpretation is “permissible,” which is to say reasonable in light of the underlying law.17 It is an understatement to say that a great deal of judicial and academic attention has been paid to the foundations and meaning of Chevron’s two-step inquiry.18 But in the last period, the most important and confusing questions have involved neither step. Instead they involve Chevron Step Zero—the initial inquiry into whether the Chevron framework applies at all.19 The Supreme Court has issued several important Step Zero decisions,20 which clarify a number of questions but also offer complex and conflicting guidance. As we shall see, the area is pervaded by legal fictions about congressional understandings, and the proliferation of fictions has vindicated the 13 See INS v. Cardoza-Fonseca, 480 U.S. 421 (1987); Babbitt v. Sweet Home Chapter of Communities for a Great Oregon, 515 U.S. 687 (1995); FDA v. Brown & Williamson, 529 U.S. 120 (2000). 14 A detailed discussion can be found in Lisa Schultz Bressman, How Mead Has Muddled Judicial Review of Agency Action, VALD. L. REV. (forthcoming 2005). 15 467 U.S. at 842-44. 16 Id. at 842-43. 17 Id. at 843. 18 See, e.g., Thomas Merrill, Judicial Deference to Executive Precedent, 101 YALE L.J. 969 (1992); Clark Byse, Judicial Review of Administrative Interpretation of Statutes: An Analysis of Chevron’s Step Two, 2 ADMIN. L.J. 255 (1988); Thomas Merrill, Textualism and the Future of the Chevron Doctrine, 72 WASH U. L.Q. 351 (1994); Richard Pierce, Chevron and its Aftermath: Judicial Review of Agency Interpretations of Statutory Provisions, 41 VAND. L. REV. 301 (1988); Note, “How Clear is Clear” in Chevron’s Step One?, 118 HARV. L. REV. 1687 (2005); Kenneth Starr, Judicial Review in the Post-Chevron Era, 3 YALE J. REG. 283 (1986). 19 I borrow the term from Thomas W. Merrill and Kristin E. Hickman, Chevron’s Domain, 89 GEO. L.J. 833, 836 (2001). 20 Christensen v. Harris County, 529 U.S. 576 (2000); United States v. Mead, 533 U.S. 218 (2001); Barnhart v. Walton, 535 U.S. 212 (2002). fears of those who have insisted on the importance of a simple answer to the Step Zero question.21 
My principal purpose here is to provide an understanding of the foundations and nature of the Step Zero dilemma, and to suggest how that dilemma should be resolved. Step Zero has become the central location of an intense and longstanding disagreement between the Court’s two administrative law specialists, Justice Breyer and Justice Scalia.22 In fact it is impossible to understand the current debates without reference to this disagreement. In the 1980s, the two converged, apparently independently, on a distinctive understanding of Chevron, one that roots the decision in a theory of implicit congressional delegation of law-interpreting power to administrative agencies.23 Both justices explicitly recognized that any understanding of legislative instructions is a “legal fiction”24; both approved of resort to that fiction. But the two sharply disagreed about its meaning and content. Here, as elsewhere, Justice Scalia seeks clear and simple rules, intended to reduce the burdens of decision-making for lower courts and litigants.25 And here, as elsewhere, Justice Breyer prefers a case-by-case approach, one that eschews simplicity in the interest of (what he sees as) accuracy.26 This kind of disagreement, involving a classic rules-standards debate,27 echoes throughout the law, but as we shall see, it has distinctive resonance in the context of judicial review of agency interpretations of law. 
On an important matter, Justice Scalia’s approach has largely triumphed, at least thus far: When agency decisions have the force of law, or follow a formal procedure, Chevron supplies a simple rule, notwithstanding early efforts to cabin its reach.28 In recent years, however, Justice Breyer’s approach has enjoyed a partial but significant victory, on the theory that Chevron should not be taken to cede law-interpreting power to 21 See Mead, 533 U.S. at 239, 245-47 (Scalia, J., dissenting); Adrian Vermeule, Mead in the Trenches, 71 GEO. WASH. L. REV. 847 (2003). 22 Justice Breyer taught administrative law for many years at Harvard Law School; Justice Scalia did the same at the University of Virginia Law School and the University of Chicago Law School. 23 Stephen Breyer, Judicial Review of Questions of Law and Policy, 38 ADMIN. L. REV. 363 (1986); Antonin Scalia, Judicial Deference to Agency Interpretations of Law, 1989 DUKE L.J. 511. 24 This point is emphasized and explored in David J. Barron and Elena Kagan, Chevron’s Nondelegation Doctrine, 2001 SUP. CT. REV. 201. 25 See Antonin Scalia, The Rule of Law As a Law of Rules, 56 U. CHI. L. REV. 1175 (1989). 26 See Stephen Breyer, On the Uses of Legislative History in Interpreting Statutes, 65 S. CAL. L. REV. 845 (1992). 27 See Louis Kaplow, Rules v.s Standards: An Economic Analysis, 42 Duke LJ 557 (1992); Kathleen Sullivan, Foreword: The Justices of Rules and the Justices of Standards, 106 Harv L Rev 22 (1992). 28 INS v. Cardoza Fonseca, 480 U.S. 421 (1987). As we shall see, this claim must be qualified by reference to recent developments involving major questions. See infra 198-231. agencies in circumstances in which it is implausible to infer a congressional delegation. A trilogy of cases, unambiguously directed to Step Zero, has suggested that when agencies have not exercised delegated power to act with the force of law, Chevron may not provide the governing framework.29 In a separate trilogy of cases,30 the Court has also raised Step Zero questions simply because it has suggested the possibility that deference will be reduced, or even nonexistent, if a “major question” is involved, one that goes to the heart of the regulatory scheme at issue. The apparent theory is that Congress should not be taken to have asked agencies to resolve those questions. 
I suggest that both trilogies point in unfortunate directions. As for the first: The “force of law” test is a crude way of determining whether Chevron deference is appropriate, and it introduces far too much complexity into the deference issue. The Court is apparently seeking to allow Chevron deference only or mostly when agency decisions have followed procedures that guarantee a kind of deliberation and reflectiveness. But that goal, however appealing, cannot justify the high level of complexity that the first trilogy has introduced. As for the second: Major questions are not easily distinguished from less major ones, and the considerations that underlie Chevron apply with more, not less, force when major questions are involved. To be sure, it is possible to defend a background principle that limits agency discretion when constitutionally sensitive interests are at stake.31 But that principle should not be converted into a general presumption in favor of limiting agency authority—a presumption that would encode a kind of status quo bias, or possibly even a strong antiregulatory “tilt,” into the Chevron framework. 
My argument, in short, is that where fairly possible, the Step Zero question should be resolved in favor of applying the standard Chevron framework—a framework that has the dual virtues of simplifying the operation of regulatory law and giving policymaking authority to institutions that are likely to have the virtues of specialized competence and political accountability. The Court’s emerging steps in favor of a more complex framework, calling for independent judicial judgment in certain circumstances, are a product of an evident desire to constrain agency discretion when such discretion seems particularly unlikely to be fairly exercised.32 But the Court’s goals can be accomplished in much simpler and better ways, above all by insisting on the rule of law constraints embodied in Steps One and Two, and on continued judicial review for arbitrariness. 
This Article comes in five parts. Part II explores the early debates over Chevron, with particular emphasis on the striking contrast between then-Judge Breyer’s effort to domesticate the decision by reading it to permit case-by-case inquiries and Justice Scalia’s insistence that Chevron is a dramatic development that establishes an across-theboard presumption. Part III investigates Step Zero trilogy in which the Court has held that Chevron applies to agency decisions having the force of law or backed by relatively formal procedures, while requiring a case-by-case inquiry into whether Chevron applies to less formal agency action. Part IV explores cases in which the Court has also failed to apply Chevron in the ordinary way, apparently on the theory that major questions, involving the basic reach of regulatory statutes, are for courts rather than agencies. Part V briefly concludes. 
In Chevron, the Court announced its two-step approach without giving a clear sense of the theory that justified it. The case itself involved the decision of the Environmental Protection Agency (EPA) to define “stationary source”33 under the Clean Air Act as an entire factory, rather than each pollution-emitting unit within the plant. The Supreme Court insisted that because the statute was ambiguous, the EPA could supply whatever (reasonable) definition it chose. But why, exactly, should agencies be permitted to interpret statutory ambiguities as they see fit, subject only to the limitations of reasonableness? The Court emphasized that Congress sometimes explicitly delegates law-interpreting power to agencies34; if the Clean Air Act had said “stationary source (as defined by the Administrator),” judges would have to accept administrative judgments. 32 See Bressman, supra note 14. 33 42 U.S.C. § 7502 (c). 34 467 U.S. at 844. Of course the Clean Air Act contained no explicit delegation. But the Court added that “sometimes the legislative delegation to an agency on a particular question is implicit rather than explicit.”35 If so, the court must accept any reasonable interpretation. 
But why should a court find an implicit delegation in the Administrative Procedure Act (APA) or the Clean Air Act, the governing statutory provisions in Chevron itself? The APA does not appear to delegate law-interpreting power to agencies; on the contrary, it specifies that the “reviewing court shall decide all relevant questions of law, [and] interpret statutory provisions.”36 This phrase seems to suggest that ambiguities must be resolved by courts and hence that the Chevron framework is wrong. But by empowering the EPA to issue regulations, perhaps the Clean Air Act is best taken to say that the agency is implicitly entrusted with the interpretation of statutory terms. If so, the reviewing court must continue to follow the APA and decide “all relevant questions of law”; but the answer to the relevant questions will depend on what the EPA has said, because under the Clean Air Act, the law is what the EPA says it is.37 In Chevron, the Court referred to this possibility, noting that Congress might have wanted the agency to strike the relevant balance with the belief “that those with great expertise and charged with responsibility for administering the provision would be in a better position [than courts or Congress itself] to do so.”38 But the Court did not insist that Congress in fact so thought. On the contrary, it said that Congress’s particular intention “matters not.”39 
Instead the Court briefly emphasized judges’ lack of expertise and, in more detail, their lack of electoral legitimacy. In interpreting law, the agency may “properly rely upon the incumbent administration’s views of wise policy to inform its judgments. While agencies are not directly accountable to the people, the Chief Executive is.”40 Hence it would be appropriate for agencies, rather than judges, to resolve “competing interests which Congress itself either inadvertently did not resolve, or intentionally left to be resolved in light of everyday realities.”41 35 Id. 36 5 U.S.C. § 706. 37 See Henry Monaghan, Marbury and the Administrative State, 83 COLUM. L. REV. 1 (1983). 38 467 U.S. at 865. 39 Id. 40 Id. 41 Id. 
The Chevron Court’s approach was much clearer than the rationale that accounted for it. The Court’s reference to expertise suggested one possible rationale: Perhaps the Court was saying that the resolution of statutory ambiguities sometimes calls for technical expertise, and that in such cases deference would be appropriate. On this view, having roots in the New Deal’s enthusiasm for technical competence,42 specialized administrators, rather than judges, should make the judgments of policy that are realistically at stake in disputes over ambiguous terms. But the Court’s emphasis on accountability suggested a second possibility: Perhaps the two-step inquiry is based on a healthy recognition that in the face of ambiguity, agency decisions must rest on judgments of value, and those judgments should be made politically rather than judicially. On this view, having roots in legal realism,43 value choices are a significant part of statutory construction, and those choices should be made by democratically accountable officials. This reading suggests a third and more ambitious possibility: Perhaps Chevron is rooted in separation of powers, requiring courts to accept executive interpretations of statutory ambiguities in order to guard against judicial displacement of political judgments.44 
In the 1980s, then-Judge Breyer45 and Justice Scalia, both administrative law specialists, rejected these readings of Chevron. They agreed that Chevron must rest on a simple idea: Courts defer to agency interpretations of law when and because Congress has told them to do so. As we shall see, this reading of Chevron has prevailed. If Congress wanted to do so, it could entrench Chevron, by providing that statutory ambiguities must be resolved by agencies; and if Congress sought to overrule Chevron, by calling for independent judicial judgments about legal questions, it could do precisely that. Judge Breyer and Justice Scalia agreed that the national legislature retains control of the deference question, and in this sense Chevron must rest on an understanding of what Congress has instructed courts to do. But their shared emphasis on implicit delegation led Judge Breyer and Justice Scalia to quite different understandings of Chevron’s scope and limitations. Where Judge Breyer sought to domesticate Chevron, treating it as a kind of 42 See JAMES LANDIS, THE ADMINISTRATIVE PROCESS (1935). 43 See Karl Llewellyn, Some Realism About Realism, 44 HARV. L. REV. 1222 (1931). 44 See Douglas Kmiec, Judicial Deference to Executive Agencies, 2 ADMIN. L.J. 269, 277-28 (1988). 45 For ease of exposition, I shall henceforth refer to “Judge Breyer” when speaking of his 1986 essay, Stephen Breyer, Judicial Review of Questions of Law and Policy, 38 Admin. L. Rev. 363 (1986). “problem” to be solved by reference to established principles, Justice Scalia saw Chevron as a genuinely revolutionary decision, one that would fundamentally alter the relationship between agencies and reviewing courts, and renovate what had long been the law. 
B. Against “Any Simple General Formula”: Breyer’s Plea for Complexity 
Chevron was decided in 1984. In that same year, Judge Breyer, writing for the United States Court of Appeals for the First Circuit, tried to make sense of the Court’s decision.46 His explanation of Chevron pointed to a delegation of law-interpreting authority to agencies. When Congress has not made an express delegation, Judge Breyer wrote, “courts may still infer from the particular statutory circumstances an implicit congressional instruction about the degree of respect or deference they owe the agency on a question of law.”47 The inference would be intensely particularistic; it would rest on an inquiry into “what a sensible legislator would have expected given the statutory circumstances.”48 The expectations of the sensible legislator would depend on an inquiry into institutional competence: “The less important the question of law, the more interstitial its character, the more closely related to the everyday administration of the statute and to the agency's (rather than the court's) administrative or substantive expertise, the less likely it is that Congress (would have) ‘wished’ or ‘expected’ the courts to remain indifferent to the agency's views. Conversely, the larger the question, the more its answer is likely to clarify or stabilize a broad area of law, the more likely Congress intended the courts to decide the question themselves.”49 
Thus Judge Breyer’s approach squarely endorsed the implicit delegation theory, but in a way that required a case-by-case inquiry into what “a sensible legislator would have expected given the statutory circumstances.” With an interstitial question closely connected to “the everyday administration of law,” or calling for agency expertise, deference would be warranted. But with a “larger” question, one whose answer would “stabilize a broad area of law,” an independent judicial assessment would be required. 46 Mayburg v. HHS, 740 F.2d 100, 106 (1st Cir. 1984). 47 Id. 48 Id. 49 Id. 
In 1986, Judge Breyer explored these issues far more systematically, in an essay that has proved to be extremely, and indeed increasingly, influential.50 Judge Breyer’s basic claim was straightforward. In the immediate aftermath of Chevron, existing doctrine seemed to argue for deferential judicial review of agency interpretations of law but stringent judicial review of agency judgments about policy.51 In this sense, the governing standards were “anomalous,” because a rational system would call for “stricter review of matters of law, where courts are more expert, but more lenient review of matters of policy, where agencies are more expert.”52 In Judge Breyer’s view, judicial review should be specifically tailored to the “institutional capacities and strengths” of the judiciary.53 For that tailoring, the simple approach set out in Chevron was hopelessly inadequate. 
Judge Breyer began by emphasizing that before Chevron, courts had been inconsistent on the question of judicial review of agency interpretations of law, with competing strands of deference and independence.54 In order to reconcile the conflict, Judge Breyer noted that courts might defer to agencies either because agencies have a “better understanding of congressional will”55 or because Congress explicitly or implicitly delegated interpretive power to agencies.56 Judge Breyer added, crucially, that the idea of a “legislative intent to delegate the law-interpreting function” is “a kind of legal fiction.”57 When courts find such an intent, they are really imagining “what a hypothetically ‘reasonable’ legislator would have wanted (given the statute’s objectives),” and “looking to practical facts surrounding the administration of a statutory scheme.”58 
In Judge Breyer’s view, this imagining should lead to a case-by-case inquiry into Congress’s hypothesized intentions. If the question calls for special expertise, the agency is more likely to be able to answer it correctly; hence an ordinary question of agency administration would call for deference.59 But if the question is “an important one,” an 50 See Stephen Breyer, Judicial Review of Questions of Law and Policy, 38 ADMIN. L. REV. 363 (1986). 51 Id. at 364-65. 52 Id. at 397. 53 Id. at 398. 54 Id. at 365-67. 55 Id. at 368. 56 Id. at 369. 57 Id. at 370. 58 Id. 59 Id. independent judicial approach is preferable. “Congress is more likely to have focused upon, and answered, major questions, while leaving interstitial matters to answer themselves in the course of the statute’s daily administration.”60 (That sentence has proved to be especially important, as we shall soon see.) Judge Breyer added that a court should “consider the extent to which the answer to the legal question will clarify, illuminate or stabilize a broad area of the law,” and whether “the agency can be trusted to give a properly balanced answer.”61 Judge Breyer insisted that the reconciliation of the apparently conflicting lines of cases depends on inquiries of this sort. 
At this stage Judge Breyer was confronted with an obvious question about the relationship between his views and the Court’s approach in Chevron. To answer that question, he embarked on a new discussion with a revealing title: “The Problem of the Chevron Case.”62 He noted that Chevron could be read as embodying “the complex approach” that he endorsed; but it could also be seen “as embodying a considerably simpler approach,” one that accepts any reasonable agency interpretation in the face of ambiguity.63 Not surprisingly, Judge Breyer argued strenuously against that latter approach. Notwithstanding “its attractive simplicity,” he urged, the broad reading could not survive “in the long run.”64 
Judge Breyer offered three reasons for this conclusion. The first involves the sheer diversity of situations in which courts might be asked to defer to agency interpretations. No simple formula can fit so “many different types of circumstances, including different statutes, different kinds of application, different substantive regulatory or administrative problems, and different legal postures.”65 Second, and ironically, a simple rule will increase delay and complexity. Under Chevron, courts will sometimes have to remand a case to an agency to establish a reasonable interpretation; because judges are at least as likely to produce the correct interpretation, such Chevron remands will be “a waste of time.”66 Third, the simple view “asks judges to develop a cast of mind 60 Id. 61 Id. at 371. 62 Id. at 372. 63 Id. at 373. 64 Id. at 373. 65 Id. 66 Id. at 378. that often is psychologically difficult to maintain.”67 The reason is that after a detailed examination of a legal question, it is difficult “to believe both that the agency’s interpretation is legally wrong, and that its interpretation is reasonable.”68 
In the end, Judge Breyer concluded, these “factors will tend to force a less univocal, less far-reaching interpretation of Chevron.” Inevitably, “we will find the courts actually following more varied approaches,” without adhering to any “single simple judicial formula.”69 Judge Breyer urged, in short, that Chevron should be read in accordance with the most sensible understanding of what had preceded it, which entailed a case-specific inquiry into Congress’s fictional instructions on the question of deference.70 Far from being a revolution, or even a radical departure, Chevron should be taken to codify the best understanding of existing law. 
Writing just three years later, Justice Scalia defended Chevron in exactly the same terms as Judge Breyer (though without referring to his essay).71 He began by insisting that the decision ultimately rested on a reading of congressional instructions—and hence that prominent justifications for the decision, pointing to agency expertise and separation of powers, were irrelevant.72 Quoting a lower court, Justice Scalia said that the deference judgment must be “a function of Congress’ intent on the subject as revealed in the particular statutory scheme at issue.”73 For Justice Scalia, as for Judge Breyer, the central issue is what Congress has told courts to do, for the national legislature maintains ultimate authority over the deference question. 
Justice Scalia also agreed with Judge Breyer’s reading of pre-Chevron law. The lower courts had tried to decide the deference question on a case-by-case basis, producing a statute-by-statute evaluation that was a recipe for confusion. “Chevron, however, if it is 67 Id. at 379. 68 Id. at 379. 69 Id. at 381. 70 Candor compels an acknowledgement that an extremely young man, writing in the same period, analyzed the Chevron issue in terms akin to those used by Judge Breyer. See Cass R. Sunstein, Constitutionalism After the New Deal, 101 HARV. L. REV. 421, 465-70 (1987). A somewhat older man believes that the conclusion in id., favoring case-by-case inquiries into the deference question, was mistaken. 71 Antonin Scalia, Judicial Deference to Agency Interpretations of Law, 1989 Duke L.J. 511. 72 Id. at 512-13. 73 Id. at 516. to be believed, replaced this statute-by-statute evaluation (which was assuredly a font of uncertainty and litigation) with an across-the-board presumption that, in the case of ambiguity, agency discretion is what is meant.”74 Here again Justice Scalia is in complete accord with Judge Breyer; but where Judge Breyer challenges the presumption as unacceptably simplistic, Justice Scalia defends it on exactly that ground—and hence as a dramatic departure from what preceded it. 
How might that presumption be defended? Returning to the touchstone of legislative instructions, Justice Scalia acknowledges that Chevron is “not a 100% accurate estimation of modern congressional intent”75; deference does not always capture what Congress wants. But “the prior case-by-case evaluation was not so either”76—a point that might be buttressed with the suggestion that such evaluations will increase the burdens of decision while also producing a degree of error from inevitably fallible judges. In the end, Justice Scalia agrees with Judge Breyer on yet another point: Any account of congressional instructions reflects “merely a fictional, presumed intent.”77 A judgment about that fictional and presumed intent, Justice Scalia seems to say, should also be based on a judgment about what would amount to a sensible instruction by a sensible legislature. 
But what makes sense should be informed by a central point: any fictional or presumed intent will operate “principally as a background rule of law against which Congress can legislate.”78 And by emphasizing this point, Justice Scalia marks his crucial departure from Judge Breyer. “If we are speaking of fictional intent, Chevron, taken to provide a simple background rule, is unquestionably better than what preceded it,” simply because “Congress now knows that the ambiguities it creates, whether intentionally or unintentionally, will be resolved, within the bounds of permissible interpretation, not by the courts but by a particular agency, whose policy biases will ordinarily be known.”79 Thus Justice Scalia offers a dynamic rather than static understanding of Chevron. Where Judge Breyer asks whether a univocal deference rule accurately reflects (fictive) congressional understandings, Justice Scalia focuses on the effects of a deference rule on 74 Id. 75 Id. at 517. 76 Id. 77 Id. 78 Id. 79 Id. subsequent congressional activity – a focus that, in his view, argues for clarity and simplicity. 
To this Justice Scalia added two points about the scope of Chevron, thus defended. First, the emphasis on “real or presumed legislative intent to confer discretion” should obliterate the old idea that longstanding and consistent interpretations would receive more deference than recent and inconsistent ones.80 Second, and more fundamentally, Justice Scalia suggested the distinct possibility that under Chevron, it would be necessary to revise a “distinction of yesteryear,” which involves “the distinction among the various manners in which the agency makes its legal views known.”81 Even mere litigating positions might receive Chevron deference, for “if the matter at issue is one for which the agency has responsibility, if all requisite procedures have been complied with, and if there is no doubt that the position urged has full and considered approval of the agency head, it is far from self-evident that the agency's views should be denied their accustomed force simply because they are first presented in the prosecution of a lawsuit.”82 
At this point Justice Scalia offered a jurisprudential suggestion, one that has turned out to be quite prescient. In his view, “there is a fairly close correlation between” enthusiasm for Chevron and a commitment to textualist methods of interpretation.83 “One who finds more often (as I do) that the meaning of a statute is apparent from its text and from its relationship with other laws, thereby finds less often that the triggering requirement for Chevron deference exists.”84 Those who reject plain meaning, and are “willing to permit the apparent meaning of a statute to be impeached by the legislative history, will more frequently find agency-liberating ambiguity, and will discern a much broader range of ‘reasonable’ interpretation that the agency may adopt and to which the courts must pay deference. The frequency with which Chevron will require that judge to accept an interpretation he thinks wrong is infinitely greater.”85 80 Id. at 519. 81 Id. 82 Id. 83 Id. at 521. 84 Id. 85 Id. 
Justice Scalia noticed that Chevron had not yet marked a revolution in the law. “The opinions we federal judges read, and the cases we cite, are full of references to the old criteria of ‘agency expertise,’ ‘the technical and complex nature of the question presented,’ ‘the consistent and long-standing agency position’—and it will take some time to understand that those concepts are no longer relevant, or no longer relevant in the same way.”86 
“in the long run Chevron will endure and be given its full scope—not so much because it represents a rule that is easier to follow and thus easier to predict (though that is true enough), but because it more accurately reflects the reality of government, and thus more adequately serves its needs.”87 
Where Judge Breyer predicted a disintegation of Chevron’s “univocal” approach, on the ground that it was ill-suited to reality, Justice Scalia contended that Chevron would be given its full scope, and amount to a major and novel development, precisely because of its univocal quality. As we shall see, Breyer’s prediction appears to have proved to be more accurate; but in important respects the jury is still out. 
It should be clear that the disagreement between Judge Breyer and Justice Scalia involves the pervasive choice between standards and rules. Judge Breyer urged that no rule could solve the deference problem, simply because it would produce so much inaccuracy. Justice Scalia can be taken to have responded that a rule is likely to be as accurate as any standard and that it has the further advantage of reducing decisional burdens on courts. Seeing a deference rule as relevant to Congress’ subsequent performance, Justice Scalia emphasizes, as Judge Breyer does not, that a simple rule will provide better guidance to subsequent legislators. If the choice between rules and standards turns in part on the costs of error and the costs of decisions,88 then Judge Breyer and Justice Scalia might be seen as disagreeing about exactly how to assess those costs. 86 Id. 87 Id. 88 See Kaplow, supra note. 
D. Reading Deference Doctrines Jurisprudentially: Chevron As Erie If Chevron is read in light of the shared concerns of Judge Breyer and Justice Scalia, it can be understood as a natural outgrowth of the twentieth-century shift from judicial to agency lawmaking.89 In numerous contexts, judge-made law has been replaced by administrative regulation, often pursuant to vague or open-ended guidance. The replacement has been spurred by dual commitments to specialized competence and democratic accountability—and also by an understanding of the need for frequent shifts in policy over time, with new understandings of fact and with new values as well. For banking, telecommunications, and environmental protection – among many other areas – changing circumstances often require agencies to adept old provisions to unanticipated problems. Despite the Court’s lack of ambition for its decision, the Chevron opinion, approving a bold and novel initiative by the Reagan Administration, did speak explicitly of the role of expertise and accountability in statutory interpretation. And if interpretation of unclear terms cannot operate without some judgments of the interpreter’s own,90 then the argument for Chevron, as the appropriate legal fiction, seems overwhelming. Indeed, Chevron can be seen in this light as a close analogue to Erie Railroad v. Tompkins91—as a suggestion that law and interpretation often involve no “brooding omnipresence in the sky” but instead discretionary judgments to be made by appropriate institutions. For resolution of statutory ambiguities, no less than for identification of common law, federal courts may not qualify as appropriate. 
I am suggesting, then, that Justice Scalia’s argument about the need for a clear background rule can be strengthened with a strong emphasis on Judge Breyer’s claims about expertise, an appreciation of the pressing need for agency flexibility over time, and a recognition that when agencies interpret ambiguities, a judgment of value, operating under the President, is often involved. As we shall see, many of the post-Chevron cases, 89 An illuminating study is Price Fishback and Shawn Kantor, A Prelude to the Welfare State: The Origins of Workers’ Compensation (1999). See also Jerry Mashaw, Due Process in the Administrative State (1983), for a valuable discussion in the context of social security disability determinations. 90 There is a connection here between Chevron and Ronald Dworkin’s view on interpretation, as set out in Law’s Empire (1985). Dworkin contends that interpretation requires a judicial judgment about “fit” with existing materials and also about “justification” of those materials; his conception of law as integrity asks judges to put existing materials in their “best constructive light.” In modern government, courts are often less capable of accomplishing this task than are agencies, precisely because of their comparatively lesser expertise and accountability. Hence acceptance of Dworkin’s account of interpretation, or any account in the same general family, is easily enlisted on behalf of Chevron. 91 304 U.S. 64 (1938). read in their context, testify to the importance of these points. But if Chevron is read both broadly and ambitiously, it runs immediately into Judge Breyer’s objection that it is too crude and univocal. 
The disagreements between Justices Scalia and Breyer could manifest themselves at multiple points. Suppose that the question involves Chevron Step One. We should expect a degree of simplicity from Justice Scalia, in the form of deference to the agency’s interpretation unless the text unambiguously forbids it; and the expectation is met in many cases.92 We might expect Justice Breyer to be less willing to find statutory language to be plain and hence to be willing to defer to agencies even when Justice Scalia is not; and there is evidence to this effect as well.93 
In these respects, the tempting idea that Justice Scalia’s enthusiastically proChevron approach will be more deferential to agencies is only partly right. If Justice Scalia is correct to say that Chevron enthusiasts are also likely to insist on plain meaning, then those who favor the “simple” reading of Chevron will be more likely to find Step One violations. There is some evidence that this is true.94 And in an important case that actually upholds an agency’s interpretation, the Court went out of its way to reject the strong and simple version of Chevron in favor of the weaker and more complex version—with explicit citation to the 1986 essay by then-Judge Breyer.95 
In fact, the 1980s disagreement might have been expected to involve something far larger than Step Zero. While Justice Scalia would adopt a general rule of deference to agency interpretations, Justice Breyer would call for a case-by-case inquiry into (fictional, hypothesized) legislative expectations. The major locus of the disagreement, however, has become much narrower. It involves the threshold question whether Chevron is applicable at all—a question ignored by Judge Breyer in 1986 and prominently 92 See Babbitt v. Sweet Home Chapter of Communities for a Great Oregon, 515 U.S. 687, 699 (1995) (Scalia, J., dissenting); Gen. Dynamics Land Sys. v. Cline, 540 U.S. 581 (2004); Smith v. City of Jackson, 125 S. Ct. 1536 (2005). 93 See MCI Telecommunications Corp. v. AT&T, 512 U.S. 218 (1994). 94 Babbitt v. Sweet Home Chapter of Communities for a Great Oregon, 515 U.S. 687 (1995); MCI Telecommunications Corp., 512 U.S.. 95 Babbitt v. Sweet Home Chapter of Communities for a Great Oregon, 515 U.S. 687 (1995). presaged by Justice Scalia in 1989. In Chevron’s first decade, this question was largely invisible, at least in the Court itself; and a number of decisions applied the Chevron framework without serious consideration of any Step Zero. 
Consider, for example, Young v. Community Nutrition Institute.96 That case involved a provision saying that “the Secretary [of Health and Human Services] shall promulgate regulations limiting the quantity [of any poisonous or deleterious substance added to any food] to such extent as he finds necessary for the protection of public health."97 The interpretive question, one of considerable practical importance, was whether “to such extent as he finds necessary” modified “shall promulgate,” so as to allow the Secretary not to act at all, or instead modified “limiting the quantity,” so as to require him to issue regulations, but with such severity as he chose. The agency had settled on the former interpretation, but not through any formal procedure; instead the agency’s informal understanding was involved. Without even pausing to consider the applicability of the Chevron framework, the Court gave deference to the agency and upheld its interpretation.98 
Does Young suggest that all agency interpretations of law should receive Chevron deference? If the underlying theory involves implicit (and fictional) delegation, the real question is when Congress should be understood to have delegated law-interpreting power to a regulatory agency. The broadest imaginable answer would be simple: Whenever an agency makes an interpretation of law, that interpretation falls under the Chevron framework. This answer would eliminate Step Zero altogether. But everyone should be willing to agree that the answer is too broad. Suppose, for example, that an agency is interpreting the Administrative Procedure Act. Is the FDA permitted to interpret the APA’s provisions governing reviewability, and hence to decide, within the bounds of reasonableness, whether its own decisions are reviewable? Is the NLRB’s understanding of the APA’s substantial evidence test controlling, or does the Court interpret that test on its own? The clear answer to such questions is that Chevron is 96 476 U.S. 974 (1986). 97 21 U.S.C. § 346. 98 FDIC v. Philadelphia Gear Corp., 476 U.S. 426, 438-439 (1986); Mead Corp. v. Tilley, 490 U.S. 714 (1989); NationsBank of N.C., N.A. v. Variable Annuity Life Ins. Co., 513 U.S. 251, 255 (1995); Pension Benefit Guaranty Corporation v. LTV Corp., 496 U.S. 633, 642-643, 647-648 (1990). appear best to permit the resolution of ambiguities to come from a politically accountable actor. 
Nor is this particular example hypothetical. It lies at the heart of the looming litigation over the validity of the EPA’s recent decision to regulate mercury, a hazardous pollutant, under a trading system.184 Indeed the example is a version of Chevron itself, which involved the EPA’s decision to adopt a modest kind of (intrafirm) trading system through its definition of “source.” Chevron hardly involved an interstitial question of the sort at issue in the everyday administration of the Clean Air Act; it involved a significant rethinking of the definition of the statutory term “source.”185 Of course we are speaking of legal fictions, but why should it be assumed that Congress (fictionally) intended the courts, rather than agencies, to define that term? Isn’t that a bad and unhelpful fiction? 
Judge Breyer’s central response is that “Congress is more likely to have focused upon, and answered, major questions, while leaving interstitial matters to answer themselves in the course of the statute’s daily administration.”186 But this is unconvincing. If Congress has in fact focused upon, and answered, major questions, agencies must accept those answers under Chevron Step One. By hypothesis, we are dealing not with such cases but with those in which statutes are ambiguous, and the only question is whether to accept an agency’s resolution or instead to rely on the interpretation chosen by a federal court. For “major questions,” the agency’s specialized fact-finding competence and democratic accountability might well be more relevant, not less. 
A better justification for the distinction between interstitial and major questions would involve agency incentives. Perhaps there is less reason to trust agencies when they are making large-scale judgments about statutory meaning. Perhaps parochial pressures, such as those imposed by interest groups, will distort agency decisions in one or another direction; perhaps agency self-interest, such as the expansion of administrative authority, will increase the likelihood of bias. Judge Breyer might believe that on interstitial questions, involving everyday administration, agencies can be trusted, but that the same is not true when a major decision is involved. And if agencies were systematically less 184 See Lisa Heinzerling & Rene Stenzor, A Perfect Storm: Mercury and the Bush Administration, 34 ENV. L. REPT. 10297 (2004). 185 See ROBERT PERCIVAL ET AL., ENVIRONMENTAL LAW AND POLICY (2003). 186 Id. reliable on major questions, the argument for a reduced degree of deference would be quite plausible. 
Nonetheless, that argument faces large problems. As I have noted, the line between interstial and major questions is thin. If the line can be drawn, it is hardly clear that courts are in a substantially better position to resolve the “major” questions than agencies are. Perhaps agencies are responding to parochial pressures, but it is also possible that their judgments are a product of specialized competence and democratic will; no sustained evidence justifies the suggestion that when agencies make decisions on major questions, bias and self-interest are the motivating factors. In any case, Chevron deference does not give agencies a blank check. It remains the case that agency decisions must not violate clearly expressed legislative will, must represent reasonable interpretations of statutes, and must not be arbitrary in any way.187 These constraints produce significant checks on agency self-interest and bias if they should be present. 
As we have seen, Judge Breyer’s challenge to the simple reading of Chevron refers more generally to the psychological difficulty, for judges, of believing that an agency interpretation is both reasonable and wrong; and that difficulty is likely to be heightened for major questions. Perhaps he is right. But it is hardly unfamiliar for judges to think “wrong but reasonable.” They might believe, for example, that a jury’s verdict is incorrect but not clearly erroneous, or that some statutes, even major ones, are hard to defend but not “irrational.”188 In any case, doctrines of deference ought not to be based on the psychological difficulties of judges. Perhaps Judge Breyer’s point should be taken as purely predictive—as a claim that judges are unlikely to follow the simplified version of Chevron generally or for major questions. If so, the path of the law is certainly a point in his favor; indeed, the decisions of the past twenty years suggest that he was uncannily prescient. But he clearly means his point as a normative one—as a challenge to the simplified reading of Chevron—and to this extent, the psychological point is neither here nor there.189 187 See note supra. 188 See, e.g., Williamson v. Lee Optical, 348 U.S. 483 (1955). 189 Perhaps Judge Breyer ought to be taken to be venturing a version of “ought implies can,” through the suggestion that the Supreme Court should not ask judges to maintain an attitude that is psychologically unrealistic. So stated, the argument is logical, but the use of lenient standards of review suggests that judges can indeed maintain that attitude. Chevron applies to jurisdictional disputes,190 and this particular Step Zero question remains unsettled in the lower courts.191 It is easy to understand the opposing views. Chevron rests on a theory of implied delegation, and perhaps Congress should not be taken to have intended to delegate to agencies the power to decide on the scope of their own authority. That question, it might be thought, ought to be answered by an independent institution, not by the agency itself. Thus Justice Brennan urged that judgments about jurisdiction “have not been entrusted to the agency” and might well “conflict with the agency's institutional interests in expanding its own power.”192 In his view, “agencies can claim no special expertise in interpreting a statute confining its jurisdiction,” and Congress cannot be presumed to ask “an agency to fill ‘gaps’ in a statute confining the agency's jurisdiction.”193 
On the other hand, any exemption of jurisdictional questions is vulnerable on two grounds. First, the line between jurisdictional and nonjurisdictional questions is far from clear, and hence any exemption threatens to introduce more complexity into the world of Chevron. Thus Justice Scalia argued that “there is no discernible line between an agency's exceeding its authority and an agency's exceeding authorized application of its authority.”194 Second, the theory that underlies Chevron might well support, rather than undermine, its application to jurisdictional questions. If an agency is asserting or denying jurisdiction over some area, it is either because democratic forces are leading it to do so or because its own specialized competence justifies its jurisdictional decision. Thus Justice Scalia urged that “Congress would naturally expect that the agency would be responsible, within broad limits, for resolving ambiguities in its statutory authority or jurisdiction.”195 Of course the claim about what “Congress would naturally expect” is a fiction; but perhaps it is the most useful one. 194 Id. at 345 (Scalia, J., concurring in the judgment). 195 Id. 190 Miss. Power & Light Co. v. Mississippi, 487 U.S. 357 (1988). 191 See, e.g., Alaska v. Babbitt, 72 F.3d 698 (9th Cir. 1995) (deferring on jurisdictional issue involving definition of “public lands”); Cavert Acquisition Corp. v. NLRB, 83 F.3d 598 (3d Cir. 1996) (deferring on jurisdictional issue involving definition of “employee”); United Trans. Union v. Surface Transp. Bd., 183 F.3d 606 (7th Cir. 1999) (refusing to defer on jurisdictional issue). A recent discussion can be found in NRDC v. Abraham, 355 F.3d 179 (2d Cir. 2004), in which the court, after finding a Step One violation, adds that “it seems highly unlikely that a responsible Congress would implicitly delegate to an agency the power to define the scope of its own power” — and then suggested that Mead (!) provided the appropriate framework. See id. at 199-200. 192 487 U.S. at 387 (Brennan, J., dissenting). 193 Id. 
Consider a prominent example. During the Clinton Administration, the EPA contended that it could assert jurisdiction over greenhouse gases under ambiguous provisions of the Clean Air Act.196 During the Bush Administration, the EPA contended that it could not do so.197 The difference is undoubtedly attributable to some combination of political commitments and readings of relevant evidence. If this is so, there is every reason, one might well think, to give deference to the agency’s jurisdictional judgment under Chevron. 
While the Court has not recently spoken to the question of jurisdiction, it has issued three decisions that bear on the Step Zero question whether Chevron applies to major questions. The meaning of these decisions is far from clear, and it remains to be seen whether they will be taken to carve out a distinct exception to Chevron. But the Court has given strong indications that this is what it means to do, in a way that has an unmistakable link with Judge Breyer’s distinction between incremental and major questions. Ironically, Justice Breyer dissented from the most important and explicit of these decisions—which borrowed, with citation, his central argument. 
(1) An “unlikely” delegation: MCI. The first case in the trilogy is MCI Telecommunications Corp. v. American Telephone and Telegraph.198 The 1934 Communications Act permits the Federal Communications Commission (FCC) to “modify” the statutory requirement that carriers file tariffs and charge customers in accordance with the tariffs that have been filed.199 As part of a program of partial deregulation, the FCC issued a regulation providing that only AT&T, as the dominant long-distance carrier, would have to file tariffs; other carriers need not do so. 
At first glance, this was a straightforward Step One question, and by a 6 to 3 vote, the Court held that the agency’s decision was unlawful under Step One. Much of Justice Scalia’s opinion for the Court emphasized the dictionary definition of “modify”—a 196 See Note, Trapped in the Greenhouse?, 54 DUKE L.J. 147 (2004); Note, Carbon Dioxide: A Pollutant in the Air, but Is the EPA Correct that It is Not an Air Pollutant?, 104 Colum. L. Rev. 1996 (2004). 197 Id.; see Memorandum from Robert E. Fabricant, EPA General Counsel, to Marianne L. Horinko, Acting EPA Administrator, at http://www.epa.gov/airlinks/co2petitiongcmemo8-28.pdf. 198 512 U.S. 218 (1994). 199 49 U.S.C. § 203. definition that, in the Court’s view, permitted only “moderate” or “modest” rather than fundamental change. But as Justice Stevens’s dissenting opinion pointed out, a longestablished meaning of “modify”—“to limit or reduce in extent or degree”—was fully compatible with the FCC’s decision. Perhaps, then, MCI could be taken as an odd case in which the Court found a Step One violation because of an emphasis, characteristic in Justice Scalia’s opinions, on what most dictionaries say. And perhaps Justice Scalia’s opinion should be taken as a vindication of his promise, in 1989, that as an advocate of “plain meaning” approaches to interpretation, he would be entirely willing to invoke Chevron Step One to strike down agency action. 
But the Court offered a strong clue that something else was involved. It noted that rate filings are “the essential characteristic of a rate-regulated industry.”200 In this light, it “is highly unlikely that Congress would leave the determination of whether an industry will be entirely, or even substantially, rate-regulated to agency discretion.”201 Thus the Court’s analysis of whether the agency’s decision was a “modification” was undertaken with reference to “the enormous importance to the statutory scheme of the tariff-filing provision.”202 This discussion, and the suggestion about what “Congress would leave” to the agency, seems to suggest that a kind of Step Zero inquiry might be involved, one that raised a question whether Congress intended to delegate this “enormous” question to a regulatory agency. And indeed, the Court’s emphasis on the important, and hardly interstitial, nature of the question at issue might easily be taken as a partial endorsement of Justice Breyer’s approach to the Chevron question. 
(2) Where’s Chevron?: “some degree of deference.” The Court provided a related clue in Babbitt v. Sweet Home Chapter of Communities for a Great Oregon.203 At issue was a provision of the Endangered Species Act that makes it unlawful to “take” a member of an endangered species.204 The word “take” is in turn defined to mean “harass, harm, pursue, hunt, shoot, wound, kill, trap, capture, or collect, or to attempt to engage in any such conduct.”205 By regulation, the Department of the Interior adopted a broad understanding of the word “take,” interpreting it to include “significant habitat 200 512 U.S. at 231. 201 Id. 202 Id. 203 515 U.S. 687 (1995). 204 16 U.S.C. § 1538(a)(1). 205 16 U.S.C. § 1532(19). modification or degradation where it actually kills or injures wildlife by significantly impairing essential behavioral patterns, including breeding, feeding, or sheltering.”206 Small landowners and logging companies challenged this definition as unlawfully broad. 
Two different opinions would have been unsurprising from the Court. The Court might have upheld the regulation by treating the case as a simple rerun of Chevron itself, involving an ambiguous term (“harm”) and a reasonable agency interpretation of that term. Alternatively, the Court might have struck down the regulation under Chevron Step One on the ground that the term “harm” appears in the context of verbs suggesting intentionality (“harass, pursue, hunt, short, wound, kill, trap, capture, or collect”). Perhaps the context forbids an interpretation that would include mere habitat modification and unintended injury to members of endangered species. And indeed, Justice Scalia, true to his account in 1989 and continuing his emphasis on dictionaries in MCI, was willing to find a “plain meaning” of the statute that prohibited the agency’s understanding.207 But the Court did neither of these things. Instead it embarked on its own, entirely independent construction of the statute, suggesting the correctness of the broad construction. For most of the Court’s opinion, it would be reasonable to ask: Where is Chevron? Why does the Court fail to point to statutory ambiguity and the agency’s interpretation? 
After parsing the statute independently, the Court turned to Chevron in a brief paragraph, noting (finally!) that Congress had “not unambiguously manifested its intent” to forbid the regulation.208 And then the Court added a singularly odd sentence: “The latitude the ESA gives the Secretary in enforcing the statute, together with the degree of regulatory expertise necessary to its enforcement, establishes that we owe some degree of deference to the Secretary’s reasonable interpretation.” At that point the Court did not cite Chevron or indeed any of its other decisions on the general question. Instead its citation reads, in full: “See Breyer, Judicial Review of Questions of Law and Policy, 38 Admin. L. Rev. 363, 373 (1986).”209 As it happens, p. 373 of this essay contains the heart of Judge Breyer’s attack on Chevron, on the ground that the proper judicial attitudes 206 50 C.F.R. 17.2 (1994). 207 515 U.S. at 714 (Scalia, J., dissenting). The Court might have invalidated the regulation by reference to some canon of construction, perhaps involving the protection of property rights; and indeed, Justice Scalia’s opinion contains an opening gesture in that direction. Id. 208 Id. at 703. 209 Id. at 703-04. toward review of questions of law cannot “be reduced to any single simple verbal formula.” 
The key paragraph in Sweet Home is cryptic, to be sure, but the Court’s opinion might well be read in a way that fits with the general approach in MCI. The scope of the words “take” and “harm” is not an interstitial question; it goes to the heart of the Endangered Species Act. Narrow definitions would constrict the range of the statutory ban; broad definitions work, in a sense, to expand the agency’s jurisdiction. For this reason, Sweet Home did not present the sort of minor question, involved in everyday administration, which Judge Breyer treated as the core case for judicial deference to agency interpretation. On the other hand, both expertise and accountability are relevant to interpretation of this provision of the Act, and a judgment about the breadth of the term “harm” certainly requires knowledge of the underlying facts. Unlike in MCI, the agency was not fundamentally altering any central feature of the statute. Hence “some degree of deference” was due. But the Court’s refusal to produce a simple Chevron opinion, and its only citation, appear to endorse Breyer’s position on the proper approach. 
(3) “Congress could not have intended to delegate”: Brown & Williamson. It might have been an overreaction to see MCI and Sweet Home as offering a serious qualification of Chevron, or as suggesting that major questions would be treated in any special way. But consider FDA v. Brown & Williamson.210 At issue there was whether the FDA had authority to regulate tobacco and tobacco products. The agency pointed to the statutory language, which defines drug to include “articles (other than food) intended to affect the structure or any function of the body.”211 It would certainly be plausible to argue that under this language and with the assistance of Chevron, the FDA could assert authority over tobacco. At the same time, it could have been concluded that this was a question of agency jurisdiction, for which Chevron deference was inapposite. But the Court took a far more complicated route. Much of its opinion emphasized the wide range of tobacco-specific legislation enacted by Congress in the last decades—legislation that, in the Court’s view, should “preclude an interpretation of the FDCA that grants the FDA authority to regulate tobacco products.”212 This argument had a degree of fragility, as the Court appeared to appreciate; repeals by implication are disfavored,213 and the Court’s failure to allow the agency to interpret ambiguous terms, merely because of subsequent legislation, was the equivalent of a finding of an implied repeal. 
Perhaps for this reason, the Court added a closing word. It said that its inquiry into the Step One question “is shaped, at least in some measure, by the nature of the question presented.”214 Chevron, the Court noted, is based on “an implicit delegation,” but in “extraordinary cases,” courts should “hesitate before concluding that Congress has intended such an implicit delegation.”215 Just as in Sweet Home, the Court cited no case for this key proposition, but instead resorted to only one source: Judge Breyer’s 1986 essay. On this occasion, however, it went beyond the citation to offer a quotation, encapsulating one of Judge Breyer’s central arguments, that there is a difference between “major questions,” on which “Congress is more likely to have focused,” and “interstitial matters.” At that point the Court drew a direct connection with MCI: “As in MCI, we are confident that Congress could not have intended to delegate a decision of such economic and political significance to an agency in so cryptic a fashion.”216 
Ironically, Justice Breyer dissented from the Court’s conclusion; even more ironically, he offered a powerful rebuttal to his own argument in 1986. He acknowledged the possibility that courts should accept a “background canon of construction” to the effect that decisions with enormous social consequences “should be made by democratically elected Members of Congress rather than by unelected agency administrators.”217 In this way he suggested, even more clearly than the Court, that some decisions, rejecting agency interpretations of statutes, might be rooted in nondelegation principles that reflect a reluctance to take ambiguous provisions as grants of “enormous” discretion to agencies. But he found any such background principle inapplicable to the problem at issue. The reason was that the decision to regulate tobacco is one for which the incumbent administration “must (and will) take responsibility.”218 212 529 U.S. at 157. 213 TVA v. Hill, 437 U.S. 153 (1978). 214 529 U.S. at 159. 215 Id. 216 Id at 160. 217 Id. at 190 (Breyer, J., dissenting). 218 Id. 
Because of its high visibility, that decision will inevitably be known to the public, and officials will be held accountable for it. “Presidents, just like Members of Congress, are elected by the public. Indeed, the President and Vice President are the only public officials whom the entire Nation elects.”219 An agency’s “decision of this magnitude— one that is important, conspicuous, and controversial” will inevitably face “the kind of public scrutiny that is essential in any democracy. And such a review will take place whether it is the Congress or the Executive Branch that makes the relevant decision.”220 
There is a close link between Justice Breyer’s arguments on this count and the Court’s emphasis on procedural safeguards in Mead. Those safeguards might be seen as a check on administrative arbitrariness, a check that reduces the need for independent judicial scrutiny of agency interpretations of law. So too, a high degree of public visibility, ensuring the operation of political safeguards, might be seen as a surrogate for independent judicial scrutiny. I believe that Justice Breyer is correct on this point and that his argument casts serious doubt on his own argument to the contrary in 1986. It is that issue to which I now turn. 
Suppose that Brown & Williamson is taken to suggest that Chevron deference is not due for agency decisions involving questions of great “economic and political significance.” This position would have exceedingly large implications. Return, for example, to an important and disputed question: Does the EPA have the authority to regulate greenhouse gases under the Clean Air Act?221 Let us simply stipulate that the relevant provisions of the Act are ambiguous. Under Chevron, the EPA would appear to have the power to regulate greenhouse gases if it chooses to do so. But under Mead and Brown & Williamson, it would be easy to argue that Congress, and not the EPA, should decide whether the EPA ought to be regulating greenhouse gases, a fundamental question 219 Id 220 Id. at 191. 221 See sources cited supra note 196. about the reach of federal environmental law. And so, in fact, the EPA’s General Counsel has argued under President Bush.222 
MCI and Brown & Williamson should be read to establish an independent Step Zero constraint on the application of Chevron, suggesting that certain fundamental questions must be resolved judicially rather than administratively. I believe that despite some of their language, MCI and Brown & Williamson should not be so understood; for the future, they are best regarded as Step One cases, not as Step Zero cases. The reason is that there is no justification for the conclusion that major questions should be resolved judicially rather than administratively. In fact, there are two problems with that conclusion. The first is that as with the distinction between jurisdictional and nonjurisdictional questions, the difference between interstitial and major questions is extremely difficult to administer; and if sensibly administered, it raises doubts about an array of decisions, including Chevron itself. The second is that expertise and accountability, the linchpins of Chevron’s legal fiction, are highly relevant to the resolution of major questions. 
Assume, for example, that the statutes in MCI and Brown & Williamson were genuinely ambiguous—that the relevant sources of interpretation could plausibly be read to support or to forbid the agency action at issue. If so, the argument for judicial deference would be exceptionally strong. In MCI, the FCC was deciding fundamental questions about the structure of the telecommunications market—hardly an issue for judicial resolution, and one for which expertise and accountability are relevant. In Brown & Williamson, the FDA was taking action against one of the nation’s most serious public health problems, in a judgment that had a high degree of public visibility and required immersion in the subject at hand. Perhaps Congress could not easily be taken to delegate the resolution of these questions to administrative agencies. But would it really be better to understand Congress to have delegated the resolution of those questions to federal courts? I have referred to the concern that on major questions, interest-group power and agency self-dealing might produce a real risk, one that is sufficient to call for a reduced degree of judicial deference. But that concern is not well-grounded in any empirical 222 See Memorandum from Robert E. Fabricant, supra note 197. literature, and in any case the standard Chevron framework provides ample constraints on bias and self-dealing. 
Brown & Williamson should not be understood to say that major questions will be resolved by courts rather than agencies. They should be taken to impose a more powerful limit on administrative discretion, in the form of a background principle to the effect that in the face of ambiguity, agencies will be denied the power to interpret ambiguous provisions in a way that would massively alter the preexisting statutory scheme.223 
On this view, Mead and Brown & Williamson are not Step Zero decisions; they are discretion-denying decisions. They do not say that courts, rather than agencies, will interpret ambiguities. They announce, far more ambitiously, that ambiguities will be construed so as to reduce the authority of regulatory agencies. But it would not much matter if this principle is described in terms of Chevron Step Zero or Chevron Step One. The key idea is that agencies would not receive deference when they attempt to exercise their authority in ways that produce large-scale changes in the structure of the statutory programs that they are administering. 
The best justification for this conclusion would rely on an analogy. In some cases, well-established background principles operate to “trump” Chevron. Agencies are not permitted to interpret statutes so as to apply beyond the territorial boundaries of the United States.224 Nor are they allowed to interpret ambiguous statutes to apply retroactively.225 An agency cannot construe an ambiguous statute so as to raise serious constitutional doubts.226 In these and other contexts, courts have insisted on a series of nondelegation canons, which require legislative rather than merely executive deliberation on the issue in question.227 Congress will not lightly be taken to have delegated to agencies the choice of how to resolve certain sensitive questions. Perhaps MCI and Brown & Williamson can be understood to build on these nondelegation canons to 223 For a valuable discussion, see John Manning, The Nondelegation Doctrine as A Canon of Avoidance, 2000 S. CT. REV. 223. 224 EEOC v Arabian American Oil Co, 499 U.S. 244, 248 (1991). 225 Bowen v Georgetown University Hospital, 488 U.S. 204, 208 (1988). 226 See id. at 208–09 (1988). 227 For general discussion, see Cass R. Sunstein, Nondelegation Canons, 67 U. CHI. L. REV. 315 (2000). suggest a more general principle: Fundamental alterations in statutory programs, in the form of contractions or expansions, will not be taken to be within agency authority.228 
For those who are enthusiastic about the nondelegation doctrine, this background principle will have considerable appeal, above all because the principle requires Congress, rather than agencies, to decide critical questions of policy (including, plausibly, the question whether significant deregulation of communications should occur, or whether the FDA should be authorized to regulate tobacco produces). But the principle faces three problems. The first is the uncertain foundations of the argument for the nondelegation doctrine itself.229 As a matter of text and history, the doctrine does not have a clear constitutional pedigree, and to say the least, it is controversial to base a principle of statutory construction on a doctrine that cannot easily be rooted in the Constitution. The second is the difficulty of administering the line that the principle would require courts to maintain. As I have emphasized, the distinction between major questions and non-major ones lacks a metric. On its facts, Chevron itself might seem to be wrong, and perhaps Sweet Home as well. 
The third and most fundamental problem is that expertise and accountability are entirely relevant to questions about contraction or expansion of statutory provisions.230 If a nondelegation principle is meant to prevent agencies from making significant alterations in statutory programs on their own, in a way that goes beyond the ordinary operation of Step One, it would embed an unhealthy status quo bias into administrative law. MCI could well be understood as embodying such a bias. But because regulatory programs last for decades, and across significantly changed circumstances, agencies should be taken to have the discretion to construe ambiguities reasonably, even if their constructions lead to large changes in the statute that they are administering. Indeed, this flexibility is a primary benefit of Chevron itself, allowing adaptation to new understandings of both facts and values. 
But suppose that the nondelegation principle is understood not to include a general status quo bias, but simply to ban agencies from expanding their authority, again in a way that goes beyond the ordinary operation of Step One. If so, then it is a modern 228 Cf. Manning, supra note 223 (discussing and challenging the use of nondelegation principles as a tool of statutory construction). 229 See Eric A. Posner & Adrian Vermeule, Interring the Nondelegation Doctrine, 69 U. CHI. L. REV. 1721 (2002). 230 See Manning, supra note 223. version of the old (and discredited) idea that statutes in derogation of the common law should be narrowly construed.231 Brown & Williamson could be understood to reflect that idea. If so, it should be repudiated. In the modern state, agencies should not have the authority to expand their authority in violation of statutory limitations. But so long as agencies are reasonably interpreting statutory ambiguities, they ought to receive deference under Chevron, at least if their interpretation does not violate a particular interpretive principle, such as the principles against extraterritoriality, retroactivity, and serious constitutional doubts. 
I conclude that MCI and Brown & Williamson are best read as Step One decisions. Despite the more general language that I have explored here, they should not be taken to suggest an additional reason to deny deference under Step Zero. 
In 1984, it was not entirely clear whether Chevron was a synthesis of existing law, as the Court appeared to believe at the time,232 or instead a genuine revolution, signaling a new era in the relationship between courts and regulatory agencies. Justice Scalia saw its revolutionary potential and sought to justify a broad reading of the decision as well-suited to the realities of modern government, above all by virtue of its clarity and simplicity. Judge Breyer, also a long-time specialist in administrative law, sought to domesticate the decision and to treat it as a codification of the best of existing practice, which called for caseby-case inquiries into the fictional instructions of rational legislators. The most ambitious readings of Chevron see it as a recognition that resolution of statutory ambiguities often calls for judgments of both policy and principle, and as a firm suggestion that such judgments should be made by administrators rather than judges. So understood, Chevron is a natural outgrowth of the twentieth-century shift from judicial to administrative lawmaking. 231 See FTC v. American Tobacco Co., 264 U.S. 298, 305-06 (1924); FTC v. Eastman Kodak, 264 U.S. 619, 623-25 (1920); Jefferson Fordham & J. Russell Leach, Interpretation of Statutes In Derogation of the Common Law, 3 VAN. L. REV. 438 (1950). 232 See Percival, supra note 2. 
For the most part, current disagreements have taken the form of a dispute over Chevron Step Zero—the inquiry into whether the Chevron framework applies at all. Justice Breyer has succeeded in ensuring case-by-case assessments of whether Congress intended to delegate law-interpreting power to agencies. To be sure, those assessments are less case-by-case than he suggested that they should be in 1986; if the agency action has the force of law, Chevron applies, and agency decisions that result from formal procedures are taken to have the force of law.233 But even if agency action lacks the force of law and is not a product of formal procedures, Chevron might apply on the basis of an inquiry into the particular nature of the decision at stake. After Mead and Barnhart, then, Chevron seems to supply a clear rule, but only in the domain of agency decisions having the force of law. The Christensen-Mead-Barnhart trilogy thus represents a significant triumph for Justice Breyer’s efforts to domesticate Chevron. 
At the same time, Sweet Home, MCI, and Brown & Williamson seem to be Step Zero decisions in Step One guise.234 They are informed, and explicitly so, by a doubt about whether Congress should generally be taken to have given agencies the authority to restructure administrative schemes, either by significantly reducing or significantly expanding their nature and coverage. 
These restrictions on the reach of Chevron create a great deal of complexity, and in a way that disregards the best justifications for the deference rule. The Court seems to have opted for standards for rules in precisely the context in which rules make most sense: numerous and highly repetitive decisions in which little is to be gained, in terms of accuracy, by a more particularized approach. Because the scope of review is a threshold issue is nearly every administrative law case, the rise of sustained controversy over the meaning of Step Zero introduces needless uncertainty. I have suggested that courts can handle the initial Step Zero trilogy either by noticing that the choice between Chevron and Skidmore usually will not matter, or by treating most cases as Barnhart cases rather than as Christensen or Mead cases. Just as Mead threatens to domesticate Chevron, future courts can use Barnhart to domesticate Mead. I have also suggested that MCI and Brown & Williamson, if rightly decided, are best read as Step One cases; it follows that future 233 As I have noted, some agency action does not have the force of law even if based on formal procedures; the decisions of the NLRB are examples. 234 Cf. Manning, supra note 223 (exploring Brown & Williamson in nondelegation terms). courts should downplay the Court’s unnecessary emphasis on what Congress could not have meant to delegate. That emphasis threatens to give courts a kind of interpretive primacy with respect to the very questions for which the Chevron framework is best suited. 
Constraints on administrative discretion, rooted in the rule of law, remain a central part of administrative law, and indeed serve to give that subject its basic point. But those constraints can and should be supplied through other means, above all through an emphasis on the limitations recognized in Chevron itself. Sometimes legal epicycles are necessary to ensure against the arbitrariness introduced by inflexible rules. But in this context, the extraordinary complexity introduced by the emerging law of Step Zero serves no useful purpose. Professor Cass R. Sunstein University of Chicago Law School 1111 East 60th Street Chicago, IL 60637 csunstei@uchicago.edu  2.  William M. Landes, Copyright Protection of Letters, Diaries and Other Unpublished Works: An  Economic Approach (July 1991)  Richard A. Epstein, The Path to The T. J. Hooper: The Theory and History of Custom in the Law of  Tort (August 1991)  Cass R. Sunstein, On Property and Constitutionalism (September 1991)  Richard A. Posner, Blackmail, Privacy, and Freedom of Contract (February 1992)  Randal C. Picker, Security Interests, Misbehavior, and Common Pools (February 1992)  Tomas J. Philipson & Richard A. Posner, Optimal Regulation of AIDS (April 1992)  Douglas G. Baird, Revisiting Auctions in Chapter 11 (April 1992)  William M. Landes, Sequential versus Unitary Trials: An Economic Analysis (July 1992)  William M. Landes & Richard A. Posner, The Influence of Economics on Law: A Quantitative Study  (August 1992)  Alan O. Sykes, The Welfare Economics of Immigration Law: A Theoretical Survey With An  Analysis of U.S. Policy (September 1992)  Douglas G. Baird, 1992 Katz Lecture: Reconstructing Contracts (November 1992)  Gary S. Becker, The Economic Way of Looking at Life (January 1993)  J. Mark Ramseyer, Credibly Committing to Efficiency Wages: Cotton Spinning Cartels in Imperial  Japan (March 1993)  Cass R. Sunstein, Endogenous Preferences, Environmental Law (April 1993)  Richard A. Posner, What Do Judges and Justices Maximize? (The Same Thing Everyone Else Does)  (April 1993)  Lucian Arye Bebchuk and Randal C. Picker, Bankruptcy Rules, Managerial Entrenchment, and  Firm‐Specific Human Capital (August 1993)  J. Mark Ramseyer, Explicit Reasons for Implicit Contracts: The Legal Logic to the Japanese Main  Bank System (August 1993)  William M. Landes and Richard A. Posner, The Economics of Anticipatory Adjudication  (September 1993)  Kenneth W. Dam, The Economic Underpinnings of Patent Law (September 1993)  Alan O. Sykes, An Introduction to Regression Analysis (October 1993)  Richard A. Epstein, The Ubiquity of the Benefit Principle (March 1994)  Randal C. Picker, An Introduction to Game Theory and the Law (June 1994)  William M. Landes, Counterclaims: An Economic Analysis (June 1994)  J. Mark Ramseyer, The Market for Children: Evidence from Early Modern Japan (August 1994)  Robert H. Gertner and Geoffrey P. Miller, Settlement Escrows (August 1994)  Kenneth W. Dam, Some Economic Considerations in the Intellectual Property Protection of  Software (August 1994)  Cass R. Sunstein, Rules and Rulelessness, (October 1994)  David Friedman, More Justice for Less Money: A Step Beyond Cimino (December 1994)  Daniel Shaviro, Budget Deficits and the Intergenerational Distribution of Lifetime Consumption  (January 1995)  Douglas G. Baird, The Law and Economics of Contract Damages (February 1995)  Daniel Kessler, Thomas Meites, and Geoffrey P. Miller, Explaining Deviations from the Fifty  Percent Rule: A Multimodal Approach to the Selection of Cases for Litigation (March 1995)  Geoffrey P. Miller, Das Kapital: Solvency Regulation of the American Business Enterprise (April  1995)  Richard Craswell, Freedom of Contract (August 1995)  J. Mark Ramseyer, Public Choice (November 1995)  Kenneth W. Dam, Intellectual Property in an Age of Software and Biotechnology (November 1995)  Cass R. Sunstein, Social Norms and Social Roles (January 1996)  37.  
David A. Weisbach, Measurement and Tax Depreciation Policy: The Case of Short‐Term Assets  (January 2003)  Randal C. Picker, Understanding Statutory Bundles: Does the Sherman Act Come with the 1996  Telecommunications Act? (January 2003)  Douglas Lichtman and Randal C. Picker, Entry Policy in Local Telecommunications: Iowa Utilities  and Verizon (January 2003)  William Landes and Douglas Lichtman, Indirect Liability for Copyright Infringement: An  Economic Perspective (February 2003)  Cass R. Sunstein, Moral Heuristics (March 2003)  Amitai Aviram, Regulation by Networks (March 2003)  Richard A. Epstein, Class Actions: Aggregation, Amplification and Distortion (April 2003)  Richard A. Epstein, The “Necessary” History of Property and Liberty (April 2003)  Eric A. Posner, Transfer Regulations and Cost‐Effectiveness Analysis (April 2003)  Cass R. Sunstein and Richard H. Thaler, Libertarian Paternalizm Is Not an Oxymoron (May 2003)  Alan O. Sykes, The Economics of WTO Rules on Subsidies and Countervailing Measures (May  2003)  Alan O. Sykes, The Safeguards Mess: A Critique of WTO Jurisprudence (May 2003)  Alan O. Sykes, International Trade and Human Rights: An Economic Perspective (May 2003)  Saul Levmore and Kyle Logue, Insuring against Terrorism—and Crime (June 2003)  Richard A. Epstein, Trade Secrets as Private Property: Their Constitutional Protection (June 2003)  Cass R. Sunstein, Lives, Life‐Years, and Willingness to Pay (June 2003)  Amitai Aviram, The Paradox of Spontaneous Formation of Private Legal Systems (July 2003)  Robert Cooter and Ariel Porat, Decreasing Liability Contracts (July 2003)  David A. Weisbach and Jacob Nussim, The Integration of Tax and Spending Programs (September  2003)  William L. Meadow, Anthony Bell, and Cass R. Sunstein, Statistics, Not Memories: What Was the  Standard of Care for Administering Antenatal Steroids to Women in Preterm Labor between 1985  and 2000? (September 2003)  Cass R. Sunstein, What Did Lawrence Hold? Of Autonomy, Desuetude, Sexuality, and Marriage  (September 2003)  Randal C. Picker, The Digital Video Recorder: Unbundling Advertising and Content (September  2003)  Cass R. Sunstein, David Schkade, and Lisa Michelle Ellman, Ideological Voting on Federal Courts  of Appeals: A Preliminary Investigation (September 2003)   Avraham D. Tabbach, The Effects of Taxation on Income Producing Crimes with Variable Leisure  Time (October 2003)  Douglas Lichtman, Rethinking Prosecution History Estoppel (October 2003)  Douglas G. Baird and Robert K. Rasmussen, Chapter 11 at Twilight (October 2003)  David A. Weisbach, Corporate Tax Avoidance (January 2004)  David A. Weisbach, The (Non)Taxation of Risk (January 2004)  Richard A. Epstein, Liberty versus Property? Cracks in the Foundations of Copyright Law (April  2004)  Lior Jacob Strahilevitz, The Right to Destroy (January 2004)  Eric A. Posner and John C. Yoo, A Theory of International Adjudication (February 2004)  Cass R. Sunstein, Are Poor People Worth Less Than Rich People? Disaggregating the Value of  Statistical Lives (February 2004)  Richard A. Epstein, Disparities and Discrimination in Health Care Coverage; A Critique of the  Institute of Medicine Study (March 2004)  Richard A. Epstein and Bruce N. Kuhlik, Navigating the Anticommons for Pharmaceutical Patents:  Steady the Course on Hatch‐Waxman (March 2004)  Richard A. Esptein, The Optimal Complexity of Legal Rules (April 2004)  Eric A. Posner and Alan O. Sykes, Optimal War and Jus Ad Bellum (April 2004)  Alan O. Sykes, The Persistent Puzzles of Safeguards: Lessons from the Steel Dispute (May 2004)    
Luis Garicano and Thomas N. Hubbard, Specialization, Firms, and Markets: The Division of Labor  within and between Law Firms (April 2004)  Luis Garicano and Thomas N. Hubbard, Hierarchies, Specialization, and the Utilization of  Knowledge: Theory and Evidence from the Legal Services Industry (April 2004)  James C. Spindler, Conflict or Credibility: Analyst Conflicts of Interest and the Market for  Underwriting Business (July 2004)  Alan O. Sykes, The Economics of Public International Law (July 2004)  Douglas Lichtman and Eric Posner, Holding Internet Service Providers Accountable (July 2004)  Shlomo Benartzi, Richard H. Thaler, Stephen P. Utkus, and Cass R. Sunstein, Company Stock,  Market Rationality, and Legal Reform (July 2004)  Cass R. Sunstein, Group Judgments: Deliberation, Statistical Means, and Information Markets  (August 2004, revised October 2004)  Cass R. Sunstein, Precautions against What? The Availability Heuristic and Cross‐Cultural Risk  Perceptions (August 2004)  M. Todd Henderson and James C. Spindler, Corporate Herroin: A Defense of Perks (August 2004)  Eric A. Posner and Cass R. Sunstein, Dollars and Death (August 2004)  Randal C. Picker, Cyber Security: Of Heterogenity and Autarky (August 2004)  Randal C. Picker, Unbundling Scope‐of‐Permission Goods: When Should We Invest in Reducing  Entry Barriers? (September 2004)  Christine Jolls and Cass R. Sunstein, Debiasing through Law (September 2004)  Richard A. Posner, An Economic Analysis of the Use of Citations in the Law (2000)  Cass R. Sunstein, Cost‐Benefit Analysis and the Environment (October 2004)  Kenneth W. Dam, Cordell Hull, the Reciprocal Trade Agreement Act, and the WTO (October 2004)  Richard A. Posner, The Law and Economics of Contract Interpretation (November 2004)  Lior Jacob Strahilevitz, A Social Networks Theory of Privacy (December 2004)  Cass R. Sunstein, Minimalism at War (December 2004)  Douglas Lichtman, How the Law Responds to Self‐Help (December 2004)  Eric A. Posner, The Decline of the International Court of Justice (December 2004)  Eric A. Posner, Is the International Court of Justice Biased? (December 2004)  Alan O. Sykes, Public vs. Private Enforcement of International Economic Law: Of Standing and  Remedy (February 2005)  Douglas G. Baird and Edward R. Morrison, Serial Entrepreneurs and Small Business Bankruptcies  (March 2005)  Eric A. Posner, There Are No Penalty Default Rules in Contract Law (March 2005)  Randal C. Picker, Copyright and the DMCA: Market Locks and Technological Contracts (March  2005)  Cass R. Sunstein and Adrian Vermeule, Is Capital Punishment Morally Required? The Relevance of  Life‐Life Tradeoffs (March 2005)  Alan O. Sykes, Trade Remedy Laws (March 2005)  Randal C. Picker, Rewinding Sony: The Evolving Product, Phoning Home, and the Duty of  Ongoing Design (March 2005)  Cass R. Sunstein, Irreversible and Catastrophic (April 2005)   James C. Spindler, IPO Liability and Entrepreneurial Response (May 2005)  Douglas Lichtman, Substitutes for the Doctrine of Equivalents: A Response to Meurer and Nard  (May 2005)  Cass R. Sunstein, A New Progressivism (May 2005)  Douglas G. Baird, Property, Natual Monopoly, and the Uneasy Legacy of INS v. AP (May 2005)  Douglas G. Baird and Robert K. Rasmussen, Private Debt and the Missing Lever of Corporate Governance (May 2005) Cass R. Sunstein, Administrative Law Goes to War (May 2005) Cass R. Sunstein, Chevron Step Zero (May 2005) 
On January 11, 1944, Fraklin Delano Roosevelt delivered one of the most influential speeches of the twentieth century. With victory in World War II on the horizon, he contended that "the one supreme objective for the future," the objective for all nations, was captured "in one word: Security." This term meant "not only physical security which provides safety from attacks by aggressors," but also "economic security, social security, moral security." Building on this claim, Roosevelt contended that the United States had come to accept "a second Bill of Rights under which a new basis of security and prosperity can be established for all--regardless of station, race, or creed." This Second Bill of Rights would include, among other things, the right to a good education; the right to freedom from unfair competition and domination by monopolies; the right to a decent home; the right to adequate medical care; the right to adequate protection from the fears associated with old age, sickness, accident, and unemployment; and the fight to a useful and remunerative job. The influence of Roosevelt's speech can be seen in its impact on the Universal Declaration of Human Rights, which includes the rights he championed. The same influence appears in dozens of the world's constitutions, which borrow directly from Roosevelt's Second Bill of Rights. Even the interim Constitution of Iraq, written with a great deal of American influence, shows Roosevelt's influence, boldly proclaiming that "the individual has the right to security, education, health care, and social security." In The Second Bill of Rights: FDR's Constitutional Vision and Why We Need It More Than Ever (New York: Basic, 2004), my first goal was to recover a lost part of U.S. history--to trace the origins of social and economic rights in American culture and to understand the historical background that led Roosevelt to propose his Second Bill. Too often, Americans (and Europeans) see the United States as committed to a form of individualism that is incompatible with rights of the sort that Roosevelt championed. Simply as a matter of history, this understanding is a blunder; one of America's greatest presidents sought those rights precisely in the name of individualism. Indeed, he saw those rights as a supplement to and compatible with what he described as the "sacred possessive rights" of property and freedom of contract, which preceded them. 
In recovering Roosevelt's Second Bill, I had two other goals. I wanted to give a sympathetic account of the early-twentieth-century American attack on the distinction between negative and positive rights--an attack that found its way into the White House. Over a period of years, many people argued that the so-called negative rights, including the right to private property and the right to freedom of contract, depended for their existence on law, legal institutions, and aggressive government. Without government protection of property, people cannot enjoy property, in the way that a capitalist society requires. In this sense, property is itself a positive right, and so too is contractual liberty. It is all very well to say that people should be able to make contracts as they choose (subject to the limitations of the criminal law), but without a legal system to enforce contractual agreements, contractual liberty is often meaningless. Those who made these arguments did not mean to attack capitalism, to which they were committed, but they thought that legal rights should be accepted, or not, after an inquiry into their functions in promoting human well-being. That inquiry should not be clouded by the effort, a conceptual disaster from the legal point of view, to distinguish negative rights from positive rights. 
In my view, Roosevelt's Second Bill was made possible in part by claims of this kind. As early as 1932, Roosevelt proposed "an economic declaration of rights, an economic constitutional order," that would recognize that "every man has a right to live," which entailed "a right to make a comfortable living." In making this argument, Roosevelt stressed that even "[Thomas] Jefferson realized that the exercise of the property rights might so interfere with the rights of the individual that the government, without whose assistance the property rights could not exist, must intervene, not to destroy individualism but to protect it." The key point here is that "property rights could not exist" without government, which makes such rights real in the world. Note in this regard that government intervention--in Roosevelt's view--is necessary to protect individualism rather than to destroy it. 
Roosevelt did not believe that the Constitution should be amended to include the Second Bill of Rights. He was generally skeptical about both constitutional change and federal courts. He thought that the Second Bill should be seen as akin to the Declaration of Independence--as a statement of basic commitments to which the citizenry and its representatives would look in deciding on its basic obligations. 
I am grateful to Max Hocutt for his courteous and careful treatment of my argument. Before I get to the central point, let me offer a few clarifications. Hocutt is correct to say that people can have moral rights that lack legal backing. My friends have a right to be treated with respect, but that right is not legally enforceable. Hocutt is also correct to say that moral sanctions for rights violations can be at least as strong as legal ones. My argument is not meant to take a stand on the question of moral rights, so it is not entirely clear what Hocutt seeks to accomplish by his emphasis on the existence of legally unenforceable moral rights. Does he mean to say that people might have a moral right to, say, education and housing without also having a legal right to these goods? Does he mean to say that their moral rights should be satisfied by private charity and entrepreneurship rather than by government? If so, Roosevelt's only objection (and mine) is intensely pragmatic: private steps are most unlikely to do what needs to be done. In U.S. history, and in other nations' histories, private actions have never come close to providing people with decent opportunities and minimal security. In Roosevelt's words, "Government has a final responsibility for the well-being of its citizenship. If private co-operative endeavor fails to provide work for willing hands and relief for the unfortunate, those suffering hardship from no fault of their own have a right to call upon the Government for aid; and a government worthy of its name must make fitting response." In my view, Roosevelt was right. 
Of course, this point is not meant as a criticism of voluntary private efforts to help or of government efforts to enlist moral suasion and economic incentives in the interest of relieving human suffering. Those who believe in the Second Bill of Rights should be entirely content if private efforts greatly reduce the need for governmental help. (Incidentally, Hocutt is wrong to say that I admire European social democracies; I don't.) For the future, it would be best to emphasize Roosevelt's goals, but also to seek less-rigid, more market-friendly and incentive-based methods for achieving those goals. Consider, for example, the earned income tax credit, which is far better than the minimum wage as a method for responding to the problems that the working poor face. A great deal of creativity--on the left, right, and center--has already been devoted to market-friendly methods of assisting those in need; we need many more such efforts. 
Hocutt's principal objection, however, lies elsewhere. He believes that positive rights do indeed differ from negative rights in the sense that the former impose "weightier and more irksome duties." This "difference in weight," in his view, imposes a heavy burden of justification on "all who advocate adding positive economic rights to our constitutionally protected list of negative political rights." Hocutt emphasizes here what he sees as Locke's view that rights to life, liberty, and property reflect a purely negative duty, to the effect that other people must refrain from interfering with them. Hence, "Locke's was a scheme of purely negative rights," as is "the scheme of rights set out in the U.S. Constitution." In this view, "people are called on not to do things, but rather to refrain from doing things, and the government's most important job is to see that they behave accordingly." Also according to Hocutt, Locke was a social-contract theorist who saw a central truth, which is "that positive rights differ from negative rights in that (1) they impose more burdensome and onerous duties, and (2) their protection is less essential to the maintenance of public order." Thus, it is "more irksome" to be asked to feed another man's children than to be told not to take food out of their mouths; it is similarly more irksome to share my house with you than to be told to refrain from taking your house. Hocutt concludes that the real trouble with positive rights is that they legitimate "a system of government coercion to expropriate the earnings of the talented, the hard-working, and the fortunate for redistribution to the untalented, the indigent, the irresponsible, and the unlucky." This argument seems to me deeply puzzling. To see why, let us begin with the U.S. Constitution. It is simply not correct to say that our founding document creates "purely negative rights" in the sense of a duty of noninterference. The Sixth and Seventh Amendments create a right to a jury trial, which transparently requires government to create and taxpayers to fund an expensive (and sometimes irksome) apparatus, affirmatively requiring citizens and taxpayers to act, not simply to refrain from acting. Article I, section 10, forbids any state from enacting a "Law impairing the Obligation of Contracts." This provision does not require the government or ordinary citizens to refrain from acting. It requires governments to ensure the availability of a court system that is ready and able to enforce contractual agreements. It also requires ordinary citizens to comply with their contractual obligations. (We can define that duty as an obligation to "refrain" from violating other people's rights, but is it really helpful to adopt that definition?) Or consider the Fifth Amendment's prohibition on the "taking" of private property except for "public use" and with just compensation. If the government abolished the trespass laws, in whole or in part, it would be violating the Fifth Amendment, which requires government to protect property rights, not to refrain from acting. 
Hocutt asserts that "freedom of speech is a right not to be prevented from saying what one thinks," but he offers an incomplete account of constitutional law. Freedom of speech also entails a right to the existence and flee availability of public parks and streets, which must be open for expressive activity at taxpayer expense (subject to restrictions of reasonable time, place, and manner). In numerous places, the government is under an affirmative duty to safeguard rights. Perhaps Hocutt will respond that many (not all) of these examples merely require government to ensure that some people do not interfere with other people--that government is merely protecting our rights to immunity from interference by others. If he makes that response, we should notice right away that he is acknowledging one of my central points, which is that government is under a positive duty to protect certain rights. He should not be embarrassed by that acknowledgment. But there is a much deeper problem. What exactly are the (prelegal) rights that government must affirmatively protect from private interference? Hocutt's major answer seems to be that those rights, which he calls negative, do not impose "weightier and more irksome duties." But this answer is more puzzling still. How do we know whether a duty is weighty and irksome? Is this matter an empirical question? If it is, Hocutt's judgment is difficult to defend. If you are wealthy, it is not very irksome to be told that you must give twenty dollars each month to the relief of poverty or to feed someone else's child. If you are homeless, it is extremely irksome to be told that you cannot use someone else's house. Hocutt wants to accept the more conventional civil and political rights and to reject a right to decent opportunity and minimal security (which Roosevelt favored), but the ideas of "irksomeness" and "weight" do not come close to justifying Hocutt's 
Hocutt does have another argument. He thinks that protection of the Second Bill of Rights "is less essential to the maintenance of public order." This also appears to be an empirical claim, and Hocutt does not say enough to defend it. Is the right to a jury trial more essential to the maintenance of public order than the right to education? Is the right to medical care less essential to the maintenance of public order than the "public-use" requirement of the Fifth Amendment? What definition of "public order" will enable us to answer these questions? Still, grant Hocutt his premise and suppose, for example, that the right to freedom from unreasonable searches and seizures does in fact contribute more to the maintenance of public order than the right to a decent home. So what? It would be perfectly plausible to say that government should protect the latter right as best it can, even though public order is less jeopardized by violation of this right than by violations of other rights. 
I suspect that Hocutt's position has nothing to do with irksomeness or with maintenance of public order. Nor is it at all helpful to say that he is concerned with protecting rights of noninterference. What counts as "noninterference" depends on our antecedent theory of entitlement. If people have a right to food and housing, then the failure to provide food and housing is "interference." If people have a right to property, then takings of land count as "interference." What is Hocutt's theory of entitlement? Clearly he thinks that the conventional rights are real and that Roosevelt's are not--that Roosevelt was actually proposing to violate fights, properly understood, by calling for expropriation and by "surrender[ing] to government the right to seize the property, restrict the liberty, and diminish the lives of some people in order to promise economic security to others." This issue, I think, is the heart of the matter. Here, however, Hocutt is left with an assertion, not an argument. If government protects the fights to an education and to freedom from monopoly and unfair competition, whose property is being unacceptably "seized"? If government takes steps to ensure that everyone has adequate medical care or decent food and clothing, why must those steps involve an unacceptable seizure of property or restriction of liberty? Hocutt must believe that people have some kind of moral or prepolitical right to the holdings that they currently have, but he has said nothing to establish the existence of such a right. 
These claims should not be misunderstood. Roosevelt rejected socialism, and he was no social democrat. He was firmly committed to capitalist institutions; moreover, he was committed to experimentation. He knew that those who seek economic equality will end up with neither equality, nor prosperity. His plea for a Second Bill of Rights was a commitment to ends, not means. Market-friendly approaches, relying heavily on the private sphere, are to be welcomed if they work. Like Roosevelt, I do not favor judicial enforcement of the Second Bill of Rights, certainly not for the United States. To defend any set of policy initiatives, it is necessary to examine their consequences, including their incentive effects. When I argue that all rights are positive, I mean only to say that in order for rights to be realized, government's presence, not its abstinence, is required. Those who purport to reject "government interference" depend on it every day of every year. 
These claims are not incompatible with the view that the rights to private property and freedom of contract are extremely important or that they have some special moral status. We can accept the Second Bill of Rights on various grounds without questioning a wide range of reasonable views about the foundations of moral and political rights. But we must reject the suggestion that people have some kind of moral entitlement to whatever holdings they have now. Roosevelt was right to find that view preposterous. The sooner we are rid of it, the better. 
Cass R. Sunstein is the Karl N. Llewellyn Distinguished Service Professor in the Law School and the Department of Political Science at the University of Chicago. 
COPYRIGHT 2005 Independent Institute Copyright 2005 Gale, Cengage Learning. All rights reserved. 
Some judges are less ambitious than others; they have minimalist tendencies. Minimalists are unambitious along two dimensions. First, they seek to rule narrowly rather than broadly. In a single case, they do not wish to resolve other, related problems that might have relevant differences. They are willing to live with the costs and burdens of uncertainty, which they tend to prefer to the risks of premature resolution of difficult issues. Second, minimalists seek to rule shallowly rather than deeply, in the sense that they favor arguments that do not take a stand on the foundational debates in law and politics. They prefer incompletely theorized agreements, by which diverse people, from their different perspectives, can unite behind modest rather than immodest theorizing. They believe that such agreements recognize the difficulty of resolving foundational debates, and that they also allow people, including judges, to show one another a large measure of mutual respect. 
In prominent cases, some judges favor minimalism, and others do not. Justice O'Connor, for example, has often shown a preference for case-bycase judgments that leave the most difficult questions for another day.3 Justice Scalia, by contrast, is no minimalist.4 He endorses an ambitious theory of constitutionalism-"originalism"-and he often uses that theory to decide cases.5 Much of the time, he prefers to rule broadly rather than narrowly, because of his preference for rule-bound judgments that give clear guidance for the future. Of course minimalism and maximalism should be seen as relative rather than absolute. Justice O'Connor is a minimalist, much of the time, but she does not always follow a minimalist path, and even when she does, she does not say that her rulings are limited to people with the same initials as the parties to the particular litigation. * 
Karl N. Llewellyn Distinguished Service Professor of Jurisprudence, University of Chicago Law School and Department of Political Science. A.B. 1975, J.D. 1978, Harvard.-Ed. The pragmatic judge tends to favor narrow over broad grounds of decision in the early stages in the development of a legal doctrine.... What the judge has before him is the facts of the particular case, not the facts of future cases. He can try to imagine what those cases will be like, but the likelihood of error in such an imaginative projection is great. Working outward, in stages, from the facts before him to future cases with new facts that may suggest the desirability of altering the contours of the applicable rules, the judge avoids premature generalization 
See Cass R. Sunstein, Incompletely Theorized Agreements, 103 HARV. L. REV. 1733 (1995). Justice Scalia has maximalist tendencies, but he does not try, in a single equal protection case, to resolve all imaginable equal protection cases. 
I am extremely grateful to Neil Siegel for his generous and careful analysis of my claims about minimalism and the Supreme Court.6 Siegel offers three objections to those claims.7 First, he contends that minimalism has not been precisely defined, and that the presence of diverse and inconsistent definitions makes it difficult to test the claim that any particular court, or any particular decision, is minimalist in character. Second, Siegel claims that the most usable definition of minimalism is palpably inconsistent with Court's behavior during the 2003 term (and probably more generally). Third, he contends that minimalism, suitably defined, is unattractive, among other things because it violates the Supreme Court's roles as guide and as guardian. These are instructive objections. But the disagreement between Siegel and me is smaller (more minimal!) than it appears, and I hope that a few clarifying remarks will help to illuminate both the nature and the uses of the minimalist project. 

What is minimalism? Let us begin with three distinctions. First, proceduralminimalism entails an effort to limit the scope and ambition of judicial rulings; procedural minimalism should be distinguished from what I call minimalism's substance, which entails an identifiable set of substantive commitments (to, for example, fair procedures and the rule of law).' Second, procedural minimalism as a general category should be distinguished from the subcategory of democracy-forcing minimalism, which involves an effort to issue narrow rulings that do not mandate ultimate outcomes but that force decisions by politically accountable actors.9 The Court might, for example, refuse to resolve a hard constitutional problem and rule more narrowly that Congress must authorize an intrusion on constitutionally sensitive interests.'0 Third, denials of certiorari and justiciability doctrines, by which the Court refuses to reach the merits, should be distinguished from narrow and incompletely theorized judgments, by which the Court resolves the merits 

but without much foreclosing the future." When the Court denies certiorari, or holds that a decision is unreviewable, it is adopting a form of procedural minimalism, but that form is importantly different from narrow and incompletely theorized judgments. I hope that I made these distinctions adequately in earlier writings, but very possibly not; Siegel is certainly correct to insist on their importance. 
Siegel's principal concern, and mine as well, is procedural minimalism in the form of narrow and incompletely theorized rulings. Quoting from a 2004 op-ed of mine, Siegel suggests that procedural minimalism requires courts to decide "the largest issues of the day ... as narrowly as possible.' 2 Hence he offers a testable hypothesis, to the effect that a decision is minimalist if it involves an "intentional choice by a majority of the Justices ... to decide a case on the narrowest and shallowest grounds reasonably open to them, even though broader and deeper rationale(s) were reasonably available."" 
Siegel should certainly be commended for offering a testable hypothesis, as I did not. He is right to say that there is a great deal of room for empirical work on the Supreme Court's uses of minimalism. But I never meant to suggest that members of the Court pursue minimalism with the intensity and rigor suggested by Siegel's hypothesis. To be sure, it is possible to imagine a set of judges, or perhaps even a court, taking the trouble to identify the possible rationales for certain decisions and consistently selecting the narrowest and less ambitious of these. But no real-world court is likely to act in this way; and for reasons that Siegel identifies, such a court would be nothing to celebrate, if only because it would give so little guidance for the future. A court of this kind would turn minimalism into a kind of dogma or theology. tIitmweosusludpnpootrtuiste. 4minimalism on the intensely pragmatic grounds that some 
When I suggest that the current Supreme Court (often) favors procedural minimalism, then, I mean to say only that in the most difficult and controversial domains, the Court tends to choose relatively narrow and unambitious grounds." The Court has not accepted a large-scale theory of constitutional interpretation; it proceeds by building cautiously on precedent, in the fashion of common law courts. Unfortunately, my claim-that minimalists prefer relatively narrow and unambitious 
See SUNSTEIN, ONE CASE AT A TIME, supra note 7, at 39-41. 
Siegel, supra note 6, at 1954 (quoting Sunstein, The Smallest Court,supra note 7). 
Siegel's critique, that a decision that is both wide and deep might also leave many issues undecided. Consider, for example, a decision to the effect that people have a constitutional right, on autonomy grounds, to avoid "undue burdens" on their medical choices. Courts that leave things undecided after broad and ambitious rulings should not be treated as minimalist. 
suggestion that the Court favors "relatively narrow and shallow holdings" is "less clearly inaccurate," and notes that this claim could be tested empirically. Id. at 2019. I agree. grounds-is not entirely easy to test empirically. To test that claim, it would be necessary to identify a large number of cases, to specify the possible grounds for decision, and to see how often the Court selected narrower grounds in the face of competing possibilities. It would also be necessary to specify what counts as minimalism along all relevant dimensions, and to decide how to count a decision in which minimalism along one dimension (say, avoiding the merits) produces maximalism along another dimension (say, through a wide ruling on standing). A great deal of coding would be necessary by people with internal understanding of the relevant cases. Notwithstanding the difficulties, there is a great deal of room for empirical testing of minimalism. In the absence of empirical work, I can suggest only that in many of the most prominent cases in recent years, the Court has re16jected both width and depth. Siegel himself offers a number of examples. 
II. MINIMALisM FALSIFIED? 
As I have said, Siegel understands minimalism in absolute rather than relative terms, as involving an intentional decision "to decide a case on the narrowest and shallowest grounds reasonably open to" the Court. 7 He explores a number of decisions from the October 2003 term to see whether the Court was minimalist in that sense. He finds that if this is the right account of minimalism, the Court has not consistently followed it. 
Consider a few examples. In Blakely v. Washington," the Court did refuse to decide the validity of the Federal Sentencing Guidelines, but that should be unsurprising; what matters is that the Court could have ruled on the Sixth Amendment question in a way that was more narrowly limited to the facts. In McConnell v. Federal Election Commission,'9 the Court left many issues undecided and generally avoided width, but it did not rule on the narrowest possible grounds, especially in its expansive understanding of "corruption" in politics.2 Siegel agrees that the Court pursued a fairly minimalist path in Locke v. Davey2' and Tennessee v. Lane,22 though more in the latter case than in the former.23 But he does not find minimalism in two of the Court's most eagerly awaited decisions, Elk Grove Unified School District v. Newdow 24 and Rumsfeld v. 
note 7, at 76-205. 
Padilla2.5 He acknowledges that Newdow, refusing to resolve the constitutionality of the Pledge of Allegiance, was in a sense minimalist,26 but he thinks that Padilla, involving a merits-avoiding procedural ground, announced a broad (procedural) rule with large implications for other issues.27 
In a series of instructive discussions, Siegel convincingly shows that, in a number of cases in the 2003 term, the Court did not rule in the narrowest imaginable way. But is that big news? Let us identify a continuum of possible outcomes, from a denial of certiorari, to a refusal to reach the merits, to a fact-bound decision that goes barely beyond the immediate parties, to a decision that states an identifiable if narrow rule with an identifiable if shallow rationale, to a decision that offers an identifiable but broad rule, and so on, culminating in a truly maximalist (if also unfathomable) decision that resolves all questions for all time by reference to the most fundamental of principles. Focusing on the 2003 term, Siegel makes a special target of my 2004 op-ed, which, to be sure, does not analyze minimalism in much detail. But the essential claims in that little op-ed may nonetheless hold. In Newdow, the Court did refuse to assess the Pledge of Allegiance on the merits. In Padilla,the Court did refuse to rule on the merits of an exceptionally controve2r8sial issue of presidential authority; the same 2is true of the Cheney case. In cases involving sexually explicit material29 and enemy combatants, 3° the Court did say more than was strictly speaking "necessary," but it also showed a tendency toward both narrowness and shallowness insofar as it deliberately refused to resolve some of the key questions raised by the litigants.3' 
Siegel's most interesting point, it seems to me, is that a decision that is minimalist along one dimension may be wide or deep on another. If a court invokes a procedural ground to avoid the merits, that very ground might be ambitiously reasoned or apply to a wide range of problems not before the Court. Siegel is right to emphasize this possibility. He is also right to identify many ways during the 2003 term in which the Court failed to choose the narrowest possible rationale for its decision. But I wonder if that is the appropriate test of the minimalist hypothesis. Notwithstanding a regrettably loose phrase in an op-ed ("as narrowly as possible"), those who find strong minimalist tendencies in the Supreme Court are inclined to think that Siegel may have mounted an attack, illuminating to be sure, on a strawman. 
2004 Sup. CT. REV. 47. 

Id. at 1985. 

Ashcroft v.ACLU, 542 U.S. 656 (2004). 

Siegel dislikes minimalism; he believes that it is normatively unattractive, because it leaves so much uncertainty. He emphasizes that "often it is critical that the Court provide guidance, either to the lower courts or to the political process. '3 2 Objecting to the uncertainty introduced by Rasul v. Bush,33 Newdow,34 and Lane,35 he thinks that the Court should try to provide clear rules for others, so as to reduce aggregate decision costs. Siegel fears that minimalist decisions often leave important problems to lower courts, not to citizens and their representatives, and to that extent such decisions do not promote democratic goals. He believes that the Supreme Court is a guardian as well as a guide, and he insists that it is appropriate for constitutional theory to specify the areas in which ambitious rulings are justified, even when those rulings reject the outcomes of political processes. 
I am not sure how much Siegel and I disagree on the normative questions. I have not argued, and I do not believe, that minimalism is generally or always the right path.36 When planning is important, minimalism is hazardous; when minimalism imposes high decisional burdens on others, the argument for minimalism is weakened. Hence minimalism must be evaluated in terms that have become familiar from the rules-standards debate in many domains of the law.37 If it is desirable for the Supreme Court to leave decisions to lower courts, it is partly because lower court decisions are less final, and a degree of percolation can occur there at the same time that deliberative debate takes place within the citizenry as a whole. In the end Siegel and I agree that the argument for minimalism is strongest in an identifiable class of cases: those in which American society is morally divided, those in which the Court is not confident that it knows the right answer, and those in which the citizenry is likely to profit from more sustained debate and reflection. 
If Siegel and I have a normative disagreement, it is because he is more confident than I am about what he calls "the Supreme Court's role-and comparative advantage-in our constitutional system of separate but interrelated powers."3 9 Invoking Brown v. Board of Education,4° which he labels "heroic, 4' he emphasizes that as compared to other institutions, "the Justices are more insulated from the pressures of majoritarian politics and therefore better equipped to protect minority rights. 42 This is of course a plaus•ibl4e1 and time-honored view, defended in whole or in part by the early Bickel, Dworkin," and Ely.45 Certainly it is easy to find cases in which the Court's insulation served the nation well. But if we are going to celebrate Brown, 6 we had better not forget about Dred Scott v. Sandford7 or Lochner v. New York48 or Coppage v. Kansas49-- or, for thatCmityatotefrB,oUenrniteedv.SFtaltoersesv,.2 MBooarrridson, Kimel v. FloridaBoard of Regents,5 of Trustees of the University ofAlabama v. Garrett5,3 Adarand Constructors, Inc. v. Pena5,4 and Gratz v. Bollinger." The Court's conception of what principle requires, and its understanding of what it means to defend "minority rights," should not be taken as unerring. From the moral point of view, insulation from majoritarian pressures is sometimes the problem, not the solution. 
But good minimalists do not mean to attack the structure of judicial review. They mean to insist instead on Learned Hand's suggestion, in the midst of World War II, that "[tihe spirit of liberty is that spirit which is not too sure that it is right .... ,56 In many of its best moments, the Rehnquist Court has respected that spirit, not least in decisions involving free speech, 7 sex equality, 8 the war on terrorism, 9 and even federalism.6 In its own small way, minimalism can be heroic too. 
We are most grateful to John Donohue, Justin Wolfers, and Carol Steiker for their valuable and illuminating responses to our article.' Donohue and Wolfers explore empirical questions, 2 on which we have little to say. Steiker investigates the moral issues, 3 and here our Reply must be more extensive. 
Donohue and Wolfers believe that, with respect to the death penalty, "existing evidence for deterrence is surprisingly fragile." 4 They attack the peerreviewed empirical work of a number of social scientists, including Hashem Dezhbakhsh, Paul Rubin, Joanna Shepherd, H. Naci Mocan, R. Kaj Gittings, and Paul Zimmerman. 5 They highlight theoretical claims by Lawrence Katz, Steven Levitt, and Ellen Shustorovich, who emphasize the infrequency of capital punishment and who thus doubt the claim of deterrence. 6 (Interestingly, Katz, Levitt, and Shusterovich do find that prison deaths have massive effects in deterring murders and other crimes. 7) Most importantly, their own work, * Karl N. Llewellyn Distinguished Service Professor, the University of Chicago Law 
using existing data, suggests that deterrence has not been shown. 
Wolfers misunderstand the point of our article. Let us distinguish among three purposes for which one might discuss the recent deterrence evidence: (1) to argue that, in fact, capital punishment deters murder; (2) to argue that the evidence has reached a threshold of reliability such that policymakers should change laws now, adopting capital punishment; and (3) to argue that the evidence has reached a threshold of reliability, much lower than in (2), such that it is worthwhile to consider the moral implications of the evidence. We do not mean to take a stand on either (1) or (2). We do not know whether deterrence has been shown; and contrary to 
Wolfers's suggestion, we do not insist "that it would be irresponsible for government to fail to act upon the studies." 8 Nor do we conclude that the evidence of deterrence has reached some threshold of reliability that permits or requires government action upon it right now.9 Plainly, Donohue and Wolfers have a quarrel with other social scientists, but not with us--except, perhaps, insofar as we are willing to take the recent evidence as a motivation for rethinking the moral issues.1 0 
Suppose that Donohue and Wolfers are fundamentally right and that their own analysis shows that current evidence of deterrence is weak. Even if that were true, we could certainly imagine a regime of capital punishment that should not be required for policymakers to take action, but that is a far more modest claim. 
Vermeule's] claim that it would be irresponsible for government to fail to act upon the studies and vigorously prosecute the death penalty."), with Sunstein & Vermeule, supra note 1, at 715 ("In any event, our goal here is not to reach a final judgment about the evidence. It is to assess capital punishment given the assumption of a substantial deterrent effect. In what follows, therefore, we will stipulate to the validity of the evidence, and consider its implications for morality and law."). 
impression that we have a strong commitment to the recent studies: 
While Lawrence Katz, Steven Levitt, and Ellen Shustorovich found no robust evidence in favor of deterrence, several researchers claim to have uncovered compelling evidence to the contrary. This latter research appears to have found favor with Cass Sunstein and Adrian Vermeule, who describe it as "powerful" and "impressive," and they refer to "many decades' worth of data about [capital punishment's] deterrent effects." While they claim not to endorse any specific analysis, these "sophisticated multiple regression studies" are "[t]he foundation of [their] argument," and they specifically rely on many of the recent studies that we will reexamine as buttressing their premise that "capital punishment powerfully deters killings." Donohue & Wolfers, supranote 2, at 793-94 (internal citations omitted). 
We hope that, taken as a whole, our essay shows a kind of interested agnosticism. We cannot help but add that as new entrants into the death penalty debate, we are struck by the intensity of people's beliefs on the empirical issues, and the extent to which their empirical judgments seem to be driven by their moral commitments. Those who oppose the death penalty on moral grounds often seem entirely unwilling to consider apparent evidence of deterrence and are happy to dismiss such evidence whenever even modest questions are raised about it. Those who accept the death penalty on moral grounds often seem to accept the claim of deterrence whether or not good evidence has been provided on its behalf. would, in fact, deter homicides. It is worthwhile to ask how the moral issues should be assessed ifdeterrence could be established. In any case, one of our central goals is theoretical. We aim to use the area of capital punishment as a way of challenging the act/omission distinction in the context of government decisions.1 1 Our hope is that this challenge is relevant to a wide range of issues, not merely capital punishment. In our view, regulation is pervaded by life-life tradeoffs, and criminal law is illuminatingly analyzed as a form of regulation. 
Carol Steiker does not accept this latter claim, and hence she engages our arguments directly. 12 But she does not defend the act/omission distinction. For government, at least, she seems to agree that this distinction is unhelpful, at least outside the context of criminal justice. Even for criminal justice, she does not insist on the value of the act/omission distinction. 13 Nonetheless, she rejects our argument on three grounds. The first involves what she sees as the need to distinguish between purposeful and nonpurposeful acts. 14 The second points to considerations of justice that seem to raise serious doubts about capital punishment. 15 The third involves slippery slopes that, in her view, make our argument unacceptable. 16 We take up these claims in sequence. 
Purposeful versus nonpurposeful action. Steiker notes that in the criminal law, it is entirely standard to distinguish between purposeful and nonpurposeful acts. Those who purposefully cause harm are punished more severely than those who are negligent or even reckless. Steiker believes that the same distinction, pointing to different degrees of mens rea, greatly matters when the government is the actor. A government that takes life is doing so purposefully, whereas a government that fails to protect life is acting negligently or at worst recklessly.17 Steiker is puzzled that we seem to ignore this conventional"quotidian"'8-- distinction. Because capital punishment involves the purposeful taking of life, and because the failure to impose capital punishment does not, Steiker believes that it is wrong to speak of life-life tradeoffs in this context.19 
We shall argue that even though the distinction between purposeful and nonpurposeful actions is important for the purpose of criminal punishment, it is not important for the purpose of evaluating what governments should be doing. A government that negligently fails to prevent hurricane damage may be less blameworthy than a government that chooses to create hurricane damage, but governments that have failed to prevent hurricane damage had better start doing so. As a preliminary matter, two closely related points are important to underline: (1) where governments are concerned, the distinction between acts and omissions is both conceptually obscure and morally irrelevant; 20 and (2) governments do not have "intentions" or "purposes," at least not in the way that people do.2 1 Steiker stipulates to the first point, 22 but she may not fully appreciate its force. In our view, the first point, properly appreciated, requires acceptance of the second point as well. 
As for the first: Suppose that we have rejected the act/omission distinction for governments and that we accept the evidence of deterrence for purposes of argument. It follows that, where government chooses not to adopt a policy of capital punishment that would deter significant deaths, 23 government is itself acting with willful disregard of the resulting deaths. True, government does not know the precise identities of the victims, but that should be neither here nor there. A lifeguard who left her post, knowing to a practical certainty that a number of people would predictably drown as a result, would be a murderer, even if the identities of the victims were unknown ex ante. Steiker recognizes that, under the Model Penal Code and in most jurisdictions, conduct of this sort would itself constitute legal "murder." 24 Perhaps such conduct would not be murder in the first degree, depending upon the relevant criminal code and upon the factual details. But it is unclear why that legal difference is relevant and unclear why it should underwrite a wholesale objection to capital punishment at the level of policy evaluation. We return to this point shortly. 
As for the second point: In the example above, we have treated government as a really big person, as does Steiker. It is worth underscoring, however, that the same structural features of government policymaking that render the act/omission distinction obscure, for government, also render the distinctions among various shadings of culpable intention obscure, for government. Steiker claims to accept, at least for the sake of argument, that the act/omission distinction fails for governments. But Steiker then uses unusual locutions about governmental intention, referring to governmental mens rea 25 (What mens could possibly be referred to here?) and to the "'intent' of capital punishment statutes." 26 In our view, Steiker's own quotation marks around "intent" indicate a healthy recognition that a metaphor has been stretched beyond the breaking omissions may not be intelligible in this [government] context, and even if it is, the distinction does not make a morally relevant difference."). 

discuss in our article. See Sunstein & Vermeule, supra note 1,at 709 n.10. 
point. Individuals qua individuals have intentions to take actions, as do individual officials. But government itself is not a person, and the underlying individual actions are usually inputs into complicated collective decisionmaking processes, such as voting in legislatures and multimember courts. The outputs of such processes need not correspond to anyone's individual intentions and are hard to classify into the fine shades that the criminal law uses for assessing the culpability of individual intentions. 
Steiker rightly observes that people speak about degrees intention with respect to government policy, as in the case of disasotfer cruellpieafb.l2e7 But the question is whether this freighted talk is really about individual-level morality or is instead a kind of shorthand or heuristic for evaluating complicated institutional questions, such as whether particular governmental officials are acting as faithful agents for the polity as whole. Steiker assumes the former, but in our view the latter is more plausible. Moralized talk about whether "governmental" intentions are culpable is metaphorical shorthand for institutional criticism and for moral criticism of particular individuals who happen to occupy government posts. In pressing the distinction between purposeful and nonpurposeful action for government actors, Steiker takes the metaphor too literally. 
Let us put these points aside; still, we believe that Steiker's argument from mens rea is unconvincing. No one doubts that mens rea matters in the criminal law. Whatever the foundations of punishment, special sanctions should be imposed on intentional wron2g8doing; the decision to do so can be defended on grounds of both deterrence and retribution. Steiker rightly contends that "those who purposefully transgress are more blameworthy." 29 But how does this claim bear on our argument? A governor of a state may well be more blameworthy if he intentionally causes a disaster than if he stands by and negligently allows a disaster to occur. But no one is talking about the appropriate punishment of governors or about how much to "blame" individual public officials. The question is how to evaluate official policies. Let us stipulate that a mayor who encourages and promotes domestic violence is more blameworthy than one who negligently permits such violence to occur. But how does that point relate to the evaluation of policies to control domestic violence-or to the legitimacy of capital punishment? Even if we accept the view that the criminal law should distinguish between purposeful and nonpurposeful action, it hardly follows that the government should decline to impose capital punishment if the effect of its decision is to condemn significant responsibility for the disaster in New Orleans on the ground that its failure to maintain the levees was merely an omission for which it was not responsible."). 
Analysis, 111 HARV. L. REV. 869 (1998). 
numbers of innocent people to death. 
In other words, the distinction between purposeful and nonpurposeful action is drawn for particular purposes, not in the abstract. Suppose that a standardized test is graded incorrectly, and the question is whether the incorrect grade should be changed. The usual answer is that it should indeed be changed; that answer does not depend on the state of mind of the initial grader (maybe it was a computer). Now suppose the question is the right mix of policies to discourage crime. The answer to that question does not turn on the state of mind of the government officials who developed the initial policy. If officials have negligently adopted a policy that allows one group of people-say, Hispanics, women, or gays and lesbians-to be subject to criminal violence, they had better change that policy, even if the resulting punishments are adopted intentionally and the earlier policy caused harm only as a result of negligence. 
Justice. Steiker's more fundamental objection is that public executions are unjust. To support this conclusion, she makes three independent arguments. First, she contends that all criminal sentences must be proportionate to the crime. In her view, capital punishment is not proportionate in view of the difficult circumstances of those who are subject to it. Victims of capital punishment very frequently are extremely intellectually limited, are suffering from some form of mental illness, are in the powerful grip of a drug or alcohol addiction, are survivors of childhood abuse, or are the victims of some sort of societal deprivation (be it poverty, racism, poor e3d0ucation, inadequate health care, or some noxious combination of the above.). 
Steiker's plea for proportionality is meant as a deontological check on permissible punishments. Second, Steiker argues that capital punishment suffers from a failure of equality because of racial disparities in its administration. Here she points to evidence that African-American defendants, and also those defendants who kill white people, are disproportionately likely to receive the death penalty. 3 1 Third, Steiker contends that capital punishment violates human dignity. On this count, her preferred argument is that the death penalty "destroy[s] the distinctive human capacities of the society" that administers it3.2 With respect to dignity, the problem with capital punishment is not what it does to convicted criminals; it is "what it does to all of us." 33 
We accept many of Steiker's concerns, but we do not think that she has offered a convincing response to our arguments. Steiker contends that the death penalty represents a failure of proportionality, in part because of the deprivations frequently faced by those who commit the most heinous murders. 34 Nothing in our arguments is inconsistent with the suggestion that a proportionality requirement should constrain permissible punishments; given certain circumstances, such a requirement might have good consequences, deontological justifications, or both. But suppose, not implausibly, that some or many hostage takers are mentally ill, drug addicts, or otherwise products of severe deprivation. Would it follow that police officers should not be permitted to kill hostage takers? 35 Now suppose that capital punishment saves lives. If so, many of those who would be saved also face severe deprivation; not irrelevantly, one deprivation that they would otherwise face involves the loss of their life. Why should they be sacrificed for the sake of their killers? 36 This last question suggests that Steiker's deontological sympathies include an unacknowledged baseline. Steiker seems to assume that capital punishment "uses" murderers for the sake of innocent people. But it would be equally plausible to say that an abolition of capital punishment "uses" innocent people for the sake of murderers. 
In any case, Steiker's proportionality argument would seem to raise serious questions about life imprisonment as well. Life imprisonment, especially without hope of parole, might be disproportionate to any offense if an offender's deprived background is taken into account. And it is reasonable to think that those who are subject to life imprisonment "frequently are extremely intellectually limited, are suffering from some form of mental illness, are in the powerful grip of a drug or alcohol addiction, are survivors of childhood abuse, or are the victims of some sort of societal deprivation." 37 Is life imprisonment to that extent unacceptable, in Steiker's view? A proportionality argument would certainly prohibit capital punishment for minor crimes, and perhaps for all crimes short of the most egregious murders, but the view that it forbids capital punishment for those murders must be parasitic on some independent, and suppressed, normative argument that capital punishment is always and everywhere impermissible (at least on certain assumptions about the 
at 762 n.40, 783 n.106. For our analogy, see Sunstein & Vermeule, supra note 1, at 740 ("Police officers are permitted to kill those who have taken hostages, at least if the killing is reasonably believed to be necessary to save human lives. If capital punishment is deemed different, it might be because the lives to be saved are merely statistical, as compared with the lives of hostages, which are entirely vivid."). Steiker refers to self-defense, Steiker, supra note 3, at 758 n.21, but it is permissible to kill hostage takers to protect hostages, not merely oneself. See N.Y. PENAL LAW § 35.15(2) (McKinney 2005), quoted by Steiker, supra note 3, at 758 n.21. Steiker contends that one can kill hostage takers even when there are more hostage takers than hostages, Steiker, supra note 3, at 758 n.21, but that fact makes our argument a fortiori. Steiker adds that hostage situations are emergencies that feature imminent threats, id. at 783 n.106, but the expansion of the temporal horizon does not undermine the analogy so long as evidence of deterrence is convincing. 
"lifelife tradeoff" in the capital punishment debate). 
backgrounds of those subject to it). That is not a proportionality claim: it is a substantive bar on a particular type of punishment, no matter how proportional it may be. At bottom, it is a conclusion, not an argument. 
Issues of racial equality are certainly important in this domain, and hence we emphasize that if deterrence occurs, African-Americans have more to gain from capital punishment than white people do.3 8 Steiker replies that racial animus plays a role in the imposition of capital punishment, whereas most murders of African-Americans do not have any such racial component.3 9 It is not clear that her premise is correct. If African-Americans disproportionately are victims of murder, a racially discriminatory past is almost certainly a contributing factor, and there is reason to wonder about a criminal justice system that does not provide African-Americans equal protection against the risk of homicide. In addition, the term "racial animus" description of existing inequalities in the system of capital pmunaiyshnmoetntb.e40 an apt 
But let us grant Steiker her premises. What follows? Is capital punishment impermissible on grounds of equality if it turns out that (a) its administration is infected by racial bias, but (b) African-Americans are disproportionately beneficiaries of its deterrent effect? That conclusion seems implausible. If racial animus is present, the natural solution is to eliminate racial bias, not to eliminate capital punishment. And if that solution proves impossible, a small racial bias in administration of the death penalty might not be fatal if that penalty has a large "tilt" toward protection of the lives of innocent AfricanAmericans. Suppose that a particular penalty provides massive and disproportionate protection to Catholics but that the penalty is, occasionally, imposed on Catholics just because of anti-Catholic animus. Should the penalty be eliminated as a way of preventing discrimination against Catholics? This is hardly clear, especially if a feasible alternative is simply to police the penalty's imposition more closely to root out animus in specific cases. 4 1 
indicates a discrepancy that appears to correlate with race. Apparent disparities in sentencing are an inevitable part of our criminal justice system... . In light of the safeguards designed to minimize racial bias in the process .... we hold that the Baldus study does not demonstrate a constitutionally significant risk of racial bias affecting the Georgia capital sentencing process."). We do not mean to take a stand on this question; we simply suggest that the idea of "racial animus" is contested in this domain. 
discrimination and deterrence. Suppose that white murderers, and those who kill AfricanAmericans, are less likely to receive the death penalty. If so, these actors will face less deterrence, in a way that will (assuming that deterrence occurs) lead to more murders by whites and more murders of African-Americans. Or suppose that African-American murderers, and those who kill whites, are unusually likely to receive the death penalty. If so, there will be extra deterrence of African-American murderers (in a way that might help African-American victims, since most murders are not cross-racial) and extra deterrence of murders of white people (in a way that will not help African-American victims). Our 
Steiker's dignity argument 42 is not our absolute favorite, because we fail to see what it adds to her other claims. True, it is possible to assert that capital punishment, by its very nature, threatens the human capacities of societies that use it. Steiker does not contend that this is an empirical claim; she is not arguing that capital punishment actually has harmful effects on society's capacities. If her claim is not empirical, what kind of argument is it, and what does it add to her other objections to the death penalty? We assume that Steiker also believes that a society's human capacities are undermined if that society fails to prevent homicide. Now suppose that capital punishment deters significant numbers of murders; if so, the failure to use it is plausibly taken as a threat to the human capacities of societies that fail to stop preventable murders. Steiker here seems to reassert the distinction between government acts and government omissions, despite her earlier acknowledgement that the distinction should be abandoned. With the appeal to human dignity, it is not clear that Steiker has supplied a distinctive argument against the death penalty. 
Slippery slopes. Steiker concludes with a set of slippery slope arguments. She believes that the logic of our argument would require execution of innocent people, including members of the killer's family. 43 Let us suppose, with Steiker, that significant deterrence would result from a system in which the state executed not only offenders but also their spouses and their children. Nothing in our argument is inconsistent with the claim that there is a deontological check on deliberate decisions to execute innocent people. On any theory, the killing of innocent people is a prima facie moral wrong. What we do insist upon is the presence of life-life tradeoffs; we are discussing the particular context of decisions to protect people from being killed. We therefore emphasize that (1) killing is on both sides of the question where governmental rules about capital punishment are concerned, because government cannot help but act;44 and that (2) almost all deontological views recognize a consequentialist override to deontological rules. 45 We have hardly argued for execution of innocent people. But if the execution of an innocent person were genuinely necessary to save an exceedingly large number of people (10,000, 100,000, or 100 million?), we believe that the execution might well be justified, and we doubt that Steiker would disagree. For both consequentialists and deontologists, the killing of innocent people could be justified in some imaginable world. assumption here is that race-neutral deterrence is best and that a discriminatory system of capital punishment is a problem, even if it might lead, on some assumptions, to benefits for members of traditionally disadvantaged groups. 
because they destroy the distinctive human capacities of the society in whose name they are publicly inflicted."). 

But in our world, it is exceedingly unlikely that such a justification could ever be convincing, even on consequentialist grounds. 46 As we have emphasized, the execution of innocent people would dilute the deterrent signal that the consequentialist wishes to strengthen. 47 As Rawls emphasizes, a policy of that sort would itself have systemic costs that would affect its desirability.48 If the conse uentialist objection to the killing of innocents is "unsatisfactorily contingent,"19 so be it; almost all consequentialist arguments are contingent, in the sense that they depend on (contingent) consequences. And as we have noted, our argument is consistent with the claim that deontological arguments forbid the execution of innocent people, so long as the override threshold is not met. What we emphasize is that the situation with capital punishment is most unlikely to require execution of the innocent, in which case the deontological position is unaffected. 
In any case, policy evaluation should not be driven by bizarre hypotheticals of this sort. In the United States, at least, no one is likely to execute innocent people in order to produce greater deterrence. In reality, the worst possible risk is that officials will administer a system of capital punishment in which almost all are guilty, but officials know, at the statistical level, that a very few (comparatively speaking) innocents may have fallen into the net, yet cannot practically be sorted from the guilty. The execution of innocents, if it ever occurs, might be "intentional" or "purposeful" in the sense that an execution is intentionally or purposefully carried out, but only in that sense; Steiker is not arguing that officials deliberately execute those they know to be innocent. Even on Steiker's view, the officials who conduct an execution have a kind of aggregate-level knowledge without purpose-a state of mind that is less culpable than a deliberate intention to kill the innocent. On our view, the execution of any number of innocent people is a good reason to increase the accuracy of the system of capital punishment. Standing by itself, however, it is not a sufficient reason to abolish the death penalty if there is strong evidence of deterrence. 
Finally, it is important to see that innocence is on both sides of the issue here. We have emphasized that if capital punishment deters murders, it saves innocent lives, perhaps many more than would be saved by abolishing capital punishment. If Steiker really accepts that government omissions are to be evaluated on par with government acts, then she faces a tradeoff between the might, in the end, reject capital punishment, because such punishment might not ultimately result in a net saving of lives. Steiker, supra note 3, at 785-88. Our argument is based on the assumption that a net saving occurs; if it does not, we agree that there is no good moral argument for capital punishment. 

also Sunstein & Vermeule, supra note 1, at 735-36 (discussing Rawls's argument). few innocent lives that might occasionally be lost under capital punishment and the innocent lives (certainly far more numerous) that capital punishment will save. 
Most generally, Steiker reads us to say "that there is nothing intrinsically wrong with individuals or governments 'using' the lives of some to promote the greater good."50 In so saying, she appears to suggest that we reject the deontological claim that people should be treated as ends rather than merely as a means. But we do not believe that this claim is properly or even plausibly understood to forbid punishment that is motivated in part by deterrence goals. Suppose that the government imposes life imprisonment on certain rapists, partly because it believes that life imprisonment will deter people from being rapists. Is the government "using" the lives of rapists to promote the general good, and, if it is, does Steiker mean to condemn that? If it is legitimate to punish wrongdoers in part to deter wrongdoing, then our argument should not violate any principle against the "use" of human lives. Steiker goes far be 1ond the deontological imperative against treating people merely as a means; 5 " she comes close to saying that it is always morally impermissible to consider consequences in choosing criminal punishments-a view that would bar deterrence-based justifications for the issuance of parking tickets. 
We have tried to offer an account of capital punishment that is agnostic about the contest between consequentialist and nonconsequentialist accounts of morality. Our minimal claim is that, in evaluating criminal penalties, deterrence should play a significant role in moral judgments, even for those whose central commitment is to human life and human liberty. And if capital punishment has significant deterrent effects, then the moral argument for the ultimate penalty is greatly strengthened--even, we think, to the point of raising the possibility that capital punishment may be morally required. 

THE METAPHYSICS OF MORALS (Mary Gregor ed. & trans., Cambridge Univ. Press 1998) (1785). 
that sort, the use of such force is obligatory from the moral point of view. See supra note 35 and accompanying text. 
Follow this and additional works at: https://chicagounbound.uchicago.edu/law_and_economics Recommended Citation 
Cass R. Sunstein, "Fast, Frugal, and (Sometimes) Wrong" ( John M. Olin Program in Law and Economics Working Paper No. 265, 2005). PUBLIC LAW AND LEGAL THEORY WORKING PAPER NO. 110  
Cass R. Sunstein  
T H E   L A W   S C H O O L   T H E   U N I V E R S I T Y  O F   C H I C A G O   
  
This paper can be downloaded without charge at the John M. Olin Program in Law and  Economics Working Paper Series: http://www.law.uchicago.edu/Lawecon/index.html and at the  
Public Law and Legal Theory Working Paper Series:   http://www.law.uchicago.edu/academics/publiclaw/index.html  
and  The Social Science Research Network Electronic Paper Collection:  
http://ssrn.com/abstract_id=844964   
forthcoming in Walter Sinnott-Armstrong, ed., The Cognitive Science of Morality All rights reserved 
Do moral heuristics operate in the moral domain? If so, do they lead to moral errors? This brief essay offers an affirmative answer to both questions. In so doing, it responds to an essay by Gerd Gigerenzer on the nature of heuristics, moral and otherwise. While focused on morality, the discussion bears on the general debate between those who emphasize cognitive errors, sometimes produced by heuristics, and those who emphasize the frequent success of heuristics in producing sensible judgments in the real world. General claims are that it is contentious to see moral problems as ones of arithmetic, and that arguments about moral heuristics will often do well to steer clear of contentious arguments about what morality requires. 
For many problems, Gerd Gigerenzer celebrates heuristics. He believes that they are simple, fast, frugal, and remarkably accurate. He emphasizes that heuristics can be prescriptive, in the sense that they may well lead to good outcomes in the real world. In the moral domain, Gigerenzer is properly cautious about whether heuristics produce moral or immoral behavior. What I would like to do here is to emphasize the imperfect reliability of heuristics in general, and to suggest that their imperfect reliability raises serious cautionary notes about some of Gigerenzer’s broader claims. 
which enables baseball players (and others) to make otherwise difficult catches. Gigerenzer, who has often explored this particular heuristic, is quite right to emphasize that people who use heuristics are often not aware that they are doing so. But even a casual understanding of sports requires some qualification of Gigerenzer's claims. Stupid tennis players tend to use fast and frugal heuristics, which contribute to their stupid tennis. Often they think, for example, that they should hit the ball hard and deep whenever the opportunity arises—an intuition, or thought, that can get them into serious trouble. Stupid athletes adopt simple heuristics that make them dumb. By contrast, smart tennis players are immensely flexible, and they are able to rethink their rules of thumb as the occasion demands. The best athletes have an exceedingly complex set of heuristics, fast but not at all simple, which they deploy as the situation requires. The moral domain is not so very different Gigerenzer suggests, but they often misfire, and good moral agents are aware of that fact. 
immense importance of moral framing and the possibility that people use “simple heuristics that make us good” logic, it is important to see that many heuristics do point us in the right direction– and hence to stress, as did Tversky and Kahneman Gigerenzer, that heuristics can lead to excellent judgments in the actual world. If people believe that they ought not to lie, or harm innocent people, they will often do the right thing—especially in light of the fact that case-by-case inquiries into the morality of lying, or harming innocent people, could produce self-serving conclusions that produce grievous moral wrong. (The case of Nazi massacres, explored by Gigerenzer, can be understood as an example.) Moral heuristics, understood as simple rules of thumb, might well have a rule-utilitarian defense, in the sense that they might, on balance, produce morally preferable behavior even if they lead to unfortunate results in particular cases. 
form of simple rules of thumb, lead to moral error on any plausible view of morality. Consider, for example, the idea, emphasized by Gigerenzer, that one ought to do as the majority does, a source of massive moral blunders fast and frugal idea that one ought not to distort the truth—a heuristic that generally works well, but that also leads (in my view) to moral error when, for example, the distortion is necessary to avoid significant numbers of deaths. Or consider the actomission distinction, which makes moral sense in many domains, but which can lead to unsupportable moral judgments as well (Baron, 2004). 
Gigerenzer notes, usefully, that it may be possible to modify people’s judgments, including their moral judgments, by altering the background. The idea is hardly original (see Sunstein and Thaler, 2004), but it is true that a default rule in favor of organ donations might well increase what, on one view, is morally desirable behavior (id.). Indeed there are many applications of this point. If default rules matter, an employer, including the state qua employer, could dramatically increase charitable contributions by presuming that (for example) each employer would like to devote 2% of wages to charitable causes. Of course the use of default rules to steer behavior raises normative questions of its own (id.). The only point is that default rules greatly matter to choices, including those with a moral component. 
Thus far, then, Gigerenzer’s general argument seems both plausible and illuminating, and I am merely underlining the possibility that even good heuristics will go wrong, for morality as for other questions. But on an important issue, Gigerenzer seems to me to miss some of the complexity of moral argument. His objections to maximization theories treat moral judgments as involving a kind of moral arithmetic, and this is a most contentious understanding. 
To be sure, Gigenenzer is correct to stress the cognitive difficulties of undertaking a full ex ante calculation of the consequences of social actions. Human beings do not have unlimited cognitive abilities, and hence they are often unable to specify the effects of one or another course of action. Gigerenzer believes that satisficers, using moral heuristics, have important advantages over optimizers. For some questions, this is undoubtedly correct. But to understand the relationship between heuristics and the moral domain, much more must be said. Three points are especially important here. 
they know exactly what Gigerenzer emphasizes, and they favor clear and simple moral rules for that very reason lead to error, even if it would be preferable if properly applied. Because people are selfserving, and because their on-the-spot judgments are unreliable, they might do best to follow simple moral rules, or one-reason decision making. There are interesting relationships between Gigerenzer’s understanding of heuristics and rule-utilitarian approach to morality. 
is one form of consequentialism, but because it require all goods and bads to be described along the metric of utility, it is controversial, even among consequentialists. When Gigerenzer speaks of the limits of maximization theories, and even of consequentialism, he appears to be operating under a utilitarian framework, without exploring the problem of plural and incommensurable goods. We might, for example, endorse a form of consequentialism that sees rights violations (so understood on nonutilitarian grounds) as a set of (very) bad consequences problems does not recognize the complexities in consequentialist accounts of morality. 
Consider the injunction to treat people as ends, not means, an injunction that runs afoul of many versions of consequentialism important point—it is not enough for Gigerenzer to show that moral heuristics do a good (enough) real-world job of achieving what we would achieve if we were optimizers with unlimited abilities of calculation. Perhaps some heuristics, in some contexts, violate deontological commands. 
Return to Gigerenzer’s first example: Should a Nazi massacre be evaluated in utilitarian or consequentialist terms? To make the calculation, does it matter if, for example, there were many more Nazis than Jews, and that many Germans had a great deal to gain, economically and otherwise, from mass murders? Many people would respond that this moral atrocity counts as such whatever the outcome of a utilitarian or consequentialist calculus—and hence that Gigerenzer’s emphasis on the impossibility of ex ante calculations is often beside the point (or worse). Perhaps many moral heuristics, followed by most people and even most soldiers (putting Nazi soldiers to one side), should be seen as fast and frugal ways not of satisficing rather than optimizing, but of ensuring that people do what is required by nonconsequentialist accounts of morality. 
makes it all the more difficult to argue that moral heuristics function well. If certain fast and frugal heuristics are defensible on utilitarian or consequentialist grounds, they might still be objectionable from the moral point of view. In my view, it is for this reason productive to explore heuristics that might be defensible, or indefensible, on the basis of any view of what morality requires, or on the basis of the least contentious views of what morality requires 
Gigerenzer seems to think that moral heuristics might be shown to be prescriptive if a full consequentialist calculus is not possible; but this thought too quickly treats morality as a problem of arithmetic. If morality ought not to be so understood, as many people believe, then it is not clear what is shown by Gigerenzer’s emphasis on the cognitive problems associated with optimizing. I emphasize that prescriptive treatments of moral heuristics are likely to be productive; but they should steer clear of the most contentious arguments about the foundations of morality.                       Readers with comments should address them to: Professor Cass Sunstein University of Chicago Law School 1111 East 60th Street Chicago, IL 60637 csunstei@uchicago.edu  
(Second Series)  209.  210.  211.  212.  213.  214.  215.  216.  217.  218.  219.  220.   221.  222.  223.  224.  225.  226.  227.  228.  229.  230.  231.  232.  233.  234.  235.  236.  237.  238.  239.  240.  241.  242.  243.  244.  245.  252.  
Follow this and additional works at: https://chicagounbound.uchicago.edu/law_and_economics Part of the Law Commons Recommended Citation Cass R. Sunstein, "Irreversible and Catastrophic" ( John M. Olin Program in Law and Economics Working Paper No. 242, 2005). 
JOHN M. OLIN LAW & ECONOMICS WORKING PAPER NO. 242          This paper can be downloaded without charge at the John M. Olin Program in Law and Economics Working  
Paper Series: http://www.law.uchicago.edu/Lawecon/index.html   and at the Social Science Research Network Electronic Paper Collection:  
 http://ssrn.com/abstract_id=705323   Irreversible and Catastrophic  
Cass R. Sunstein  
T H E   L A W   S C H O O L   T H E   U N I V E R S I T Y   O F   C H I C A G O   
April 2005  Preliminary draft 3/24/05 All rights reserved 
As many treaties and statutes emphasize, some risks are distinctive in the sense that they are potentially irreversible or catastrophic; for such risks, it is sensible to take extra precautions. When a harm is irreversible, and when regulators lack information about its magnitude and likelihood, they should purchase an “option” to prevent the harm at a later date—the Irreversible Harm Precautionary Principle. This principle brings standard option theory to bear on environmental law and risk regulation. And when catastrophic outcomes are possible, it makes sense to take special precautions against the worst-case scenarios—the Catastrophic Harm Precautionary Principle. This principle is based on three foundations: an emphasis on people’s occasional failure to appreciate the expected value of truly catastrophic losses; a recognition that political actors may engage in unjustifiable delay when the costs of precautions would be incurred immediately and when the benefits would not be enjoyed until the distant future; and an understanding of the distinction between risk and uncertainty. The normative arguments are illustrated throughout with reference to the problem of global warming; other applications include injunctions in environmental cases, genetic modification of food, protection of endangered species, and terrorism. 
Many losses are irreversible. Once a species is gone, it is gone forever. Transgenic crops can impose irreversible costs as a result of increasing pest resistance.1 Because some greenhouse gases stay in the atmosphere for centuries, the problem of global warming may be irreversible, at least for all practical purposes. Global warming could be catastrophic as well, potentially endangering many millions of people.2 * Karl N. Llewellyn Distinguished Service Professor of Jurisprudence, University of Chicago, Law School and Department of Political Science. I am extremely grateful to Robert Goodin, Robert Hahn, Eric Posner, Richard Posner, Adrian Vermeule, and David Weisbach, and participants in a workshop at Columbia Law School for valuable comments; special thanks to Vermeule and Weisbach for many helpful discussions. 
1 See Justus Wesseler, Resistance Economics of Transgenic Crops under Uncertainty: A Real Options Approach, in Battling Resistance to Antibiotics and Pesticides 214 (Rmanan Lazminarayan ed. 2003); to the same general effect, see Benoit Morel et al., Pesticide Resistance to Bt Corn: Real Option and Rational Option Approaches to Decisionmaking, in id. at 184. 
2 See Richard Posner, Catastrophe: Risk and Response 43-58 (2004). 
Irreversible or catastrophic risks pose distinctive problems for regulators; they require significant adjustments in the standard forms of cost-benefit analysis.3 In any case specialists in risk perception have long emphasized the fact that under some circumstances, people are especially averse to risks that are irreversible, potentially catastrophic, or both.4 
The Precautionary Principle, used in many international documents,5 is often said to have a special place in the context of irreversibility and catastrophe.6 Consider a few examples:    
The closing Ministerial Declaration from the United Nations Economic Conference for Europe in 1990 asserts, “In order to achieve sustainable development, policies must be based on the Precautionary Principle. . . . Where there are threats of serious or irreversible damage, lack of full scientific certainty should not be used as a reason for postponing measures to prevent environmental degradation.”7 The 1992 Rio Declaration states, “Where there are threats of serious or irreversible damage, lack of full scientific certainty shall not be used as a reason for postponing cost-effective measures to prevent environmental degradation.”8 The United Nations Framework Convention on Climate Change states: “Where there are threats of serious or irreversible damage, lack of full scientific certainty should not be used as a reason for postponing [regulatory] measures, taking into account that policies and measures to deal with climate change should be cost-effective so as to ensure global benefits at the lowest possible cost.”9 
The Final Declaration of the First European “Seas At Risk” conference says that if “the ‘worst case scenario’ for a certain activity is serious enough then even a small amount of doubt as to the safety of that activity is sufficient to stop it taking place.”10 
In American law, similar ideas are at work. A special precautionary principle underlies the analysis of preliminary injunctions in cases involving a risk of irreparable environmental harm.11 San Francisco has adopted its own precautionary principle, with an emphasis on seriousness and irreversibility: “Where threats of serious or irreversible damage to people or nature exist, lack of full scientific certainty about cause and effect shall not be viewed as sufficient reason for the City to postpone cost effective measures to prevent the degradation of the environment or protect the health of its citizens.”12 At the national level, the National Environmental Policy Act requires agencies to discuss “any irreversible and irretrievable commitments of resources which would be involved in the proposed action should it be implemented.”13 Courts have been careful to insist that environmental impact statements should be prepared at a time that permits consideration of environmental effects before irretrievable commitments have been made.14 A number of federal statutes, especially in the environmental context, specifically refer to irreversible losses and make their prevention a high priority.15 
For a long period, both courts and the executive branch also required agencies to engage in “worst-case analysis,” focusing on potential catastrophes.16 That requirement has been eliminated by the Council on Environmental Quality,17 but agencies continue to be directed to explore “impacts which have catastrophic consequences, even if their probability of occurrence is low.”18 Under the Clean Air Act, the Environmental Protection Agency is asked to build an “adequate margin of safety” into health-based national ambient air quality standards.19 This explicitly precautionary requirement is not limited to irreversible or catastrophic harms, but it might well be understood as an effort to ensure against them. 
The central notions here—irreversibility and catastrophe—play a critical role in many domains, and they lie at the heart of countless discussions of how to deal with risks to safety, health, and the environment. The problem is that both notions are exceedingly ambiguous, and it is by no means clear how regulators should understand them. The central purpose of this Article is unpack the ambiguities and to identify the proper role of both concepts in law and policy. I shall show that standard option theory, emphasizing the importance of irreversibility, 20 has important implications for environmental law, and indeed that some statutes and doctrines show an implicit appreciation of this point. I shall also show that regulators should sometimes attempt to eliminate worst-case scenarios, even or perhaps especially if they cannot assign a probability to its occurrence. When no such probability can be assigned, the best approach is to assess what is gained, and what is lost, by eliminating the most catastrophic outcomes—a point that helps discipline the inquiry into many risk-related problems, including global warming, terrorism, and injunctions in environmental cases. 
These general points lead to two refined versions of the Precautionary Principle. The first involves irreversibility: When regulators lack information about the likelihood and magnitude of a risk, it makes sense to spend extra resources to buy an “option” to protect against irreversible harm until future knowledge emerges. The value of the option is that of delaying the decision until better information is available. An emphasis on options and irreversibility leads to a distinctive principle, which I shall call the Irreversible Harm Precautionary Principle. 
The second principle involves catastrophe: When risks have extremely bad worstcase scenarios, it makes sense to pay special attention to those risks, even if they are unlikely to come to fruition and even if existing information does not enable regulators to make a reliable judgment about the probability that they will occur. An emphasis on the 19 42 USC 7409 (b)(1). An “ample margin of safety” is mentioned in connection with hazardous air pollutants lacking a safe thresholds, see 42 USC 7412 (d)(4). 
20 See Arrow and Fisher, supra note, for an early treatment. need to attend to potentially catastrophic risks also yields a distinctive principle, which I shall call the Catastrophic Harm Precautionary Principle. 
At first glance, these two principles seem to justify aggressive action to combat many environmental risks, including those posed by global warming, threats to endangered species, and genetic modification of food. Perhaps societies should invest substantial resources in the reduction of greenhouse gases, first to prevent what might turn out to be an irreversible loss, and second to stop the worst-case scenarios.21 Outside of the environmental domain, the two principles bear on appropriate steps to prevent terrorist attacks, epidemics, asteroid collisions, earthquakes, and more. An emphasis on the two principles also has the advantage of suggesting the possibility of a rapprochement between the risk judgments of experts and the risk judgments of ordinary people.22 As risk analysts have long emphasized, ordinary people sometimes pay a great deal of attention to whether risks are irreversible or potentially catastrophic.23 If the refined precautionary principles can be defended, then ordinary intuitions turn out to be plausible, and experts should accept them. Indeed the two principles might be combined, in certain cases, into an Irreversible and Catastrophic Harm Precautionary Principle,24 which provides the strongest basis for aggressive regulation of greenhouse gases. 
At the same time, I shall show that both principles are subject to important qualifications. The unifying claim is that the refined precautionary principles should be implemented with wide rather than narrow viewscreens. They must be attentive to the full range of consequences, not simply to a subset. A focus on irreversibility, and on catastrophic harm, threatens to violate this principle through a kind of selective perception. More particularly, the idea of irreversibility is exceedingly ambiguous; because time is linear, every step is in a literal sense irreversible. In the technical 21 See R. Posner, supra note, at 162, 184-86, 197, and in particular emphasizing “the practically irreversible effect of greenhouse-gas emissions on the atmospheric concentration of those gases. . . . Making shallower cuts now can be thought of as purchasing an option to enable global warming to be stopped or slowed at some future at a lower cost.” Id. at 162. 

23 See id. These psychometric studies coexist, however, with evidence that people dismiss many lowprobability risks of catastrophe, as discussed below. A simple example is that people do not pay much attention to the risk of asteroid collisions, even though there is a good argument that they should do so. See Posner, supra note, at 24-29. 
24 An early treatment of relevant points is Talbot Page, A Generic View of Toxic Chemicals and Similar Risks, 7 Ecology L.Q. 207 (1978). literature, the problem of ambiguity is solved through a particular definition of irreversibility.25 But under that definition, irreversibilities are usually on all sides of environmental problems. If significant steps are taken to reduce greenhouse gases, those very steps will inflict irreversible losses, environmental or economic—making it necessary to explore their likelihood and magnitude in order to decide what to do. An Irreversible Harm Precautionary Principle is both coherent and sensible, but it calls for precautions against the irreversible harms associated with environmental protection, as well as against the irreversible harms associated with environmental neglect. 
In addition, it can be costly, and even environmentally harmful, to avoid worstcase scenarios. If those scenarios are exceedingly unlikely, then there are clear limits on how much regulators should do to eliminate them. If it would cost a great deal to avoid those scenarios, or if doing so would subject people to high probabilities of very bad-case scenarios, then avoiding the worst-case may not be sensible. It is both necessary and possible, in short, to explore what is gained and what is lost by eliminating worst-case scenarios. As we shall see, an understanding of the uses and limits of the refined versions of the Precautionary Principle, focusing on irreversibility and catastrophe, casts new light on the foundations of environmental law, and indeed on all aspects of law that deal with the reduction of serious risks to safety and health.26 
This Article comes in four parts. Part I briefly explores the conventional Precautionary Principle, with an emphasis on the fact that precautionary steps often produce risks of their own. Part II discusses the question of irreversibility. The key point here is that because environmental harms are often irreversible, it is appropriate to spend resources to maintain flexibility for the future; the theory of real options has important implications for the theory and practice of environmental law, and indeed for regulation in general. An understanding of the problem of irreversibility also helps to explain continuing debates over the issuance of preliminary injunctions in environmental cases. Armed with that understanding, we can discipline the analysis of injunctions in such cases. 
Part III turns to the risk of catastrophic harm. A particular focus is the difference between risk, where probabilities can be assigned to various outcomes, and uncertainty, where no such probabilities can be assigned. With respect to catastrophe, risk aversion is perfectly sensible, but it is hard to defend the idea that regulators should generally seek to eliminate the worst-case scenario, whatever the environmental and other costs of doing so. Part III also offers some experimental evidence that people do not focus on the worstcase scenario under circumstances of either risk or uncertainty. The most important point here is that a form of cost-benefit balancing, perhaps with distributional weights, can inform the decision whether to eliminate the most catastrophic outcomes. Part IV offers some brief remarks on the question whether experts and ordinary people display “rival rationalities,” and on the relationship of irreversibility and catastrophe to that question. 
In the face of risks of environmental harm, it has become common to invoke the Precautionary Principle, an increasingly influential idea for environmental protection.27 My principal concerns here are irreversibility and catastrophe, but in order to understand those problems, it is necessary to explore the Precautionary Principle more generally. 
Unfortunately, there are twenty or more definitions of that principle, and they are not compatible with one another.28 The most cautious and weak versions suggest, quite sensibly, that a lack of decisive evidence of harm should not be a ground for refusing to regulate. Controls might be justified even if it is impossible to establish a definite connection between, for example, low-level exposures to certain carcinogens and adverse effects on human health. Thus the Ministerial Declaration of the Second International Conference on the Protection of the North Sea, held in London in 1987, sensibly suggests: “Accepting that in order to protect the North Sea from possibly damaging effects of the most dangerous substances, a Precautionary Principle is necessary which may require action to control inputs of such substances even before a causal link has been established by absolutely clear scientific evidence.”29 
The widely publicized 
Wingspread 
Declaration, from a meeting of environmentalists in 1998, goes much further: “When an activity raises threats of harm to human health or the environment, precautionary measures should be taken even if some cause and effect relationships are not established scientifically. In this context the proponent of the activity, rather than the public, should bear the burden of proof.”30 In Europe, the Precautionary Principle is sometimes understood in a still stronger way, suggesting that it is important to build “a margin of safety into all decision making.”31 According to one definition, the Precautionary Principle means “that action should be taken to correct a problem as soon as there is evidence that harm may occur, not after the harm has already occurred.”32 The word “may” is the crucial one here, because it signals the need for corrective action even in the face of merely speculative evidence that the risk is serious.33 
In a comparably strong version, it is said that “the Precautionary Principle mandates that when there is a risk of significant health or environmental damage to others or to future generations, and when there is scientific uncertainty as to the nature of that damage or the likelihood of the risk, then decisions should be made so as to prevent such activities from being conducted unless and until scientific evidence shows that the damage will not occur.”34 What is striking about this passage is its requirement that potentially hazardous activities be prevented until they are shown to be safe. I have noted that the Final Declaration of the First European “Seas At Risk” conference goes so far as to say that if “the ‘worst case scenario’ for a certain activity is serious enough then even a small amount of doubt as to the safety of that activity is sufficient to stop it taking place.”35 A. Hazardous Precautions 
The weak versions of the Precautionary Principle are unobjectionable and important. Every day, individuals and nations take steps to avoid hazards that are far from certain. We do not walk in moderately dangerous areas at night; we exercise; we buy smoke detectors; we buckle our seatbelts; we might even avoid fatty foods (or carbohydrates). Sensible governments regulate risks that, in individual cases or even in the aggregate, have a well under 100% chance of coming to fruition.36 An individual might ignore a mortality risk of 1/500,000, because that risk is quite small, but if 100 million citizens face that risk, the expected number of deaths is 200, and the nation had better take the problem seriously. 
For the moment let us understand the Precautionary Principle in a strong way, to suggest that regulation is required whenever there is a possible risk to health, safety, or the environment, even if the supporting evidence remains speculative and even if the economic costs of regulation are high. Recall that under the Final Declaration of the First European “Seas At Risk” conference, a serious worst-case scenario is said to justify abandonment of activity even if there is only “a small amount of doubt as to the safety of that activity”; recall too that under the National Environmental Policy Act, agencies must pay close attention to risks that have only a small probability of occurrence.37 To avoid absurdity, any emphasis on the idea of “possible risk” must be understood to require a certain threshold of scientific plausibility. To support regulation, no one thinks that it is enough if someone, somewhere, urges that a risk is worth taking seriously. But under the Precautionary Principle in its stronger forms, the threshold burden is minimal, and once it is met, there is a presumption in favor of regulatory controls. This version, as we shall see, helps to clarify a significant problem with the idea of precaution, in a way that bears on the more refined versions as well. 
The real problem with the Precautionary Principle, thus understood, is that it offers no guidance—not that it is wrong, but that it forbids all courses of action, including regulation. Taken seriously, it is paralyzing, banning the very steps that it simultaneously requires. In some cases, it should be easy to see that in its own way, stringent regulation would actually run afoul of the Precautionary Principle. Consider the “drug lag,” produced whenever the government takes a highly precautionary approach to the introduction of new medicines and drugs onto the market.38 If a government insists on such an approach, it will protect people against harms from inadequately tested drugs, in a way that fits well with the goal of precaution; but it will also prevent people from receiving potential benefits from those very drugs, and hence subjects people to serious risks that they would not otherwise face. Is it “precautionary” to require extensive premarketing testing, or to do the opposite? Or consider the case of DDT, often banned or regulated in the interest of reducing risks to birds and human beings.39 The problem with such bans is that in poor nations, they eliminate what appears to be the most effective way of combating malaria—and thus significantly undermine public health.40 Precautionary steps seem both mandated and forbidden by the idea of precaution in its strong forms. 
Similar issues are raised by the continuing debate over whether certain antidepressants impose a (small) risk of breast cancer.41 A precautionary approach might seem to caution against use of such antidepressants because of their carcinogenic potential. But the failure to use those depressants might well impose risks of its own, certainly psychological and possibly even physical (because psychological ailments are sometimes associated with physical ones as well). Or consider the decision, by the Soviet Union, to evacuate and relocate more than 270,000 people in response to the risk of adverse effects from the Chernobyl fallout. It is not clear that on balance, this massive relocation project was justified on health grounds: “A comparison ought to have been made between the psychological and medical burdens of this measure (anxiety, psychosomatic diseases, depression and suicides) and the harm that may have been prevented.”42 More generally, a sensible government might want to ignore the small risks associated with low levels of radiation, on the ground that precautionary responses are likely to cause fear that outweighs any health benefits from those responses.43 
The Precautionary Principle is often invoked in connection with genetic modification of food44—a plausible invocation in light of the multiple risks created by that practice.45 But many people believe that a failure to allow genetic modification might well result in numerous deaths, and a small probability of many more.46 The reason is that genetic modification holds out the promise of producing food that is both cheaper and healthier—resulting, for example, in “golden rice,” which might have large benefits in developing countries.47 The point is not that genetic modification will likely have those benefits, or that the benefits of genetic modification outweigh the risks. The claim is only that if the Precautionary Principle is taken literally, it is offended by regulation as well as by nonregulation. 
The example suggests that regulation sometimes violates the Precautionary Principle because it gives rise to substitute risks, in the form of hazards that materialize, or are increased, as a result of regulation.48 It is possible to go much further. A great deal of evidence suggests the possibility that an expensive regulation can have adverse effects on life and health.49 An early study found that a statistical life can be lost for every expenditure of $7 million50; one study suggests that an expenditure of $15 million produces a loss of life.51 Another suggests that poor people are especially vulnerable to this effect—that a regulation that reduces wealth for the poorest 20% of the population will have twice as large a mortality effect as a regulation that reduces wealth for the wealthiest 20%.52 To be sure, both the phenomenon and the underlying mechanisms are disputed.53 It is unnecessary to accept any particular amount here, or even to suggest that there has been an unambiguous demonstration of an association between mortality and regulatory expenditures. The only point is that reasonable people believe in that association. It follow that a multimillion dollar expenditure for “precaution” has—as a worst case scenario—significant adverse health effects, with an expenditure of $600 million as leading to perhaps as many as sixty lives lost. 
This point makes the Precautionary Principle hard to implement not merely where regulation removes “opportunity benefits,” or introduces or increases substitute risks, but in any case in which the regulation costs a significant amount. If this is so, the Precautionary Principle, for that very reason, raises doubts about many regulations. If the principle argues against any action that carries a small risk of imposing significant harm, then regulators should be reluctant to require large expenditures to reduce risks, simply because those expenditures themselves carry risks. Here is the sense in which, the Precautionary Principle is paralyzing: It stands as an obstacle to regulation and nonregulation, and to everything in between. 
A nation-by-nation study commissioned by the German Federal Environmental Agency goes so far as to conclude that there are two separate camps in the industrialized world: “precaution countries” (Germany, Sweden, the Netherlands, and the United States) and “protection countries” (Japan, France, and the United Kingdom).54 If the argument thus far is correct, this conclusion is implausible—not empirically but conceptually. The universe of risks is far too large to permit categorizations of this kind. The most general point is that no nation is precautionary in general and costly precautions are inevitably taken against only those hazards that seem especially salient or insistent.55 Taken in its strongest and crudest forms, the Precautionary Principle wrongly suggests that nations can and should adopt a general form of risk aversion. It is possible to take precautions against particular risks, but it is not possible to take precautions against all of them. It is possible to display aversion to particular hazards, but it is not possible to display aversion to all of them.56 54 See Sand, supra note, at 448. 
55 See David Vogel, The Hare and the Tortoise Revisited: The New Politics of Consumer and Environmental regulation in Europe, 33 B. J. Pol. S. 557, 570-71 (2003). for a demonstration of this point for Europe. 
56 It might be tempting to defend the Precautionary Principle—certainly in the context of health, safety, and the environment—on the ground that early warnings, in the form of merely suggestive evidence of harm, often turn out to be correct. See The Precautionary Principle in the Twenty-First Century, supra note. And it is right to insist that indisputable proof of harm should not be required to justify regulation; this is the sense in which the weak version of the principle is both unobjectionable and important. But the fact that suggestive evidence must be taken seriously does not render the strong version coherent, simply because B. A Note on Distributional Issues 
Those who endorse the Precautionary Principle often do so on grounds of fairness, believing that the principle will assist the most vulnerable members of society.57 Does the principle actually have that effect? In the United States, the Clean Air Act takes a highly precautionary approach, requiring an “adequate margin of safety” and hence regulation in the face of scientific uncertainty.58 At the same time, the Clean Air Act delivers especially large benefits to poor people and members of minority groups—larger benefits, on balance, than it gives to wealthy people.59 In the international domain, aggressive action to combat climate change would be more beneficial to poor countries than to wealthy ones.60 In the context of global warming, at least, the Precautionary Principle might be invoked to prevent especially severe burdens on those in the worst position to bear them. 
It makes a great deal of sense to emphasize the distribution of domestic or international risks, and the distributional effects of global warming are among the strongest points in favor of aggressive regulation of greenhouse gases.61 But in many cases, the Precautionary Principle, as applied, would have unfortunate distributional effects. Consider the case of DDT. A ban on DDT, often supported by reference to the suggestive evidence of harm is often on all sides. In any case, suggestive evidence of harm has often been found not to be an early warning worth heeding, but instead a false alarm, producing unjustified fear and significant social losses of many kinds. See Aaron Wildavsky, But Is It True? (1996). Consider, for example, fears of contaminated cranberries in 1959, of MSG in Chinese restaurants in 1968, of cyclamates in 1968, of flouordated water in 1950, and of mercury in tuna in 1970. All of these widely publicized “hazards” turned out to pose no serious risk. See Allan Mazur, True Warnings and False Alarms 110-141 (2004). For a valuable effort to distinguish between prescient warnings and false ones, see id. at 97-109, and in particular the suggestion tat the “clearest hallmark of a true public warning during the period 19481971 was a reputable scientific news source. Warnings reaching the press from scientists operating in a conventional way at an orthodox scientific institution were true more than twice as often as those reaching the news from government officials or citizen advocates.” Id. at 87. 
57 See, e.g., Frank Ackerman and Lisa Heinzerling, Priceless (2004). 58 42 USC 7409 (b)(1). 59 See Matthew E. Kahn, The Beneficiaries of Clean Air Act Regulation, 24(1) Regulation 34 (2001). 60 See, e.g., Joseph Aldy, Peter Orszag, & Joseph Stiglitz, Climate Change: An Agenda for Gloval Collective Action (unpublished manuscript 2001); Bjorn Lomborg, The Skeptical Environmentalist 291302 (New York: Cambridge University Press, 2001). 
especially high priority, in light of the many needs of those countries, needs that might be addressed by wealthier nations. For controversial but illuminating treatments, see Bjorn Lomborg, The Skeptical Environmentalist 322-23 (2001); Indur Goklany, The Precautionary Principle: A Critical Appraisal of Environmental Risk Assessment 71-8 Precautionary Principle, is eminently justified in wealthy nations. But such a ban is likely to have—and is actually having—unfortunate effects in at least some poor countries, where DDT is the cheapest and most effective way of combating serious diseases, most notably malaria.62 The case of genetic modification of food might well be a similar example; according to some projections, the benefits are likely to be enjoyed by poor people, not the wealthy.63 And recall that expensive regulations have a disproportionately serious effect on poor people, simply because any price increases are hardest for them to handle, and because the costs of regulation may well manifest themselves in fewer and less remunerative jobs.64 
Distributional issues should indeed be a central concern of a system of risk regulation,65 but the Precautionary Principle is a crude, indirect, and sometimes perverse way of incorporating distributional concerns. As a result, an emphasis on distribution does not rescue the Precautionary Principle from the charge of incoherence. The real question is whether more refined understandings of the principle can be developed. 
It is possible to identify an Irreversible Harm Precautionary Principle, applicable to a subset of environmental risks.66 On plausible assumptions, the problem of irreversibility does justify aggressive measures to combat environmental risks, under a general attitude of “act, then learn,” as opposed to the tempting alternative of “wait and learn.” With respect to global warming, for example, some people believe that it is most sensible to use research as the first line of defense, refusing to commit substantial resources until evidence of harm is clear.67 But if research alone allows continued emissions to produce irreversible risks, it might be best to take precautions now as a way of preserving future flexibility.68 In the environmental context in general, this point suggest that regulators should proceed with far more aggressive measures than would otherwise seem justified.69 
Begin with the monetary valuation of an environmental amenity, such as a pristine area. Some people will be willing to pay to use the area; others will be willing to pay to preserve it, even if they will not use it. Hence “existence value” is sometimes included in the valuation of environmental amenities,70 and indeed federal courts have insisted that agencies pay attention to that value in assessing damages to natural resources.71 But people are also willing to pay for the option to use an environmental amenity in the future, even if they are unsure whether they will exercise that option.72 Suppose that a pristine area might be developed in a way that ensures irreversible change. Many people would be willing to pay a significant amount to preserve their option. Under federal law, option value must also be considered in the assessment of natural resource damages.73 Many regulations explore the role of option value in the environmental context.74 
Here, then, is a simple sense in which irreversible environmental harm causes a loss that is not adequately captured in the standard economic measure of value. Some skeptics contend that it “is hard to imagine a price for an irreversible loss,”75 but people do identify prices for such losses.76 
The idea of option value, as used in the monetary valuation literature just discussed, is closely related to the use of the notion of “options” in the domain that I shall be emphasizing here. The simple claim is that when regulators are dealing with an irreversible loss, and when they are uncertain about the timing and likelihood of that loss, they should be willing to pay a sum—the option value—in order to maintain flexibility for the future.77 The option might not be exercised if it turns out that the loss is not a serious one. But if the option is purchased, regulators will be in a position to forestall the loss if it turns out to be large. 
In the domain of finance, options take multiple forms.78 An investor might be willing to purchase land that is known to have deposits of gold; even if the cost of extraction is too high to justify mining, ownership of the land creates an option to mine if the cost falls.79 A standard “call option” is a right to purchase an asset prior to a specific date at a specified price.80 In another variation, people might seek the right to abandon a project at a fixed price, perhaps on the occurrence of a specified set of events. Alternatively, they might obtain the right to scale back a project, to expand it, or to extend its life. Options that recognize multiple sources of uncertainty, of the sort that can be found for many environmental problems, are termed “rainbow options.”81 
Option theory has countless applications outside of the domain of investments. Suppose, for example, that because of law or social norms, it is difficult to divorce, so willing to incur costs to maintain their flexibility before marrying—higher costs than they would be willing to incur if divorce were easier. Narrow judicial rulings, of the sort celebrated by judicial minimalists,83 can be understood as a way of “buying” an option, or at least of “paying” a certain amount in return for flexibility. Judges who leave things undecided, and who focus their rulings on the facts of particular cases, are in a sense forcing themselves, and society as a whole, to purchase an option to pay for flexibility in the resolution of subsequent problems. Whether that option is worthwhile depends on its price and the benefits that it provides. 
It should be readily apparent how an emphasis on option value might explain the emphasis, in NEPA and other environmental statutes, on irreversible losses. The central point of NEPA is to ensure that environmental factors receive serious consideration,84 and if irreversible losses are involved, the delay produced by the duty to generate an environmental impact statement can be seen as payment for an option. It should also be clear that the idea of option value might help give content to the Precautionary Principle, which would, on this view, be understood as requiring societies to pay a kind of premium in the face of potentially irreversible losses.85 An important implication involves global warming; the argument for a global carbon tax is significantly strengthened by an appreciation of the option value of conserving the atmospheric environment.86 Let us elaborate the argument for the Irreversible Harm Precautionary Principle. 
In a classic essay, Arrow and Fisher demonstrate that the ideas of uncertainty and irreversibility have considerable importance to the theory of environmental protection.87 They imagine a situation in which the question is whether to preserve a virgin redwood forest for wilderness recreation or instead to open it to clear-cut logging. Assume that if the development option is chosen, the destruction of the forest is effectively assess the costs or benefits of a proposed development. If development produces “some irreversible transformation of the environment, hence a loss in perpetuity of the benefits from preservation,” then it is worth paying something to wait to acquire the missing information. Their suggestion is that “the expected benefits of an irreversible decision should be adjusted to reflect the loss of options it entails.”89 
Much more recently, Fisher has generalized this argument to suggest that “[w]here a decision problem is characterized by (1) uncertainty about future costs and benefits of the alternatives, (2) prospects for resolving or reducing the uncertainty with the passage of time, and (3) irreversibility of one or more of the alternatives, an extra value, an option value, properly attaches to the reversible alternative(s).”90 To pass a costbenefit test, it follows that an irreversible decision must clear a higher hurdle. The intuition here is both straightforward and appealing: More steps should be taken to prevent harms that are effectively final than to prevent those that can be reversed at some cost. If an irreversible harm is on side, and a reversible one on the other, an understanding of option value suggests that it is worthwhile to spend a certain amount to preserve future flexibility, by paying a premium to avoid the irreversible harm. 
Judge Richard Posner has invoked a point of this sort as a justification for aggressive steps to combat global warming.91 Posner acknowledges that the nature of the threat of global warming is disputed, and hence it is tempting to wait to regulate until we have more information. But there is a serious problem with waiting, which is “the practically irreversible effect of greenhouse-gas emissions on the atmospheric concentration of those gases.”92 Thus Posner concludes that making “shallower cuts now can be thought of as purchasing an option to enable global warming to be stopped or 88 The assumption may be unrealistic, under ordinary understandings of what counts as irreversibility. If seeds are retained, the forest can be recreated, though perhaps with a significant interim loss. I deal below with some of the complexities in the notion of irreversibility. 
89 Id. at 319. 90 See Anthony Fisher, Uncertainty, Irreversibility, and the Timing of Climate Policy 9 (2001). 91 See Richard A. Posner, Catastrophe 161-62 (2004). A more technical discussion to the same effect is Graciela Chichilnisky and Geoffey Heal, Global Environmental Risks, 7 J Econ Perspectives 65 (1993), emphasizing the need for a distinctive approach to “risks that are poorly understood, endogenous, collective, and irreversible.” Id. at 67; see especially pp. 76-84 for a treatment of option value and irreversibility. 
92 Posner, supra note, at 161. uncertainty. Perhaps regulators are usually able to assign probabilities to outcomes; and if if not, perhaps they can assign probabilities to probabilities (or, if this proves impossible, probabilities to probabilities of probabilities). In many cases regulators might be able to specify a range of probabilities—saying, for example, that the probability of catastrophic outcomes from global warming is above 2% but below 30%.212 Many scientists and economists believe that global warming is not likely to create catastrophic harm, and that the real costs, human and economic, will be high but not intolerable. In their view, the worst-case scenarios can be responsibly described as improbable.213 
Perhaps we can agree that pure uncertainty is rare. Perhaps we can agree that at worst, environmental problems involve problems of “bounded uncertainty,” in which we cannot assign probabilities within specified bands. It is possible to think, for example, that the risk of a catastrophic outcome is above 1% but below 10%, without being able to assign probabilities within that band. The pervasiveness of uncertainty depends on what is actually known. As I have emphasized, Posner believes that “no probabilities can be attached to the catastrophic global-warming scenarios, and without an estimate of probabilities an expected cost cannot be calculated.”214 Note in this regard that a 1994 survey of experts showed an extraordinary range of estimated losses from global warming, varying from no economic loss to a 20% decrease in gross world product.215 This finding, it has been suggested, is enough to support the view that uncertainty is real and must be taken seriously in environmental policy.216 In my view, uncertainty is both real and rare in the environmental domain; but this is an empirical judgment, and it may be wrong. 
difficult question: What is the appropriate approach to genuine uncertainty? Is maximin a rational strategy? I begin with some points about actual behavior and then turn to normative issues. show a degree of uncertainty aversion, in the sense that they will avoid gambles to which probabilities are not assigned. The relevant work was done by Daniel Ellsberg.217 Assume that people are asked to choose among two lotteries, each involving an urn with 100 balls. All of the balls are either black or red. For the first lottery, the urn contains an equal division of black and red balls. For the second lottery, the urn contains an unknown proportion of black balls and red balls. People receive a specified amount of money for correctly guessing the color of balls randomly chosen from the urn. It turns out that most people prefer the first lottery to the second, and thus display aversion to uncertainty.218 On the assumption of uncertainty aversion, it might be possible to defend maximin as a decision rule.219 
Note, however, that uncertainty aversion is bounded. So long as uncertainty aversion is not infinite, maximin will not always be the preferred decision rule. And indeed it is reasonable to think that most people will reject maximin if the question is properly framed. To test this possibility, I asked seventy-one University of Chicago law students the following problem: 
The government is considering two environmental problems. For the first, the government is able to estimate the probability that a bad outcome will occur. It believes that there is a 90% chance that 600 people will die (and the death of 500 people is the worst-case scenario). It also believes that there is a 10% chance that 400 people will die. For the second problem, the government cannot assign probabilities to the various outcomes. The “worst-case scenario” is that 700 people will die. 
Do you think: (a) the first problem has higher priority? (b) the second problem has higher priority? (c) the two problems have equal priority? 
No fewer than 63% chose (a), with the remainder equally divided between (b) and (c). As noted above, law students at any particular institution may offer idiosyncratic responses to such questions; but within the general population, it is reasonable to 217 See Daniel Ellsberg, Risk, Ambiguity, and the Savage Axioms, 75 Q J Econ 643 (1961). 218 Id. 219 Chu and Liu, supra note, at 265-66; Woodward and Bishop, supra note, at 496-98. conjecture that most people will show no consistent preference for maximin, and that they will reject an approach that eliminates the worst worst-case scenario, under circumstances of uncertainty, in favor of an approach that eliminates a highly probable but somewhat less bad worst-case scenario. 
Why is this? The Principle of Insufficient Reason says that when people lack information about probabilities (say, 1% to 40%), they should act as if each probability is equally likely.220 Whatever its normative status,221 actual decisions may well use that principle, which fits well with the results in the experiment just described. Consider another experiment with a larger group of law students from two institutions (the 
One thousand people are at risk from an environmental hazard. (a) If one approach is taken, a minimum of 400 people will die, and a maximum of 500 people will die. Regulators are unable to assign probabilities to the various outcomes. (b) If another approach is taken, a minimum of 10 people will die, and a maximum of 600 people will die. Regulators are unable to assign probabilities to the various outcomes. Which approach should be chosen? (a) The first approach (b) The second approach 
No less than 85.5% of respondents rejected maximin and chose (b). Why did (b) seem better to so many respondents? On a reasonable interpretation, people begin by presuming at least roughly equal probabilities under circumstances of uncertainty, and conclude that they would much rather go the route that has a much higher expected value, given that presumption. This interpretation is supported by the results of the following experiment,223 which asks people to compare a choice under risk with a choice under uncertainty: 
The government is considering two environmental problems. For the first, the government is able to estimate the probability that a bad outcome will occur. It believes that there is a 60% chance that 500-600 people will die (and the death of 600 people is 220 See Duncan Luce and Howard Raiffa, Games and Decisions 284 (1958). 221 Id.; Isaac Levi, On Indeterminate Probabilities, 71 J. Phil 391 (1974). 
222 One hundred and seventy-three law students were surveyed, seventy-one from the University of Chicago, one hundred and two from the University of Alabama. Interestingly, the answers from the two groups were essentially identical. 
223 This experiment was limited to seventy-one University of Chicago law students. the worst-case scenario). It also believes that there is a 40% chance that 200-400 people will die. For the second problem, the government cannot assign probabilities to the various outcomes. The worst-case scenario is that 700 people will die. 
Do you think: (a) (b) (c) the first problem has higher priority? the second problem has higher priority? the two problems have equal priority? 
For this problem, maximin was also rejected by a majority of respondents, but here the margin was much thinner: 52% favored (a), 25% were undecided, and 22% favored (b). The rejection of maximin is the most striking result here, and it is not entirely clear why the choice seemed relatively difficult. But the Principle of Insufficient Reason is consistent with that difficulty. Under the second problem, the expected number of deaths is 350 is equal probabilities are assigned, a number that is close to the expected number of deaths for the first problem; with a small degree of risk aversion, the choice between the two problems becomes extremely difficult. 
To see the role of the Principle of Insufficient Reason, suppose that people are asked to choose between: a 99.5% chance of a loss of 200 lives, and a .5% chance of a loss of 2 lives, with an uncertain chance of losing between 2 lives and 205 lives. 
For most people, it is reasonable to suppose that a) is much worse than b)—and hence that b) will be the overwhelming choice. Here, then, people will select a choice that eliminate the worst-case scenario. But compare: c) d) a risk of 60% of a loss of 200 lives, and 40% of 2 lives, with an uncertain chance of losing between 205 lives and 2 lives. 
For most people, the choice here is much less clear, and it is likely that many people will choose c) and refuse to follow maximin. It follows that people would have a great deal of difficulty in choosing between a 51% risk of a loss of 200 lives, and a 49% chance of a loss of 1 life, as compared with an uncertain risk that threatens to produce losses of between 200 and 1 lives, with no possibility of assigning probabilities to the various possibilities. 
The precise role of the Principle of Insufficient Reason, and the nature of people’s choices under circumstances of uncertainty, remain to be established. The discussion thus far should be enough to show that people will often reject maximin and that the Principle of Insufficient Reason is a starting point for their intuitions. The implication for environmental protection, and for other problems involving safety and health, is clear. People will not consistently follow maximin under circumstances of uncertainty. If the worst-case scenario is extremely vivid, and if it is drawn to their attention, they might neglect the issue of probability and attempt to eliminate it.224 But under ordinary circumstances, they will select maximin only when the Principle of Insufficient Reason, accompanied by a degree of risk aversion, suggests that they should. 
(b) A cost-benefit analysis of maximin? A great deal of work explores the question whether people should follow maximin under circumstances of uncertainty.225 Some of this work draws on people’s intuitions, in a way that illuminates actual beliefs but may tell us little about what rationality requires.226 Those intuitions, of the sort described by the experiments above, may be based on some kind of confusion. Other work is highly formal, 227 adopting certain axioms and seeing whether maximin violates them. The results of this work are not conclusive.228 Certainly maximin has not been ruled out as a candidate for rational choice under uncertainty. 
I cannot resolve these difficult issues here, but will rest content with a general suggestion. In deciding whether to follow maximin in the environmental context, a great deal should turn on two questions: (a) How bad is the worst-case scenario, compared to other bad outcomes? (b) What, exactly, is lost by choosing maximin? Of course it is possible that choosers, including regulators, will lack the information that would enable them to answer these questions. But in the regulatory context, answers to both (a) and (b) may well be possible even if it is not possible to assign probabilities to the various outcomes with any confidence. By emphasizing the relative badness of the worst-case scenario, and the extent of the loss from attending to it, I am attempting to build on the Rawls/Gardiner suggestion that maximin is the preferred decision rule when little is lost from following it. I have objected that this suggestion threatens to trivialize the case for maximin; but it is possible to develop the underlying intuition into a far more general, and useful, method for orienting both private and public choice. 
To see the relevance of the two questions, suppose that you are choosing between two options. The first has a best-case outcome of 10 and a worst-case outcome of –5. The second has a best-case outcome of 15 and a worst-case outcome of –6. It is impossible to assign probabilities to the various outcomes. Maximin would favor the first option, to avoid the worse worst-case; but to justify that choice, we have to know something about the meaning of the difference s between 10 and 15 on the one hand and –5 and –6 on the other. If 15 is much better than 10, and if the difference between –5 and –6 is a matter of relative indifference, then the choice of the first option is hardly mandated. But if the difference between –5 and –6 greatly matters—if it is a matter of life and death—then maximin is much more attractive. 
These points have the important implication of suggesting the possibility of a (rough) cost-benefit analysis229 of maximin under conditions of uncertainty. Sometimes a rejection of maximin is compelled by that analysis because the worst-case scenario is not much worse than the second worst-case scenario (and hence the benefits of maximin are low),and because maximin imposes extremely high costs.230 But sometimes the worst229 By suggesting this possibility, I do not mean to assign dollar equivalents to the various outcomes or to say anything controversial about the ingredients of the analysis. I am referring generally to an all-thingsconsidered approach to social welfare. 
230 It might be suggested that when people reason in this way, they are implicitly assigning probabilities to the relevant events. Suppose that the choice is between (a), with uncertain payoffs of 15 or 5, and (b) with uncertain payoffs of 8 or 6; suppose too that 15 is much better than 8 and that 5 is only a little worse than 6. On the analysis in text, (a) should be chosen. But perhaps that judgment depends on an implicit assignment of a significant probability, in case (a), to 15, and of an assignment of some probability of less than (say) 99%, in case (b), to 5. Without some such implicit assignment of probabilities, why would (a) be preferable? I cannot prove the point here, but I believe that this question, and this counterargument, are based on a denial of the possibility of acting under conditions of genuine uncertainty. If such conditions exist, and if people act under them, then they might indeed be assigning implicit probabilities; but they might also be acting in terms of the principles described in the text. case is the worst by far, and sometimes we lose relatively little by choosing maximin. It is typically thought necessary to assign probabilities in order to engage in cost-benefit balancing; without an understanding of probabilities, such balancing might not seem able to get off the ground.231 But a useful form of cost-benefit balancing is possible even without reliable information about probability. For the balancing exercise to work, of course, it must be possible to produce cardinal rankings among the outcomes—that is, it must be possible to rank them not merely in terms of their badness but also in at least rough terms of how much worse each is than the less-bad others. That approach will not work if cardinal rankings are not feasible—as might be the case if (for example) it is not easy to compare the catastrophic loss from global warming with the loss from huge expenditures on reductions of greenhouse gas emissions. Much of the time, however, cardinal rankings are possible. 
Irreversibility becomes highly relevant as part of this analysis. Recall that some of the costs of precautions are irreversible. If governments invest a great deal to control greenhouse gas emissions, they will be forcing private and public actors to incur irreversible costs. It follows that if governments follow maximin, they will be limiting their own flexibility , expending a great deal even though future information might move the situation from uncertainty to risk, as regulators learn more about the problem. Suppose that no probability can now be assigned to the catastrophic risk associated with abrupt global warming, and that for this reason regulators are tempted to spend a great deal to eliminate that risk. The relevant expenditures will greatly reduce future flexibility, ensuring sunk costs for a danger that might turn out to be quantifiable or even trivial. This point is not decisive against large expenditures, but it should be part of the analysis of whether worst-case scenarios ought to be eliminated. In this sense, there can be some tension between an Irreversible Harm Precautionary Principle and a Catastrophic Harm Precautionary Principle. 
Imagine, then, two polar situations with respect to global warming. First, suppose that the catastrophic dangers associated with global warming could be eliminated if every nation contributed $10 million to a fund to combat that risk. On reasonable assumptions, that cost would be fully acceptable. Second, suppose that the catastrophic dangers 231 See Ackerman and Heinzerling, supra note. associated with global warming could be eliminated only if every nation contributed enough resources to reduce standards of living by 50% world-wide, with a corresponding increase in global poverty. If global warming really does pose an uncertain danger of total catastrophe, maximin argues in favor of this extraordinary reduction in world-wide standards of living. But to incur costs of this magnitude, we might want to insist that the danger of catastrophe rise about the minimal threshold—that there be demonstrable probability, and a not-so-low one, that the catastrophic risk will occur. 
To appreciate this point, and the need for an analysis of the effects of following maximin, imagine an individual or society lacking the information that would permit the assignment of probabilities to a series of hazards with catastrophic outcomes; suppose that the number of hazards is ten, or a twenty, or a thousand. Suppose too that such an individual or society is able to assign probabilities (ranging from 1% to 90%) to an equivalent number of other hazards, with outcomes that range from bad to extremely bad, but never catastrophic. Suppose finally that every one of these hazards can be eliminated at a cost—a cost that is high, but that does not, once incurred in individual cases, inflict harms that count as extremely bad or catastrophic. The maximin principle suggests that our individual or society should spend a great deal to eliminate each of the ten, or twenty, or hundred potentially catastrophic hazards. But once that amount is spent on even one of those hazards, there might be nothing left to combat the extremely bad hazards, even those with a 90% chance of occurring. We could even imagine that a poorly informed individual or society would be condemned to real poverty and distress, or even worse, merely by virtue of following maximin. In these circumstances, maximin should be rejected. 
This suggestion derives indirect support from the empirical finding that when asked to decide on the distribution of goods and services, most people reject the two most widely discussed principles in the philosophical literature: average utility, favored by Harsanyi, and Rawls’ difference principle (allowing inequalities only if they work to the advantage to the least well-off).232 Instead people choose average utility with a floor constraint—that is, they favor an approach that maximizes overall well-being, but subject 232Norman Frohlich and Joe A. Oppenheimer, Choosing Justice: An Experimental Approach to Ethical Theory (1992). to the constraint that no member of society may fall below a decent minimum.233 Insisting on an absolute welfare minimum to all, they maximize over that floor. Their aversion to especially bad outcomes leads them to a pragmatic threshold in the form of the floor. So too, very plausibly, in the context of precautions against risks. A sensible individual, or society, would not always choose maximin under circumstances of risk or uncertainty. Everything depends on what is lost, and what is gained, by eliminating the worst-case scenario; and much of that time, available information makes it possible to answer those questions at least in general terms. 
Nothing here is meant as a proof that maximin is forbidden, or even not required, by rationality. To decide on the relationship between rationality and maximin strategies, it is necessary to specify the right account of rationality.234 I am doubtful that any such specification can establish the status of maximin without making contentious assumptions. My claim is instead that maximin makes most sense when the worst-case scenario, under one course of action, is much worse than the worst-case scenario under the alternative course of action, and when the choice of maximin does not result in extremely significant losses. 
The most general conclusion is that a degree of risk aversion should be expected in cases of catastrophic risks; for such risks, margins of safety are entirely sensible. For this reason, a Catastrophic Harm Precautionary Principle, of the sort suggested by several understandings of the Precautionary Principle, is a coherent and defensible part of environmental policy.235 Indeed, such a principle might well be the best understanding of the Precautionary Principle itself. It has many uses, not only in environmental policy but in health and safety regulation as a whole, including the war on terrorism. 
But maximin is not generally a sensible strategy in the environmental context or elsewhere. First, it is senseless under circumstances of risk, unless we assume an implausibly high degree of risk aversion. Second, regulators are rarely operating under circumstances of pure uncertainty; often rough probabilities can be ascribed to serious 233 Id. 234 See Luce and Raiffa, supra note, at 286-97. 235 See notes supra. outcomes, and if not, at least rough probabilities can be ascribed to probabilities. Third, adoption of maximin, under circumstances of genuine uncertainty, is most reasonable when the worst-case scenario is exceptionally bad and when removal of that scenario does not inflict serious losses of its own. 
It follows that a Catastrophic Harm Precautionary Principle is best understood to embody a form of risk aversion for the most dangerous risks. Its central domain involves . uncertain dangers of catastrophe when the costs of reducing those dangers are not huge and when incurring those costs does not divert substantial resources from extremely pressing problems. Four qualifications are important: social risks; it makes no sense to take steps to avert catastrophe if those very steps would create catastrophic risks of their own. If a preventive war, designed to reduce the risks of terrorism from one source, would increase those very risks from another source, then the Catastrophic Harm Precautionary Principle is indeterminate. This point is a simple extension of that made earlier with respect to the unrefined Precautionary Principle and the Irreversible Harm Precautionary Principle. which requires regulators to choose the least costly means of achieving their ends. In the context of global warming, there are many methods by which to reduce the relevant risks.236 Both nations and international institutions should choose those methods that minimize costs. The same is true for efforts to combat terrorism. applied in a way that reduces extreme burdens on those least able to bear them. For global warming, there is a particular need to ensure that citizens of poor nations are not required to pay a great deal to contribute to the solution of a problem for which those in wealthy nations are most responsible.237 If an antiterrorism policy would impose special burdens on members of racial and religious minority groups— consider racial profiling—it is worth considering other policies that reduce or eliminate those burdens. 236 A good discussion is Nordhaus and Boyer, Warming the World, supra note, at 121-44. 237 See the overview in R. Percival et al., supra note. their expense. In cases of the kind I am discussing, where the worst-case scenario is truly catastrophic and when probabilities cannot be assigned, a large margin of safety makes a great deal of sense.238 
There is a final point. It is possible to combine a concern about catastrophe with a focus on irreversible harm, in a way that generates an Irreversible and Catastrophic Harm Precautionary Principle. Suppose that by adopting environmental controls at the present time, regulators can maintain flexibility to prevent a risk that is not only irreversible but potentially catastrophic as well. Suppose too that the likelihood of catastrophe cannot be specified with much confidence, or even that it is in the domain of uncertainty rather than risk. Risk-neutral, risk-averse, or uncertainty-averse regulators might be willing to pay a great deal to maintain the flexibility that would permit them to avoid the worst-case scenarios. We have seen that significant expenditures can reduce flexibility too; many problems involve not irreversibility, but irreversibilities. At the same time, the most important irreversibilities may well turn out to be environmental in character. 
This argument provides the strongest basis for aggressive measures to combat global warming.239 The natural objections would either point to the irreversible costs of maintaining flexibility or question the probability that catastrophe will actually ensue. The appropriate conclusion rests on an assessment of the empirical questions,240 but in my view, an appreciation of irreversibility and catastrophe argues for otherwise excessive steps to reduce greenhouse gases.241 
238 See Woodward and Bishop, supra note, at 505: “If one considers a spectrum of choice problems from pure uncertainty to pure risk, almost all of the attention of economics has been on one extreme. . . . This has led to policy advice and analysis that either implicitly or explicitly requires policymakers to divine probability distributions. We argue, however, that there are important cases where probability distributions cannot be reasonably formulated and under such conditions dramatically different decision criteria may be rational.” 239 See Posner, supra note, at 161-65. 240 See Fisher, supra note, for a good discussion. 241 See id.; William Cline, Climate Change, in Global Crises, Global Solutions, supra note, at 13, 15-21. 
When it comes to risk, why do experts disagree with ordinary people? Many people think that the reason lies in the fact that ordinary people have a “rival rationality.”242 On this view, experts are concerned with statistics, and, above all, with the number of lives at stake.243 By contrast, ordinary people are concerned with a range of qualitative factors that make certain risks a special cause of concern. Irreversibility and catastrophe are said to loom especially large in ordinary people’s reactions.244 Where experts simply calculate expected values, ordinary people, and lay rationality, show special aversion to potentially irreversible and catastrophic harm. If this is so, ordinary people display “rival rationalities,” and each “side must respect the insights and intelligence of the other.”245 
According to a competing view, the rival rationality of ordinary people is mostly a product of cognitive illusions, ignorance, and confusion.246 For the critics, ordinary people are also concerned with the central question, which is the number of lives at stake. Unfortunately, they are unable to answer that question well. When ordinary people show a concern about irreversible and catastrophic harms, it is because they fear that many lives are at risk, no less and no more. 
The discussion thus far suggests a possible rapprochement between the apparently rival rationalities. Sensible experts do not and should not believe that there is any particular magic in irreversibility, and they do and should insist that the line between catastrophic and non-catastrophic harm is one of degree. But they should also agree that irreversibility matters, in the sense that it makes sense to spend resources to maintain 246 See Howard Margolis, Dealing With Risk (1995). 
1061-1085 (1990). Note that the evidence describe above suggests that in certain settings, potentially catastrophic outcomes will receive less, not more, attention than their expected value, perhaps because people treat very low-probability risks (eg, 1 in 1 million) as if they were zero. The framing of the question undoubtedly matters a great deal here. Probably the best generalization is that people sometimes give low probability risks of catastrophe more attention than its expected value and sometimes give them more less. See Gary H. McClelland et al., supra note. I suspect that in some of the relevant studies, the evidence of grave concern for catastrophic harms stems from the affect heuristic, through which a general affective reaction to a risk—often a negative one—leads people to show concern about all aspects of the relevant risk. See Paul Slovic et al., The Affect Heuristic, in Heuristics and Biases: The Psychology of Intuitive Judgment 397 
flexibility for the future. Insofar as the National Environmental Policy Act247 instructs agencies to spend time to acquire relevant information before ensuring irreversible losses, it is on firm ground. Experts agree that any cost-benefit analysis that ignores option value is missing an important variable—a standard point in finance though not yet in environmental protection.248 In this sense, ordinary people are correct to see the importance of irreversibility, and to emphasize the importance, some of the time, of adopting a strategy of “act, and then learn.” 
What of catastrophic risks? Experts may have little to say on the question whether it is worse to create a 1/100 risk that 100,000 people will die, or a 1/100,000 risk that 1 million people will die. But they should agree that when regulators are unable to assign probabilities to catastrophic risks, it is worth doing a great deal to avoid those risks—or at least to spend resources while more information is acquired. Experts cannot rule out the choice of maximin under circumstances of uncertainty. At the very least, experts know that elimination of the worst-case scenario is sometimes justified by a kind of cost-benefit analysis, one that pays attention to the relative egregiousness of the worst case and the costs of eliminating it. Sensible experts are interested not only in the expected value of catastrophic risks, but also in producing strategies for eliminating them when probabilities cannot be confidently assigned. For these reasons, an understanding of irreversibility and catastrophe help not only to refine but also to vindicate intuitions that have been found to play a significant role in ordinary risk perceptions. 
The ideas of irreversibility and catastrophe have had a major impact on domestic and international law, and they play a large role in private and public decisions. My major goal in this Article is to unpack these ideas and to bring them to bear on law and policy. I have suggested the possibility of replacing the Precautionary Principle, which is incoherent, with more refined principles that embody an understanding of the distinctive problem of irreversible losses and of the need to attend to low-probability risks of disaster. 
247 42 USC 102 (C)(5). 248 See Dixit and Pindyck, supra note, at 4-7. and coherent. Drawing on the idea of real options, it suggests that regulators, including those who make environmental policy, should find it worthwhile to invest resources to preserve flexibility for the future. In the context of global warming, the Irreversible Harm Precautionary Principle argues for substantial current investments, above all because emissions of carbon dioxide stay in the atmosphere for an extremely long time. The difficulty here is that emissions reductions also impose irreversible costs. An emphasis on irreversibility does not always favor aggressive environmental regulation, or anything like an attitude of “act, then learn.” It is even possible to imagine an Irreversible Harm Precautionary Principle that in many cases argues for a plan of “wait and learn.”249 Everything depends on the magnitude and likelihood of the full range of irreversible losses. In the context of global warming, the best approach is probably a world-wide agreement to cap greenhouse emissions, with the size of the cap decreasing over time as the expense of controls diminish.250 
If expected values matter, then societies should not ignore low probability risks of catastrophe. A minimal response would be a Catastrophe Harm Precautionary Principle, one that attempts to counteract the serious risk that both individuals and societies will treat small risks as if there were zero.251 The argument for this principle is strengthened by the fact that a catastrophic harm typically has secondary effects that ensure adverse effects that go far beyond a simple multiple of the number of people who are killed.252 A less minimal approach would build a degree of risk aversion into the Catastrophe Harm Precautionary Principle, so as to treat catastrophic harms as worth more than their expected value. A much more aggressive approach would be to adopt maximin, by which regulators identify the worst-case scenario and attempt to eliminate it. I have argued that for most environmental problems, this approach is senseless. Under circumstances of risk, maximin is far too cautious, and it would inflict serious harms—often including environmental harms—for no sufficient reason. Usually environmental problems involve 249 See Mendelsohn, supra note, at 45. 250 See Nordhaus and Boyer, supra note, at 121-44. 
251 On some of the cognitive mechanisms here, see Bazerman, Predictable Surprises, supra note; on the need to attend to low-probability risks, see Posner, supra note, at 245-65. 
risk, in the sense that a range of probabilities can be assigned, or at least in the sense that probabilities can be assigned to probabilities. 
As a matter of theory, pure uncertainty cannot be ruled out of bounds. The fact that people assign probabilities to uncertain outcomes does not negate the possibility of (objective) uncertainty. Under circumstances of uncertainty, maximin has some appeal if the worst-case scenario is truly catastrophic. And if it is not terribly costly to eliminate that scenario, regulators should certainly do so. But maximin can be an unappealing strategy, certainly under risk, and also when the worst-case scenario is not much worse than the second-worst case scenario and when the costs of eliminating the worst-case scenario are extremely high. To operate sensibly, precautionary steps must be attentive to the full range of consequences, not simply to a subset of them. But a Catastrophic Harm Precautionary Principle, applied with a wide viewscreen, has an important role in environmental law as well as ordinary life,253 and it is a strong candidate for replacing and refining any more general Precautionary Principle.   253 See William Cline, Climate Change, in Global Crises, Global Solutions, supra note, at 13, 18-19. 2.  William M. Landes, Copyright Protection of Letters, Diaries and Other Unpublished Works: An  Economic Approach (July 1991)  Richard A. Epstein, The Path to The T. J. Hooper: The Theory and History of Custom in the Law of  Tort (August 1991)  Cass R. Sunstein, On Property and Constitutionalism (September 1991)  Richard A. Posner, Blackmail, Privacy, and Freedom of Contract (February 1992)  Randal C. Picker, Security Interests, Misbehavior, and Common Pools (February 1992)  Tomas J. Philipson & Richard A. Posner, Optimal Regulation of AIDS (April 1992)  Douglas G. Baird, Revisiting Auctions in Chapter 11 (April 1992)  William M. Landes, Sequential versus Unitary Trials: An Economic Analysis (July 1992)  William M. Landes & Richard A. Posner, The Influence of Economics on Law: A Quantitative Study  (August 1992)  Alan O. Sykes, The Welfare Economics of Immigration Law: A Theoretical Survey With An  Analysis of U.S. Policy (September 1992)  Douglas G. Baird, 1992 Katz Lecture: Reconstructing Contracts (November 1992)  Gary S. Becker, The Economic Way of Looking at Life (January 1993)  J. Mark Ramseyer, Credibly Committing to Efficiency Wages: Cotton Spinning Cartels in Imperial  Japan (March 1993)  Cass R. Sunstein, Endogenous Preferences, Environmental Law (April 1993)  Richard A. Posner, What Do Judges and Justices Maximize? (The Same Thing Everyone Else Does)  (April 1993)  Lucian Arye Bebchuk and Randal C. Picker, Bankruptcy Rules, Managerial Entrenchment, and  Firm‐Specific Human Capital (August 1993)  J. Mark Ramseyer, Explicit Reasons for Implicit Contracts: The Legal Logic to the Japanese Main  Bank System (August 1993)  William M. Landes and Richard A. Posner, The Economics of Anticipatory Adjudication  (September 1993)  Kenneth W. Dam, The Economic Underpinnings of Patent Law (September 1993)  Alan O. Sykes, An Introduction to Regression Analysis (October 1993)  Richard A. Epstein, The Ubiquity of the Benefit Principle (March 1994)  Randal C. Picker, An Introduction to Game Theory and the Law (June 1994)  William M. Landes, Counterclaims: An Economic Analysis (June 1994)  J. Mark Ramseyer, The Market for Children: Evidence from Early Modern Japan (August 1994)  Robert H. Gertner and Geoffrey P. Miller, Settlement Escrows (August 1994)  Kenneth W. Dam, Some Economic Considerations in the Intellectual Property Protection of  Software (August 1994)  Cass R. Sunstein, Rules and Rulelessness, (October 1994)  David Friedman, More Justice for Less Money: A Step Beyond Cimino (December 1994)  Daniel Shaviro, Budget Deficits and the Intergenerational Distribution of Lifetime Consumption  (January 1995)  Douglas G. Baird, The Law and Economics of Contract Damages (February 1995)  Daniel Kessler, Thomas Meites, and Geoffrey P. Miller, Explaining Deviations from the Fifty  Percent Rule: A Multimodal Approach to the Selection of Cases for Litigation (March 1995)  Geoffrey P. Miller, Das Kapital: Solvency Regulation of the American Business Enterprise (April  1995)  Richard Craswell, Freedom of Contract (August 1995)  J. Mark Ramseyer, Public Choice (November 1995)  Kenneth W. Dam, Intellectual Property in an Age of Software and Biotechnology (November 1995)  Cass R. Sunstein, Social Norms and Social Roles (January 1996)  37.  
David A. Weisbach, Measurement and Tax Depreciation Policy: The Case of Short‐Term Assets  (January 2003)  Randal C. Picker, Understanding Statutory Bundles: Does the Sherman Act Come with the 1996  Telecommunications Act? (January 2003)  Douglas Lichtman and Randal C. Picker, Entry Policy in Local Telecommunications: Iowa Utilities  and Verizon (January 2003)  William Landes and Douglas Lichtman, Indirect Liability for Copyright Infringement: An  Economic Perspective (February 2003)  Cass R. Sunstein, Moral Heuristics (March 2003)  Amitai Aviram, Regulation by Networks (March 2003)  Richard A. Epstein, Class Actions: Aggregation, Amplification and Distortion (April 2003)  Richard A. Epstein, The “Necessary” History of Property and Liberty (April 2003)  Eric A. Posner, Transfer Regulations and Cost‐Effectiveness Analysis (April 2003)  Cass R. Sunstein and Richard H. Thaler, Libertarian Paternalizm Is Not an Oxymoron (May 2003)  Alan O. Sykes, The Economics of WTO Rules on Subsidies and Countervailing Measures (May  2003)  Alan O. Sykes, The Safeguards Mess: A Critique of WTO Jurisprudence (May 2003)  Alan O. Sykes, International Trade and Human Rights: An Economic Perspective (May 2003)  Saul Levmore and Kyle Logue, Insuring against Terrorism—and Crime (June 2003)  Richard A. Epstein, Trade Secrets as Private Property: Their Constitutional Protection (June 2003)  Cass R. Sunstein, Lives, Life‐Years, and Willingness to Pay (June 2003)  Amitai Aviram, The Paradox of Spontaneous Formation of Private Legal Systems (July 2003)  Robert Cooter and Ariel Porat, Decreasing Liability Contracts (July 2003)  David A. Weisbach and Jacob Nussim, The Integration of Tax and Spending Programs (September  2003)  William L. Meadow, Anthony Bell, and Cass R. Sunstein, Statistics, Not Memories: What Was the  Standard of Care for Administering Antenatal Steroids to Women in Preterm Labor between 1985  and 2000? (September 2003)  Cass R. Sunstein, What Did Lawrence Hold? Of Autonomy, Desuetude, Sexuality, and Marriage  (September 2003)  Randal C. Picker, The Digital Video Recorder: Unbundling Advertising and Content (September  2003)  Cass R. Sunstein, David Schkade, and Lisa Michelle Ellman, Ideological Voting on Federal Courts  of Appeals: A Preliminary Investigation (September 2003)   Avraham D. Tabbach, The Effects of Taxation on Income Producing Crimes with Variable Leisure  Time (October 2003)  Douglas Lichtman, Rethinking Prosecution History Estoppel (October 2003)  Douglas G. Baird and Robert K. Rasmussen, Chapter 11 at Twilight (October 2003)  David A. Weisbach, Corporate Tax Avoidance (January 2004)  David A. Weisbach, The (Non)Taxation of Risk (January 2004)  Richard A. Epstein, Liberty versus Property? Cracks in the Foundations of Copyright Law (April  2004)  Lior Jacob Strahilevitz, The Right to Destroy (January 2004)  Eric A. Posner and John C. Yoo, A Theory of International Adjudication (February 2004)  Cass R. Sunstein, Are Poor People Worth Less Than Rich People? Disaggregating the Value of  Statistical Lives (February 2004)  Richard A. Epstein, Disparities and Discrimination in Health Care Coverage; A Critique of the  Institute of Medicine Study (March 2004)  Richard A. Epstein and Bruce N. Kuhlik, Navigating the Anticommons for Pharmaceutical Patents:  Steady the Course on Hatch‐Waxman (March 2004)  Richard A. Esptein, The Optimal Complexity of Legal Rules (April 2004)  Eric A. Posner and Alan O. Sykes, Optimal War and Jus Ad Bellum (April 2004)  Alan O. Sykes, The Persistent Puzzles of Safeguards: Lessons from the Steel Dispute (May 2004)  
Volume 23 
Winter 2005-2006 Number 1 ELEVENTH ANNUAL LLOYD K. GARRISON LECTURE ON ENVIRONMENTAL LAW 
Warming, Terrorism, and Other Problems, 
Thank you very much. It's a pleasure to be here. I had a fantastic afternoon on the premises. I'm really grateful for the warmth and the substantive conversations. 
What I'm going to be focusing these comments on is something called the precautionary principle. And if you haven't heard of the precautionary principle, you will before long. The precautionary principle is very popular in Europe. It is an idea for organizing environmental protection and regulation of risk in general. The United States in the last few years has fought hard against the precautionary principle on grounds that are occasionally obscure. But one thing that we can say is that the United States, in the last few years, has in one sense firmly endorsed the precautionary principle. In defending the Iraq War and many measures to fight against terrorism, the United States emphasized that you don't need certainty that harm will occur; you at Pace University School of Law on April 25, 2005. The lecture drew heavily on three works: Cass Sunstein, Irreversibleand Catastrophic,CORNELL L. REV. (forthcoming 2006); CAss SUNSTEIN, LAws OF FEAR: BEYOND THE PRECAUTIONARY PRINCIPLE (2005); and Cass Sunstein, Beyond the PrecautionaryPrinciple, 151 U. PA. L. REV. 1003 (2003) [hereinafter Beyond the PrecautionaryPrinciple]. 
* Karl N. Llewellyn Distinguished Service Professor of Jurisprudence, University of Chicago Law School and Department of Political Science. should take preventive action because of the risk that harm will occur. The basic idea behind the precautionary principle is that we should build a margin of safety into our decisions-an idea that environmentalists in Europe emphasize in thinking about global warming, in thinking about destruction of the ozone layer, in thinking about clean air and clean water in general.2 With respect to terrorism, the Iraq War was explicitly defended on the grounds that we need a margin of safety in all decision-making. The White House did not believe that Saddam Hussein definitely had weapons of mass destruction. It thought that there was some probability that he had weapons of mass destruction, and that that was enough to justify action. 
I'm going to be making three suggestions here. The first is that the precautionary principle is incoherent. Risks are on all sides of social situations. It's just not possible to take precautions against all of them at the same time. So what Europe celebrates is an unfortunate basis for environmental and other decision-making, because it leads to paralysis and not to environmental protection. 
My second claim is that the precautionary principle gives the misleading appearance of helpfulness just because of the way the human mind works. When we think about risks-and maybe this will resonate with your experience in the last six months and possibly in the next six months-when we think about risks, human minds tend to single out one or two from the background and fixate on them without thinking about the extent to which efforts to reduce the risks on which we're fixating may give rise to risks of their own. So if you think, I'm nervous about going into a bad neighborhood at night, I think I'll stay home, it may be that staying home introduces its own risks with respect to health or otherwise. So what I'm going to suggest is that the precautionary principle, which is incoherent, gives the appearance of coherence and usefulness because human minds select some of the universe of relevant risks. 
Those are negative claims. Those are points against our friends in Europe. The third claim is an effort to rescue the pre2. See, e.g., INDOR GOKLANY, THE PRECAUTIONARY PRINCIPLE: A CRITICAL APPRAISAL OF ENVIRONMENTAL RISK ASSESSMENT 5 (2001) ("'In order to protect the environment, the precautionary approach shall be widely applied by States according to their capabilities'" (quoting U.N. Conference on Environment & Development (UNCED), June 3-14, 1992, Rio Declarationon Environment and Development, Principle 15, U.N. Doc. A/CONF.151/26 (Aug. 12, 1992) [hereinafter Principle15])). cautionary principle by suggesting that if we focus on a subset of risks, including environmental risks, we can make progress toward a more sensible way of orienting our laws and policies. And the subset that I'm going to identify includes those risks that are irreversible and catastrophic. Hence my title, Irreversible and Catastrophic-mycheerful title-the thought being that irreversible and catastrophic risks are ones against which we ought to take precautions, and those are the ones for which the precautionary principle is sensible. The rest is just going to be footnotes. But in law-professor fashion, there are going to be a lot of footnotes and they're going to be pretty long. So here goes. 
Footnote one: What is the precautionary principle with which I started? There are some versions that are weak and there are some versions that are strong. The weak version suggests that a lack of decisive evidence of harm ought not to be a ground for refusing to regulate. Think for example about smoking and tobacco regulation in the 1960s. Then, we didn't have decisive evidence that smoking was going to cause cancer, but sensible people didn't smoke on the ground that they were going to build a margin of safety into their decision by saying, The fact that there isn't definitive evidence isn't a reason to refrain from precaution. So the 1992 Rio Declaration involving climate change says, "'lack of full scientific certainty shall not be used as a reason for postponing cost-effective measures to prevent environmental degradation."'3 Think for example about global warming or air pollution or water pollution, where we don't have a full sense of how large the harm is. 
The ministerial declaration of an international conference on the protection of the North Sea is in the same weak vein: "'[A] precautionary principle is necessary which may require action to control inputs of [dangerous] substances even before a causal link has been established by absolutely clear scientific evidence."' 4 Straightforward and sensible. 
Here's a stronger version, from a group of environmentalists in the 1990s: "When an activity raises threats of harm to human STATE OF THE WORLD 348 (Hugh Matthews trans., Cambridge Univ. Press 2001) (quoting Principle 15, supra note 2). 
THE PRECAUTIONARY PRINCIPLE 1, 3 (Julian Morris ed., 2000) (quoting Second International Conference on the Protection of the North Sea (SICPNS), MinisterialDeclaration (1987)). health or the environment, precautionary measures should be taken, even if some cause and effect relationships are not fully established scientifically. In this context, the proponent of the activity, rather than the public, should bear the burden of proof."5 It should be clear that that's a stronger version because it puts the burden of proof on those who want to expose people to risk. This version doesn't merely say lack of certainty of harm is not a reason for refraining from regulation. So the proponent of an activity bears the burden of proof. 
In Europe, it's sometimes said that a margin of safety must be built into all decision-making. 6 And think of that as the strong version of the precautionary principle-a margin of safety in all decision-making. So, in one European understanding, "[A]ction should be taken to correct a problem as soon as there is evidence that harm may occur.. . ."7 The possibility that harm may occur is a justification for corrective action. 
And, "The precautionary principle mandates where there is a risk of significant damage to others or to future generations, then decisions should be made so as to prevent such activities from being conducted unless and until scientific evidence shows that the damage will not occur."8 That doesn't build a margin of safety. It doesn't just shift the burden of proof. It says that it must be shown by a proponent of action that damage will not occur. 
The weak version-that is the version that says lack of full scientific certainty isn't a reason to refrain from action-is fine and good. I don't have anything to say against that. What I'm going to try to criticize as incoherent is the strong version, that is, the version that requires a demonstration of absolute safety by the proponents of the activity. 
Now, to get at the problem we need some examples. And let me just give four in which the precautionary principle has been invoked, both in the United States and in Europe, as a basis for thinking about environmental protection. One example, obviously, is genetic engineering of organisms. Many people who like the precautionary principle (especially in Europe, and there's a movement to this effect in the United States), say that genetic modifications of organisms give rise to risks. 9 We don't know their magnitude. In the view of some people, we should take a precautionary approach and not allow genetic engineering. 10 
Nuclear power is another one, where the halting of nuclear power plants in the United States in the last couple of decades has been driven in part by a precautionary idea that nuclear power gives rise to serious risks.11 
Global warming is an area where the United States' perceived intransigence has been criticized in Europe for failing to be precautionary. It's not clear that global warming will occur in a way that's extremely damaging, or at least some people so think. And that, Europeans say, is not a reason for refraining from action; the fact that it's a possible risk means that we should take very aggressive regulatory action. 12 
The Bush Administration's most controversial early decision, see if you remember this, involved arsenic in drinking water, in which the Bush Administration suspended the Clinton Administration's proposal, which was to reduce arsenic levels from fifty parts per billion to ten parts per billion. 13 
In all four of those areas, the precautionary principle is said to require aggressive protection. What I'm going to try to argue is that that's just not so, that for each of these problems the precautionary principle forbids the very steps that it requires. Let's go at it first by thinking about genetic engineering of food and arsenic. Genetic engineering of food, it's true, gives rise to risk; and the precautionary principle calls for precautions against risk. The problem is, if we don't allow genetic engineering of food, we will give rise to risks of starvation; because there's some hope, speculative to be sure, that genetic engineering of food can deliver safe and nutritious and inexpensive food to countries where this is literally lifesaving. 14 A ban on genetic engineering of food is literally dangerous to people who have a great deal to gain from genetic modification. So my suggestion is that the precautionary principle forbids genetic modification of food because it gives rise to risk, but the precautionary principle also forbids forbidding of genetic engineering of food because forbidding genetic engineering of food gives rise to risk. 
Let's explore another example: arsenic. The former head of the Environmental Protection Agency, Administrator Christine Whitman, said in the early days of trying to defend the suspension of regulation, that there's something we're worried about, which is the expense of the rule that costs $210 million, and for some water systems that burden is going to be very hard.15 And for some users of water that's going to be very expensive-up to $400 per year or more.16 And what we're worried about is they're going to stop relying on their water companies and they're going to start using wells, local wells.' 7 What worries us about those local wells is that they are contaminated.' 8 What Administrator Whitman argued, and it wasn't an implausible worry (it may have been wrong, but it wasn't implausible), was that the arsenic regulation would give rise to risks of its own in the form of increased use of highly contaminated, dirty well water. 
Now, mind you, for the purposes of the precautionary principle, the mere fact that the Administrator has legitimate concerns should be decisive. Remember? The burden of proof switchesthe possibility that harm may occur is the reason for the invoking of the principle. The arsenic regulation stands both mandated and condemned by the precautionary principle. Mandated, because there's a risk of cancer from levels of arsenic in drinking water that President Bush was thinking of allowing.19 But the precautionary principle also forbids the very regulation because it gives rise to substitute risks. 
Think for a moment, if you would, about nuclear power and global warming, my other two examples. Nuclear power seems condemned by the precautionary principle. But its proponents say the following: If you're really worried about air pollution and global warming, you ought to rely on nuclear power, which is much better than fossil fuels which come from coal-fired power plants. 20 So they say that aggressive regulation of nuclear power runs afoul of the precautionary principle because it gives rise to risks in the form of increased reliance on coal-fired power plants, which make the air dirty and which contribute to global warming. Forbidding nuclear power seems required by the precautionary principle, and in a way it is. But forbidding nuclear power is also unacceptable under the precautionary principle because if you forbid nuclear power, you're going to increase reliance on coal-fired power plants. 
What I'm saying with these examples-genetic modification, nuclear power, and arsenic-is that risks are on all sides of social situations. Not just one. If you push against one risk, it's inevitable that you'll be giving rise to another risk. This should be familiar in everyday life. Think, for example, of that risk-avoidance strategy called "staying at home all day." Probably not very good for your health. Or think, for example, of the strategy of driving, rather than flying, for those who fear the risks associated with airplanes. Driving is risky, too. There is no way of avoiding risks. 
What about global warming? Well, the simple fact is that regulations, especially in poor countries, that cost a lot to reduce carbon dioxide emissions, will increase-significantly-expenditures. 21 And if the expense is high enough, those expenditures will give rise to risks, for example in the form of dying of too much heat-if energy prices increase, you might not be able to afford fans or air-conditioning. That last point about global warming suggests that the very fact that risk regulation is often expensive creates risks of its own. 22 And in fact, there are studies suggesting that in the United States, every time the government requires industry to spend $10 million to $30 million, at least one person dies. 23 The mechanism by which the multi-million dollar expenditure produces death isn't agreed upon, 24 but think for example of the simple fact that a very expensive regulation will probably increase unemployment and therefore poverty, neither of which is good for your health. So my suggestion is that under the precautionary principle (which forbids government from allowing possible risk to occur), any regulation of an environmentally risky activity is both required by the precautionary principle and forbidden by the precautionary principle for the simple reason that it imposes a possible risk of death, against which we ought to take precautions. 
So this is why I suggest that the precautionary principle, in its strong forms, is incoherent. It can be made sensible only if we single out some subset of the risks that social situations present. But it's impossible to be universally precautionary. We can be precautionary with respect to some risks, but not with respect to risks in general. And if this seems implausible, try it. Try to be precautionary in general in the next day or two. Chances are that precautions will give rise to the very risks that precautions induce. 
Why then are Europeans and many others excited about the precautionary principle? I suggest that the precautionary principle becomes operational because of the way human minds work, and more particularly, because in assessing risks we tend to rely on something that has an elaborate name-but it's a pretty straightforward idea-called the "availability heuristic." The availability heuristic says that we assess risks by thinking about the ease with which we can recall an example in which the risk came to fruition. 25 So if we don't know a lot about certain situations, we ask, Can we think of an example in which the risk occurred? 
Here's a simple way of illustrating the use of the availability heuristic: Suppose that you ask people how many words on a page, Were Told (Mar. 6, 1997), http://cato.org/testimony/ct-mg030697.html (written testimony before Senate Cancer Coalition). 

Heuristicsand Biases, in JUDGMENT UNDER UNCERTAINTY: HEURISITCS AND BIAsEs 3, 11-14 (Daniel Kahneman et al. eds., 1982). a random page, have as the last three letters "ing." Most people tend to say twenty, twenty-five, fifteen. If you ask people how many words on this page have as the second-to-last letter "n," people say four, five. The reason is that it's easy to think of "ing" words, but not easy to think of words that have as the second-tolast letter "n." So, too, in thinking about whether something gives rise to risks: Available instances often drive judgments. 
An example is a recent cross-national study of risk from terrorism and risk from SARS, the illness that struck Canada, and China much worse, in the last few years. 26 The studies found that Americans believed that the terrorism risk is much higher than the SARS risk, and Canadians just the opposite.27 The disparity was so large that it couldn't easily be justified by reference to reality.28 It's just that Americans have an acute sense, because of the September 11th attacks, of the risk of terrorism, and the sense of the risk is probably wildly inflated. People tend to think that the chance that they, themselves, will die in a terrorist attack next year is about 8 percent-far too high29-whereas we don't have experiences with SARS. So the fact that terrorism is available cognitively and comes to our minds, and SARS doesn't, helps to drive judgments. 
A study of risk perceptions, in the sense of public concerns about risks in the United States for the past thirty years or so, showed-and this is a happy fact-that our concern about risks basically tracks fluctuations in real risks.30 So as certain risks increase, people are more worried, and as other risks decrease, people are less worried. But there was one exception to this happy finding, which is panic bred by vivid illustrations in which harm came to fruition.31 So the mistakes, in which people had an unjustly inflated sense of risk, came from fear of suicide, fear of herpes, and fear of AIDS, in contexts in which the vivid new example made people far more scared than reality warranted. 32 So the conclusion is when people got it wrong, it was because a particularly vivid case received considerable media attention. 33 A Cross-NationalComparison,69 Mo. L. REV. 991 (2004). 

A study of Kenya and Malawi tried to figure out when people are worried about and taking precautions against AIDS.34 When do they use in their own behavior precautionary principles to guard against the AIDS risk? The answer was clear: Had they observed or heard about an illness or death?35 The availability heuristic drove people's risk assessment. 
My suggestion is, when the United States takes precautions against the risks posed by terrorism, we're not following the precautionary principle in the abstract; it's because that particular risk is available to our minds. When Europeans are especially concerned about mad-cow disease or other risks, it's because those are the risks that are cognitively available to them. I am suggesting that Europe does not in fact practice precaution in general-that it's not a possibility. It adopts a precautionary principle against a particular subcategory of risks that are readily available to European thinking, just because of their own recent experiences. 
Okay, that's the end of that footnote, which ends with the claim that it is not possible to be precautionary in general-Europe isn't. The apparent workability of the precautionary principle rests on the fact that human minds naturally fixate on some sets of risks and not on others. When in the context of genetic modification of food, many people are nervous about the risks posed by genetic modification-and it does pose risks-they are fixating on those, and are ignoring the risks posed by refusing to use a technology that has a lot of promise.36 I'm not suggesting that we go forward with genetic modification more than we already have, or that those who like it are right and those who don't like it are wrong. I'm only suggesting that the precautionary principle isn't a helpful entry into the debate. 
Okay, now these footnotes have been destructive. Now we're going to try to get constructive. What I'm going to try to do is to figure out a way in which the precautionary principle can be made 1 (Penn. Inst. for Econ. Res., Working Paper No. 03-007, 2003), availableat http:l! ssrn.com/abstract=382844. 

Baert Wiener, Confronting Risk Tradeoffs, in RISK Vs. RISK: TRADEOFFS IN PROTECTING HEALTH AND THE ENVIRONMENT 1, 1-41 (John D. Graham & Jonathan Baert Wiener eds., 1995); Cass R. Sunstein, Health-Health Tradeoffs, in RISK AND REASON: SAFETY, LAW, AND THE ENVIRONMENT 133, 133-52 (2002); Nielson & Anderson, supra note 14. workable for purposes of environmental law and environmental protection, possibly with regulation in general, conceivably even for human life. 
A first way of trying to get at this might be to suggest that in the environmental context there's a distinctive argument for a precautionary principle. Meaning, there is an argument that would say that environmental risks in particular deserve precautionary thinking, even if risks in general don't and can't. The idea might be that environmental risks are frequently long-term risks and not short-term risks-which is to say incidents of their occurrence won't readily come to mind like the risks associated with workplace accidents-and also that the risks often are faced by a lot of people at once in the form of a statistical probability, rather than a concentrated group of people who are organized. In addition, environmental risks are often faced by wildlife and animals, which don't have a voice in environmental protection. You might say, for these reasons, there is a kind of a built-in problem, which is that environmental risks will receive less attention than they deserve. It's possible. 
But I want to try to build up a coherent precautionary principle more narrowly by focusing on the problems of irreversibility and potential catastrophe. Let's notice that some subsets of the risks that the human species faces are irreversible if they come to fruition-meaning that if they happen, they happen permanently or they can't be stopped except at extraordinary cost. Extinction of species from which human beings benefit or not, or which are important to human beings for moral or other reasons-that's permanent. It's an irreversible risk. Genetic modification has three irreversible risks associated with it, one of which is its effect on pest resistance; that may be either literally irreversible or extremely costly to reverse.37 The risks associated with global warming have an irreversible feature in that carbon dioxide is in the atmosphere for a long, long time-hundreds of years.38 So what we're doing now will face future generations; unless technologies change, there's not much they can do about that. 
Uncertainty: A Real Option Approach, in BATTLING RESISTANCE TO ANTIBIOTICS AND PESTICIDES, supra note 9, at 214, 215-16. 
Carbon Dioxide, http://www.ace.mmu.ac.uk/Resources/Teaching-PacksfKey-Stage_4/ClimateChange/02t.html (last visited Sept. 22, 2005). 
It just so happens that some formulations of the precautionary principle are alert to this point. The United Nations Economic Conference for Europe in 1992 said, "'Where there are threats of ... irreversible damage, lack of full scientific certainty shall not be used as a reason for postponing ... measures to prevent environmental degradation."' 39 The final declaration of the first European Seas at Risk Conference says that "'if the worst case scenario for a certain activity is serious enough, then even a small amount of doubt as to safety of that activity is sufficient to stop it taking place,"' with clear emphasis in context on the irreversible nature of the worst-case scenario. 40 
One way to get at the problem of irreversibility is just to notice that when human beings are asked how much they value environmental goods, they tend to put their values into three separate categories: (1) How much would you value to use the good? Call it a pristine area or a beach. And then there's a dollar amount: (2) How much would you be willing to pay to have an option to use the good in the future? Then there's an amount that's separate and not identical to the amount that people would pay to use the good. And (3), there's also an amount that people would be willing to pay to maintain the existence of the good. What I want to emphasize here is that people are willing to pay something to have an option to prevent a harm that could prove to be irreversible in the future. 
If we notice the existence of option value, we can just say that when a decision, a regulatory decision, involves uncertainty about what the future will bring, and when it's possible to resolve the uncertainty as time goes on, and when one or more alternatives is irreversible, it just makes a lot of sense to pay something extra to protect against the irreversible bad outcome. So my suggestion is, if there's an irreversible harm on the one side and a reversible one on the other, we should pay the extra amount to protect ourselves against the irreversible harm. 
What might this mean concretely? Well, I've referred to global warming as a problem of irreversibility. People do dispute the nature of the threat from global warming. And so it's tempting to say what President Bush has said, which is that we should wait to regulate until we have more information. Often it does Uncertainty, in AN INTRODUCTION TO THE LAW AND ECONOMICS OF ENVIRONMENTAL POLICY: ISSUES IN INSTITUTIONAL DESIGN (2002). make sense to say, if you don't know the magnitude of the problem, you should wait until you have clearer information about it before spending money to prevent it. But in the context of global warming, there's a big problem with waiting, which is the irreversible effect of greenhouse gas emissions on atmospheric concentrations of those gases.41 So if we make emission cuts now, we're essentially purchasing an option to enable global warming to be stopped or slowed in the future at a lower cost. The basic idea is that because of the irreversible nature of carbon dioxide emissions, to expend resources now serves to protect the future wealth by reducing their costs. 
This is just to describe a narrow version of the precautionary principle. Let's call it the Irreversible Harm Precautionary Principle, which doesn't run afoul of the problem of incoherence, which doesn't depend on any selectivity in human minds, depending on whether examples are evident. It just draws attention to a distinctive feature of certain kinds of risks-environmental risks and some risks posed by terrorists. 
Now let's shift from the problem of irreversible harm to the problem of catastrophic harm. Let's try to figure out how to make these go away, these harms, or to protect the human race against them. Let's have a little thought experiment. And if I had paper or a mind-reading machine, or had enough paper to pass out to you, we actually could find the answers. But think, if you will, about the following problem: Which do you think is a higher priority-a one-in-ten risk that 2000 people will die? Or a one-in-onemillion chance that 200 million people will die? Suppose that the government is deciding between two programs: one that will eliminate a one-in-ten risk that 2000 people will die, and one that will eliminate the one-in-a-million risk that 200 million people will die. Which should the government choose? They cost the same. 
One question you might ask yourself, if you're the President of the United States is, Which of those should you give higher priority to? Another question you might ask yourself is, What do you think most people would say in answering that question? 
I've now asked hundreds of people that question, and the answer is clear. The vast majority of people think that the one-in-amillion chance of 200 million deaths is lower priority. Most people think the one-in-ten chance of 2000 deaths deserves more attention than the one-in-one-million chance of 200 million deaths. 
There are a couple of ways to think about this public judgment that the higher probability of that outcome is worth more than the very low probability catastrophe. The first thing to think of, if we're economists, involves expected value.42 And let's just notice that the two problems I gave have identical expected values: 200 lives. Each involves 200 lives. So if human beings consider the one-in-ten chance of 2000 deaths to be higher priority than the tiny chance of 200 million deaths, there's possibly something wrong there. On standard accounts of rationality, they should be treated the same. That's the first cut of the problem: that there seems to be something in human cognition that sometimes treats tiny probabilities, even of extreme disaster, as worth less than their expected value. 43 And maybe the United States' caution about global warming has something to do with the undervaluing of low risks of catastrophe. 
But I think it's worse than that-that people's answers are even worse than they seem. Think for a moment what it would mean for 200 million people to die. Would that be a simple arithmetic multiplier of 2000 people dying-is it just a matter of adding the right number of zeroes? Probably it's a lot worse than that. If 200 million Americans die-and that's what I've given in the survey-then, our nation is devastated. What kind of institutions would we have if two-thirds of our country is lost? What kind of government could we have? How many generations would it take to replicate the system we now have? Two hundred million deaths is a lot worse than adding the relevant zeros to 2000. And human beings seem not easily to understand that fact. 
This is just a suggestion that catastrophic risks deserve some kind of precautionary principle, if only to overcome the human tendency to treat them as worth less than their expected value, if you simply do the multiplication; and worth much less than their real expected value, if you consider the ancillary effects. In some domains we want to be risk averse. That is, we want to build in a big margin of safety. And for a risk of catastrophic harm, that makes a lot of sense. 
I am attempting to sketch a distinctive precautionary principle. It has a name, which won't surprise you: the Catastrophic Harm Precautionary Principle. And the Catastrophic Harm Precautionary Principle is intended as a narrow replacement for the 
Bimodal Response to Unlikely Events, 7 J. RISK & UNCERTAINTY 95, 95, 102 (1993). precautionary principle. We have to make a few qualifications. One is that if you make efforts to reduce catastrophic harm, you might give rise to catastrophic risk just by doing that. It's possible. And so the Catastrophic Harm Precautionary Principle sometimes does run into the incoherence problem. 
Example: the Iraq War. Put to one side the question whether you're for it or against it. The Iraq War was best defended on the grounds that it averted a catastrophic harm, and best criticized on the grounds that it gave rise to a catastrophic harm in the form of increased hatred of the United States and better resources and manpower for terrorist activity. So, this is just to suggest that the Catastrophic Harm Precautionary Principle is coherent, but it can't work in a case in which its deployment gives rise to a catastrophic harm of its own. 
Now turn to global warming. Those who think we ought not to be taking very expensive steps to avert global warming have, as the best arrow in their quiver, the suggestion that those very steps give rise to risk of catastrophic harm. This argument would be plausible if the cost of reducing global warming significantly were so high that it would produce mass unemployment and poverty. But in a context in which there is a catastrophic harm on one side and no catastrophic harm introduced by reducing it, then we have an argument for the Catastrophic Harm Precautionary Principle. 
I want to be a little more concrete about all this by introducing a couple of suggestions for the use of the Catastrophic Harm Precautionary Principle. The first is that even for the Catastrophic Harm Precautionary Principle, the cost matters. If it costs a fortune to eliminate a catastrophic harm, that's different from if you can eliminate the catastrophic harm at essentially zero cost. The second thing to say about the Catastrophic Harm Precautionary Principle is, we want to know who is being protected from catastrophic harm and who is paying the cost of eliminating catastrophic harm. In the context of global warming, it's just the case that the United States has probably relatively little to fear from global warming. First, because our economy does not depend fundamentally on agriculture, 44 and second because we're rich and can adapt pretty well.45 So even in the worst-case scenarios for global warming, the United States is probably not at severe risk. Poor countries, on the other hand, for which agriculture is MODELS OF GLOBAL WARMING 69-98 (2000). 
everything, are in very severe danger.46 They have a risk of economic devastation, of very serious human health problems, and they don't have the wealth to respond to the risk.47 Global warming is plausibly seen (it's not typically analyzed in these terms, but it's plausibly seen in these terms), as a kind of tort imposed by wealthy countries on poor ones through their global greenhouse gas emissions. And that has to bear on the analysis of what the obligations of wealthy countries are. 
If we bear that in mind, we are well on the way to developing-to tie all bits of the argument together-an Irreversible and Catastrophic Harm Precautionary Principle. The principle could be used for the context of global warming-suggesting that while the Kyoto Protocol was imperfect, other approaches would make sense as a way of combating global warming. Consider an approach that would require general and broad participation by India and China, as well as by Germany and Russia and the United States. Such an approach might give a permit for greenhouse gas emissions to poor countries because they need them in order to have their economies grow. Poor countries would be allowed to sell their emissions rights to the United States, which has more money-and that would be good on distributional grounds, because it would help the poor. A sensible approach would have increasing intensity of reductions over time, so it doesn't cause massive poverty and unemployment in the short run, but allows reductions to occur with increasing severity as technology changes. In the United States, we might have a kind of cap-andtrade system, where we cap our carbon dioxide emissions, or greenhouse contributions generally, and allow trading internally. 
Some system of this general kind would work; it wouldn't bankrupt us; it would be a response to the tort-like character of the current situation; and it would adopt not a general idea of precaution, but precaution in this particular context of catastrophic and irreversible risk. 
I'm just about done. My first and negative submission has been that the precautionary principle is incoherent. It forbids the very steps that it requires. My second argument has been that the precautionary principle gives the illusion of coherence because human minds naturally fixate on a subset of risks that human beings encounter, and so the precautionary principle operates with disregard for the other risks that are inevitably part of social situations and that are sometimes introduced by precautions themselves. The Irreversible Harm Precautionary Principle, which maybe underlies the Endangered Species Act,48 is coherent. To evaluate it, we have to know the magnitude of the risk that the Irreversible Harm Precautionary Principle is combating. The Catastrophic Harm Precautionary Principle is coherent. The major qualification is that we have to make sure that our steps that are reducing catastrophic harm aren't themselves introducing catastrophic harm. 
So my main suggestion has been that to operate sensibly, any counsel for precaution has to be attentive to the full range of consequences, not simply to a subset of them. But suitably modified, a precautionary principle focused on irreversibility and potential catastrophe deserves a prominent place not only in the law of environmental protection, but in the everyday life of human beings. Thank you. 

Cass R. Sunstein and Adrian Vermeule  
T H E   L A W   S C H O O L   T H E   U N I V E R S I T Y  O F   C H I C A G O   
  March 2005  
  
This paper can be downloaded without charge at:  The Chicago Working Paper Series Index: http://www.law.uchicago.edu/Lawecon/index.html  and at the Social Science Research Network Electronic Paper Collection:  
http://ssrn.com/abstract_id=691447   Preliminary draft 3/22/05 Not for citation, circulation, or quotation without express permission All rights reserved 
Is Capital Punishment Morally Required? 
The Relevance of Life-Life Tradeoffs 
Cass R. Sunstein* Adrian Vermeule** 
Recent evidence suggests that capital punishment may have a significant deterrent effect, preventing as many eighteen or more murders for each execution. This evidence greatly unsettles moral objections to the death penalty, because it suggests that a refusal to impose that penalty condemns numerous innocent people to death. Capital punishment thus presents a life-life tradeoff, and a serious commitment to the sanctity of human life may well compel, rather than forbid, that form of punishment. Moral objections to the death penalty frequently depend on a distinction between acts and omissions, but that distinction is misleading in this context, because government is a special kind of moral agent. The familiar problems with capital punishment— potential error, irreversibility, arbitrariness, and racial skew—do not argue in favor of abolition, because the world of homicide suffers from those same problems in even more acute form. The widespread failure to appreciate the life-life tradeoffs involved in capital punishment may depend on cognitive processes that fail to treat “statistical lives” with the seriousness that they deserve. 
Many people believe capital punishment is morally impermissible. In their view, executions are inherently cruel and barbaric.1 Often they add that capital punishment is not, and cannot be, imposed in a way that adheres to the rule of law.2 They contend that as administered, capital punishment ensures the execution of (some) innocent people, and also that it reflects arbitrariness, in the form of random or invidious infliction of the ultimate penalty.3 
*Karl N. Llewellyn Distinguished Service Professor of Jurisprudence, The University of Chicago Law School, Department of Political Science and the College. 
**Bernard D. Meltzer Professor of Law, The University of Chicago. The authors thank Robert Hahn, Dan Kahan, Steven Levitt, Richard Posner, and Eugene Volokh for helpful suggestions, and Blake Roberts for excellent research assistance and valuable comments. 
1 See, e.g., Furman v. Georgia, 408 U.S. 238, 371 (1972) (Marshall, J., concurring). 
Punishment, in DEBATING THE DEATH PENALTY 152 
3 See, e.g., JAMES LEIBMAN ET AL., A BROKEN SYSTEM: ERROR RATES IN CAPITAL CASES, 1973 – 1995 (Columb. L. School, Pub. L. Res. Paper No. 15, 2000). 
Defenders of capital punishment come in two different camps. Some are retributivists.4 Following Kant, they claim that for the most heinous forms of wrongdoing, the penalty of death is morally justified or perhaps even required. Other defenders of capital punishment are consequentialists and often also welfarists.5 They contend that the deterrent effect of capital punishment is significant and that it justifies the infliction of the ultimate penalty. Consequentialist defenses of capital punishment, however, tend to assume that capital punishment is (merely) morally permissible, as opposed to being morally obligatory. 
Our goal here is to suggest that the debate over capital punishment is rooted in an unquestioned assumption, and that the failure to question that assumption is a serious moral error. The assumption is that for governments, acts are morally different from omissions. We want to raise the possibility that an indefensible form of the act-omission distinction is crucial to the most prominent objections to capital punishment—and that defenders of capital punishment, apparently making the same distinction, have failed to notice that on the logic of their theory, capital punishment is morally obligatory, not just permissible. We want to suggest, in other words, that capital punishment may be morally required not for retributive reasons, but in order to prevent the taking of innocent lives.6 
constitutional questions. In invalidating the death penalty for juveniles, for example, the Supreme Court did not seriously engage the possibility that capital punishment for juveniles may help to prevent the death of innocents, including the deaths of juvenile innocents.7 And if our suggestion is correct, it is connected to many questions outside of 4 See, e.g., Luis Pojman, Why the Death Penalty is Morally Permissible, in DEBATING THE DEATH PENALTY 51, 55-58 
5 Arguments along these lines can be found in id. at 58-73. 
6 In so saying, we are suggesting the possibility that states are obliged to maintain the death penalty, not that they must inflict that penalty is every individual case of a specified sort; hence we are not attempting to enter into the debate over mandatory death sentences, as invalidated in Woodson v North Carolina, 428 US 280 (1986); Lockett v. Ohio, 438 US 586 (1978). For relevant discussion, see Martha Nussbaum, Equity and Mercy, 22 Phil & Pub Aff 83 (1993). 
7 Roper v. Simmons, 125 US 1183 (2005). Here is the heart of the Court’s discussion: “As for deterrence, it is unclear whether the death penalty has a significant or even measurable deterrent effect on juveniles, as counsel for the petitioner acknowledged at oral argument. . . . [T]he absence of evidence of deterrent effect is of special concern because the same characteristics that render juveniles less culpable than adults suggest as well that juveniles will be less susceptible to deterrence. . . . To the extent the juvenile death penalty might have residual deterrent effect, it is worth noting that the punishment of life imprisonment without the possibility of parole is itself a severe sanction, in particular for a young person.” These are speculations at the context of capital punishment. If omissions by the state are often indistinguishable, in principle, from actions by the state, then a wide range of apparent failures to act—in the context not only of criminal and civil law, but of regulatory law as well—should be taken to raise serious moral and legal problems. Those who accept our arguments in favor of the death penalty may or may not welcome the implications for government action in general. In many situations, ranging from environmental quality to highway safety to relief of poverty, our arguments suggest that in light of imaginable empirical findings, government is obliged to provide far more protection than it now does, and that it should not be permitted to hide behind unhelpful distinctions between acts and omissions. 
The foundation for our argument is a large and growing body of evidence that capital punishment may well have a deterrent effect, possibly a quite powerful one. A leading study suggests that each execution prevents some eighteen murders, on average.8 The particular numbers do not much matter. If the current evidence is even roughly correct, then a refusal to impose capital punishment will effectively condemn numerous innocent people to death. States that choose life imprisonment, when they might choose capital punishment, are ensuring the deaths of a large number of innocent people.9 On moral grounds, a choice that effectively condemns large numbers of people to death seems objectionable to say the least. For those who are inclined to be skeptical of capital punishment for moral reasons—a group that includes one of the current authors—the task is to consider the possibility that the failure to impose capital punishment is, prima facie and all things considered, a serious moral wrong. 
Judgments of this sort are often taken to require a controversial commitment to either a consequentialist or a deontological view about the foundations of moral best, and they do not engage with the empirical literature; of course, that literature does not dispose of the question whether juveniles are deterred by the death penalty. 
8 See Hashem Dezhbakhsh et al., , Does Capital Punishment Have a Deterrent Effect? New Evidence from Postmoratorium Panel Data, 5 AM. L. & ECON. REV 344, 344 (2003). In what follows, we will speak of executions saving eighteen lives on average. We are of course suppressing many issues in that formulation, simply for expository convenience. For one thing, that statistic is a national average, as we emphasize in Part IV. For another thing, future research might find that capital punishment has diminishing returns: if the first 100 executions deter 1800 murders, it does not follow that another 1,000 executions will deter another 18,000 murders. We will take these and like qualifications as understood in the discussion that follows. 
9 In recent years, the number of murders in the United States has fluctuated between 15,000 and 24,000. Federal Bureau of Investigation, Crime in the United States: 2003 Tabl.1 (2003), available at http://www.fbi.gov/ucr/03cius.htm. evaluation. One of our principal points, however, is that the choice between consequentialist and deontological approaches to morality does not seem crucial here; we will suggest that on certain empirical assumptions, theorists of both stripes might converge on the idea that capital punishment is morally obligatory. On consequentialist grounds, the death penalty seems morally obligatory if it is the only or most effective means of preventing significant numbers of murders; and much of our discussion will emphasize this point. For deontologists, a killing is a wrong under most circumstances, and its wrongness does not depend on its consequences or its effects on overall welfare. Many deontologists (of course not all) believe that capital punishment counts as a moral wrong. But in the abstract, any deontological injunction against the wrongful infliction of death turns out to be indeterminate on the moral status of capital punishment if it is necessary to prevent significant numbers of killings. 
An unstated assumption animating much opposition to capital punishment, especially among self-conscious or intuitive deontologists, is that capital punishment counts as an “act,” while the refusal to impose it counts as an “omission,” and that the two are altogether different from the moral point of view. We shall investigate this claim in some detail. But we doubt that the act-omission distinction can bear the moral weight given to it by the critics of capital punishment. Whatever its value as a moral concept where individuals are concerned, the act-omission distinction misfires in the general setting of government regulation. If government policies fail to protect people against air pollution, occupational risks, or racial discrimination, it is inadequate to put great moral weight on the idea that the failure to act is a mere “omission.” No one believes that government can avoid responsibility to protect people against serious dangers, as for example by refusing to enforce regulatory statutes, simply by contending that such refusals are unproblematic omissions.10 If state governments impose light penalties on offenders, or treat certain offenses (say, domestic violence) as unworthy of attention, they should not be able to escape public retribution by contending that they are simply refusing to act. Where government is concerned, failures of protection, through refusals to punish and deter private misconduct, cannot be justified by pointing to the distinction between acts and omissions. 
It has even become common to speak of “risk-risk tradeoffs,” understood to arise when regulation of one risk (say, the risks associated with use of DDT) gives rise to another risk (say, the spread of malaria, against which DDT has been effective).11 Or suppose that an air pollutant creates adverse health effects but also has health benefits, as appears to be the case for ground-level ozone.12 No one believes that for moral reasons, social planners should refuse to take account of such tradeoffs; there is general agreement that whether a particular substance ought to be regulated depends on the overall effect of regulation on human well-being. 
As an empirical matter, criminal law is pervaded by its own risk-risk tradeoffs. If the deterrent signal works, a failure to impose stringent penalties on certain crimes will increase the number of those crimes. A refusal to impose such penalties is, for that reason, problematic from the moral point of view. The very idea of “equal protection of the laws,” in its oldest and most literal sense, attests to the importance of enforcing the criminal and civil law so as to safeguard the potential victims of private violence.13 What we are suggesting is that the death penalty produces a risk-risk tradeoff of its own, indeed what we will call a life-life tradeoff, to the extent that a refusal to impose capital punishment yields a significant increase in the number of deaths of innocent people. 
Of course this point does not resolve the capital punishment debate. By itself, the act of execution may be a wrong, in a way that cannot be said for an act of imposing civil or criminal penalties on (say) environmental degradation. But the existence of life-life tradeoffs raises the possibility that for those who oppose killing, a rejection of capital punishment is not necessarily mandated. On the contrary, it may well be morally compelled. At the very least, those who object to capital punishment, and do so in the name of protecting life, must come to terms with the fact that the failure to inflict capital punishment might fail to protect life—and must, in our view, justify their position in ways that do not rely on question-begging claims about the distinction between acts and omissions. 
We begin, in Part I, with the facts. Contrary to widely-held beliefs, based on partial information or older studies, a wave of recent evidence suggests the possibility that capital punishment saves lives. One study finds that as a national average, each execution deters some eighteen murders. Our question whether capital punishment is morally obligatory is motivated by these findings; our central concern is that foregoing any given execution may be equivalent to condemning some eighteen unidentified people to a premature and violent death. Of course social science can always be disputed in this contentious domain, and we mean to outline, rather than to defend, the relevant evidence here. But the current findings do provide evidence of deterrence, and we think that it is illuminating to take those findings as given for purposes of analysis of the moral issues. Those who would like to abolish capital punishment, and who reject the social science, might find it useful to ask whether they would maintain their commitment to abolition if they were persuaded that capital punishment does have a strong deterrent effect; that is the principal issue that we mean to raise here. 
In Part II, the centerpiece of the paper, we offer a few remarks on moral foundations and examine some standard objections to capital punishment that might seem plausible even in light of the current findings. We focus in particular on the crucial view that capital punishment is objectionable because it requires affirmative and intentional state “action,” not merely an “omission.” That distinction, we suggest, systematically misfires when applied to government, which is a moral agent with distinctive features. The act-omission distinction may not even be intelligible in the context of government, which always faces a choice among policy regimes, and in that sense cannot help but “act.” 
Even if the distinction between acts and omissions can be rendered intelligible in regulatory settings, its moral relevance is obscure. Some acts are morally obligatory, while some omissions are morally culpable. If capital punishment has significant deterrent effects, we suggest that for government to omit to impose it is morally blameworthy, even on a deontological account of morality. Deontological accounts typically recognize a consequentialist override to baseline prohibitions; if each execution saves eighteen lives, on average, then it is plausible to think that the override is triggered, in turn triggering an obligation to adopt capital punishment. 
Once the act/omission distinction is rejected where government is concerned, it becomes clear that the most familiar, and plausible, objections to capital punishment deal with only one side of the ledger: the objections fail to take account of the exceedingly arbitrary deaths that capital punishment apparently deters. We consider rule-of-law concerns about the irreversibility of capital punishment and its possibly random or invidious administration; a strict-scrutiny principle that capital punishment should not be permitted if other means for producing the same level of deterrence are available; and concerns about slippery slopes from capital punishment to other practices. We suggest that while some of these complaints have merit, they do not count as decisive objections to capital punishment, because they embody a flawed version of the act-omission distinction, and generally overlook the fact that the moral objections to capital punishment apply even more strongly to the murders that capital punishment deters. 
In Part III, we conjecture that various cognitive and social mechanisms, lacking any claim to moral relevance, cause many individuals and groups to subscribe to untenable versions of the distinction between acts and omissions, or to underestimate the life-saving benefits of capital punishment while exaggerating the harms that it causes. An important concern here is a sort of misplaced concreteness, stemming from heuristics such as salience and availability. The single person executed is often more visible and more salient in public discourse than are the (on average) eighteen abstract statistical persons whose murders a single execution would deter. If those people, and their names and faces, were highly visible, we suspect that many of the objections to capital punishment would at least be shaken. As environmentalists have often argued, “statistical persons” should not be treated as irrelevant abstractions.14 The point holds for criminal justice no less than for pollution controls. 
Part IV expands upon the implications of our view and examines some unresolved puzzles. Here we emphasize that we hold no brief for capital punishment across all contexts, or in the abstract. The crucial question is what the facts show in particular domains. We mean to include here a plea for a disaggregated approach. The evidence that 14 Lisa Heinzerling, The Rights of Statistical People, 24 HARV. ENV. L. REV. 189, 189 (2000). capital punishment strongly deters murder is aggregate evidence based on national averages; future research and resulting policies would do well to take separate account of various regions and of various classes of offenders and offenses. We also emphasize that our argument is limited to the setting of life-life tradeoffs—settings in which the taking of a life by the state will reduce the number of lives taken overall. We express no view about cases in which that condition does not hold—for example, the possibility of capital punishment for serious offenses other than killing, with rape being the principal historical example, and with rape of children being a currently contested problem. Such cases involve distinctively difficult moral problems that we mean to bracket here. A brief conclusion follows. 
I. 
For many years, the deterrent effect of capital punishment was sharply disputed.15 But a great deal of recent evidence strengthens the claim that capital punishment has large deterrent effects.16 The reason for the shift is that a wave of sophisticated econometric studies have exploited a newly-available form of data, so-called “panel data” that uses all information from a set of units (states or counties) and follows that data over an extended period of time. A leading study used county-level panel data from 3,054 U.S. counties between 1977 and 1996.17 The authors find that the murder rate is significantly reduced by both death sentences and executions. The most striking finding is that on average, each execution results in 18 fewer murders.18 
Other econometric studies also find a substantial deterrent effect. In two papers, Paul Zimmerman uses state-level panel data from 1978 onwards to measure the deterrent effect of execution rates and execution methods. He estimates that each execution deters Gittings find that each execution deters five murders on average.20 They also find that increases in the murder rate come from removing people from death row and also from commutations in death sentences. Yet another study, based on state-level data from 19971999, finds that a death sentence deters 4.5 murders and an execution deters three murders.21 The same study investigates the question whether executions deter crimes of passion and murders by intimates. The answer is clear: these categories of murder are deterred by capital punishment.22 The deterrent effect of the death penalty is also found to be a function of the length of waits on death row, with a murder deterred for every 2.75 years of reduction in the period before execution.23 
In the period between 1972 and 1976, the Supreme Court produced an effective moratorium on capital punishment, and an extensive study exploits that fact to estimate the deterrent effect. Using state-level data from 1960–2000, the authors make before-andafter comparisons, focusing on the murder rate in each state before and after the death penalty was suspended and reinstated.24 The authors find a substantial deterrent effect. After suspending the death penalty, 91% of states faced an increase in homicides—and in 67% of states, the rate was decreased after reinstatement of capital punishment.25 by state basis, Joanna Shepherd finds that the nation-wide deterrent effect of capital punishment is entirely driven by only six states—and that no deterrent effect can be found in the twenty-one other states that have restored capital punishment.27 What distinguishes the six from the twenty-one? The answer lies in the fact that states showing a deterrent effect are executing more people than states that do not. In fact the data show a “threshold effect”: deterrence is found in states that had at least nine executions between 1977 and 1996. In states below that threshold, no deterrence can be found.28 This finding is intuitively plausible. Unless executions reach a certain level, murderers may act as if the death is so improbable as not to be worthy of concern.29 Her main lesson is that once the level of executions reaches a certain level, the deterrent effect of capital punishment is substantial. 
All in all, the recent evidence of a deterrent effect from capital punishment seems impressive. But in studies of this kind, it is hard to control for confounding variables, and a degree of doubt inevitably remains. It remains possible that these findings will be exposed as statistical artifacts or will be found to rest on flawed econometric methods. More broadly, skeptics are likely to question the mechanisms by which capital punishment has a deterrent effect. On the skeptical view, many murderers lack a clear sense of the likelihood and perhaps even the existence of executions in their state; further problems for the deterrence claim are introduced by the fact that capital punishment is imposed infrequently and after long delays.30 In any case many murders are committed in a passionate state that does not lend itself to an all-things-considered analysis on the part of perpetrators. 
As mentioned above, and as we discuss in Part IV, these suppositions are in some tension with existing evidence. But let us suppose that these doubts are reasonable. If so, 26 JOANNA M. SHEPHERD, DETERRENCE VERSUS BRUTALIZATION: CAPITAL PUNISHMENT’S DIFFERING IMPACTS AMONG STATES (Emory Legal Scholarship Working Paper No. 1, 2004). 
27 Id. at 38. 28 Id. at 36 – 38. 
29 Less intuitively, Shepherd finds that in thirteen of the states that had capital punishment, but executed few people, capital punishment actually increased the murder rate. She attributes this puzzling result to what she calls the “brutalization effect,” by which capital punishment devalues human life and teaches people about the legitimacy of vengeance. Id. at 37 – 38. 
30 See Steven Levitt, Understanding Why Crime Fell in the 1990s, 18 J Econ Persp 163 (2004). should current findings be deemed irrelevant for purposes of policy and law? That would be an odd conclusion. In regulation as a whole, it is common to embrace some version of the Precautionary Principle31—the idea that steps should be taken to prevent significant harm even if cause-and-effect relationships remain unclear and even if the risk is not likely to come to fruition. Even if we reject strong versions of the Precautionary Principle,32 it hardly seems sensible that governments should ignore evidence demonstrating a significant possibility that a certain step will save large numbers of innocent lives. 
For capital punishment, critics often seem to assume that evidence on deterrent effects should be ignored if reasonable questions can be raised about it. But as a general rule, this is implausible. In most contexts, the existence of reasonable questions is hardly an adequate reason to ignore evidence of severe harm. If it were, many environmental controls would be in serious jeopardy.33 We do not mean to suggest that government should commit what many people consider to be, prima facie, a serious moral wrong simply on the basis of speculation that this step will do some good. But a degree of reasonable doubt does not seem sufficient to doom capital punishment, if the evidence suggests that significant deterrence occurs. 
In any event, we will proceed by stipulating to the validity of this evidence, in order to isolate the question of its moral significance. Our primary concern here is not to reach a final judgment about the evidence, but how to evaluate capital punishment given the assumption of a substantial deterrent effect. Those who doubt the evidence might ask themselves how they would assess the moral questions if they were ultimately convinced that life-life tradeoffs were actually involved—as, for example, in hostage situations in which officials are authorized to use deadly force to protect the lives of innocent people. 
31 See generally ARIE TROUWBORST, EVOLUTION AND STATUS OF THE PRECAUTIONARY PRINCIPLE IN INTERNATIONAL LAW (2002); INTERPRETING THE PRECAUTIONARY PRINCIPLE (Tim O'Riordan & James Cameron eds., 2002). 
32 See, e.g., Julian Morris, Defining the Precautionary Principle, in RETHINKING RISK AND THE PRECAUTIONARY PRINCIPLE (2002). 
33 Indeed, those skeptical of capital punishment invoked evidence to the effect that capital punishment did not deter, and argued, plausibly, that it would be a mistake to wait for definitive evidence before ceasing with a punishment that could not be shown to be reducing homicide. See Richard O. Lempert, Desert and Deterrence: An Assessment of the Moral Bases of the Case for Capital Punishment, 79 Mich. L. Rev. 1177, 1222-24 (1981). Here is a kind of precautionary principle, arguing against the most aggressive forms of punishment if the evidence suggested that they did not deter. We are arguing for a mirror-image precautionary principle when the evidence goes the other way. 
If capital punishment does have a strong deterrent effect, there is a crucial implication: it must be the case that capital punishment is not a wholly capricious system of punishment, pervaded by false positives. At the very least, some or many prospective murderers must believe that the system has a high degree of accuracy. The simple reason is that if capital punishment were thoroughly error-prone and seen as such, the deterrent signal of the punishment would be so diluted that it would be extremely unlikely to produce such strong and consistent empirical traces as those described above. At the limit, if capital punishment were entirely random, falling with utter arbitrariness upon innocent and guilty alike, there would be no reason for any prospective criminal to factor it into calculations about the costs and benefits of crime. In this sense, it turns out, Justice Potter Stewart’s comparison of capital punishment to being struck by lightning does not hold for current systems (a point on which we will expand below).34 We do not mean to overstate this point. Of course it remains undeniable that capital punishment is sometimes imposed erroneously, and undeniable too that it is sometimes imposed arbitrarily or on invidious grounds within the set of guilty defendants. Nothing we say here is meant to suggest that states should be content with erroneous or arbitrary death sentences. But the evidence suggests that there is at least a high degree of accuracy, in the sense of avoiding false positives, in the infliction of capital punishment. 
Assume, then, that capital punishment does save significant numbers of innocent lives. On what assumptions should that form of punishment be deemed morally unacceptable, rather than morally obligatory? Why should the deaths of those convicted of capital murder, a large fraction of whom are guilty in fact, be considered a more serious moral wrong than the deaths of a more numerous group who are certainly innocents? 
We consider, and ultimately reject, several responses. Our first general contention is that opposition to capital punishment trades on a form of the distinction between acts and omissions. Whatever the general force of that distinction, its application to government systematically fails, because government is a distinctive kind of moral agent. 34 Furman v. Georgia, 408 U.S. 238, 309 – 310 (1972) (Stewart, J., concurring). Our second general contention is that, apart from direct state involvement, the features that make capital punishment morally objectionable to its critics are also features of the murders that capital punishment deters. The principal difference, on the empirical assumptions we are making, is that in a legal regime without capital punishment, far more people die, and those people are innocent of any wrongdoing. No one denies that arbitrariness in the system of capital punishment is a serious problem. But even if the existing system is viewed in its worst light, it involves far less arbitrariness than does the world of homicide. Let us begin, however, with foundational issues. 
On a standard view, it is impossible to come to terms with the moral questions about capital punishment without saying something about the foundations of moral judgments. We will suggest, however, that sectarian commitments at the foundational level are for the most part irrelevant to the issues here. If it is stipulated that the evidence discussed in Part I is correct, both consequentialist and deontological accounts of morality will or should converge upon the view that capital punishment is morally obligatory. Consequentialists will do so because capital punishment minimizes killings overall. Deontologists will do so because an opposition to killing is, by itself, indeterminate in the face of life-life tradeoffs; because a legal regime with capital punishment has a strong claim to be more respectful of life’s value than does a legal regime lacking capital punishment; and because modern deontologists typically subscribe to a consequentialist override or escape-hatch, one that makes otherwise impermissible actions obligatory if necessary to prevent many deaths—precisely what we are assuming is true of capital punishment. Only those few deontologists who both insist upon a strong distinction between state actions and state omissions, and who reject a consequentialist override, will believe the deterrent effect of capital punishment irrelevant in principle. 
Suppose that we accept consequentialism and believe that government actions should be evaluated in terms of their effects on aggregate welfare. If so, the evidence of deterrence strongly supports a moral argument in favor of the death penalty—which, by hypothesis, seems to produce a net gain in overall welfare. Of course there are many complications here; for example, the welfare of many people might increase as a result of knowing that capital punishment exists, and the welfare of many other people might decrease for the same reason. A full consequentialist calculus would require a more elaborate assessment than we aim to provide here. The only point is that if capital punishment produces significantly fewer deaths on balance, there should be a strong consequentialist presumption on its behalf. To be sure, it is also possible to imagine forms of consequentialism that reject welfarism as implausibly reductionist and that see violations of rights as part of the set of consequences that must be taken into account in deciding what to do.35 For some such consequentialists, killings are, under ordinary circumstances, a violation of rights, and this point is highly relevant to any judgment about killings. But even if the point is accepted, capital punishment may be required, not prohibited, on consequentialist grounds, simply because and to the extent that it minimizes rights violations. 
But imagine that we are deontologists, believing that actions by government and others should not be evaluated in consequentialist terms; how can capital punishment be morally permissible, let alone obligatory? Suppose, for example, that under ordinary circumstances, killing a human being is a wrong, and its wrongness does not depend on an inquiry into whether it produces a net increase in welfare. For many critics of capital punishment, a deontological intuition is central; evidence of deterrence is irrelevant because moral wrongdoing by the state is not justified even if it can be defended on utilitarian grounds. Compare a situation in which a state seeks to kill an innocent person, knowing that the execution will prevent a number of private killings; deontologists believe that the unjustified execution cannot be supported even if the state is secure in its knowledge of its beneficial effects. Of course it is contentious to claim that capital punishment is a moral wrong. But if it is, then significant deterrence might be entirely beside the point. 
Despite all this, our claims here do not depend on accepting consequentialism or on rejecting the deontological objection to evaluating unjustified killings in consequentialist terms. The argument is instead that by itself and in the abstract, this objection is indeterminate on the moral status of capital punishment. To the extent possible, we intend to bracket the most fundamental questions and to suggest that 35 Amartya Sen, Rights and Agency, 11 PHIL. & PUB. AFF. 3, 15 – 19 (1982). whatever one’s view of the foundations of morality, the objection to the death penalty is difficult to sustain under the empirical assumptions that we have traced. Taken in its most sympathetic light, a deontological objection to capital punishment is unconvincing if states that refuse to impose the death penalty produce, by that very refusal, significant numbers of additional deaths. For deontologists who emphasize life’s value and object to the death penalty, the problem is acute if the refusal to impose that penalty predictably leads to significant additional murders. In a hostage situation, police officers are permitted to kill (execute) those who have taken hostages if this step is reasonably deemed necessary to save those who have been taken. If the evidence of deterrence is convincing, why is capital punishment so different in principle? 
Of course we could envision a form of deontology that refuses any exercise in aggregation—one that would refuse to authorize, or compel, a violation of rights even if the violation is necessary to prevent a significantly larger number of rights violations. But most modern deontologists reject this position, instead admitting a consequentialist override to baseline deontological prohibitions.36 Although the threshold at which the consequentialist override is triggered varies with different accounts, we will suggest below that if each execution deters some eighteen murders, the override is plausibly triggered. 
To distill these points: the only moral accounts that are inconsistent with our argument are those that both (1) embrace a distinction between state actions and state omissions and (2) reject a consequentialist override. To those who subscribe to this complex of views, and who consider capital punishment a violation of rights, our argument will not be convincing. In the end, however, we believe that it is difficult to sustain the set of moral assumptions that would bar capital punishment if it is the best means of preventing significant numbers of innocent deaths. Indeed, we believe that those who think that they hold those assumptions are motivated by other considerations— especially a failure to give full weight to statistical lives—on which we focus in Part III. 
36 For an overview, see Larry Alexander, Deontology at the Threshold, 37 SAN DIEGO L. REV. 893, 898900 (2000). B. Acts and Omissions 
A natural response to our basic concern would invoke the widespread intuition that capital punishment involves intentional state “action,” while the failure to deter private murders is merely an “omission” by the state. In our view, this appealing and intuitive line of argument goes rather badly wrong. The critics of capital punishment have been led astray by uncritically applying the act-omission distinction to a regulatory setting. Their position condemns the “active” infliction of death by governments, but does not condemn the “inactive” production of death that comes from the refusal to maintain a system of capital punishment. The basic problem is that even if this selective condemnation can be justified at the level of individual behavior, it is difficult to defend for governments.37 A great deal of work has to be done to explain why “inactive,” but causal, government decisions should not be part of the moral calculus. Suppose that we endorse the deontological position that it is wrong to take human lives, even if overall welfare is promoted by taking them. Why does the system of capital punishment violate that position, if the failure to impose capital punishment also takes lives? 
Perhaps our argument about unjustified selectivity is blind to morally relevant factors that condemn capital punishment and that buttress the act-omission distinction in this context. There are two possible points here, one involving intention and the other involving causation. First, a government (acting through agents) that engages in capital punishment intends to take lives; it seeks to kill. A government that does not engage in capital punishment, and therefore provides less deterrence, does not intend to kill. The deaths that result are the unintended and unsought by-product of an effort to respect life. Surely—it might be said—this is a morally relevant difference. Second, a government that inflicts capital punishment ensures a simple and direct causal chain between its own behavior and the taking of human lives. When a government rejects capital punishment, the causal chain is much more complex; the taking of human lives is an indirect consequence of the government’s decision, one that is mediated by the actions of a murderer. The government authorizes its agents to inflict capital punishment, but does not 37 Compare debates over going to war: Some pacifists insist, correctly, that acts of war will result in the loss of life, including civilian life. But a refusal to go to war will often result in the loss of life, including civilian life. authorize private parties to murder; indeed it forbids murder. Surely that is a morally relevant difference too. 
In our view, both the argument from causation and the argument from intention go wrong by overlooking the distinctive features of government as a moral agent. Whatever the general status of the act-omission distinction as a matter of moral philosophy,38 the distinction is least impressive when applied to government.39 The most fundamental point is that unlike individuals, governments always and necessarily face a choice between or among possible policies for regulating third parties. The distinction between acts and omissions may not be intelligible in this context, and even if it is, the distinction does not make a morally relevant difference. Most generally, government is in the business of creating permissions and prohibitions. When it explicitly or implicitly authorizes private action, it is not omitting to do anything, or refusing to act.40 Moreover, the distinction between authorized and unauthorized private action—for example, private killing—becomes obscure when the government formally forbids private action, but chooses a set of policy instruments that do not adequately or fully discourage it. 
A system of punishments that only weakly deters homicide, relative to other feasible punishments, does not quite authorize homicide; but it is not properly characterized as an omission, and little turns on whether it can be so characterized. Suppose, for example, that government fails to characterize certain actions—say, sexual harassment—as tortuous or as violative of civil rights law, and that it therefore permits employers to harass employees as they choose, or to discharge employees for failing to submit to sexual harassment. It would be unhelpful to characterize the result as a product of governmental “inaction.” If employers are permitted to discharge employees for failing to submit to sexual harassment, it is because the law is allocating certain entitlements to employers rather than employees. Or consider the context of ordinary torts. When homeowner B sues factory A, complaining of air pollution, a decision not to rule for B is not a form of inaction; it is the allocation to factory A of property right to pollute. 
Let us apply these points, beginning with the causal version of the argument in favor of an act-omission distinction. For concreteness, suppose that government officials face a choice between two (and only two) packages of policies for reducing the murder rate. Suppose that Package A contains a range of legal instruments, such as ordinary imprisonment, imprisonment without parole (perhaps for life), post-incarceration programs to prevent recidivism, and so on. Package B contains all the same instruments plus capital punishment. 
Stipulating to the validity of the evidence discussed in Part I, the crux of the issue is this: whatever the nature of the causal chain, Package A will inevitably ensure a significant increase in the number of deaths. Why should the length of the causal chain matter? In this setting, it is hard to make sense of the claim that capital punishment involves causal government “action” in some morally distinctive way. For government to opt for Package A—even in the sense of simply leaving in place previously-enacted laws that adopted Package A—is no less an “action” than it is to opt for Package B. Some criminal-justice policy or other will necessarily be in place. The only interesting or even meaningful question government ever faces is not whether to act, but what action should be taken—what mix of criminal-justice policies government ought to pursue. The policy mix that does not include capital punishment is not an “omission” or a “failure to act” in any meaningful sense. If a government chooses that mix, it is allocating a certain set of rights to both murderers and their victims; the latter are certainly given a right to be free from murder, but the right is limited by the terms of the anticipated punishment. In the extreme case, suppose that a state failed to punish certain classes of murders (say, those of African-Americans), or that it punished such murders only infrequently, or that it punished such murders with a slap on the wrist. If so, the distinction between authorizing murder, and failing to prevent it, would become thin. 
The allied idea that capital punishment involves “intentional” action, whereas merely allowing (undeterred) private murders to proceed does not, misfires for parallel reasons. Consider a situation in which regulators refuse to adopt motor vehicle or drug safety regulations that would prevent significant numbers of statistical deaths; is the 
Perhaps the torture example fails to get to the heart of what is most objectionable in our argument. Return to a problem raised above and focus on the following question: “Suppose that holding a show trial to frame and convict one innocent person of murder would deter eighteen real murders. Wouldn’t you be obliged to defend that, crazy as it is?” On one view, the question is fair. It is possible to imagine a finding that a show trial of this sort would deter murders, so that the failure to hold show trials is, in effect, condemning large numbers of people to unjustified deaths. On utilitarian grounds, the show trials might be permissible or even mandatory. On deontological grounds, the answer is at first clear: The state cannot take the lives of innocent people. But if our argument is correct, the deontological argument might not be so clear after all: Might not the failure to conduct show trials be a way of taking the lives of innocent people, too? 
The problem with slippery slope questions of this kind is that they often obscure more than they clarify.66 First, as Rawls pointed out long ago, the systemic effects of a government policy that allowed sham convictions of the innocent, including debilitating uncertainty for other innocents, would themselves have to be folded into the overall assessment.67 Only a conspiracy to keep the policy secret would prevent the unraveling. No such conspiracy is likely to succeed. Put differently, such a policy would violate the publicity constraint emphasized by Rawls and others. It could not be defended publicly and still accomplish its central goal.68 The publicity constraint is a principle of political morality that can be given both deontological and consequentialist justifications,69 so moral theorists of many stripes could reject this sort of hypothetical. 
Second, it is not clear how policymakers could have reliable evidence about the deterrent effects of show trials, torture or other disturbing practices without first experimenting on hapless victims; and the necessary experimentation might well be impermissible on moral grounds ex ante, even if the policies themselves would be 66 Note that a widely held view, which we do not mean to endorse, insists that there is a kind of moral floor for the infliction of severe punishment, a floor that rules out punishment of the innocent, or punishment that is grossly excessive in comparison to the crime, but that permits the death penalty, if the evidence supports it, after people have been convicted, under stringent standards, of committing especially egregious murders.. Of course any floor will be controversial, and the most adamant opponents of capital punishment might believe that it is below the floor by its very nature. 
67 See John Rawls, Two Concepts of Rules, 67 PHILOSOPHICAL REVIEW 3-32 (1955). 68 On the publicity condition, see John Rawls, A Theory of Justice (1971). 
69 For an overview of issues, see David Luban, The Publicity Principle, in THE THEORY OF INSTITUTIONAL DESIGN 154 (Robert Goodin ed. 1996). permissible given certain experimental findings ex post. Capital punishment, however, is already the status quo in most states, and policymakers already have many decades’ worth of reliable data about its deterrent effects. 
Finally, and most fundamentally, we doubt that the intuitions that drive extreme hypotheticals of this sort have moral significance in any event. Of course it is prima facie objectionable, worse than outrageous, if the state proposes to kill people whom it knows to be innocent. The widely held moral and legal norm against executions of innocent people is certainly an individual and social good, whatever one’s views about the foundations of morality. But suppose that a situation arises in which execution of an innocent person really is the only way to save eighteen, or eighty, or eighteen hundred innocent people. There is no reason to think that intuitions about the extreme cases are reliable trackers of moral truth, or to assume that such intuitions have any privileged connection to what a considered moral theory would permit or require; consider the obvious tension between any such intuitions and the accepted practice of killing hostagetakers. We offer more remarks on the moral status of particular intuitions below, and in Part III. 
What is the relationship between the foregoing argument, particularly our rejection of the act-omission distinction as applied to government’s policy choices, and standard debates about deontological and consequentialist approaches in moral theory? We do not think this is a crucial analytic lens for the questions we address.70 As we have emphasized, our argument does not challenge deontological claims as such, except insofar as they apply the act-omission distinction to government and reject any consequentialist override of deontological injunctions. The simple injunction “thou shalt not kill” is too general to cut between the relevant options at the lower level of policy choice. If capital punishment strongly deters killings, and if the government that eschews capital punishment can fairly be charged with those killings, then the government’s only 70 For a recent argument that most or all substantive moral positions can be formulated at will in either deontological or consequentialist terms, see Campbell Brown, Consequentialise This (2004), at http://socpol.anu.edu.au/~cbrown/papers/ConsThis.pdf. choices are to kill more or to kill fewer; the deontological injunction might then be interpreted to require rather than to forbid capital punishment. 
To be sure, some opponents of capital punishment tend to build the act-omission distinction directly into the deontological injunction itself. “Thou shalt not kill” might be interpreted just to mean that the state and its agents shall not themselves kill. Moreover, opponents sometimes assume away the problem of consequentialist overrides. But as we have seen, both the act-omission distinction, and the idea that deontological injunctions are absolute, are highly contentious assumptions; they require independent arguments on their behalf. In many cases, no such argument is offered; all that is typically offered is an intuition that the state must not kill, period. 
We do not believe that, upon reflection, the intuition can be defended, nor do we think that case-specific intuitions should be morally dispositive. In part this is because of familiar arguments in moral theory that commitments to generalizable moral principles should trump intuitions in particular cases.71 In part it is because the reliability of such intuitions is highly suspect. Recent research in cognitive psychology, which we discuss in Part III, suggests that the intuitions underpinning the act-omission distinction may represent cognitive errors, without any moral relevance or larger importance. The only point we emphasize here is that, in light of the recent evidence that capital punishment powerfully deters killings, an opposition to killing is most naturally understood to support capital punishment rather than to undercut it. Opponents of capital punishment who build the act-omission distinction directly into an absolutist deontological injunction, by stating a position against state killing with no consequentialist override, face the prospect that their position will ultimately come to rest upon little more than an inarticulate intuition, a conclusion masquerading as an argument. 
Overall, the crucial question is what the facts show, a point to which we return in Part IV. Perhaps capital punishment might best be restricted to certain classes of offenders or offenses, or even to certain geographic regions; different polities might, in their different factual circumstances, each do well by adopting or rejecting capital punishment as appropriate. It might even turn out that the system of capital punishment is so riddled with errors and arbitrariness that it would be unwise and unjust to adopt it, 71 See generally R.M. HARE, MORAL THINKING: ITS LEVELS, METHODS, AND POINT (1981). simply because it is a hopelessly defective tool of criminal justice. Although we doubt very much that this is so, for the reasons given in Part I, it could be so, and a belief that it is so would supply a very respectable reason for opposing capital punishment. What does not supply such a reason is an indeterminate and unprocessed intuition that the state should not “kill.” 
None of this is to suggest that intuitionism is the only possible basis for opposing capital punishment; of course it is not. Perhaps the death penalty is opposed (as it is sometimes endorsed) on expressivist grounds; perhaps the social meanings of capital punishment are what drive opponents as well as advocates.72 But if the evidence outlined here is correct, expressivist opposition is not so easy to sustain. A failure to protect workers against severe occupational risks is objectionable on expressive grounds because it reflects contempt for the safety of workers.73 A failure to take steps that would prevent significant numbers of murders is itself expressively objectionable, on parallel grounds. 
There is a final point, involving democracy itself. On one view, government’s central obligation is to follow the public will, assuming that it has been properly focused and channeled through processes of public deliberation.74 If the public opposes capital punishment, and insist on an act-omission distinction, then officials should oppose it too, unless, perhaps, opposition can be shown to be ill-informed or to have failed the minimal requirements of political deliberation. And on this view, public support for capital punishment would be presumptively binding as well. We do not mean to say anything contentious about democratic legitimacy here. Both citizens and representatives must ask themselves about what morality requires. If a life-life tradeoff is involved, the moral question is inevitably affected. And if capital punishment saves large numbers of innocent lives, then participants in democracies are obliged to take note of that fact. 
Those who object to capital punishment, and who are not much moved by evidence of deterrence, believe that they are operating in accordance with a freestanding moral principle. They will not be enthusiastic about the suggestion that their moral 72 See Dan M. Kahan, The Secret Ambition of Deterrence, 113 Harv L Rev 413 (1999). 73 See Elizabeth Anderson, Value In Ethics and Economics (1993). 74 See William Bessette, The Mild Voice of Reason (1993). judgments are instead a product of some kind of cognitive error. But in the regulatory domain as a whole, it has become standard to say that cognitive processes contribute to large mistakes, at least on questions of fact.75 For risk regulation, people do seem to focus on a subset of the harms at stake, in a way that produces both excessive and insufficient reactions to environmental problems.76 More generally, a form of “tradeoff neglect” pervades regulatory policy77; and it is easy to imagine that the moral domain has its own kinds of tradeoff neglect. For example, it is common, in the environmental domain, to focus on the risks associated with some kind of environmental degradation, but to neglect the risks associated with environmental regulation; and those who focus on the costs of regulation often neglect the risks of inaction.78 
In a related vein, a great deal of recent work has emphasized the possibility that heuristics and biases can be found in the moral arena, ensuring that deeply felt moral intuitions are a result of errors and confusions.79 This is a possibility and no more. Certainly we cannot demonstrate that opposition to capital punishment is rooted in selective attention or an identifiable heuristic. But return to the hostage situation to which we have referred: Police officers are permitted to kill those who have taken hostages, at least if the killing is reasonably believed to be necessary to save human lives. If capital punishment is deemed differently, it might be because the lives to be saved are merely statistical, as compared with the lives of hostages, which are entirely vivid. More generally, consider two points, the first involving salience, the second involving the foundations of the act-omission distinction. 
75 Roger G. Noll & James E. Krier, Some Implications of Cognitive Psychology for Risk Regulation, 19 J. LEGAL STUD. 747, 777 (1990). 
76 This is the theme of HOWARD MARGOLIS, DEALING WITH RISK: Why the Public and the Experts Disagree on Environmental Issues (1996). 
77 Id. 78 Id. 
79 See, e.g., Jonathan Baron, Heuristics and Biases in Equity Judgments: A Utilitarian Approach, in PSYCHOLOGICAL PERSPECTIVES ON JUSTICE (Barbara A. Mellers & Jonathat Baron eds., 1993); Jonathan Haidt, The Emotional Dog and Its Rational Tail: A Social Intuitionist Approach to Moral Judgment, 108 PSYCHOL. REV. 814 (2001); Cass R. Sunstein, Moral Heuristics (2d John M. Olin L. & Econ. Working Paper No. 180, 2003). A. Salience 
It is obvious that people’s reactions to factual and moral questions are much affected by vividness or salience.80 If an event, such as a terrorist attack, seems salient, people will think it is more likely to occur. And if an event is salient, people may not pay much attention to less salient possibilities, such as the risk that a response to a terrorist attack will cause a significant number of deaths of its own. When people neglect tradeoffs, it is often because one aspect of the situation is highly visible, or “on screen,” while other aspects are less visible or perhaps invisible. 
Consider in this regard the life-life tradeoffs involved with capital punishment. Those subject to capital punishment are real human beings, with their own backgrounds and narratives. Some of them have been subject to multiple forms of unfairness, in the legal process and elsewhere. At least some were wrongly convicted. By contrast, those whose lives are or might be saved by virtue of capital punishment are mere “statistical people.”81 They are both nameless and faceless, and their deaths are far less likely to count in moral deliberations. It is, for this reason, perhaps, that the advocates of capital punishment often focus on the heinousness of the (salient) offender, while the abolitionists focus on his or her humanity. We suspect that the discussion would take a different form if the victims of a regime lacking capital punishment were salient too82; and the example of police behavior in hostage situations supports the suspicion. None of this establishes the claim that the death penalty is morally required if it saves far more lives than it ends. But it does raise the possibility that moral intuitions, for many people, are a product of the salience of one set of deaths and the invisibility or speculativeness of another. 
80 See SCOTT PLOUS, THE PSYCHOLOGY OF JUDGMENT AND DECISION MAKING 125-26, 178-80 (1993) (discussing the salience heuristic and the closely related heuristics of vividness and availability). Cf. Robert M. Reyes, William C. Thompson & Gordon H. Bower, Judgmental Biases Resulting from Differing Availabilities of Arguments, 39 J. PERSONALITY & SOC. PSYCHOL. 2, 5-12 (1980) (demonstrating that vivid, concrete information exerts greater influence on mock jury deliberations than abstract, pallid information). 
81 See Lisa Heinzerling, The Rights of Statistical People, 24 HARV. ENV. L. REV. 189, 189 (2000). 82 Cf. PHILIP K. DICK, THE MINORITY REPORT (2002). B. Acts, Omissions, and Brains 
Outside of the domain of government, we have not questioned the act-omission distinction here. But in some settings, it may be worth considering the possibility that the act-omission distinction operates as a heuristic for a more complex and difficult assessment of the moral issues at stake.83 Consider in this regard the dispute over two well-known problems in moral philosophy.84 These problems do not involve the actomission distinction, but they implicate closely related concerns. We suggest that people’s asymmetrical reactions to the two problems say a great deal about the operation of the act-omission distinction. 
The first, called the trolley problem, asks people to suppose that a runaway trolley is headed for five people, who will be killed if the trolley continues on its current course. The question is whether you would throw a switch that would move the trolley onto another set of tracks, killing one person rather than five. Most people would throw the switch. The second, called the footbridge problem, is the same as that just given, but with one difference: the only way to save the five is to throw a stranger, now on a footbridge that spans the tracks, into the path of the trolley, killing that stranger but preventing the trolley from reaching the others. Most people will not kill the stranger. What is the difference between the two cases, if any? A great deal of philosophical work has been done on this question, much of it trying to suggest that our firm intuitions can indeed be defended in principle.85 
Without engaging these arguments, consider a suggestive experiment designed to see how the human brain responds to the two problems.86 The authors do not attempt to answer the moral questions in principle, but they find “that there are systematic variations in the engagement of emotions in moral judgment,”87 and that brain areas associated with emotion are far more active in contemplating the footbridge problem than in contemplating the trolley problem. An implication of the authors’ finding is that human 83 The argument is ventured in JONATHAN BARON, JUDGMENT MISGUIDED: INTUITION AND ERROR IN PUBLIC DECISION MAKING (1998). 
84 See JUDITH JARVIS THOMSON, RIGHTS, RESTITUTION, AND RISK: ESSAYS IN MORAL THEORY 94-116 (1986). 
85 Id. 
86 Joshua Green et al., An fMRI Investigation of Emotional Engagement in Moral Judgment, 293 SCIENCE 2105 (2001). 
87 Id. at 2107. brains are hard-wired to distinguish between bringing about a death “up close and personal” and doing so at a distance. 88 
Of course this experiment is far from decisive; emotions and cognition are not easily separable,89 and there may be good moral reasons why certain brain areas are activated by one problem and not by the other. Perhaps the brain is closely attuned to morally relevant differences. But consider the case of fear, where an identifiable physical region of the brain makes helpfully immediate but not entirely reliable judgments.90 So too, very plausibly, in the context of morality.91 
For capital punishment, the implication is straightforward. For many people, the prospect of lethal executions is akin to the prospect of throwing the stranger in the footbridge problem; we expect that the relevant part of the brain would light up for most people who sincerely imagine themselves in the position of executioner. But for almost everyone, no such lights would be found after learning that the consequence of abolishing capital punishment was to ensure somewhere between eight and twenty-eight statistical deaths for each foregone execution. No unambiguous moral lesson follows from an understanding of the operation of the human brain. But it is perhaps illuminating if moral judgments are caused by rapid intuitions that do not involve a great deal of cognitive work, and that have no obvious connection to the morally relevant features of the situation. 
Our general claims might be thought to run up against Bernard Williams’ wellknown critique of utilitarianism, rooted largely in a story about an unfortunate tourist named Jim.92 In Williams’ tale. Jim is a tourist in a small town in South America. He notices that twenty Indians are about to be shot. The leader of the shooters gives Jim a chance to save all but one of the Indians, for a price: Jim has to shoot one of them. Williams believes that for utilitarians, the moral answer is clear: Jim should shoot. But in 88 Id. 89 See MARTHA C. NUSSBAUM, UPHEAVALS OF THOUGHT: THE INTELLIGENCE OF EMOTIONS (2001). 90 See Joseph E. LeDoux & Jeff Muller, Emotional Memory and Psychopathology, 352 Philosophical Transactions: Biological Sciences 1719, 1719 (1997). 
91 Joshua Green & Jonathan Haidt, How (And Where) Does Moral Judgment Work?, 6 Trends in Cognitive Sciences 517, 522 – 23 (2002). 
92 See J. J. C. SMART & BERNARD WILLIAMS, UTILITARIANISM: FOR AND AGAINST 98-99 (1973). his view, it is not clear that this is what morality requires. In Williams’ words, utilitarianism “cuts out a kind of consideration which for some others makes a difference to what they feel about such cases: a consideration involving the idea, as we might first and very simply put it, that each of us is especially responsible for what he does, rather than for what other people do.”93 Williams asks: “how can a man, as a utilitarian agent, come to regard as one satisfaction among others, and a dispensable one, a project or attitude round which he has built his life, just because someone else’s projects have so structured the causal scene that that is how the utilitarian sum come out?”94 
We believe that an intuition akin to Williams’ helps to explain opposition to capital punishment even in the face of evidence of the sort that we have outlined. But there are three responses. Most modestly: Capital punishment, on the evidence given here, more closely resembles a hostage situation than Jim’s case. Recall that police officers are permitted, even expected, to use deadly force against those who have taken hostages if that is the only way to save innocent people. Those who are subject to capital punishment are (almost always) egregious wrongdoers, not innocents. Somewhat less modestly: An understanding of the attitudes around which people have built their lives might constrain people, but such an understanding does not constrain states, at least not in the same way. If Jim is a police officer, asked to save twenty hostages, he might well be morally obliged to shoot one of them if that is the only way to save the other nineteen. If Jim is the head of state, asked to engage in military intervention to prevent a mass slaughter (say, in Rwanda), it is not so clear that he should refuse even if he knows that military intervention will also result in innocent deaths at the hands of his own military. At the very least, the moral question cannot be answered, at the level of the state, by insisting “that each of us is especially responsible for what he does, rather than for what other people do.” Less modestly still: Williams ‘ own arguments seem to us to rely on question-begging claims about causation, and about what counts as intentional action. He is right to say that we are particularly responsible for what we do; but in Jim’s case, how does that precept cash out, exactly? Granted that Jim faces bad choices, through no fault 93 Id. at 99. 94 Id at 116. of his own; but can Jim disclaim responsibility for the unnecessary death of nineteen people? IV. Implications and Future Problems 
Here we expand upon the implications of our approach and highlight some remaining puzzles, not with a view to resolving all issues, but with a view to indicating the contours of existing puzzles and some possible directions for future research. We also emphasize that our argument is limited to the setting of life-life tradeoffs: cases in which capital punishment is used to deter killing, rather than other offenses. 
The statistic that each execution saves eighteen lives, on average, is an aggregated statistic based on national-level data—something like a national average. Averages can be misleading, of course, and a look at regional variation suggests a potentially complicated picture. The most recent work suggests that states fall into three sets: a set of states in which capital punishment deters very strongly, a set in which it has no deterrent effect at all, and even a set in which capital punishment has perverse effects, slightly raising murder rates. The pronounced deterrent effect at the level of the national average occurs because, where capital punishment does deter, it deters powerfully. As we mentioned in Part I, Joanna Shepherd suggests there is a “threshold effect” at work here: in states that execute very few, capital punishment either has no effect or even backfires, perhaps because (Shepherd conjectures) a “brutalization effect” operates; while states that execute a larger number see large deterrent effects. Whatever the validity of the particular mechanisms Shepherd proposes, it seems plausible that capital punishment deters strongly in one set of states and has little effect in others. If this is correct, does it undermine our thesis? 
The simple answer is no, not at all, because we hold no brief to promote capital punishment everywhere, at all times and places. Where capital punishment is a powerful deterrent, we have suggested that states may well be morally obligated to adopt it. Where capital punishment does not powerfully deter, the empirical predicate for that obligation disappears. Retributivists might continue to argue for capital punishment on other grounds, but we are not retributivists and see no inherent moral necessity for capital punishment if it produces little in the way of benefits in the protection of human life. If future work were to overturn the recent evidence that capital punishment deters, that work would also in our view overturn the case for capital punishment altogether. 
What holds for variation across states within the United States holds a fortiori for variation across liberal democratic polities. The European Union and its member states firmly reject capital punishment as violative of human dignity; more broadly, the United States is one of only a small number of nations that permit capital punishment. How does this bear upon our thesis? The short answer is that we have nothing to say about such polities, because the relevant facts are not yet known. It might turn out that, due to variation in some relevant factor, capital punishment is appropriate for our circumstances but not for the circumstances of (some set of) other polities; nothing in our view excludes this. If capital punishment turns out to deter strongly in some populations, or given some background legal and economic systems, but not otherwise, then the scope of the moral obligation to adopt capital punishment would vary accordingly. Israel does not execute terrorists, in part because of a belief that executions of terrorists would breed more terrorism; if the belief is correct, as seems plausible, then the failure to use capital punishment is correct too. (Those who favor capital punishment on retributive grounds might ask whether they reject Israel’s policy.) 
Other dimensions of variation include differences in classes of offenses (murders for profit versus murders animated by passion) and in classes of offenders (juvenile murders, mentally disabled murders, and so on). As we noted in discussing the argument from slippery slopes, no a priori argument either precludes or mandates extending capital punishment to all such cases, because a priori argument is not helpful here. 
Let us consider the example of juvenile offenders. In a recent decision, the Supreme Court held that capital punishment may not be inflicted upon offenders who were under 18 years old at the time of the offense.95 Acknowledging the paucity of 95 Roper v. Simmons, 125 S. Ct. 1183, 1198 (2005). evidence either way, the Court speculated that such offenders are less deterrable than adults.96 Below, we will review findings suggesting, by analogy to crimes of passion, that the speculation is mistaken. What if the facts turn out to be otherwise than the Court guessed? Suppose that there turns out to be a significant class of 15-year-old murderers, perhaps predominantly murdering other 15-year-olds; and suppose that relevant data suggested that allowing capital punishment for 15-year-olds would significantly deter those murders. 
In our view, there is a strong argument that states would then be morally obligated to extend capital punishment to such cases (bracketing whether such a course would be legally permitted). If the obligation would attach, it is precisely because killing 15-yearolds is morally unacceptable, and because capital punishment would be the best system for reducing overall the number of such killings; only an implausible version of the act/omission distinction would suggest otherwise. On the other hand, suppose that the Court’s speculation is correct, and that murders by juveniles turn out to be genuinely undeterrable, perhaps because juveniles are not yet capable of reasoning clearly about the expected costs and benefits of crime, or frequently act out of uncontrollable passion. Then the factual predicate for our view would simply fail to apply. 
The same holds true for classes of offenses. The recent work has suggested, contrary to the intuitions of many, that capital punishment may well deter types of murders previously thought undeterrable.97 In particular, the frequent conjecture that murders animated by passion are undeterrable turns out to be erroneous. Should future work identify such a category of undeterrable murders, however, our view would straightforwardly entail that capital punishment ought not extend to such cases. 
Every case we have discussed so far involves what we have called life-life tradeoffs: cases in which state execution deters many private executions. A very different set of moral and institutional problems arises when the issue is whether capital punishment should be used to deter serious offenses other than killing. Consider the 96 Id. at 1196. 97 Shepherd, supra note 21, at 305. sentence of capital punishment for particularly serious forms of rape, which the Supreme Court invalidated as cruel and unusual punishment.98 Apart from constitutional constraints, would states be morally obligated to employ capital punishment if it could be shown, empirically, that each such execution would deter many future rapes? 
We have given simple answers to the foregoing questions about variation in regional and international circumstances, and in classes of offenders and offenses, but no simple answers are possible here. The reason is that, unlike the other cases, the harms on the two sides of the ledger are not the same. Where capital punishment of murderers is at issue, the taking of life is the only morally relevant action in the picture, and the state’s taking of life is entirely commensurable with the crimes it deters. (This assumes that one rejects, as we did in Part II, the attempt to build the act-omission distinction right into the definition of the relevant action. We have rejected, in other words, the idea that the state’s taking of life is itself a morally different act from the “private” taking of life that is made possible by the state’s choice of a criminal-justice policy that does not include capital punishment). When other offenses are at issue, additional moral questions of commensurability and of aggregation arise. May the state inflict a permanent deprivation of all personhood (death) upon a few to deter serious, but temporary, deprivations of autonomy (rape) that would otherwise be inflicted upon many? What if the many are children? The moral analysis here will necessarily be more complicated. If a utilitarian or consequentialist framework is used, the effects must be “translated” in some way so as to permit tradeoffs to occur. Suppose that an execution of a rapist would deter thirty rapes; how should capital punishment be evaluated in that event? It is most unclear how to think about these and similar cases; here we mean to bracket such questions entirely. 
We conjecture that something like the following set of views about capital punishment has been and probably still is widespread in the legal academy. Capital punishment does not deter, or at least the evidence that it does so is essentially 98 See Coker v. Georgia, 433 U.S. 584, 598 (1977). nonexistent99; some categories of murders, especially crimes of passion, are undeterrable (at least by capital punishment);100 even if capital punishment has a deterrent effect, the effect is marginal, perhaps because of the relatively small number of capital sentences and the long time lags between sentencing and execution;101 the system of capital punishment is rife with error and arbitrariness.102 
The recent evidence raises doubts about all of these views. Capital punishment may well have strong deterrent effects; there is evidence that few categories of murders are inherently undeterrable, even so-called crimes of passion; some studies find extremely large deterrent effects; error and arbitrariness undoubtedly occur, but the evidence of deterrence suggests that prospective murderers are receiving a clear signal. 
The moral and legal commentary on capital punishment ought to be sensitive to any significant revision in what we know. Life-life tradeoffs are inescapably involved. In light of recent evidence, a government that settles upon a package of crime-control policies that does not include capital punishment might well seem, at least prima facie, to be both violating the rights and reducing the welfare of its citizens—just as would a state that failed to enact simple environmental measures promising to save a great many lives. 
The most common basis for resisting this conclusion, and our principal target here, is some version of the distinction between acts and omissions. Opponents of capital punishment frequently appeal to an intuition that intentional killing by the government and its agents is morally objectionable in a way that simply allowing private killings is not. Whatever the general merits of the distinction between acts and omissions in the moral theory of individual conduct, we think it gets little purchase on questions of governmental policy. Government cannot help but act in ways that affect the actions of citizens; where citizens decide whether or not to kill each other in light of government’s policies, it is not clear even as a conceptual matter what it would mean for government not to act. For government to adopt a mix of criminal-justice policies that happens not to include capital punishment is not an “omission” or a “failure to act” in any meaningful sense. Likewise, deontological injunctions against unjustified killing, which we have not 99 See Richard O. Lempert, Desert and Deterrence: An Assessment of the Moral Bases of the Case for Capital Punishment, 79 Mich. L. Rev. 1177, 1222 (1981). 
100 See id. at 1193 – 94. 101 See id. at 1192 – 93. 102 See id. at 1224. questioned here, are of little help in these settings. Unjustified killing is exactly what capital punishment prevents. 
If this argument is correct, it has broad implications, some of which may not be welcomed by advocates of capital punishment. Government engages in countless omissions, many of which threaten people’s health and safety; consider the failure to reduce highway fatalities, to regulate greenhouse gas emissions, to prevent domestic violence, to impose further controls on private uses of guns, even to redistribute wealth to those who most need it. Suppose that it is not sensible, in these and other contexts, to characterize government omissions as such, or suppose that even if the characterization is sensible, it lacks moral relevance. If so, then government might well be compelled, on one or another ground, to take steps to protect people against statistical risks, even if those steps impose costs and harms; much will depend on what the facts show.103 
Any objection to capital punishment, we believe, must rely on something other than abstract injunctions against the taking of life. If the recent evidence of deterrence is shown to be correct, then opponents of capital punishment will face an uphill struggle on moral grounds. If each execution is saving many lives, the harms of capital punishment would have to be very great to justify its abolition, far greater than most critics have heretofore alleged. There is always residual uncertainty in social science and legal policy, and we have attempted to describe, rather than to defend, recent findings here. But if those findings are ultimately shown to be right, capital punishment has a strong claim to being, not merely morally permissible, but morally obligatory, above all from the standpoint of those who wish to protect life. 
103 For a general attack on the act-omission distinction from a utilitarian perspective, see Jonathan Baron, Judgment Misguided (1998). If our argument is correct, some of Baron’s arguments should be appealing to deontologists as well, and his controversial commitment to utilitarianism is not a necessary foundation for his conclusions. 2.  William M. Landes, Copyright Protection of Letters, Diaries and Other Unpublished Works: An  Economic Approach (July 1991)  Richard A. Epstein, The Path to The T. J. Hooper: The Theory and History of Custom in the Law of  Tort (August 1991)  Cass R. Sunstein, On Property and Constitutionalism (September 1991)  Richard A. Posner, Blackmail, Privacy, and Freedom of Contract (February 1992)  Randal C. Picker, Security Interests, Misbehavior, and Common Pools (February 1992)  Tomas J. Philipson & Richard A. Posner, Optimal Regulation of AIDS (April 1992)  Douglas G. Baird, Revisiting Auctions in Chapter 11 (April 1992)  William M. Landes, Sequential versus Unitary Trials: An Economic Analysis (July 1992)  William M. Landes & Richard A. Posner, The Influence of Economics on Law: A Quantitative Study  (August 1992)  Alan O. Sykes, The Welfare Economics of Immigration Law: A Theoretical Survey With An  Analysis of U.S. Policy (September 1992)  Douglas G. Baird, 1992 Katz Lecture: Reconstructing Contracts (November 1992)  Gary S. Becker, The Economic Way of Looking at Life (January 1993)  J. Mark Ramseyer, Credibly Committing to Efficiency Wages: Cotton Spinning Cartels in Imperial  Japan (March 1993)  Cass R. Sunstein, Endogenous Preferences, Environmental Law (April 1993)  Richard A. Posner, What Do Judges and Justices Maximize? (The Same Thing Everyone Else Does)  (April 1993)  Lucian Arye Bebchuk and Randal C. Picker, Bankruptcy Rules, Managerial Entrenchment, and  Firm‐Specific Human Capital (August 1993)  J. Mark Ramseyer, Explicit Reasons for Implicit Contracts: The Legal Logic to the Japanese Main  Bank System (August 1993)  William M. Landes and Richard A. Posner, The Economics of Anticipatory Adjudication  (September 1993)  Kenneth W. Dam, The Economic Underpinnings of Patent Law (September 1993)  Alan O. Sykes, An Introduction to Regression Analysis (October 1993)  Richard A. Epstein, The Ubiquity of the Benefit Principle (March 1994)  Randal C. Picker, An Introduction to Game Theory and the Law (June 1994)  William M. Landes, Counterclaims: An Economic Analysis (June 1994)  J. Mark Ramseyer, The Market for Children: Evidence from Early Modern Japan (August 1994)  Robert H. Gertner and Geoffrey P. Miller, Settlement Escrows (August 1994)  Kenneth W. Dam, Some Economic Considerations in the Intellectual Property Protection of  Software (August 1994)  Cass R. Sunstein, Rules and Rulelessness, (October 1994)  David Friedman, More Justice for Less Money: A Step Beyond Cimino (December 1994)  Daniel Shaviro, Budget Deficits and the Intergenerational Distribution of Lifetime Consumption  (January 1995)  Douglas G. Baird, The Law and Economics of Contract Damages (February 1995)  Daniel Kessler, Thomas Meites, and Geoffrey P. Miller, Explaining Deviations from the Fifty  Percent Rule: A Multimodal Approach to the Selection of Cases for Litigation (March 1995)  Geoffrey P. Miller, Das Kapital: Solvency Regulation of the American Business Enterprise (April  1995)  Richard Craswell, Freedom of Contract (August 1995)  J. Mark Ramseyer, Public Choice (November 1995)  Kenneth W. Dam, Intellectual Property in an Age of Software and Biotechnology (November 1995)  Cass R. Sunstein, Social Norms and Social Roles (January 1996)  37.  
David A. Weisbach, Measurement and Tax Depreciation Policy: The Case of Short‐Term Assets  (January 2003)  Randal C. Picker, Understanding Statutory Bundles: Does the Sherman Act Come with the 1996  Telecommunications Act? (January 2003)  Douglas Lichtman and Randal C. Picker, Entry Policy in Local Telecommunications: Iowa Utilities  and Verizon (January 2003)  William Landes and Douglas Lichtman, Indirect Liability for Copyright Infringement: An  Economic Perspective (February 2003)  Cass R. Sunstein, Moral Heuristics (March 2003)  Amitai Aviram, Regulation by Networks (March 2003)  Richard A. Epstein, Class Actions: Aggregation, Amplification and Distortion (April 2003)  Richard A. Epstein, The “Necessary” History of Property and Liberty (April 2003)  Eric A. Posner, Transfer Regulations and Cost‐Effectiveness Analysis (April 2003)  Cass R. Sunstein and Richard H. Thaler, Libertarian Paternalizm Is Not an Oxymoron (May 2003)  Alan O. Sykes, The Economics of WTO Rules on Subsidies and Countervailing Measures (May  2003)  Alan O. Sykes, The Safeguards Mess: A Critique of WTO Jurisprudence (May 2003)  Alan O. Sykes, International Trade and Human Rights: An Economic Perspective (May 2003)  Saul Levmore and Kyle Logue, Insuring against Terrorism—and Crime (June 2003)  Richard A. Epstein, Trade Secrets as Private Property: Their Constitutional Protection (June 2003)  Cass R. Sunstein, Lives, Life‐Years, and Willingness to Pay (June 2003)  Amitai Aviram, The Paradox of Spontaneous Formation of Private Legal Systems (July 2003)  Robert Cooter and Ariel Porat, Decreasing Liability Contracts (July 2003)  David A. Weisbach and Jacob Nussim, The Integration of Tax and Spending Programs (September  2003)  William L. Meadow, Anthony Bell, and Cass R. Sunstein, Statistics, Not Memories: What Was the  Standard of Care for Administering Antenatal Steroids to Women in Preterm Labor between 1985  and 2000? (September 2003)  Cass R. Sunstein, What Did Lawrence Hold? Of Autonomy, Desuetude, Sexuality, and Marriage  (September 2003)  Randal C. Picker, The Digital Video Recorder: Unbundling Advertising and Content (September  2003)  Cass R. Sunstein, David Schkade, and Lisa Michelle Ellman, Ideological Voting on Federal Courts  of Appeals: A Preliminary Investigation (September 2003)   Avraham D. Tabbach, The Effects of Taxation on Income Producing Crimes with Variable Leisure  Time (October 2003)  Douglas Lichtman, Rethinking Prosecution History Estoppel (October 2003)  Douglas G. Baird and Robert K. Rasmussen, Chapter 11 at Twilight (October 2003)  David A. Weisbach, Corporate Tax Avoidance (January 2004)  David A. Weisbach, The (Non)Taxation of Risk (January 2004)  Richard A. Epstein, Liberty versus Property? Cracks in the Foundations of Copyright Law (April  2004)  Lior Jacob Strahilevitz, The Right to Destroy (January 2004)  Eric A. Posner and John C. Yoo, A Theory of International Adjudication (February 2004)  Cass R. Sunstein, Are Poor People Worth Less Than Rich People? Disaggregating the Value of  Statistical Lives (February 2004)  Richard A. Epstein, Disparities and Discrimination in Health Care Coverage; A Critique of the  Institute of Medicine Study (March 2004)  Richard A. Epstein and Bruce N. Kuhlik, Navigating the Anticommons for Pharmaceutical Patents:  Steady the Course on Hatch‐Waxman (March 2004)  Richard A. Esptein, The Optimal Complexity of Legal Rules (April 2004)  Eric A. Posner and Alan O. Sykes, Optimal War and Jus Ad Bellum (April 2004)  Alan O. Sykes, The Persistent Puzzles of Safeguards: Lessons from the Steel Dispute (May 2004)  
Follow this and additional works at: https://chicagounbound.uchicago.edu/law_and_economics 
PUBLIC LAW AND LEGAL THEORY WORKING PAPER NO. 111  JUSTICE BREYER’S DEMOCRATIC PRAGMATISM  
Cass R. Sunstein  
T H E   L A W   S C H O O L   T H E   U N I V E R S I T Y  O F   C H I C A G O   
November 2005  
This paper can be downloaded without charge at the John M. Olin Program in Law and  Economics Working Paper Series: http://www.law.uchicago.edu/Lawecon/index.html and at the  
Public Law and Legal Theory Working Paper Series:   http://www.law.uchicago.edu/academics/publiclaw/index.html  
and  The Social Science Research Network Electronic Paper Collection:  
http://ssrn.com/abstract_id=845064   Preliminary draft 11/5/05 All rights reserved Forthcoming, Yale Law Journal 
Cass R. Sunstein* 
There have been many efforts to reconcile judicial review with democratic self-government. Some such efforts attempt to justify judicial review if and to the extent that it promotes self-rule. Active Liberty, by Justice Stephen Breyer, is in this tradition; but it is also marked by a heavy pragmatic orientation, emphasizing as it does the need for close attention to purposes and to the importance of consequences to legal interpretation. Its distinctiveness lies in its effort to forge close connections among three seemingly disparate ideas: a democratic account of judicial review; a purposive understanding of legal texts; and a neo-pragmatic emphasis on consequences. Breyer’s argument is convincing insofar as it challenges “originalist” approach on pragmatic grounds. It is more vulnerable insofar it downplays the inevitable role of judicial discretion in the characterization of purposes and the evaluation of consequences. Those who emphasize consequences, and active liberty, might well end up embracing textualism, or even broad judicial deference to legislative majorities. Moreover, it is not simple to deduce, from the general idea of “active liberty,” concrete conclusions on the issues that concern Breyer, such as affirmative action, campaign finance reform, privacy rights, and commercial advertising. Many competing approaches to these issues, and to interpretation as a whole, can also march proudly under the pragmatic banner. 
* Karl N. Llewellyn Distinguished Service Professor, Law School and Department of Political Science, University of Chicago Law School. This essay grows out of Cass R. Sunstein, The Philosopher-Justice, The New Republic (Sept. 19, 2005); I have substantially revised and expanded the discussion here, and in some ways the basic orientation has shifted. I am grateful to Adrian Vermeule for extremely valuable comments on a previous draft. 
The pragmatic method is primarily a method of settling metaphysical disputes that otherwise might be interminable. Is the world one or many?—fated or free?—material or spiritual?—here are notions either of which may or may not hold good of the world; and disputes over such notions are unending. The pragmatic method in such cases is to try to interpret each notion by tracing its respective practical consequences. What difference would it practically make to any one if this notion rather than that notion were true? —William James1 
was made up of reasonable persons pursuing reasonable purposes reasonably. —Henry Hart and Albert Sacks2 
Throughout the nation’s history, many of the most prominent constitutional theorists have tried to reconcile judicial review with the national commitment to democratic self-rule. They have argued that if the Supreme Court acts in a certain way, it can coexist comfortably with democracy after all.3 Much of this work is highly conceptual—more theoretical than pragmatic, in the sense that abstract ideas, rather than concrete consequences, are in the foreground. 
Early in the twentieth century, for example, James Bradley Thayer emphasized democratic considerations in order to argue that the Supreme Court should strike down legislation only “when those who have the right to make laws have not merely made a mistake, but have made a very clear one,—so clear that it is not open to rational believed that courts should take the same approach to challenged legislation that juries take to criminal defendants; thus he argued that the Supreme Court should uphold the actions of the elected branches unless their invalidity is “very plain and clear, clear beyond a reasonable doubt.”6 Thayer’s view was largely followed by Oliver Wendell Holmes, perhaps the greatest figure in the history of American law, who generally agreed with his plea for judicial deference to the legislature. In Holmes words, “If my fellow citizens want to go to Hell I’ll help them. It’s my job.”7 Unlike Thayer, Holmes was inspired by pragmatism,8 but his own arguments, at least as they appeared in judicial opinions, were quite abstract.9 In the period after Franklin Delano Roosevelt’s New Deal, Thayer’s approach had a significant role on the Supreme Court,10 embraced as it generally was by Felix Frankfurter,11 Holmes’ disciple. 12 
To many people, the idea of judicial deference to the elected branches lost much of its theoretical appeal in the 1950s and 1960s, when the Supreme Court, under the leadership of Chief Justice Earl Warren, was invalidating school segregation,13 protecting freedom of speech,14 striking down poll taxes,15 requiring a rule of one person, one vote,16 and protecting accused criminals against police abuse.17 Is it possible to defend the Warren Court against the charge that its decisions were fatally undemocratic? The most elaborate effort came from John Hart Ely, the Warren Court’s most celebrated expositor and defender, who famously argued for what he called a “representationreinforcing” approach to judicial review.18 Like Thayer, Ely emphasized the central importance of democratic self-rule. But Ely famously insisted that if self-rule is really our loadstar, then unqualified judicial deference to legislatures is utterly senseless. Some rights, Ely argued, are indispensable to self-rule, and the Court legitimately protects those rights not in spite of democracy but in its name.19 The right to vote and the right to speak are the central examples. Courts promote democracy when they protect those rights. 
Ely went much further. He argued that some groups are at a systematic disadvantage in the democratic process, and that when courts protect “discrete and insular minorities,” they are reinforcing democracy too.20 Ely was particularly concerned with African-Americans, whom he saw as unable to protect themselves in politics. But with some qualifications, his plea for judicial protection extended to other groups as well.21 And while Ely was clearly concerned with consequences, his argument stressed democracy’s preconditions, and indeed the general idea of equal concern and respect.22 
A third theory of constitutional interpretation, stressed most prominently by Justice Antonin Scalia23 and also favored by Clarence Thomas,24 is “originalism.” Invoking both self-government and the rule of law, originalists believe that the Constitution should be interpreted to mean what it meant at the time that it was ratified. If the Equal Protection Clause was originally understood to permit sex discrimination, then courts should permit sex discrimination. If the Second Amendment was originally understood to forbid gun control, then courts should forbid gun control. When President Bush praises “strict construction,” many people take him to be embracing originalism. 25 Originalists like Scalia do not follow Thayer’s approach, because they are quite prepared to strike down legislation that violates the original understanding. Originalists do not defend Ely’s idea of “representation-reinforcing” judicial review. But in their own way, originalists too prize democracy. They emphasize that the Constitution was ratified by We the People, who have sovereign authority, and they want to limit the discretion of federal judges, who are after all unelected.26 
It is true that those who ratified the Constitution are long dead, and this point might be thought to create a real problem for originalists who attempt to defend their approach on democratic grounds. Why—in the name of self-government?—should current citizens be bound by those who lived long ago? But democracy is central to originalist thinking about constitutional law.27 Above all, originalists fear that if judges do not follow the original understanding, they will be creating the Constitution anew, because they will give it the content of their own choosing. Originalist arguments are not always pragmatic in spirit. On the contrary, they can be highly abstract, stressing considerations of legitimacy.28 But some originalists are aware that their approach would have dramatic and perhaps intolerable consequences.29 
Inspired by pragmatic considerations, they are willing to attempt to reduce that risk.30 
25 Mona Charen, Do-Gooders: How Liberals Hurt Those They Claim to Help (and The Rest of Us) (2004). 
26 See Scalia, A Matter of Interpretation, supra note. 27 See Cass R. Sunstein, Justice Scalia’s Democratic Formalism, 107 Yale LJ 529 (1997). 28 See Robert Bork, The Tempting of America (1985); some of the arguments in Scalia, A Matter of Interpretation, are in the same vein. Consider this passage: “The principal theoretical defect of nonoriginalism, in my view, is its incompatibility with the very principle that legitimizes judicial review of constitutionality. . . . I take the need for theoretical legitimacy seriously, and even if one assumes (as many nonoriginalists do not even bother to do) that the Constitution was originally meant to expound evolving rather than permanent values, . . . I see no basis for believing that supervision of the evolution would have been committed to the courts. At an even more general theoretical level, originalism seems to me more compatible with the nature and purpose of a Constitution in a democratic system.” 
29 See Scalia, The Lesser Evil, supra note: “I can be much more brief in describing what seems to me the second most serious objection to originalism: In its undiluted form, at least, it is medicine that seems too strong to swallow. Thus, almost every originalist would adulterate it with the doctrine of state decisis-so that Marbury v. Madison would stand even if Professor Raoul Berger should demonstrate unassailably that it got the meaning of the Constitution wrong. . . . But stare decisis alone is not enough to prevent originalism from being what many would consider too bitter a pill. What if some state should enact a new law providing public lashing, or branding of the right hand, as punishment for certain criminal offenses? Even if it could be demonstrated unequivocally that these were not cruel and unusual measures in 1791, and even though no prior Supreme Court decision has specifically disapproved them, I doubt whether any 
As a law professor at Harvard Law School, Stephen Breyer specialized in administrative law. His important work in that field was marked above all by its unmistakably pragmatic foundations.31 Indeed, one of his major innovations lay in an insistence on the importance of evaluating traditional doctrines not in a vacuum, but in light of the concrete effects of regulation on the real world.32 Hence Breyer argued for a close connection between administrative law and regulatory policy.33 While some of his work touched on the separation of powers,34 constitutional law was not his field. But as a member of the Supreme Court, Breyer has slowly been developing a distinctive approach of his own, one that also has a pragmatic dimension, and that can be seen as directly responsive to his colleague Scalia and to originalism. 
This book announces and develops that theory. Its most distinctive feature is its effort to connect three seemingly disparate claims. The first is an insistence that judicial review can and should be undertaken with close reference to active liberty and to democratic goals, a point with clear links to Ely’s work. The second is an emphasis on the centrality of “purposes” to legal interpretation, a point rooted in the great legal process materials of Henry Hart and Albert Sacks and in particular their brilliant note on statutory interpretation.35 The third is a claim about the need to evaluate theories of legal interpretation with close reference to their consequences, a point whose foundations can be found in American pragmatism.36 
As we shall see, much of the interest of Breyer’s book lies in its effort to integrate these three claims. I shall be raising questions about that effort, above all on two grounds. federal judge--even among the many who consider themselves originalists-- would sustain them against an eighth amendment challenge.” 
30 Note here that Justice Scalia confesses, “I hasten to confess that in a crunch I may prove a fainthearted originalist. I cannot imagine myself, any more than any other federal judge, upholding a statute that imposes the punishment of flogging.” See id. 
31 See, e.g., Stephen Breyer, Regulation and its Reform (1985). 32 See, e.g., id; Stephen Breyer, Breaking the Vicious Circle (1993). 
33 See Stephen Breyer et al., Administrative Law and Regulatory Policy (6th ed. 2006). Full disclosure: I am among the “al.” now working on the book, and hence Breyer and I are, in a formal sense, coauthors. But Breyer, otherwise occupied, has not worked on the book since I have joined it. 
34 See Stephen Breyer, The Legislative Veto After Chadha, 72 Geo LJ 785 (1984). 
eds. 1994) First, those who emphasize active liberty and democratic self-government might well reject a purposive approach to interpretation, including purposive interpretation of the Constitution. Second, those who believe in the importance of consequences might well be drawn to an approach very different from Breyer’s, including textualism, Thayerism, and perhaps even originalism. Breyer’s arguments are unfailingly reasonable; the question is whether his general commitments are enough to justify his particular conclusions. Let us now turn to some details. 
Breyer’s organizing theme is “active liberty,” which he associates with the right of self-governance. It is noteworthy that in his own judicial work, Breyer is plausibly seen as the most consistently democratic member of the Rehnquist Court: Among its nine members, he has shown the highest percentage of votes to uphold acts of Congress37 and also to defer to the decisions of the executive branch.38 And indeed, a great deal of his book is a plea for judicial caution and deference.39 But Breyer does not mean to follow Thayer; he does not say that the Court should uphold legislation whenever the Constitution is unclear. Like Ely, Breyer does not rule out the view that courts should take an aggressive role in some areas, above all in order to protect democratic governance (p. 11). 
His short book comes in three parts. The first builds on Benjamin Constant’s famous distinction between the liberty of the ancients and the liberty of the moderns (pp. 3-7).40 The liberty of the ancients involves “active liberty”—the right to share in the exercise of sovereign power. Quoting Constant, Breyer refers to the hope that the sharing of that power would “ennoble” the people’s “thoughts and establish among them a kind of intellectual equality which forms the glory and power of a free people” (p. 4). But Constant also prized negative liberty, meaning “individual independence” from government authority. As Breyer describes Constant’s view, which he firmly endorses, it is necessary to have both forms of freedom, and thus “to combine the two together” (p. 5).41 
Breyer believes that the founders of the Constitution did exactly that. His special emphasis is on what Constant called “an active and constant participation in public power.”42 That form of participation includes voting, town meetings, and the like; but it also requires that citizens receive information and education, in order to promote their capacity to ensure effective governance. In Breyer’s view, the citizens of postrevolutionary America insisted on highly democratic forms of state government, promoting popular control. Breyer is aware of the highly ambivalent experiences of postrevolutionary governments; he know that some commentators have rejected the view that the Constitution is a democratic document.43 Nonetheless, he believes that the founders of the Constitution accepted the deepest aspirations of the American Revolution, creating a framework with a “basically democratic outlook” (p. 25). 
After all, the document begins with the words, “We the People,” and in Breyer’s view, its very structure is a testimonial to active liberty. Both the House and the Senate are subject to electoral control. Even with the electoral college, the choice of the President is ultimately traceable to voters, not to an unaccountable elite. Breyer thinks that the whole system is “difficult to reconcile with a retreat from democratic principle.” On the contrary, he claims that the Constitution can be viewed “as focusing first and foremost upon ‘active liberty.’” He thinks that constitutional interpretation should be undertaken with close reference to that overriding constitutional purpose.44 
In Breyer’s account, the Warren Court appreciated active liberty, and it attempted to make that form of liberty more real for all Americans (p. 11). By contrast, the Rehnquist Court may have pushed the pendulum too far back in the other direction (id.). In short, Breyer believes that an appreciation of active liberty has concrete implications for a wide range of modern disputes. 
The second part of his book traces those implications. He begins with free speech. An obvious question is whether the Court should be hostile or receptive to campaign finance reform. With his eye directly on the democratic ball, Breyer suggests that if we focus on the “the Constitution’s basic structural objective, ‘participatory selfgovernment’” (p. 46), then we will be receptive to restrictions on campaign contributions. A central reason is that such restrictions “seek to democratize the influence that money can bring upon the electoral process” (p. 47). He thinks that some of his colleague s, most prominently Rehnquist and Scalia, have been quite mistaken to invoke negative liberty as a rigid barrier to campaign finance restrictions. In the same vein, he insists that the free speech principle, seen in terms of active liberty, gives special protection to political speech, and significantly less protection to commercial advertising. He criticizes his colleagues on the Court for protecting advertising with the aggressiveness that they have shown in recent years. His purposive interpretation of freedom of speech thus emphasizes democratic self-government above all.45 
Affirmative action might seem to have little to do with active liberty. At first glance, it poses a conflict between the ideal of color-blindness and what Breyer calls a “narrowly purposive” (p. 80) understanding of the Equal Protection Clause, one that emphasizes the historical mistreatment of African-Americans. Directly disagreeing with some of his colleagues,46 Breyer endorses the narrowly purposive approach. But he also contends that in permitting affirmative action at educational institutions, the Court has been centrally concerned with democratic self-government. The reason, pragmatic in character, is that “some form of affirmative action” is “necessary to maintain a wellfunctioning participatory democracy” (p. 81). Breyer points to the Court’s emphasis on the role of broad access to education in “sustaining our political and cultural heritage” and in promoting diverse leadership. Underlining those points, Breyer argues that the Court’s decision to permit affirmative action made a direct appeal “to principles of fraternity , to principles of active liberty” (p. 82). In Breyer’s view, it should be no surprise that the Court selected an interpretation of the Equal Protection Clause that would, as a pragmatic matter, promote rather than undermine the operation of democracy. As serious problem with the attack on affirmative action is that it would produce intolerable consequences. 
With respect to privacy, Breyer’s emphasis is on the novelty of new technologies and the rise of unanticipated questions about how to balance law enforcement needs against the interest in keeping personal information private. Because of the difficulty of those problems, Breyer argues, on pragmatic grounds, for “a special degree of judicial modesty and caution.” He wants to avoid a “premature judicial decision” that would risk “short-circuiting, or pre-empting, the ‘conversational law-making process.” Hence his plea is for narrow, cautious judicial rulings that do not lay out long-term solutions. In Breyer’s view, such rulings serve active liberty, because they refuse to “limit legislative options in ways now unforeseeable.” By its very nature, a narrow ruling is unlikely to “interfere with any ongoing democratic policy debate.” His argument here is important, because other members of the Court, most notably Scalia, have objected to narrow rulings on the ground that they leave too much uncertainty for the future.47 
Some of the most noteworthy decisions of the Rehnquist Court have attempted to limit the power of Congress. 48 For example, the Court has struck down the Violence Against Women Act as beyond congressional authority under the Commerce Clause.49 It has also announced an “anti-commandeering” principle, one that forbids the national government from requiring state legislatures to enact laws.50 In the abstract, those decisions seem to promote active liberty, because they decrease the authority of the more remote national government, and because they promote participation and self-government at the local level. Breyer is no critic of federalism or defender of centralized government. He agrees that the federal system fits with his general theme, because that system makes “it easier for citizens to hold government officials accountable” and because it brings “government closer to home.” 47 See, e.g. Antonin Scalia, The Rule of Law is a Law of Rules, 56 U Chi L Rev 1175 (1989). 48 United States v. Morrison, 529 US 598 (2000). 49 Id 50 New York v US, 505 US 144 (1992). 
Nonetheless, he strongly objects to the Court’s recent federalism decisions. With respect to congressional enactments, “the public has participated in the legislative process at the national level, ” and hence active liberty calls for deference by the Court. Breyer’s special target is the anti-commandeering principle. Speaking in heavily pragmatic terms, Breyer thinks that this prohibition prevents valuable national initiatives to protect against terrorism, environmental degradation, and natural disasters—initiatives in which, for example, the national government requires state officials to ensure compliance with federal standards (p. 60). 
Breyer also contends that an understanding of active liberty can inform more technical debates. Here is a prominent example: In Chevron v. NRDC,51 the Court announced a principle of deference to administrative interpretations of law. The Court ruled that in the face of statutory ambiguity, courts should defer to agency interpretations so long as they are reasonable. Breyer believes that this approach is too simple and too crude, in a way that disserves democracy itself.52 When the agency has solved an interstitial question, Breyer believes that judicial deference is appropriate, because deference is what a reasonable legislature would want. But on “questions of major importance” (p. 107), involving the fundamental reach or nature of the statute, Breyer thinks that a reasonable legislature would not want courts to accept the agency’s interpretation. He thus urges that courts should take a firmer hand in reviewing agency judgments on fundamental matters than in reviewing more routine matters. Here too, he opposes Justice Scalia, who endorses a broad reading of Chevron, one that would generally defer to agency interpretations of law.53 In democracy’s name, Breyer argues on behalf of independent judicial review of agency interpretations involving major national questions. 
There is a larger interpretive question in the background. Should courts rely only on a statute’s literal text, or should they place an emphasis instead “on statutory purpose and congressional intent”? Sharply disagreeing with the more textually-oriented Scalia,54 and again emphasizing pragmatic considerations, Breyer favors purpose and intent. Here he is evidently influenced by the famous legal process materials, compiled by Henry Hart and Albert Sacks; as I have noted, those materials place “purpose” front and center, and they also insist that courts should assume that legislators are “reasonable people acting reasonably.”55 In the same vein, Breyer emphasizes that a purpose-based approach asks courts to consider the goals of “the ‘reasonable Member of Congress’—a legal fiction that applies, for example, even when Congress did not in fact consider a particular problem.”56 
In defending this approach, Breyer speaks in thoroughly pragmatic terms, emphasizing the beneficial consequences of purposivism. Breyer thinks that as compared with a single-minded focus on literal text, his approach will tend to make the law more sensible, almost by definition. He also contends that it “helps to implement the public’s will and is therefore consistent with the Constitution’s democratic purpose.” Breyer concludes that an emphasis on legislative purpose “means that laws will work better for the people they are presently meant to affect. Law is tied to life; and a failure to understand how a statute is so tied can undermine the very human activity that the law seeks to benefit” (p. 100). Thus Breyer directly links active liberty, purposive approaches to law, and an emphasis on consequences. 
The third part of Breyer’s book tackles the broadest questions of interpretive theory and directly engages Scalia’s contrary view. Breyer explicitly emphasizes that he means to draw attention to purposes and consequences above all. Constitutional provisions, he thinks, have “certain basic purposes,” and they should be understood in light of those purposes and the broader democratic goals that infuse the Constitution as a whole. In addition, consequences are “an important yardstick to ensure a given interpretation’s faithfulness to these democratic purposes.” Breyer is fully aware that many people, including his colleagues Scalia and Thomas, are drawn to “textualism” and its close cousin “originalism”—approaches that argue in favor close attention to the 55 See Hart and Sacks, supra note, at 1380. 
in Henry Hart and Albert Sacks, The Legal Process 1374-1380 1994) ought to be attributed to the statute and to any subordinate provision of it which may be involved,” id at 1374, and add that a court “should assume, unless the contrary unmistakably appears, that the legislature was made up of reasonable persons pursuing reasonable goals reasonably,” id. at 1378. meaning of legal terms at the time that were enacted. Scalia , Thomas, and their followers are likely to think that Breyer’s approach is an invitation for open-ended judicial lawmaking, in a way that compromises his own democratic aspirations.57 But he offers several responses. 
First, originalist judges claim to follow history, but they cannot easily demonstrate that history in fact favors their preferred method. The Constitution does not say that it should be interpreted to mean what it meant when it was ratified. The document itself enshrines no particular theory of interpretation; it does not mandate originalism. And if originalism cannot be defended by reference to the intentions and understandings of the framers, Breyer asks, in what way can it be defended—“other than in an appeal to consequences?” He points out that the most sophisticated originalists ultimately argue that their approach will have good consequences—by, for example, stabilizing the law and deterring judges from imposing their own views. Even Breyer’s originalist adversaries are “consequentialist in an important sense” (p. 118). They are not consequentialists in particular cases, but they adopt, and defend, their preferred approach on consequentialist grounds.58 
Breyer’s second argument is that his own approach does not leave courts at sea, for he too insists that judges must take account of “the legal precedents, rules, standards, practices, and institutional understanding that a decision will affect.” Those who focus on consequences will not favor frequent or dramatic legal change, simply because stability is important. In any case textualism and originalism cannot avoid the problem of judicial discretion. “Which historical account shall we use? Which tradition shall we apply?” (p. 127). In the end, Breyer contends that the real problem with textualism and originalism is that they “may themselves produce seriously harmful consequences—outweighing whatever risks of subjectivity or uncertainty [are] inherent in other approaches” (p. 129). His pragmatic goal is “a form of democratic government that will prove workable over 57 See Scalia, Originalism: The Lesser Evil, supra note. 
58 See id; see also the candid suggestion by Randy Barnett, a self-described originalist, see Randy Barnett, Restoring the Lost Constitution (2004): “Given a sufficiently good constitutional text, originalists maintain that better results will be reached overall if government officials—including judges—must stick to the original meaning rather than empowering them to trump that meaning with one that they prefer,” available at http://legalaffairs.org/webexclusive/debateclub_cie0505.msp (emphasis added). time,” and he believes that his kind of purposive approach, rooted in active liberty, is most likely to promote that goal. 
This is a brisk, lucid, and energetic book, written with conviction and offering a central argument that is at once provocative and appealing. It is not usual for a member of the Supreme Court to attempt to set out a general approach to his job; Breyer’s effort must be ranked among the very few most impressive such efforts in the nation’s long history. And in defending a pragmatic, purposive-oriented alternative to originalism, Breyer writes in a way that is unfailingly civil and generous to those who disagree with him—and thus provides a model for how respectful argument might occur, even in a domain that is intensely polarized. 
A large difficulty, as we shall soon see, lies in the characterization of purposes, both in particular cases and in general. Texts rarely announce their own purposes; the same is true of the Constitution itself. When Breyer asks judges to identity the purposes of reasonable legislators, he is inviting a degree of judicial discretion in the judgment of what purposes are reasonable. And while he is not wrong to say that “active liberty” helped inform the founding document, his own characterization of its purposes has a strong evaluative element. The same is true for the effort to bring active liberty to bear on concrete cases. 
There is a further point. Breyer emphasizes consequences, and rightly so. But those who think that consequences are important might well end up favoring approaches that he rejects, such as textualism, judicial deference to legislative judgments,59 respect for agency interpretations of law, and even originalism. To be sure, consequences cannot be assessed without some theory of value, and Breyer wants to use active liberty as part of the standard for assessment. But it is both to impossible both to use active liberty as the basis for evaluating consequences and to think that courts do best if they follow the ordinary meaning of statutory texts, or defer to agency interpretations of the most important questions, or uphold legislation unless it is plainly unconstitutional. 
59 For a consequentialist defense of such deference, see Adrian Vermeule, Judging Under Uncertainty (forthcoming 2006. 
I do not believe that these are decisive objections to Breyer’s particular conclusions or even to his general approach. But they suggest that the defense of those conclusions, and of that approach, must be cast in a way that acknowledges that much of the time, reasonable purposes are made, not found. And on both pragmatic and democratic grounds, that acknowledgement raises some questions about the search for purposes by the federal judiciary. It is possible that we are all pragmatists now.60 If so, the problem is that many different approaches, and not only Breyer’s, can march under the pragmatic banner.61 Breyer does not give adequate attention to the possibility that alternative approaches, especially in the domains of statutory interpretation and administrative law, might be powerfully defended on the very grounds that Breyer invokes. 
Breyer’s specific conclusions do make a great deal of sense; they are eminently reasonable. In the domain of personal privacy, for example, the Court should be aware of how little it knows about current technological developments, and narrow rulings have the valuable feature of avoiding premature solutions.62 In general, Breyer makes a sensible plea for judicial restraint, and it is most illuminating to hear that plea from one of the Court’s “liberals.” Indeed, Breyer’s survey of particular areas is unified by a general theme, which involves the need for courts to respect democratic prerogatives. But his largest claim is more general still: Any approach to legal interpretation must be defended 60 Note in this connection that Judge Richard Posner is a famous pragmatist, and his own approach to law is very different from Justice Breyer’s, certainly insofar as Posner does not stress citizen participation or active liberty. See Richard A. Posner, Law, Pragmatism, and Democracy 57-96 (2003); Richard A. Posner, Foreword, Harv L Rev (forthcoming 2005). Posner describes his own position as “everyday pragmatism,” id. at 24-25, 49-56, as distinguished from philosophical pragmatism, and Posner tries to avoid theoretically ambitious claims. He describes everyday pragmatism as “the untheorized cultural outlook of most Americans, one rooted in the usages and attitudes of a brash, fast-moving, competitive, forwardlooking, commercial, materialistic, philistine society, with its emphasis on working hard and getting ahead.” An obvious objection to “untheorized” pragmatism is that it is not possible to evaluate consequences without a contested judgment of value; and Posner’s own description of the “untheoretized culture outlook of most Americans” does suggest some kind of theoretical position. But this objection should not be overstated. It is often possible, however, for people to make a particular evaluation of consequences amidst disagreement or uncertainty about foundational questions. See Cass R. Sunstein, Legal Reasoning and Political Conflict (1996). 
61 See, e.g., Adrian Vermeule, Judging Under Uncertainty (forthcoming 2006), which defends a form of Thayerism on pragmatic grounds. 
62 See Lawrence Lessig, The Path of Cyberlaw, 104 Yale LJ 1743 (1995). in a way that plays close attention to its consequences. Despite its simplicity, this pragmatic point continues to be widely ignored. It has particular implications for the analysis of originalism. 
Of course there is a lively historical dispute about whether those who ratified the Constitution meant to hold posterity to their specific views.63 If the ratifiers did not want to bind posterity to their particular understandings, originalism stands defeated on its own premises: The original understanding may have been that the original understanding is not binding. Breyer properly notes this possibility (p. 117), and if the historical record shows that the ratifiers rejected originalism, the argument for originalism is selfdefeating. But suppose that the ratifiers had no clear view on that question, or even that the better understanding is that they did, in fact, want to hold posterity to their understandings.64 Even if so, it is up to us, and not to them, to decide whether to follow those views. It would be circular and therefore unhelpful to defend reliance on the ratifiers’ specific views on the ground that the ratifiers wanted us to respect their specific views. 
It follows that the question whether originalism is a sensible approach to constitutional law must be answered by reference to its consequences.65 Suppose that the consequence of originalism 
would be to threaten many contemporary rights and understandings. If so, why should we accept it66? And indeed it does seem probable that originalism would have this consequence. For example, it would likely mean that the national government could discriminate on the basis of race and sex, simply because the equal protection clause does not apply to the national government at all. Originalism would almost certainly authorize states to discriminate on the basis of sex, which the equal protection clause was not originally understood to forbid. Originalism might well mean that Brown v. Bd of Education was wrongly decided67; it would almost certainly eliminate the right of privacy altogether, simply because there is no such right in the document, and it is hard to show that the original understanding of any relevant provision supports the privacy right. And many originalists firmly believe that their approach would require courts to invalidate a great deal of legislation—by, for example, striking down independent regulatory agencies,68 forbidding Congress from delegating discretion to regulatory agencies,69 and imposing new limitations on national power under the commerce clause.70 
I do not mean to say that the originalist method necessarily compels all of these conclusions; the interpretive questions are complex, not simple. And even if originalism has these consequences, some originalists candidly acknowledge that established precedent has its claims, and that it must sometimes be respected even if it deviates from the original understanding. Justice Scalia, for example, says that he might well be a “faint-hearted” originalist,71 because he is willing to follow precedent even when he believes that it is wrong in principle.72 My only point is that Breyer is correct to note that the document itself does not require originalism and to argue that consequences matter to the choice of a theory of interpretation—and to insist that if we care about consequences, the argument for originalism looks increasingly implausible.73 Va L Rev 947 (1995). McConnell impressively shows that many members of Congress believed that under section five of the fourteenth amendment, Congress had the authority to abolish segregation. But it is one thing to say that many members of Congress so believed, and expressed that view in unenacted legislation; it is quite another thing to say that the fourteenth amendment was understood to create a self-executing, judicial enforceable ban on segregation. 
68 Steven Calabresi and Sai Prakash, The President’s Power to Execute the Law, 104 Yale LJ 541 (1994). 
69 Randolph May, The Public Interest Standard: Is It Too Indeterminate to be Constitutional, 53 F Comm LJ 427 (2001). 
70 See Raich v. Ashcroft, US (Thomas, J., dissenting); Richard Epstein, The Proper Scope of the Commerce Clause, 73 Va L Rev 1387 (1987); Randy Barnett, Restoring the Lost Constitution (2003); Douglas Ginsburg, On Constitutionalism, Cato Supreme Court Review 7 (2003). 
71 Scalia, A Matter of Interpretation, supra note. 
72 Scalia has said that Thomas “doesn’t believe in stare decisis, period.” Scalia remarks, “if a constitutional authority is wrong, [Thomas] would say, ‘Let’s get it right.’ I wouldn’t do that.” Quoted in Stephen Presser, Touting Thomas, Legal Affairs (Jan./Feb. 2005). 
73 There are other problems, including the arguable incoherence of the originalist enterprise. See Cass R. Sunstein, Radicals in Robes (2005). 
Breyer offers a sketch, not a sustained argument, and he leaves significant gaps. Above all, he says too little about the difficulties that judges face in assessing consequences and in describing purposes. We can describe this as a pragmatic objection to his approach—an objection that might argue in favor of second-order pragmatism, that is, a form of pragmatism that is alert to institutional considerations.74 Let us begin with some technical issues. 
Recall that Breyer argues against a broad reading of Chevron; he believes that for major questions, courts should make an independent assessment of statutory meaning, and not defer to reasonable interpretations by the executive branch. But why? The answer appears to be that reasonable legislators would want courts to assume an independent role (p. 106). But is this so clear? Assume that a statute—say, the Endangered Species Act, or the Food and Drug Act—contains an ambiguous provision on an issue of national importance. Might not reasonable legislators want a specialized, accountable agency to resolve the ambiguity, even on major questions? 
On pragmatic grounds, it might be thought that resolution of the ambiguity often calls for a difficult policy judgment, and reasonable legislatures might not want difficult policy judgments to be made by federal courts.75 On consequentialist grounds, consider the following fact: In reviewing agency interpretations of law, Republican appointees to the federal bench show a definite “tilt” in a conservative direction, and Democratic appointees should a definite “tilt” in a liberal direction.76 Why would we think that a reasonable legislator would want statutory ambiguities to be resolved in accordance with whatever “tilt” can be found on the relevant reviewing court? Or consider an additional fact: A more refined approach to Chevron, of the sort that Breyer celebrates, has produced a great deal of confusion in the lower courts.77 Does pragmatism support that outcome? 
In short, it is not clear that in this context, Breyer has properly identified the (hypothetical, constructed) instructions of a reasonable legislator; but the important point is far more general. For interpreting statutes, Breyer follows Hart and Sacks in arguing in favor of close attention to purposes, understood as the objectives of a “reasonable legislator.” Sometimes this approach is indeed useful, especially where there is a consensus on what reasonableness requires.78 But Hart and Sacks, writing in the consensus-pervaded (and complacent) legal culture of the 1950s, downplayed the possibility that disagreement, highly ideological in nature, would break out on that question. In the current period, it should be obvious that different judges may well disagree about what a reasonable legislator would like to do. Imagine that a law condemns “discrimination on the basis of sex”; suppose that a state adopts a height and weight requirement for police officers, one that excludes far more women than men. In deciding whether this requirement is “discrimination,” how shall judges characterize the purpose of a reasonable legislator? It is inevitable that courts will see their own preferred view as reasonable. Does that promote active liberty? Does pragmatism support a situation in which judges assess reasonableness by their own lights? 
Unfortunately, the problem is common. Laws rarely come with clear announcements of their purposes, and in hard cases, any characterization requires some kind of evaluative judgment from courts. It is not a matter of finding something. Suppose that the antidiscrimination statute is invoked against affirmative action programs.79 Does the purpose of the ban on “discrimination” argue for, or against, such programs? It would be easy to characterize the purpose as the elimination of any consideration of race from the relevant domain; it would also be easy to characterize the purpose as the protection of traditionally disadvantaged groups.80 If judges are asked to say what “reasonable” legislators would like to do, they are all too likely to say what they themselves would like to do. 
Hart and Sacks, Breyer’s predecessors, do offer a powerful and sensible approach to statutory interpretation, but they devote too little attention to the problem of characterizing purpose. When courts choose one purpose over another (reasonable) candidate, they are actually attempting to put the relevant text in the best constructive light.81 Of course they are selecting an interpretation that fits the text and context; if they were not doing that, they would not be engaging in interpretation at all. But when they select a reasonable purpose, they are choosing an approach that, by their own lights, makes the best sense. A judicial judgment on this count is hardly untethered—that would be a caricature—but it is a judicial judgment nonetheless. 
Many textualists distrust resort to purposes for this very reason. Emphasizing the increasing substitution of statutes for the common law, they want courts to hew closely to statutory language.82 They think that judges have paid too little attention to the rise of statutory lawmaking, and have used common law approaches, including analogical reasoning, in domains where they do not belong.83 And indeed, the Hart and Sacks materials might well be understood as a product of an early confrontation between common law thinking and a system of law that is pervaded by statutory interventions. Justice Scalia believes that an emphasis on the plain meaning of the text, which is what after all has been enacted, promotes democratic responsibility and also helps to discipline the judiciary.84 
If purpose is being characterized in a way that defies the ordinary meaning of the text, Scalia’s argument has considerable pragmatic force. Indeed, textualism might easily be defended with reference to active liberty, and in two different ways. First, textualism promotes democratic government, by encouraging the legislature to make its instructions clear. Over time, a text-oriented judiciary might well promote better accountability from 81 See Ronald Dworkin, Law’s Empire (1985). 82 See Scalia, supra note. 83 Id. 84 See Scalia, A Matter of Interpretation, supra note. legislatures. Second, textualism 
disciplines judicial creation of “intentions” and “purposes” to push statutes in judicially preferred directions.85 
To be sure, it is easy to overstate the constraints imposed by text. Where the text is ambiguous, or leaves gaps, textualism by hypothesis is inadequate, and some other interpretive tool must be invoked.86 There is a serious risks that in hard cases, preferences are likely to matter for textualists as for everyone all else.87 My only suggestion is that Breyer pays too little attention to the risk that any judgments about “reasonableness” will be the judges’ own, in a way that disserves democracy itself. 
Breyer is correct to say that any theory of interpretation has to be defended in terms of its consequences. But for interpreting statutes, it is not at all clear that a purposebased approach, focusing on consequences in particular cases, is preferable to a textbased approach, one that asks judges not to think little or not at all about consequences. A textual approach might be simpler to apply; if so, that is surely a point in its favor. And if judges cannot reliably identify reasonable purposes, textualism might also lead to better results, or consequences, all things considered.88 
None of this means that Scalia’s approach is necessarily superior to Breyer’s. But it does point out the necessity of engaging the possibility that on his own consequentialist grounds, and with an eye firmly on democratic goals, textualism in the interpretation of 85 Note in this regard the very different reaction of German and Italian judges to the emergence of fascism. German judges proceeded in a purposive fashion, abandoning text in favor of legislative goals (and consequences!) – in a way that promoted injustice an even atrocity. See Ingo Muller, Hitler’s Justice: The Courts of the Third Reich(1991). By contrast, the Italian judges made close attention to text and to plain meaning, in a way that produced much better consequences. See Guido Calabresi, Two Functions of Formalism, 67 U Chi L Rev 479 (2000), and in particular this suggestion: “To the scholars opposing Fascism, the nineteenth-century self-contained formalistic system became a great weapon. . . . What it conserved was the liberal, nineteenth century political approach . . . [and] in a time of Fascism, the important thing was that it conserved basic democratic attitudes.” Id. at 482. 
86 Consider, for example, the rule of lenity, invoked in Smith v. US, 508 US 223 (1999) (Scalia, J, dissenting). 
87 Evidence can be found in Cass R. Sunstein, Beyond Marbury: The Executive’s Power to Interpret the Law; Cass R. Sunstein and Thomas Miles, Do Judges Make Regulatory Policy? An Empirical Investigation of Chevron, U Chi L Rev. 
88 Some people appear to believe that interpretation, to count as such, necessarily calls for attention to the “intent” of those who wrote the text in question. See, e.g., Stanley Fish [citation to be added]. This is a blunder. In law, it is certainly possible to interpret texts by pointing to the ordinary meaning of the words, without speculating about authorial intentions. Whether this is desirable as well as possible is another question, one that must be resolved by reference, among other things, to consequences. ordinary statutes might be better than an approach that explores purposes.89 Of course textualism is sometimes a fake, as when the text does not have any clear meaning. In my view, hard cases, in which the text is indeterminate, are best resolved with clear reference to the views of any applicable administrative agency, and also with close attention to pertinent canons of construction—which, properly used, discipline the exercise of judicial discretion and also serve the system of separated powers.90 But this is not the place to defend that view. The only point is that Breyer has not shown that a purposive approach to statutory interpretation is unambiguously preferable to the reasonable alternatives. 
Breyer is right to say that the framers wanted to recognize both active liberty and negative liberty. But the framers saw themselves as republicans, not as democrats,91 and they did not believe in participatory democracy or in rule through town meetings. On this count, Breyer slides quickly over intense debates about what the American framers actually sought to do.92 Of course they attempted to provide a framework for a form of self-government.93 But so stated, that goal operates at an exceedingly high level of abstraction, one that cannot easily be brought to bear on concrete cases. Much of the time, it is hard to link the general idea of self-government to particular judgments about contemporary disputes in constitutional law. 
Certainly Breyer does not try to argue, in originalist fashion, that the actual drafters and ratifiers of the Constitution wanted to allow campaign finance reform, restrictions on commercial advertising, affirmative action programs, and federal commandeering of state government. He argues instead that the idea of active liberty, which animates the Constitution, helps to justify these judgments. This is reasonable enough. But exactly what kind of argument is it? The framers of the Constitution also placed a high premium on “domestic tranquility,” to which the preamble explicitly refers. Would it be right to say that because domestic tranquility is a central goal of the document, the President is permitted to ban dangerous speech—or that because, or if, affirmative action threatens to divide the races, in a way that compromises “tranquility,” color-blindness is the right principle after all? 
In any case Breyer rightly emphasizes that the Constitution attempts to protect negative liberty too. Why shouldn’t a ban on campaign finance restrictions be seen to run afoul of that goal? Nor is negative liberty the only value at stake. Such restrictions forbid people from spending their money on political campaigns, in a way that might well be taken to compromise participatory self-government. In this light we might well see campaign finance restrictions as offending, at once, both negative and active liberty. Deductive logic cannot take us from an acknowledgement of the importance of active liberty to acceptance of campaign finance restrictions; there are no syllogisms here. Instead an evaluative judgment must be made, to the effect that properly characterized, the first amendment and its goal of self-government do not condemn (the relevant) restriction on campaign contributions and expenditures. I believe that this conclusion is broadly correct, especially when we consider the general need for courts to defer to congressional judgments in hard cases.94 But the evaluative judgment is inescapable. 
Or suppose that we accept, as we should, Breyer’s claims about the centrality of active liberty to the constitutional design. Is originalism therefore off the table? Not at all. We might believe, with some constitutional theorists (including Alexander Hamilton95), that constitutional provisions, as products of an engaged citizenry, reflect the will of We the People, as ordinary legislation usually does not. If so, an emphasis on the original understanding can be taken to serve active liberty at the same time that it promotes negative liberty. It serves active liberty because it follows the specific judgments of an engaged citizenry. It promotes negative liberty because and precisely to the extent that those judgments favor negative liberty (or for that matter active liberty). I do not mean to suggest that this argument is convincing. The framers and ratifiers included only a small 94 An obvious qualification involves incumbent protection measures. If campaign finance legislation is operating to insulate incumbents against electoral challenge, there is a strong reason, on grounds of active liberty (among others), for courts to take a strong role. 
95 See The Federalist No. 78; see also Bruce Ackerman, We the People vol. 1: Foundations (1993). segment of early America, and in any case the fact that the framers and ratifiers are long dead creates serious problems for those who argue for originalism in democracy’s name. The only point is that Breyer’s emphasis on active liberty does not rule originalism out of bounds. 
Or return to Thayer’s claim that the Court should strike down legislation only if it clearly and unambiguously violates the Constitution. Despite his general enthusiasm for restraint, Breyer does not mean to follow Thayer. But why not? Thayer and his followers can claim to favor active liberty, because they allow the sovereign people to do as they choose. Indeed, Learned Hand, an apostle of judicial restraint, wanted courts to be reluctant to invalidate legislation in large part because he was committed democratic selfrule.96 Perhaps Breyer thinks that this approach undervalues both negative and active liberty. But why? Perhaps a deferential Court will ultimately produce exactly the right mix between the two kinds of freedom. Of course Ely’s approach, emphasizing reinforcement of democratic processes, can easily be rooted in active liberty; indeed, active liberty lies at its heart. Breyer writes approvingly of the Warren Court on the ground that its decisions promoted active liberty (p. 11); and Ely is the Warren Court’s most systematic defender. Does Breyer mean to endorse Ely? If not, where does he differ? A puzzling gap in Breyer’s book is the omission of any treatment of Ely’s apparently similar argument.97 
Recall that Breyer candidly acknowledges that legislative “purpose” is not something that can simply be found. “Purpose” is what judges attribute to the legislature, based on their own conception of what reasonable legislators would mean to do. If this is true for the purposes of individual statutes, it is also true for the purposes of the Constitution as well. When Breyer says that the “basic” purpose of the Constitution is to protect active liberty, so as to produce concrete conclusions on disputed questions, his own judgments about the goals of a reasonable constitution-maker are playing a central role. Fortunately, Breyer’s own judgments are indeed reasonable. But he underplays the extent to which they are his own. 
96 See Learned Hand, The Spirit of Liberty (Irving Dillard ed. 1960)/ 97 There is only one reference to Ely, presaged by “cf” see p. 1146. Note also that Frank Michelman has made closely related arguments. See Frank Michelman, Traces of Self-Government, 100 Harv L Rev. 4 (1986). 
The same point bears on Breyer’s enthusiasm for an inquiry into consequences. Consequences certainly do matter, but much of the time, it is impossible to assess consequences without reference to disputed questions of value. Return to the question of affirmative action, and suppose, rightly, that the text of the Constitution could, but need not, be understood to require color-blindness. If we care about consequences, will we accept the color-blindness principle or not? Suppose we believe that affirmative action programs create racial divisiveness and increase the risk that underqualified people will placed in important positions. If those are bad consequences, perhaps we will oppose affirmative action programs. An emphasis on consequences as such is only a start. Of course Breyer is not concerned with consequences alone; he wants to understand them with close reference to specified purposes, above all “active liberty.” But as I have suggested, that idea, taken in the abstract, is compatible with a range of different approaches to constitutional law; it need not be taken to compel Breyer’s own approach. 
None of this means that Breyer is wrong. On the contrary, I believe that he is largely right. He is right to say that the free speech principle should be understood to democratic terms. He is right to say that where the Court lacks important information, it should rule cautiously and narrowly. He is right to resist the constitutional assault on affirmative action programs (an assault that, by the way, is extremely hard to defend in originalist terms98). He is right to reject originalism. Above all, he is right to emphasize the importance of democratic goals to constitutional interpretation. 
But to make his argument convincing, he would have to offer a more sustained encounter between his own approach and the imaginable alternatives. Essentially for Breyer’s own reasons, originalism does seem unacceptable, certainly if it is unaccompanied by respect for precedent; and it is not clear if originalism, so accompanied, can be made coherent. Bipartisan restraint, of the sort championed by Thayer and Holmes, has many attractions, and the Court should probably move toward it in some areas99; but in too many contexts, it too would destabilize our rights and our institutions. For the evaluation of a democracy-centered approach to constitutional law, a great deal depends on whether courts will often wield democratic ideals as a sword 98 See Eric Schnapper, Affirmative Action and the Legislative History of the Fourteenth Amendment, 71 Va L Rev. 753 (1985). 
99 See Vermeule, supra note. against our actual democracy (as Ely urged), or whether they will more frequently use those ideals as a shield against constitutional attacks (as Breyer appears to urge). A great deal also depends on our degree of trust in those who wield those ideals, and their likely reasonableness. 
A deeper point lies in the background here. For the selection of a general theory of interpretation, a great deal turns on context. I have argued against originalism, but it is possible to imagine a world in which originalism would make a great deal of sense. Suppose, for example, that the original public meaning of the founding document would generally or always produce sensible results; that violations of the original public meaning would be unjust or otherwise unacceptable; that democratic processes that did not violate the original public meaning would not cause serious problems from the standpoint of justice or otherwise; and that judges, not following the original public meaning, would produce terrible blunders from the appropriate point of view. In such a world, originalism would be the best approach to follow. I have argued against bipartisan restraint. But in a world in which democratic processes were systematically reliable, and in which unrestrained judges would use ambiguous provisions of the Constitution to impose unjust or otherwise unsupportable policies, the argument for bipartisan restraint would be very strong. The larger point is that the Constitution itself does not contain a theory of interpretation, and no single theory would make sense in every imaginable world. 
It is also possible to doubt whether the Supreme Court should accept any ambitious or unitary theory of interpretation.100 Perhaps the Court does best, in our actual world, if it avoids ambitious accounts, and decides cases, if it can, with reference to reasons that can command agreement from those with diverse views about foundational questions, and from those who do not want to take a stand on those questions. Perhaps a commitment to “active liberty” is too sectarian to command general assent. But at least this much can be said on Breyer’s behalf: If an ambitious account is desirable, indispensable, or unavoidable, an emphasis on the commitment to democratic rule is hardly the worst place to start. 
Within the Supreme Court itself, the most powerful recent theoretical arguments have come from Justice Scalia, with his insistence on originalism and his complaint that if courts are not bound by the original understanding, they are essentially doing whatever they want.101 Breyer has now developed a distinctive argument of his own, one that demonstrates the possibility of a nonoriginalist method that, while not eliminating discretion, is hardly a blank check to the judiciary. Breyer’s originality lies in the effort to forge links among its three distinctive moving parts: an appreciation of “active liberty” and its place in our constitutional tradition; a commitment to purposive understandings of interpretation; and an insistence, inspired by American pragmatism, that theories of interpretation must be evaluated in terms of their consequences. 
I have emphasized what seems to me a central problem in Breyer’s account: the difficulty of characterizing purposes, and of counting purposes as “reasonable,” without a judgment of the interpreter’s own. In hard cases, judgments about purpose are evaluative, not descriptive.102 What is true for particular provisions is true for the founding document as a whole. Active liberty is certainly a theme of the document, but it is not easy to deduce, from that theme, particular conclusions about the legal issues raised campaign finance restrictions, affirmative action plans, privacy, and judicial review of agency action. Nor does active liberty, standing alone, make the choice between textual and purposive approaches to constitutional interpretation. On purely pragmatic grounds, purposive approaches run into serious problems once we acknowledge the role of judicial discretion in the characterization of purposes.103 
I have also suggested the possibility of endorsing a kind of second-order pragmatism, one that attempts to develop tools to discipline the judicial inquiry into both consequences and purposes. Perhaps we are all pragmatists now, in the sense that we can agree that any theory of interpretation must pay close attention to the outcomes that it 101 See Scalia, supra note. 
102 This point is emphasized and not deplored in Dworkin, Law’s Empire, supra note; Insofar as he emphasizes the constructive element in interpretation, Dworkin seems to me to make a large advance on Hart and Sacks, whose approach resembles his. 
103 Cf. Vermeule, supra note. produces.104 Whether or not we do agree on that point, we certainly should. The problem is that many diverse views can march under the pragmatic banner. 
But if Breyer’s particular conclusions are not compelled by his general themes, they are always plausible, and usually more than that. Many originalists fear that without originalism, judges will be unloosed, producing a system of interpretation that endangers self-government and is extremely hard to defend in terms of its consequences. One of the many virtues of Breyer’s book is its demonstration that these objections are overstated— that without mechanical jurisprudence or rule-   104 Note that even Dworkin describes himself as a consequentialist. See Ronald Dworkin, Order of the Coif Lecture: In Praise of Theory, 29 Ariz. St. L. J. 353, 364 (1997). 177.  250.  251.  260.  261.  
Cass R. Sunstein and Arden Rowell  
T H E   L A W   S C H O O L   T H E   U N I V E R S I T Y  O F   C H I C A G O   
  July 2005  
  
This paper can be downloaded without charge at:  The Chicago Working Paper Series Index: http://www.law.uchicago.edu/Lawecon/index.html  and at the Social Science Research Network Electronic Paper Collection:  
 http://ssrn.com/abstract_id=759804  Preliminary draft 7/3/05 All rights reserved 
On Discounting Regulatory Benefits: Risk, Money, and Intergenerational Equity Cass R. Sunstein* and Arden Rowell** 
There is an elaborate debate over the practice of “discounting” regulatory benefits, such as environmental improvements and decreased risks to health and life, when those benefits will not be enjoyed until some future date. Economists tend to think that, as a general rule, such benefits should be discounted in the same way as money; many philosophers and lawyers doubt that conclusion on empirical and normative grounds. Both sides neglect a simple point: Once government has converted regulatory benefits into monetary equivalents, what is being discounted is merely money, not regulatory benefits as such. No one seeks to discount health and life—only the money that might be used to reduce threats to these goods. To be sure, cost-benefit analysis with discounting can produce serious problems of intergenerational equity; but those problems, involving the obligations of the present to the future, require an independent analysis. Failing to discount will often hurt, rather than help, future generations. Solutions to the problem of intergenerational equity should not be conflated with the question whether to discount. 
Suppose that a proposed regulation will not produce benefits for some period of years; suppose too that an agency is asked to engage in some form of cost-benefit analysis before it proceeds with the regulation. Costs will be discounted, on the theory that a dollar today is worth more than a dollar in twenty years. But what should the agency do about future benefits, such as improved health or averted deaths? Should these too be "discounted," or should a death in 2025 be treated the same as a death today? 
In terms of ultimate outcomes, the choice matters a great deal. If an agency chooses not to discount, the benefits calculation will shift dramatically from what it would be if the agency chose a discount rate of, for example, 10%. If a human life is valued at $8 million, and no discount rate is applied, a life saved 100 years from now is * Karl N. Llewellyn Distinguished Service Professor, Law School and Department of Political Science, University of Chicago. 
** University of Chicago Law School, J.D. expected 2006. Thanks to Blake Roberts for excellent research assistance; to Matthew Adler, Robert Hahn, Bernard Harcourt, John Morrall, Eric Posner, Richard Posner, and David Weisbach for valuable comments; and to participants in an exceedingly helpful work-inprogress lunch at the University of Chicago Law School. worth the same expenditure today as a life saved now: $8 million. But at a discount rate of 10%, the same life would justify a modern expenditure of only $581.1 For regulation whose effects would be felt centuries from now, any reasonable discount rate will reduce substantial benefits, including the populations of large nations, to close to nothing.2 The Office of Management and Budget (“OMB”) suggests that agencies should prepare analyses using rates of both 3% and 7%,3 departing from its suggested 10% rate in the 1980s. But these numbers remain controversial.4 Consider the fact that the midpoint— 5%—would ensure that, if a human life is valued at $8 million, one hundred lives in one hundred years would be worth a total amount of only $6.25 million today. 
In any case, agencies are not bound by OMB guidelines, and in recent years, their own rates have ranged from as low as 3% (Food and Drug Administration, Department of Housing and Urban Development) to as high as 10% (Environmental Protection Agency).5 In fact, the same agency sometimes uses different discount rates for no apparent reason—with the EPA, for example, using the 10% rate for regulation of emissions from locomotives, but selecting 7% for regulation of drinking water and 3% for regulation of lead-based paint.6 In this domain, government practice seems inexplicably erratic. Key questions are therefore: What discount rate, if any, should agencies choose?7 Do life and health require some special discount rate—or no discount rate at all? What is the relationship between discount rates and the rights and interests of future generations? 
We shall attempt to make progress on these questions by offering two claims. First, regulatory benefits should be discounted at the same rate as money, and for a reason neglected by all sides in the debate: money is inevitably what is being discounted.8 When regulators appear to “discount” mortality or morbidity, they are actually discounting people’s willingness to pay to reduce statistical risks; and willingness to pay to reduce risks should be discounted like all other expenditures. Second, cost-benefit analysis with discounting can produce serious problems of intergenerational equity, but these problems are poorly addressed by refusing to discount. It is true that regulatory decisions based on discounting can produce morally unjustified actions by the present generation at the expense of posterity. But a refusal to discount does not solve this problem; in many cases, it could injure, rather than promote, the interests of future generations. The task of fulfilling the obligations owed by the present to the future should be addressed directly. It is not appropriately handled by a refusal to discount. 
To understand these claims, and their implications for administrative agencies, reviewing courts, and the general idea of “sustainable development,” we need to back up a bit. 
Everyone agrees that money should be discounted. It is better to have $1000 today than $1000 in ten years, if only because $1000 today can be invested and made to be worth much more than $1000 a decade hence. But for life and health, discounting is greatly disputed in both theory and practice. In an important case, a federal court said that discounting is necessary to provide an "apples to apples" comparison of costs and benefits, suggesting that agencies are legally required to use the same discount rate for health and safety benefits as for dollars.9 Other decisions have insisted on careful explanations for whatever discount rates agencies choose.10 Economists tend to believe that the argument for discounting is obvious, though the consensus has started to unravel Philosophers have raised serious doubts about the idea that a future death or illness should be discounted in the same way as money.14 Lawyers as well have questioned that idea, suggesting that it depends on contentious empirical or normative assumptions.15 
A central objection is that a life in 2025 is not obviously “worth less” than a life today. If ten people are killed twenty years hence, the outcome is not worse than if ten people are killed tomorrow. Thus one critic asks: “What is wrong with discounting numbers of lives saved? One obvious problem is that death does not recognize human accounting conventions and death does not discount.”16 In the same vein, Ackerman and Heinzerling object that “the choice implicit in discounting is between preventing harms to the current generation and preventing similar harms to future generations. Seen in this way, discounting looks like a fancy justification for foisting our problems off onto the people who come after us.”17 They emphasize that with “a discount rate of five percent, for example, the death of a billion people 500 years from now becomes less serious than the death of one person today.”18 
Defenders of discounting have responded that a refusal to use a discount rate creates a number of logical and practical conundrums. For instance, a refusal to discount might require truly extraordinary sacrifices from the present for the sake of the (infinite) future. On one view, the “failure to discount would leave all generations at a subsistence level of existence, because benefits would be postponed perpetually for the future.”19 At the very least, a zero discount rate might bias “cost-benefit analysis in favor of rules that impose excessive sacrifices on the current generation.”20 On the other hand, it also has been argued that a failure to discount the monetized equivalent of regulatory benefits would lead to less regulation, not more. Suppose that regulators are indifferent as between lives saved now and lives saved in the future, but discount costs at some positive rate. If so, it makes sense for them to delay life-saving expenditures indefinitely, simply because the cost-benefit ratio will (always) be better in the future.21 “[T]he discounting of costs but not benefits . . . has a paralyzing effect on a decisionmaker. . . . For any attractive program, there is always a superior delayed program which should be funded first. The result is that no program with a finite starting date can be selected.”22 
In any case, defenders of discounting have argued that instead of discounting lives and health as such, regulators might simply use the future discounted (monetary) cost of saving lives and health at the time when these are actually saved—an approach that is mathematically identical to ordinary discounting and hence produces the same analysis.23 Summarizing a range of arguments, a general overview suggests that failure to permit a discount rate will ensure that any cost-benefit “analysis fails to account for the opportunity cost of resources that are diverted from private investment toward investment in the proposed rule,” and could therefore “lead the agency to adopt rules that reduce the welfare of future generations, because the resources could have been invested in assets with higher rates of return.”24 But these arguments have yet to convince the numerous 19 See DAVID PEARCE & R. KELLY TURNER, ECONOMICS OF NATURAL RESOURCES AND THE ENVIRONMENT 223-24 (1990). 
20 Id. 
21 Emmett B. Keeler & Shan Cretin, Discounting of Life-Saving and Other Nonmonetary Effects, 29 MANAGEMENT SCIENCE 300 (1983). Ackerman and Heinzerling discuss this claim and reject it, PRICELESS, supra note 13, at 193-94, in part on the ground that allowing numerous current deaths would be politically unacceptable; but the claim is one of the logical implications of refusing to discount, and the fact that it entails a politically unacceptable outcome does not mean that it is wrong. 
22 Keeler & Cretin, supra note 21, at 303. The argument is criticized in Revesz, supra note 7, at 
23 John F. Morrall III, Saving Lives: A Review of the Record, 27 J. RISK AND UNCERTAINTY 221 (2003). 24 Morrison, Comment, supra note 5, at 1349. This argument has been used by OMB itself, see infra TAN 45, and it is the closest to the argument we make here; but if our argument is correct, it is unnecessary to speak of opportunity costs, because what is being discounted is the monetary value of the risk itself. For questions of intergenerational equity, the argument from opportunity costs is insufficient, because we do critics of discounting. As we shall see, it is by no means clear that the relevant resources will be “invested” for the benefit of future generations. 
Responding to the controversy, some prominent analysts have distinguished between “descriptive” and “prescriptive” approaches.25 Under descriptive approaches, the discount rate is chosen by examining the rate of return to capital that has been invested in a range of possible assets. This is the standard approach of those who advocate discounting.26 Under prescriptive approaches, the discount rate is selected on the basis of ethical judgments about the duties of one generation to those that succeed it. These approaches can lead to dramatically different rates.27 The difficulty is that to be worthy of adoption, any “descriptive” approach must ultimately be defended in “prescriptive” terms. It remains disputed whether the best prescriptive arguments require abandonment of what emerges from the preferred descriptive approach. 
An alternative possibility is to attempt to bracket the moral debates by investigating people’s actual preferences in this domain.28 Emphasizing the importance of those preferences, some defenders of discounting have attempted to show that people do discount future lives. On a standard view, “a zero discount rate is inconsistent with the observable behavior of individuals, which is arguably the best guide for policy in a democratic state.”29 But the word “arguably” suggests the normative problem in this context: Why should the interests of future generations be determined by consulting the preferences of the present generation? Those preferences might well be self-interested. not know that those savings will be invested for posterity’s benefit rather than consumed. See Cowen & Parfit, supra note 12, at 152. Note that many people believe that because of technological advances, future risks are unlikely to come to fruition, simply because new technologies will permit us to prevent them. As later discussed in the text, however, this is not a point about discounting itself. 
25 See Kenneth J. Arrow et al., Intertemporal Equity, Discounting, and Economic Efficiency, in CLIMATE CHANGE 1995: ECONOMIC AND SOCIAL DIMENSIONS OF CLIMATE CHANGE ch. 4 (J.P. Bruce et al. eds., 1996); William Cline, Discounting for the Very Long Term, in DISCOUNTING AND INTERGENERATIONAL EQUITY, supra note 2, at 131, 135, 137-39 (Paul R. Portney et al. eds., 1999). 
26 See Robert C. Lind, A Primer on the Major Issues Relating to the Discount Rate for Evaluating National Energy Options, in ROBERT C. LIND ET AL., DISCOUNTING FOR TIME AND RISK IN ENERGY POLICY (1982). 
27 See Portney & Weyant, Introduction, in DISCOUNTING AND INTERGENERATIONAL EQUITY, supra note 2, at 4. 
28 See Raymond J. Kopp & Paul R. Portney, Mock Referenda and Intergenerational Decisionmaking, in DISCOUNTING AND INTERGENERATIONAL EQUITY, supra note 2, at 87. 
29 Id. Even if there is a degree of altruism, there is no reason to think that the (bounded) altruism of the present should settle the moral entitlements of the future.30 
In any case, individual preferences in this context are not easy to identify, and they appear to be subject to framing effects. In an influential paper, Maureen Cropper and her coauthors conclude that people are indifferent between saving one life today and saving 45 lives in 100 years—a conclusion that has concrete implications for the appropriate discount rate.31 This conclusion was based on a study that asked people whether they would prefer a program that saves “100 people now” or one that saves a substantially larger number “100 years from now.” But other ways of framing the same problem yield radically different results.32 For example, most people consider “equally bad” a single death from pollution next year and a single death from pollution in 100 years—a finding that implies no preference for members of the current generation. In these ways, measurements of people’s judgments about obligations to future generations are influenced by framing effects.33 For this reason, it is far from clear that judgments about discounting can be rooted in actual preferences. Even if those preferences have moral weight, they are too labile to be a reliable basis for public policy. 
Within the legal literature, the most influential and elaborate treatment of discounting future benefits has been offered by Dean Richard Revesz.34 Revesz makes two central arguments. First, he contends that the primary reasons for discounting monetary benefits do not apply to risks to life and health.35 Money is discounted for two reasons: first, it can be invested, and second, most people have a “pure” time preference for current over future consumption.36 But human lives cannot be invested, and a life lost twenty years hence cannot be “recovered” by investing some sum, or some person, in the present. Nevertheless, Revesz acknowledges that people may well have a “pure” time preference that would treat a future risk as less troublesome than a present risk.37 Moore and Viscusi, for example, have investigated the empirical question, and find a real discount rate of about 2%, one that “accords roughly with financial market interest rates for the period, once these nominal rates are adjusted for inflation.”38 Revesz argues that the existence of time preference justifies some discount rate for future harms that will affect people now living.39 
To see the practical implication, consider the case of arsenic regulation. In its rationale for the regulation, the EPA treated an arsenic death in the future as equivalent to an arsenic death in the present, even though an arsenic death is likely to come, if it does come, decades after current exposures.40 In refusing to discount the latent harms from arsenic exposure, Revesz’s argument suggests that the EPA’s judgment was wrong, even arbitrary; some kind of discount rate is clearly appropriate. But Revesz does not argue that the EPA should adopt a discount rate that is equivalent to the appropriate discount rate for money. He contends, not implausibly, that there is no reason in the abstract to think that the time preference for health risks is the same as the time preference for dollars; and because there is no investment opportunity, any discount rate for health risks is likely to be much smaller than the market rate of return typically used to discount money. Hence the use of a market rate of return, on Revesz’s view, is likely to produce a significant undervaluation of regulatory benefits that will be enjoyed in the future.41 This is an important conclusion, because it suggests that current government practice should be substantially changed. The result would be to justify a number of regulations that cannot now satisfy a cost-benefit test. 
36 For a suggestion that a pure time preference is irrational, see Cowen and Parfit, supra note 12, at 
37 Revesz, supra note 7, at 975-76. 
38 Michael J. Moore & W. Kip Viscusi, Discounting Environmental Health Risks: New Evidence and Policy Implications, 18 J. ENVTL. ECON. & MGMT. S-59, S-61 (1990). 
39 Revesz, supra note 7, at 983-87. 40 66 Fed. Reg. at 7013. 41 Revesz, supra note 7, at 983. 
Second, Revesz contends that it is important to distinguish between latent harms and risks to future generations.42 An environmentally-induced illness that will come to fruition today is worse than an environmentally-induced illness that will come to fruition in twenty years; it is for this reason that some kind of discount rate makes a great deal of sense for latent harms. But for risks to future generations, Revesz believes that the argument for discounting is much more fragile.43 Why should a death of a ten-year-old in 2040 count less than a death of a ten-year-old today? Revesz concludes that there is no good answer to this question, and hence that the standard idea of discounting is not properly applied to harms faced by members of future generations.44 
In its guidance to federal agencies, the Office of Management and Budget (OMB) is alert to Revesz’s concerns but disagrees, calling for the same discount rate for money as for other goods, with a brief reference to opportunity costs: “It is true that lives saved today cannot be invested in a bank to save more lives in the future. But the resources that would have been used to save those lives can be invested to earn a higher payoff in future lives saved.”45 In any case people prefer immediate health gains to equivalent health gains in the future.46 And because a failure to discount would produce “perverse” results, OMB suggests that agencies should follow the “professional consensus that future health effects, including both benefits and costs, should be discounted at the same rate.”47 
We believe that both the defenders and the critics of discounting neglect an exceedingly simple point, one that supports the conclusion that an “apples to apples” comparison is indeed necessary. The point is this: Once a risk has been translated into a 42 Id. at 987. 43 Id. at 988-1003. 
44 Id. at 1005-1009. Revesz does argue for a limited role for discounting in the intergenerational context, suggesting, for example, that present generations should not “spend more when we can achieve the same result for less,” id. at 1008, and that present generations might well prefer to face environmental harms in return for “the fruits of greater investments in technological innovation.” Id. These suggestions are very much in the spirit of our discussion in Part IV below. 
45 See Circular A-4, supra note 3, at 35. 46 Id. 
47 Id. OMB overstates the professional consensus. See DISCOUNTING AND INTERGENERATIONAL EQUITY, supra note 2. monetary equivalent, it should be discounted as such; there is no need for a separate analysis of the problem of discounting life and health. 
To separate this argument from intergenerational issues, to which we turn in Part IV, let us suppose that the practice of discounting is proposed, but only for those people who are now living. Imagine that the question involves the practices of what is, in some part of the world, the Last Generation—a generation of living people who will have no successors.48 
For the Last Generation, the argument for discounting requires only one step: acceptance of the claim, now standard in the federal government and endorsed by many critics of discounting, that statistical risks should be turned into monetary equivalents. Once that claim is accepted,49 the case for a discount rate, one that cuts across all relevant costs and benefits, follows as a matter of course. It is entirely unnecessary to speak of opportunity costs, as OMB does, or to ask, as Revesz does, whether the arguments that apply to money also apply to health and life. The reason is that what is being discounted is always money, and never health or life as such. When agencies apply a discount rate to monetized regulatory benefits, they are discounting the relevant monetary amounts, not life or health. 
To understand this point, it is necessary to see how regulators translate reductions of risk into monetary equivalents. The answer comes from two kinds of evidence. The first and most important involves real-world markets, producing evidence of compensation levels for actual risks.50 In the workplace and for consumer goods, additional safety has a price; market evidence is investigated to identify that price.51 The second kind of evidence comes from contingent valuation studies, asking people how 48 Cf. Richard Dubourg & David Pearce, Paradigms for Environmental Choice: Sustainability versus Optimality, in MODELS OF SUSTAINABLE DEVELOPMENT 21, 24 (Sylvie Faucheux et al. eds., 1996) (“For maximizing a single utility function . . . over infinite time cannot help but suggest that we are dealing with a single generation which exists forever, or even a single individual”). 
49 Perhaps it should not be. But the objection to monetization as such is far more general than the objection to discounting, and we are focusing on the latter objection here. 
50 See W. KIP VISCUSI, FATAL TRADEOFFS: PUBLIC AND PRIVATE RESPONSIBILITIES FOR RISK (1992). 51 A valuable and comprehensive overview can be found in W. Kip Viscusi and Joseph E. Aldy, The Value of a Statistical Life: A Critical Review of Market Estimates Throughout the World, 27 J. RISK & UNCERTAINTY 5 (2003). much they are willing to pay to reduce statistical risks.52 Both of these approaches are controversial, of course, and we do not mean to resolve the controversy here53; the use of the relevant figures grows out of the simple idea that people should not be forced to pay for risk protection they do not want.54 
Currently, regulators use this evidence to calculate the amounts that people are willing to pay to avoid certain categories and levels of risk.55 The most frequently used calculation involves the “value of a statistical life” (VSL). Once an agency has identified the relevant studies, the calculation of VSL is a product of simple arithmetic. The EPA, for instance, relies on studies of actual workplace risk, attempting to determine how much workers are paid to assume mortality hazards.56 If workers must be paid $600, on average, to eliminate a risk of 1/10,000, the value of a statistical life would be said to be $6 million. It should be clear, however, that the very idea of valuing a statistical life is highly misleading; no one is “valuing life.” The real issue involves the valuation of statistical risks. It would be much more accurate to say that for risks of 1/10,000, the median monetary value in the relevant population is $600—or that for risks of 1/100,000, the median monetary value is $60. 
Once regulatory benefits have been monetized in this way, regulators are no longer discounting actual risks to life or health; they are merely discounting the amounts of money that people are willing to pay to avoid those risks. In discounting these monetized regulatory benefits, regulators are doing nothing more controversial than discounting money. It is appropriate to discount the money that people will be willing to spend on refrigerators, automobiles, movies, books, education, and medicine; the same is true of the money that people are willing to spend to avoid risks. All money can be invested and made to grow; it is for this reason that $100 today is worth more than $100 in 2020. To accept discounting for the Last Generation, there is no need to identify logical conundrums or implausible outcomes that seem to follow from a failure to discount. It is not necessary to embark on complex and disputed empirical studies about how people compare a health risk in 2020 to a health risk today. Only two steps are necessary: An appreciation of the theory that underlies current practice, and an understanding that what is being discounted, always, is money, and not life, health, or the environment as such. 
Return in this light to the question of latent harms and suppose that for the Last Generation, it is necessary to assign monetary values to a risk that will come to fruition in of eliminating a risk of 1/100,000 in 2020, and if the answer, in 2020, will be $80, that amount must be subject to the appropriate discount rate for money—and hence the VSL of $8 million must be discounted too. The reason has nothing to do with discounting risks or health; it is that an expenditure of $8 million in 2020 is worth only a fraction of $8 million today. Recall that $8 million in 2020 is worth some fraction of that figure now not for any exotic or theoretically contentious reason, but because the fraction can be invested and made to grow. 
How might this argument be resisted? It is correct to say that national wealth tends to increase over time, and hence people will likely be wealthier in 2020 than they now are. Because they will be wealthier, they will demand more to be subject to statistical risks. For this reason, use of the current VSL to calculate monetary amounts in the future likely produces unjustifiably low numbers.57 But these are not points against discounting. They simply suggest that the numbers that must be discounted are higher than regulators currently recognize. The proper analysis uses a multiplier for national income growth and other relevant factors, and applies a discount rate from that point. 
57 See Revesz, supra note 7; Dora L. Costa & Matthew E. Kahn, The Rising Price of Nonmarket Goods, 93 AM. ECON. REV. PAPERS & PROC. 227, 229 tbl.1 (2003) (suggesting a likely current value of $12 million). In the context of arsenic regulation, the EPA also noted in its sensitivity analysis that the appropriate adjustment would increase the VSL from $6.1 million to $6.7 million. National Primary Drinking Water Regulations; Arsenic and Clarifications to Compliance and New Source Contaminants Monitoring, 66 Fed. Reg. 6976, 7012 (Jan. 22, 2001) (codified at 40 C.F.R. pts. 9, 141, and 142). Note also that wealthier people might not merely be willing spend more because they are wealthier; certain goods, such as environmental protection, might be especially appealing to wealthier people, whose preferences and tastes might change as a result of their relative wealth. 
Similarly, it might be objected that agencies are on fragile ground in using labor market studies to estimate VSL.58 Perhaps workers, accepting a $60 premium to face a risk of 1/100,000, are insufficiently informed or are subject to some form of coercion. Perhaps the proper premium is $70, or $100, or $200. If so, agency practice would have to change significantly; but discounting itself would be unaffected. What would be discounted would be the proper monetary amounts rather than the improper ones. So long as any monetary valuation is used, discounting generally follows. To repeat: When discounting occurs, it is money that is being discounted, not the goods to which monetary amounts are being assigned. 
A separate objection would stress that in the future, technological, medical, and other changes will produce a range of improvements with respect to health, safety, and the environment. Harms that we now project, holding current practices constant, might well not materialize, simply because posterity will be in a position to prevent them. This objection is plausible in itself, but it is not a claim about discounting. It is true that regulators who are projecting future harms should attempt to make an accurate projection, and accuracy requires an appreciation of technological innovation. But a “probabilistic discount rate,”59 reflecting a judgment about such innovation, should not be confused with the issues of discounting on which we are focusing here. 
A more ambitious counterargument would suggest that the monetary values of human beings are not the proper basis for valuing some regulatory benefits. Consider, for example, the continued existence of an endangered species, or the lives of wild horses and elephants. It is controversial (as it should be60) to say that endangered species and animals should be valued by aggregating people’s willingness to pay to protect them. What might be sought is a more deliberative judgment, based on the exchange of reasons that can be offered on behalf of one or another outcome.61 And perhaps the underlying concern could be generalized to a range of benefits and amenities. We should agree that there are many problems with the claim that all goods, including other living creatures, should be valued by aggregating private willingness to pay. But any method of valuation 58 As argued in Ackerman Heinzerling, supra note 13; a general discussion can be found in Sunstein, supra note 54. 
59 See Cowen & Parfit, supra note 12, at 147. 60 See ELIZABETH ANDERSON, VALUE IN ETHICS AND ECONOMICS (1993). 61 See AMARTYA SEN, RATIONALITY AND FREEDOM 286-88 (2002). will necessarily include the explicit or implicit assignment of monetary values. So long as that assignment is made, discounting is generally appropriate, because no one doubts that it is appropriate to discount money.62 
A final objection would ask some questions about the temporal distribution of the risks to be faced by the citizens of the Last Generation. Suppose that some citizens will face risks imminently, whereas others will face risks in a decade, and still others will face risks in twenty or even thirty years. It should be readily apparent that with discounting, imminent mortality risks will receive higher values than risks that will not materialize for many years, and that the same level and kind of risk will elicit a different regulatory expenditure, depending on when the risk will be faced. Is this wrong or unfair? The simplest answer assumes that each citizen pays, in full, for the relevant risk reduction. With that assumption, it should be clear that there is no error or unfairness. It is not unfair to believe that John will pay $50 to reduce an imminent risk of 1/100,000 while also assuming that Jane—or even John himself—will pay only some fraction of that amount to reduce a risk of 1/100,000 in thirty years. That belief follows from the fact that the monetary value of the future risk can and should be discounted. 
But if each citizen will not pay, in full, for the relevant risk reduction, then the analysis cannot be so simple. When regulatory benefits are enjoyed by people who have not paid for them, regulations will have a distributional effect, and that effect should be taken into account.63 It is possible that cost-benefit analysis with discounting will create distributional problems even within the Last Generation.64 But as we shall now see, this is not a point about discounting as such, and it does not affect the analysis of discounting for the Last Generation. 
62 Insofar as (future) animals are involved, however, some of the problems described in connection with future generations apply here as well. 
63 This point is explored in Sunstein, supra note 54. 
64 To say how this might occur, suppose that everyone in the Last Generation faces a statistical risk of 1/100,000 – a risk that 30% of the population faces immediately, that 20% would face in ten years, that 20% faces in twenty years, and that 30% faces in thirty years. Suppose that regulators aggregate and then discount the total cost, and proceed on the ground that the regulation is worthwhile. Suppose finally that every member of the Last Generation is asked to spend $30, right now, to eliminate the risk. It should be clear that those who benefit immediately are being subsidized by those who benefit in the future. 
The argument thus far has not explored the question of intergenerational equity. Of course the amounts spent by future generations involve money, and at first glance that money must be discounted, simply because it is money. But critics are correct to say that discounting might contribute to serious problems of intergenerational equity.65 The reason is that with discounting, a cost-benefit analysis can lead the current generation to impose extremely high costs on future generations, and such costs might be imposed without offering compensating benefits to the losers—leading to a net welfare loss, a serious distributional problem, or both.66 To be sure, people might well have a pure time preference for money, choosing one hundred dollars today over the financially equivalent sum in a year67; but a pure time preference on the part of those now living cannot justify a discount rate with respect to harms faced by people not yet born.68 
It is possible, of course, that the current generation will effectively “pay off” the future generation, making it more than worthwhile for it to bear those costs; the problem of intergenerational equity would be appear to be resolved if future generations are in fact compensated for some risk because (for example) adequate sums of money have been invested for their eventual benefit.69 And the course of human history, with astounding improvements in wealth, health, and longevity, makes it plausible to suggest that something like this does happen over time.70 But there is no assurance that it will continue to occur, in general or for particular risks.71 It follows that in principle and all by itself, a cost-benefit analysis, based on discounting, can create genuine risks of both net welfare losses and distributional inequity.72 65 See Revesz, supra note 7. 66 See Portney & Weyant, supra note 27, at 6 (emphasizing the distributional problem). 67 As noted, a pure time preference is challenged as irrational in Cowen and Parfit, supra note 12, at 
68 See Cowen & Parfit, supra note 12, at 155. 69 This statement raises a number of puzzles, to which we will return. 
70 See Remarks of Vernon Smith, in GLOBAL CRISES, GLOBAL SOLUTIONS 630, 635 (Bjorn Lomborg ed. 2004); Remarks of Thomas Schelling, in id. at 627. 
71 See POSNER, supra note 8, at 151-53; Robert C. Lind, Analysis for Intergenerational Decisionmaking, in DISCOUNTING AND INTERGENERATIONAL EQUITY, supra note 2, at 173, 176. 
72 Baumol suggests that a low discount rate, or even a zero discount rate, might make sense in narrow circumstances, such as those in which the goal is to prevent environmental damage that is both irreversible 
It is not at all clear, however, that a refusal to discount is the best way of reducing those risks. On the contrary, any such refusal might well harm members of future generations. Our primary submission is that the question of discounting should be separated from the question of obligations to future generations; it is not productive to collapse those two questions. If cost-benefit analysis with discounting imposes a serious loss on members of future generations, the current generation should be asked to fulfill whatever moral obligations it has. A refusal to discount is a crude and possibly even perverse way of doing that. 
To see the relevant considerations, consider five problems. Of these, the fourth and fifth are most important, but they are best understood in light of those that precede them. 
will live for a great many years, even centuries. Let us call him Methuselah. Suppose that Methuselah will face a set of health risks (by hypothesis, none of them fatal) over time. Suppose that each risk of concern—those that involve a significant malady—is in the vicinity of 1/100, and that Methuselah is willing to pay $3000 to eliminate each of these risks. On standard assumptions, it is fully appropriate to discount, by the appropriate amount, the monetary value of the relevant risks. If a 1/100 risk will be faced in 2020, it is worth not $3000, but $3000 discounted to present value. Methuselah can invest that discounted amount and watch it grow.73 Money is being discounted, not health—a restatement of our conclusion in Part II. 
This conclusion might be questioned if Methuselah is seen as a series of selves extending over time and if an early self does not act as an appropriate agent for the later one.74 It is possible that Methuselah should be required to take steps to insure against serious harms in old age, especially if self-control problems loom large. But if we indulge and potentially catastrophic. See Baumol, supra note 8, at 801. In our view, this suggestion is best taken as an effort to ensure that future generations are not net losers from the actions of the current generation. On the particular problem, see Cass R. Sunstein, Irreversible and Catastrophic, CORNELL L. REV. (forthcoming 2006). 
73 By itself, this argument rejects Revesz’ claims about the appropriate treatment of latent harms. 74 See Parfit, supra note 12. the not implausible assumption that Methuselah is a good agent for his later self, discounting is fully appropriate. 
risk of 1/100,000, and that every resident of Paretoville is willing to pay $50, but no more, to eliminate that risk. The mayor of Paretoville takes this figure very seriously, and decides not to eliminate risks of 1/100,000 if the cost of doing so is greater than $50. Under plausible assumptions, involving adequate information and sufficient rationality,75 the mayor is properly using cost-benefit analysis in deciding how to proceed, and there is no objection from the standpoint of equity. The reason is that every member of Paretoville pays, in full, for risk reduction, and people should not be required to pay more than they wish unless there is a problem of inadequate information, bounded rationality, or harms to third parties.76 
In some regulatory contexts, all three problems introduce serious complications77; but we are assuming that they are absent in Paretoville. For the citizens of Paretoville, the argument for discounting is straightforward. 
Cleanville, are adjacent to one another in the large and somewhat messy state of Kaldorhicksiana. Dirtyville engages in polluting activity that produces $60 in benefits to each of its 100,000 citizens. That activity creates a risk of 1/100,000, faced by each of the 100,000 citizens of Cleanville. Each citizen of Cleanville is willing to pay $50, but no more, to eliminate the risk of 1/100,000 caused by Dirtyville’s polluting activity. On costbenefit grounds, the polluting activity should be allowed; its value is $6 million, which is higher than its $5 million cost. 
distributional issue: The citizens of Cleanville are uncompensated losers. If we were committed to economic efficiency, we would want the polluting activity to continue, but the distributional problem much complicates matters. And the problem may be worse still. Because monetized figures rather than direct measurements of welfare are involved, it is possible that the activity actually creates a net welfare loss, with the citizens of 75 See Sunstein, supra note 54, for a detailed treatment. 76 Id. 77 Id. Cleanville losing more, in welfare terms, than the citizens of Dirtyville gain.78 Suppose, for instance, that the citizens of Cleanville are relatively poor, and hence their willingness to pay only $50 to eliminate a risk of 1/100,000 is consistent with the conclusion that they are facing a huge welfare loss from their subjection to that risk. The relatively small amount each citizen is willing to pay—$50—reflects their relative poverty, not a relatively small welfare loss. On plausible assumptions about the situation, the state of Kaldorhicksiana, containing these two towns, is not living up to its name, because the losers are not, in welfare terms, losing less than the winners gain. 
equivalent to problem ( those of Cleanville, through law or some process of bargaining. But let us suppose that this is not feasible. In that event, we cannot be sure whether the efficient solution is also the solution that promotes social welfare. An additional question, a familiar one in regulatory policy, is whether there should be some kind of equitable or distributional barrier to the use of cost-benefit balancing.79 If the citizens of Dirtyville are wealthy, and those of Cleanville are poor, the barrier might well be justified, at least if there is no mechanism by which the citizens of Cleanville can capture some of the benefits of the activity. 
produces $60 in benefits to each of its 100,000 citizens. But the polluting activity does not harm citizens of Presentville or any other current place; instead it harms members of future generations. More particularly, the activity creates a risk that will materialize in one hundred years, in the town of Futureville—which, as it happens, is Presentville a century from now. In that time, the one million citizens of Futureville will face a death risk of 1/10,000—meaning that 100 people are expected to die. If the lives of the people of Futureville are valued at $8 million, it is clear that the polluting activity should stop, because $800 million is far greater than $6 million. But if money is discounted at an annual rate of 7%, each of their lives is worth only $581, and hence the polluting activity should continue, because $6 million is far greater than $58,100. 
78 Matthew Adler & Eric A. Posner, Implementing Cost-Benefit Analysis When Preferences Are Distorted, 29 J. LEGAL STUD. 146 (2000). 
79 See Portney & Weyant, supra note 27, at 6. 
But on what premises does it make sense to refuse a $6 million (current) expenditure to save 100 future lives? If all the people of Presentville and Futureville were treated as a single person extending over time, then the case would be similar to Methuselah’s, and discounting would be appropriate. In that case, the various people would amount to just one person who could invest the relevant resources and use them later. And it is tempting to suppose that if there were an intergenerational negotiation between the people of Presentville and the people of Futureville, discounting would be part of a mutually beneficial trade.80 Here is the reason: The people of Presentville could agree not to squander or to consume the benefits they receive, but instead to invest a relevant sum and offer that amount to the people of Futureville, making them better off on balance. Those who emphasize the opportunity costs of investments as a reason for discounting, including OMB, implicitly appeal to the idea that future generations will in fact benefit from the investments that current generations make.81 Hence discounting might be seen as a part of a (hypothetical) mutually beneficial intergenerational negotiation. 
But there are two problems with relying on that idea. The first is conceptual: What is the set of background entitlements against which this purely hypothetical negotiation is occurring? At first glance, the people of Presentville are literally dictators; they can decide to consume all existing resources, to ruin the environment, to impoverish posterity, even to remain childless and not create later generations at all. In the (hypothetical) negotiating process, are the people of Presentville permitted to threaten the (hypothetical) people of Futureville with nonexistence? If so, how much will Futureville be able to extract? If not, is this because hypothetical people have some entitlement to be permitted to exist? And if Presentville merely threatens Futureville with impoverishment and desperation, the people of Futureville will be in a singularly weak position to extract protection against (say) individual risks of 1/100,000. In short, the idea of a mutually beneficial deal raises serious conceptual difficulties. At the very least, it is necessary to identify some entitlements on the part of both Presentville and Futureville, setting the background against which they might bargain. To be plausible, any such specification will inevitably have to depend on an independent normative account of some kind, and that independent account, rather than a notion of intergenerational bargaining as such, will be doing the crucial work.82 
mechanism to ensure that any mutually beneficial bargain will be enforceable; the citizens of Presentville might simply consume their resources instead.83 To be sure, the problem could be solved with compensation, and the discounted value of the 100 future deaths should be used if Futureville will benefit from the investment of that sum. If so, the case would be quite similar to case ( there is no assurance that this will be the case.84 
It should be clear that as a result of cost-benefit analysis with discounting, the problem of Presentville and Futureville has many features in common with that of Cleanville and Dirtyville. But the problem of Presentville and Futureville nonetheless raises distinct questions: What does the present owe the future? Is the present obliged to compensate the future for the injuries it causes? What can the idea of “compensation” mean in this context? We will shortly return to these questions. 
Suppose, as is plausible, that the primary victims of global warming will include poor people in India and Africa.86 Suppose that social planners concerned about global warming decide what to do by engaging in cost-benefit analysis and discounting the victims’ costs to present value. If so, such victims will not be much helped, because no one is planning to invest the discounted sum to create a fund to compensate them in the future. It is true that on an optimistic view, technological innovations might mean that what we see as likely deaths end up as mere illnesses (and perhaps minor illnesses at 82 Hence Rawls rejects the idea of intergenerational bargaining in favor of a just savings principle; see below. 
note 12, at 151-52. Donohue, supra note 11, defends discounting on the ground that it “is appropriate in that, if invested, our resources are expected to grow at [the stated] rate, so that if we forego spending and invest the money instead, we can save more lives in the future with the amount foregone today.” Id. at is a kind of giant Methuselah, which it clearly is not. 
84 See Lind, supra note 71, at 176-77. 85 See POSNER, supra note 8, at 151-52. 
86 WILLIAM NORDHAUS & JOSEPH BOYER, WARMING THE WORLD: ECONOMIC MODELS OF GLOBAL WARMING (2000). that). But this possibility does not justify discounting; it is instead an effort to deny that the anticipated harms will be as large as we project. If that number is inflated, then of course the analysis must change. 
It is also true that future generations are likely to be wealthier than our own, and hence it might not make much sense for the relatively poor present to transfer resources to the relatively rich future.87 This would be a perverse form of redistribution. If future generations can be expected to be richer, that point must be part of the analysis of what equity requires. And if future generations can be expected to be richer, their anticipated wealth is produced by some combination of the efforts, investments, and altruism of their predecessors—a point that compounds the concern about perverse redistribution. But suppose, for example, that a relatively poor community is gaining $5 million as a result of activity that will cause 100 deaths in a relatively wealthier community. Is the activity justified merely because poorer people are obtaining the benefit, which by hypothesis is much smaller than the cost? That claim would be exceedingly difficult to defend. It is in this light that cost-benefit analysis with discounting can indeed produce serious problems across generations, including a net welfare loss and distributional unfairness. 
Responding to concerns of this sort, Thomas Schelling argues that “[g]reenhouse gas abatement is a foreign aid program, not a saving-investment problem of the familiar kind.”88 For long-term problems, including global warming, it might be thought that the question is whether the current generation should provide “foreign aid” to posterity. And because posterity is likely to be wealthier than we are,89 there is a serious question whether such aid will or even should be provided. As Schelling suggests, citizens of the developed world are not now willing to make significant sacrifices to help people in poor nations; it would seem extremely unlikely that such citizens would be willing to make significant sacrifices to assist people in those same nations in the distant and probably less-poor future.90 
But Schelling’s analogy runs into serious difficulties. In our example, Futureville is not merely a foreign country; it consists to a large extent of Presentville’s own 87 See Schelling, supra note 70. 
88 Thomas Schelling, Intergenerational Discounting, in DISCOUNTING AND INTERGENERATIONAL EQUITY, supra note 2, at 99, 100. 
89 Id. at 100-101. 90 Id. at 101. descendents, and the risks faced in Futureville are a direct result of Presentville’s actions—both plausible reasons to think that Futureville might have special obligations towards Presentville. The idea of “foreign aid” seems an exceedingly poor fit for problems like that of global warming, in which environmental and health risks in some Futureville are a product of actions undertaken knowingly (and perhaps negligently) by some Presentville. In that event, the present might well be seen to have committed a kind of tort on the future, and the argument for compensation is hardly a claim for some kind of subsidy, or “aid.” 
To give a stark example, imagine that present generations plant a bomb that will explode in two centuries. Is this a violation of the obligation to provide “foreign aid”? Environmental problems are rarely bombs, for they are not created with malice or any kind of self-consciously destructive goal; but if they result from activities that are projected to create risks, they must be analyzed in the general terms of tort law. This point has important implications for global warming. The risks, faced above all by poor nations, are a result of actions from which wealthier nations have benefited. 
C. Not Discounting as a Crude Response to the Intergenerational Problem present generation believes that we have moral obligations to our successors, either because those successors will be our children’s children, or because whoever they are, they will be injured by our actions. The key point—what we seek to emphasize here—is that refusing to discount is not a good way of fulfilling these obligations.91 Indeed, any such refusal might well hurt posterity. The moral obligation is best discharged not by a zero discount rate, but by asking the current generation to ask more directly about what it is morally obliged to do. 
91 As argued in Samida, supra note 80; see also Cowen & Parfit, supra note 12, at 158-59, for the brief suggestion that while remoteness in time is not a reason to care less about social harms, it might make sense to take account of the possibility that “it would be cheaper now to ensure compensation.” William D. Nordhaus, Discounting and Public Policies That Affect the Distant Future, in DISCOUNTING AND INTERGENERATIONAL EQUITY, supra note 2, at 145, provides a powerful argument against responding to the ethically unacceptable consequences of cost-benefit analysis by altering discount rates, on the view that “ad hoc manipulation of discount rates is a very poor substitute for policies that focus directly on the ultimate objective.” 
A refusal to discount, often justified as a way of assisting the future,92 is a singularly crude way of attempting to fulfill any obligation to future generations. The consequence of refusing to discount might well be to reduce investments, economic and otherwise, that will lead to long-term prosperity. If so, then discounting is hardly helpful to future generations, which greatly benefit from economic growth, not least because growth can lead increase the amounts spent on environmental protection. Alternatively, a refusal to discount might well result in the postponement of protective programs, environmental and otherwise.93 In that event as well, the future is to that extent hurt rather than helped.94 Our simple point is this: The moral obligations of current generations should be uncoupled from the question of discounting, because refusing to discount is not an effective way of ensuring that those obligations are fulfilled. The moral issues should be investigated directly, and they should be disentangled from the practice of discounting. 
complaint if current generations follow the path indicated by cost-benefit analysis with discounting. But what kind of complaint do they have? To answer that question, it is necessary to say something about the nature of intergenerational equity. 
It is tempting to think of ethical obligations in compensatory terms, as in the idea that ethical obligations are satisfied if the present can make it worthwhile for future generations to run the risks to which it subjects them. But this idea turns out to be a false start, because it is hard to know what the idea of compensation means in this context. Must the present compensate the future for each particular risk? That conclusion would be implausible; surely it would be acceptable to impose a risk of 1/100,000 on ten million future people if the very step that imposes that risk also eliminates a 1/10,000 risk that would be faced by one hundred million future people (including the ten million future people subjected to the new 1/100,000 risk). At first glance, then, the goal should be to produce an overall “risk package” for which adequate compensation has been paid. But to what, exactly, is this overall risk package being compared? To a situation in which future generations face extreme poverty and catastrophic global warming? To a situation in 92 See Revesz, supra note 7, at 987-1007; Ackerman & Heinzerling, supra note 13. 93 See Keeler & Cretin, supra note 21. 94 See id. which future generations do not exist at all? Do members of future generations have rights to exist? These questions are closely connected with the difficulty of specifying the background entitlements against which any hypothetical bargaining occurs.95 
In short, it is necessary to specify the baseline against which any “compensation” must be paid, and the real moral work is being done by that baseline, not by the idea of compensation. The relevant baseline must consist of a more general account of the ethical obligations owed by the present to the future.96 Some people believe that current generations are obliged not to make the environment worse than it is today.97 On this view, current generations are environmental trustees; as such, they must adhere to a kind of environmental nondegradation principle. But there is a problem with this position, which is its selective focus on environmental quality. Suppose that the current generation sacrifices a pristine area, or a remote island, but that as a direct result of that action, it is able to confer significant economic and even environmental benefits on posterity. Is it so clear that the sacrifice is morally unacceptable? 
John Rawls emphasizes a more promising approach, embodied in a “just savings” principle, to be chosen by people behind a veil of ignorance in which “they do not know to which generation they belong or, what comes to the same thing, the stage of civilization of their society.”98 For Rawls, the key point is the extension of the device of the veil of ignorance to the intergenerational question. What approach would people 95 Note too that Rawls’s just savings principle would be satisfied by less than full compensation. The reason is that it is possible to imagine, without full compensation for risks, a system of savings that will bring “about the full realization of just institutions and the equal liberties,” with particular reference to the “standpoint of the least advantaged of each generation.” JOHN RAWLS, POLITICAL LIBERALISM 287-88 (1993). Indeed, it is possible to imagine situations in which full compensation might well be too demanding. Suppose, for example, that the result of full compensation would be to impoverish the most disadvantaged members of the current generation, in order to ensure compensation to the already-wealthy members of future generations. Current generations might, in principle, be able to claim that full compensation is not necessary when the distributive consequences are perverse. In fact, this claim raises some causal and even conceptual difficulties: If future generations are significantly wealthier than past generations, their wealth is partly attributable to the actions and omissions of their predecessors; once the causal chains have been sorted out, we might well conclude that adequate compensation has been paid for any risks, taken not separately but as a whole. 
96 For an influential view, see JOHN RAWLS, A THEORY OF JUSTICE 251-58 (revised ed. 1999); for a helpful overview, see Lukas Meyer, Intergenerational Justice, THE STANFORD ENCYCLOPEDIA OF PHILOSOPHY (Edward N. Zalta ed. Summer 2003 ed.), available at http://plato.stanford.edu/archives/sum2003/entries/justice-intergenerational/. 
97 Edith Brown Weiss, Intergenerational Equity: A Legal Framework for Global Environmental Change, in ENVIRONMENTAL CHANGE AND INTERNATIONAL LAW: NEW CHALLENGES AND DIMENSIONS 385 (Edith Brown Weiss ed. 1991). 
98 RAWLS, supra note 96, at 254. select if they were unaware of the generation in which they will find themselves? Rawls also contends that his conception of justice as fairness ought to inform choices behind the veil. What is required, on his view, is a system of savings that will bring “about the full realization of just institutions and the equal liberties,”99 with close attention to the “standpoint of the least advantaged of each generation.”100 Under this approach, costbenefit analysis with discounting is morally problematic if it leads to decisions that (for example) greatly injure the most disadvantaged members of future societies. The proper response would be to take steps to conform to the just savings principle, chosen behind the veil of ignorance. 
On this view, for example, it would be unacceptable to refuse to take steps to protect against global warming if the refusal meant that the least advantaged members of future generations would suffer hardship well beyond that of the least advantaged members of the current generation. On the other hand the current generation would not be required to take protective measures that would produce extreme hardship for its own least advantaged members, at least if that hardship would exceed what is anticipated for the least advantaged members of future societies. And indeed, some debates over global warming devote attention to issues of exactly this sort.101 
In a later treatment, Rawls contends that it is unhelpful to “imagine a (hypothetical and nonhistorical) direct agreement between all generations.”102 Instead the parties, behind the veil of ignorance, might be “required to agree to a savings principle subject to the further condition that they must want all previous generations to have followed it.”103 This savings principle, thus understood, has the advantage of treating all generations the same, thus protecting against the dual problems of impoverishing the present and impoverishing the future. Here as well, an approach that harmed the most disadvantaged members of current generations for the sake of the future would be disfavored, and a key question would be whether that approach was necessary to protect 99 Id. at 257. 100 Id. at 258. 101 See INDUR GOKLANY, THE PRECAUTIONARY PRINCIPLE (2002). 102 RAWLS, supra note 93, at 274. 103 Id. the most disadvantaged members of future generations from still greater harm (as, on a pessimistic view, is the case of emissions of greenhouse gases104). 
Rawls’ own approach, emphasizing equal liberties and the least advantaged members of society, is not utilitarian or welfarist; it builds on his idea of justice as fairness. But it would be easy to adapt the idea of a veil of ignorance for utilitarian or welfarist purposes. From the welfarist point of view, the goal should be to maximize welfare over time. Welfarists would want current generations to give members of future generations the same moral weight that they give to existing people. Hence the current generation violates its ethical responsibilities if it engages in projects that lead to net welfare losses, measured after including the interests of all generations.105 If existing practice produce significant long-term environmental harm, in a way that lead to serious health risks for posterity, the current generation is violating its duties. 
We believe that the idea of a veil of ignorance is both helpful and appealing, and that it points in the right directions for thinking about intergenerational equity. But our aim here is to sketch rather than to solve that problem. Our simple point is that behind the veil, a refusal to discount would not be chosen, because the refusal would often hurt future generations as well as the current one. Whatever the proper approach to intergenerational equity, the debate over that issue should be separated from the debate over discounting, and the former debate should be engaged directly. 
analysis with discounting can produce serious distributional problems, and can easily lead to a net welfare loss. The proper response is to take steps to ensure that present generations do not violate their obligations to posterity. On an optimistic view, no special steps are necessary. Some combination of market forces and ordinary altruism tends to ensure that those who come later are, in all relevant respects, significantly better off than those who came before.106 
But perhaps the optimistic view is unrealistic for some problems, such as global warming. Suppose that global warming imposes truly catastrophic losses on the world as 104 See POSNER, supra note 8. 
105 We are putting to one side the question whether the focus should be on average welfare or total welfare; an emphasis on total welfare seems to lead to the puzzling suggestion that it is important to ensure that there are as many people as possible. See Parfit, supra note. 
106 See Schelling, supra note 88; Schelling, supra note 70. a whole, or at least on the most vulnerable members of the most vulnerable nations.107 Even if the losses from global warming are not catastrophic, it would be surprising if the gains from refusing to spend money on greenhouse gas emissions turned out to protect those who are most likely to suffer as a result of greenhouse gas emissions. When the optimistic view fails, the current generation is obliged take self-conscious steps to protect its successors. Our goal is not to specify the mechanisms by which the current generation fulfills that obligation, but to suggest that whether or not the optimistic view is right, a refusal to discount is not the appropriate response to the risk of intergenerational inequity. 
In recent years, a great deal of attention has been devoted to the topic of “sustainable development,” an idea that has considerable force in international law.108 Unfortunately, the idea of sustainability remains poorly defined. An influential report suggests that development is sustainable if it “meets the needs of the present without compromising the ability of future generations to meet their own needs.”109 Implicitly using a framework not unlike Rawls’, Robert Solow defines sustainability to require each generation to have the capacity to attain the same levels of welfare as those that preceded it.110 For the environmental context, Solow contends that this definition means that nonrenewable resources must be used so as not to make it impossible for future generations to acquire the same standard of living.111 Edith Brown Weiss argues that each generation has a duty not to make the environmental quality of the planet worse and also to preserve the essential options available to future generations.112 
Each of these specifications is contentious, for reasons that should be clear from the discussion thus far. But if the idea of sustainable development is designed to require present generations to pay attention to the interests of those who will follow, it points in 107 See POSNER, supra note 8, at 43-58. 
108 See Revesz, supra note 7, at 1009-1014; SUSTAINABLE DEVELOPMENT (Julian Morris ed. 2002); MODELS OF SUSTAINABLE DEVELOPMENT (Sylvie Faucheux et al. eds., 1996). 
109 See WORLD COMMISSION ON ENVIRONMENT AND DEVELOPMENT, OUR COMMON FUTURE 43 (1987). 110 Robert Solow, An Almost Practical Step Toward Sustainability, 19 RESOURCES POL’Y 162 (1993). 111 Id. 
112 Edith Brown Weiss, Intergenerational Equity: A Legal Framework for Global Environmental Change, in ENVIRONMENTAL CHANGE AND INTERNATIONAL LAW: NEW CHALLENGES AND DIMENSIONS 385 (Edith Brown Weiss ed. 1991). Brown’s account, like Solow’s, goes well beyond what is required by Rawls’s just savings principle. On the relevance of options, see Sunstein, supra note 72. the right directions and should have considerable practical importance. Of course most people are willing to sacrifice their own well-being for the benefit of their children; and as we have noted, the arc of human history suggests that the standard of living increases over time in any case. But for some goods, including some environmental amenities, long-term losses are possible unless steps are self-consciously taken to avoid them; and depletion of the ozone layer, threats to endangered species, and global warming threaten to impose large-scale risks on posterity.113 
To the extent that the idea of sustainable development is meant to require a specific policy of preserving environmental goods, it offers a valuable reminder that current actions can produce short-run economic benefits while also creating long-term environmental problems. The reminder is especially important in the face of potentially irreversible environmental change.114 But environmental protection can burden the future too, especially if it is extremely costly, and there is no abstract reason to believe that preserving a particular environmental amenity (a forest, a lake) is always better for posterity than other investments that do not involve the environment in particular (expenditures on basic research, reductions in national debt). The appropriate conclusion is that an emphasis on sustainable development must be taken as a placeholder for a set of conclusions, requiring specification and independent justification, about what intergenerational equity requires. 
What are the implications for reviewing courts and for regulatory practice? The question of judicial review is the easiest to handle. Courts are correct to require some kind of rationale for any particular discount rate (including a discount rate of zero).115 An implausibly high discount rate (say, 10%) would have to be explained, as would an implausibly low one (say, 1%). But the great complexity of the underlying issues, and the continued existence of reasonable disagreement, argue for a cautious judicial role, 113 POSNER, supra note 8; WILLIAM NORDHAUS & JOSEPH BOYER, WARMING THE WORLD: ECONOMIC MODELS OF GLOBAL WARMING (2000). 
114 See POSNER, supra note 8, at 162; Kenneth Arrow, Discounting, Morality, and Gaming, in DISCOUNTING AND INTERGENERATIONAL EQUITY, supra note 2, at 17-20; Sunstein, supra note 72. 
115 Natural Resources Defense Council, Inc v. Herrington, 768 F.2d 1355, 1410-14 (D.C. Cir. 1985); Northern California Power Agency v. FERC, 37 F.3d 1517 (D.C. Cir. 1994). especially because of the risk that judicial invalidations will simply stall desirable regulation.116 Of course extreme cases can be imagined.117 Suppose that an agency refuses to discount the monetary value of health and safety benefits at all. If so, it would be reasonable to rule that the agency’s refusal is arbitrary, and perhaps any resulting regulation should be struck down if the refusal to discount is responsible for its content. Under current circumstances, a discount rate of 10% would be extremely difficult to defend. But across a wide range of agency choices, judicial deference is the best general orientation. 
Our discussion provides considerable support for OMB’s general posture of requiring the same discount rate for all costs and benefits.118 It does so not on the basis of OMB’s unruly and complex rationale,119 but on the ground that for latent harms, what is being discounted is money, and not risks to life and health as such. To the extent that regulations will mostly affect currently living people, a uniform discount rate is fully appropriate. Unfortunately, agencies have not always followed OMB’s guidance with respect to discount rates.120 On this issue, at least, they ought to so do. 
The analysis must be more complicated when planners are affecting the welfare of future generations—as, for example, in the assessments of values associated with protection against global warming. It is standard to use a uniform discount rate for such values.121 Nothing said here suggests that the standard practice is wrong. But we have emphasized that for such problems as global warming, cost-benefit analysis with the usual discount rate can produce both welfare losses and serious unfairness.122 In this context, social planners should not base their decisions solely on such analysis with discounting; any judgments about appropriate regulation must include steps that will fulfill the present generation’s moral obligations to the future. For global warming, a separate international fund, provided mostly by wealthy nations and accompanying 116 See JERRY MASHAW & DAVID HARFST, THE STRUGGLE FOR AUTO SAFETY (1990). 117 The cost-benefit analysis for arsenic is an example. See supra note 57. 118 See supra. 119 See supra. 
120 See Morrison, Comment, supra note 5. We have not attempted to identify the appropriate discount rate, and hence have not taken a position on the two options suggested by OMB: 7% and 3%. The most careful analyses argue in favor of the lower figure. See, e.g., Arrow, supra note 114. 
121 See, e.g., NORDHAUS & BOYER, supra note 113. 122 See POSNER, supra note 8, at 151-52. reductions in greenhouse gases, may well be a place to start. Such a fund could be used to promote further reductions and also to help nations that most suffer from global warming—by, for example, furnishing technological assistance to facilitate adaptations to hotter climates. Of course it is possible that the feasible steps to assist adaptation would not be adequate, and that prevention is therefore preferable. 
The debate over discounting regulatory benefits has become both vigorous and exceedingly complicated. In our view, both advocates and critics of discounting have missed a central point. So long as monetary values are assigned to the relevant variables, it is only money, and not any variable as such, that is being discounted. If a discount rate is properly applied to money, it is properly applied to the money that public or private actors are willing to devote to regulatory benefits. There is no need for a separate assessment of the discount rate applied to “latent harms.” What is being discounted is the money that is used to combat those harms. 
In many respects, current valuations may be too low—perhaps because they do not consider national income growth, perhaps because cancer risks deserve particular attention,123 perhaps because they do not include the valuations of those whose friends and family members are at risk.124 But as a general rule, it should not be controversial to apply the monetary discount rate to monetized regulatory benefits, simply because no one doubts that money should be discounted. 
It is true that cost-benefit analysis with discounting, no less than cost-benefit analysis of any kind, can produce a net welfare loss, significant distributional difficulties, or both. For this reason, decisions based on that form of analysis can create severe ethical problems. But a refusal to discount might well fail to solve those problems. It may even 123 See James K. Hammitt & Jin-Tan Liu, Effects of Disease Type and Latency on the Value of Mortality Risk, 28 J. RISK & UNCERTAINTY 73, 80 (2004) (“The value of preventing a fatal cancer is often considered to be greater than the value of preventing a fatal trauma in a workplace or transportation accident.”); Revesz, supra note 7. Some people, however, have expressed skepticism about the argument for adjustments. See EPA, An SAB Report on EPA's White Paper Valuing the Benefits of Fatal Cancer Risk Reduction (2000), available at http://www.epa.gov/science1/pdf/eeacf013.pdf (last visited May 6, 2004) ("[T]he Committee does not believe that the current literature supports adjustments to the VSL for differences in age, health status, or risk aversion."). 
124 See Eric A. Posner & Cass R. Sunstein, Dollars and Death, U. CHI. L. REV. (forthcoming 2005). aggravate them, either by impoverishing the present (to the detriment of the future) or by requiring the delay of life-saving programs (also to the detriment of the future). Current generations do owe moral duties to posterity, and it is important to prevent actions that impose serious losses on those who will follow. We have suggested that the idea of a veil of ignorance is the appropriate foundation for thinking about the problem intergenerational equity. But that problem should be engaged directly; it should not be conflated with the question of discounting.   Professor Cass Sunstein University of Chicago Law School 1111 East 60th Street Chicago, IL 60637 csunstei@uchicago.edu  2.  William M. Landes, Copyright Protection of Letters, Diaries and Other Unpublished Works: An  Economic Approach (July 1991)  Richard A. Epstein, The Path to The T. J. Hooper: The Theory and History of Custom in the Law of  Tort (August 1991)  Cass R. Sunstein, On Property and Constitutionalism (September 1991)  Richard A. Posner, Blackmail, Privacy, and Freedom of Contract (February 1992)  Randal C. Picker, Security Interests, Misbehavior, and Common Pools (February 1992)  Tomas J. Philipson & Richard A. Posner, Optimal Regulation of AIDS (April 1992)  Douglas G. Baird, Revisiting Auctions in Chapter 11 (April 1992)  William M. Landes, Sequential versus Unitary Trials: An Economic Analysis (July 1992)  William M. Landes & Richard A. Posner, The Influence of Economics on Law: A Quantitative Study  (August 1992)  Alan O. Sykes, The Welfare Economics of Immigration Law: A Theoretical Survey With An  Analysis of U.S. Policy (September 1992)  Douglas G. Baird, 1992 Katz Lecture: Reconstructing Contracts (November 1992)  Gary S. Becker, The Economic Way of Looking at Life (January 1993)  J. Mark Ramseyer, Credibly Committing to Efficiency Wages: Cotton Spinning Cartels in Imperial  Japan (March 1993)  Cass R. Sunstein, Endogenous Preferences, Environmental Law (April 1993)  Richard A. Posner, What Do Judges and Justices Maximize? (The Same Thing Everyone Else Does)  (April 1993)  Lucian Arye Bebchuk and Randal C. Picker, Bankruptcy Rules, Managerial Entrenchment, and  Firm‐Specific Human Capital (August 1993)  J. Mark Ramseyer, Explicit Reasons for Implicit Contracts: The Legal Logic to the Japanese Main  Bank System (August 1993)  William M. Landes and Richard A. Posner, The Economics of Anticipatory Adjudication  (September 1993)  Kenneth W. Dam, The Economic Underpinnings of Patent Law (September 1993)  Alan O. Sykes, An Introduction to Regression Analysis (October 1993)  Richard A. Epstein, The Ubiquity of the Benefit Principle (March 1994)  Randal C. Picker, An Introduction to Game Theory and the Law (June 1994)  William M. Landes, Counterclaims: An Economic Analysis (June 1994)  J. Mark Ramseyer, The Market for Children: Evidence from Early Modern Japan (August 1994)  Robert H. Gertner and Geoffrey P. Miller, Settlement Escrows (August 1994)  Kenneth W. Dam, Some Economic Considerations in the Intellectual Property Protection of  Software (August 1994)  Cass R. Sunstein, Rules and Rulelessness, (October 1994)  David Friedman, More Justice for Less Money: A Step Beyond Cimino (December 1994)  Daniel Shaviro, Budget Deficits and the Intergenerational Distribution of Lifetime Consumption  (January 1995)  Douglas G. Baird, The Law and Economics of Contract Damages (February 1995)  Daniel Kessler, Thomas Meites, and Geoffrey P. Miller, Explaining Deviations from the Fifty  Percent Rule: A Multimodal Approach to the Selection of Cases for Litigation (March 1995)  Geoffrey P. Miller, Das Kapital: Solvency Regulation of the American Business Enterprise (April  1995)  Richard Craswell, Freedom of Contract (August 1995)  J. Mark Ramseyer, Public Choice (November 1995)  Kenneth W. Dam, Intellectual Property in an Age of Software and Biotechnology (November 1995)  Cass R. Sunstein, Social Norms and Social Roles (January 1996)  37.  
David A. Weisbach, Measurement and Tax Depreciation Policy: The Case of Short‐Term Assets  (January 2003)  Randal C. Picker, Understanding Statutory Bundles: Does the Sherman Act Come with the 1996  Telecommunications Act? (January 2003)  Douglas Lichtman and Randal C. Picker, Entry Policy in Local Telecommunications: Iowa Utilities  and Verizon (January 2003)  William Landes and Douglas Lichtman, Indirect Liability for Copyright Infringement: An Economic  Perspective (February 2003)  Cass R. Sunstein, Moral Heuristics (March 2003)  Amitai Aviram, Regulation by Networks (March 2003)  Richard A. Epstein, Class Actions: Aggregation, Amplification and Distortion (April 2003)  Richard A. Epstein, The “Necessary” History of Property and Liberty (April 2003)  Eric A. Posner, Transfer Regulations and Cost‐Effectiveness Analysis (April 2003)  Cass R. Sunstein and Richard H. Thaler, Libertarian Paternalizm Is Not an Oxymoron (May 2003)  Alan O. Sykes, The Economics of WTO Rules on Subsidies and Countervailing Measures (May  2003)  Alan O. Sykes, The Safeguards Mess: A Critique of WTO Jurisprudence (May 2003)  Alan O. Sykes, International Trade and Human Rights: An Economic Perspective (May 2003)  Saul Levmore and Kyle Logue, Insuring against Terrorism—and Crime (June 2003)  Richard A. Epstein, Trade Secrets as Private Property: Their Constitutional Protection (June 2003)  Cass R. Sunstein, Lives, Life‐Years, and Willingness to Pay (June 2003)  Amitai Aviram, The Paradox of Spontaneous Formation of Private Legal Systems (July 2003)  Robert Cooter and Ariel Porat, Decreasing Liability Contracts (July 2003)  David A. Weisbach and Jacob Nussim, The Integration of Tax and Spending Programs (September  2003)  William L. Meadow, Anthony Bell, and Cass R. Sunstein, Statistics, Not Memories: What Was the  Standard of Care for Administering Antenatal Steroids to Women in Preterm Labor between 1985  and 2000? (September 2003)  Cass R. Sunstein, What Did Lawrence Hold? Of Autonomy, Desuetude, Sexuality, and Marriage  (September 2003)  Randal C. Picker, The Digital Video Recorder: Unbundling Advertising and Content (September  2003)  Cass R. Sunstein, David Schkade, and Lisa Michelle Ellman, Ideological Voting on Federal Courts  of Appeals: A Preliminary Investigation (September 2003)   Avraham D. Tabbach, The Effects of Taxation on Income Producing Crimes with Variable Leisure  Time (October 2003)  Douglas Lichtman, Rethinking Prosecution History Estoppel (October 2003)  Douglas G. Baird and Robert K. Rasmussen, Chapter 11 at Twilight (October 2003)  David A. Weisbach, Corporate Tax Avoidance (January 2004)  David A. Weisbach, The (Non)Taxation of Risk (January 2004)  Richard A. Epstein, Liberty versus Property? Cracks in the Foundations of Copyright Law (April  2004)  Lior Jacob Strahilevitz, The Right to Destroy (January 2004)  Eric A. Posner and John C. Yoo, A Theory of International Adjudication (February 2004)  Cass R. Sunstein, Are Poor People Worth Less Than Rich People? Disaggregating the Value of  Statistical Lives (February 2004)  Richard A. Epstein, Disparities and Discrimination in Health Care Coverage; A Critique of the  Institute of Medicine Study (March 2004)  Richard A. Epstein and Bruce N. Kuhlik, Navigating the Anticommons for Pharmaceutical Patents:  Steady the Course on Hatch‐Waxman (March 2004)  Richard A. Esptein, The Optimal Complexity of Legal Rules (April 2004)  Eric A. Posner and Alan O. Sykes, Optimal War and Jus Ad Bellum (April 2004)  Alan O. Sykes, The Persistent Puzzles of Safeguards: Lessons from the Steel Dispute (May 2004)  213.  
Much of Justice Sandra Day O'Connor's work on the Supreme Court embodies a commitment to judicial minimalism, understoodas a preferencefor narrow rulings, closely attuned to particularfacts. In many contexts, however, that commitment is hard to justify, simply because it imposes severe decisionmakingburdens on others and may well createmore, ratherthanfewer, errors. For this reason, a general preference for minimalism is no more defensible than a generalpreferencefor rules. The choice between narrow and wide rulings cannot itselfbe made by rules or even presumptions; it requires a case-by-case inquiry. The argument is illustratedthroughout this Article with reference to the problem of affirmative action, where Justice O'Connor's preferencefor particularityresultedin the impositionofa constitutionalmandate on admissionsoffices that is not simple to defend in principle.In some contexts, however, narrow rulings are indeedpreferable,in largepart because they give flexibility to politically accountable officials. Justice O'Connor'sminimalism is best understood as reflecting a belief that in dfficult cases at the frontiers of constitutionallaw,judges woulddo best to avoidfirm rules thatthey might come to regret. * Karl N. Llewellyn Distinguished Service Professor of Jurisprudence, the University of Chicago Law School, Department of Political Science, and the College. I am grateful to Christine Jolls for helpful discussions and to James Weingarten for research assistance and excellent comments. 
1899 
INTRODUCTION 
" Program A gives a specified number of points to every AfricanAmerican applicant. More particularly, every such applicant receives ten points, simply by virtue of being African-American. Ten points are not trivial, but they are far from enough to ensure admission. Children of alumni, for example, receive fifteen points; specified academic achievements produce thirty points; athletic accomplishments result in the addition of fifteen points. Admission is unlikely unless an applicant receives at least sixty points. " Program B dispenses with a point system. Every applicant receives individualized consideration from one of eight admissions officers. In hard cases, the admissions officers meet in "teams" of three; in the hardest cases, all eight admissions officers meet as a group. For African-American applicants, race counts as a plus, though numbers are not assigned. 
Under existing law, Program A is unconstitutional because it is too rulebound.1 Program B is permissible because it calls for "holistic" consideration of individual applicants. More than anyone else, Justice Sandra Day O'Connor is responsible for the fact that constitutional law distinguishes so sharply between the two programs. While Justice O'Connor and Justice Breyer voted to strike down a variation on Program A and to uphold a variation on Program B,3 the isnevvaelnidaottihnegr boJuthstipcreosgrawmosu.l4d treat the two the same, either upholding or 
the Court has long made clear that educational institutions cannot "insulat[e] each category of applicants with certain desired qualifications from competition with all other applicants." 5 Extending that principle, the Court has also invalidated a point system, analogous to Program A, used for undergraduate of Michigan's freshman admissions policy, "which automatically distributes 20 points, or one-fifth of the points needed to guarantee admission, to every single 'underrepresented minority' applicant solely because of race," violated the Equal Protection Clause because it was "not narrowly tailored to achieve [the University's] asserted compelling interest in diversity). 
program in question "engages in a highly individualized, holistic review of each applicant's file ... "). 
concurring); Grutter,539 U.S. at 311-44 (O'Connor, J.). 

315 (1978)). admissions at the University of Michigan. 6 In that system, students received a specified set of points for various attributes, including academic performance (up to 110 points), in-state residence (ten points), having alumni parents (four points), athletic recruitment (twenty points), and being a member of an underrepresented minority group (twenty points).7 
The Court did not rule that the twenty points were too high; it ruled instead that a point system, in the context of racial preference, is invalid as such. The Court stressed "the importance of considering each particular applicant as an individual, assessing all of the qualities that the individual possesses, and in turn, evaluating that individual's ability to contribute to the unique setting of higher education." 8 The problem with the point system is that its automatic nature simply does not allow for individualized consideration. And the easy administrability of this automatic system does not constitute an adequate excuse, for "the fact that the implementation of a program capable of providing individualized consideration might present administrative challenges does not render constitutional an otherwise problematic system." 9 
By contrast, in Grutter v. Bollinger, Justice O'Connor wrote the Court's opinion permitting educational institutions to create affirmative action programs if they do not assign points or impose quotas but merely include race as a "plus" within a system of highly individualized judgment.10 At least such programs are acceptable if they remain "flexible enough to ensure that each applicant is evaluated as an individual.""l Hence, the Court permits raceconscious admissions if, in the words of Justice O'Connor's opinion, there is "a highly individualized, holistic review of each applicant's file, giving serious consideration to all the ways an applicant might contribute to a diverse educational environment." 12 When no policy gives "automatic acceptance or rejection based on any single 'soft' variable," and when there are "no mechanical, predetermined diversity 'bonuses' based on race or ethnicity," affirmative action is permissible.13 
In drawing a sharp line between rigid and more particularized programs, Justice O'Connor acted in a way that fits with her jurisprudence far more generally. She has essentially required educational institutions to proceed in a way that fits her own "holistic" practice-her preference for case-by-case judgment, unburdened by clear rules. Because of her general commitment to particularized consideration, Justice O'Connor has stood as the Court's most prominent minimalist, asking for narrow rulings rather than broad ones. In joining the Court's decision to invalidate a Chicago gang-loitering ordinance, for example, Justice O'Connor went out of her way to suggest that another such ordinance, more cautiously drawn by the city or more narrowly construed by a state supreme court, might well pass constitutional muster. 14 In addition, Justice O'Connor is largely responsible for the "undue burden" standard in the area of abortion'15_a standard that is rule-free and that calls for close attention to the details of the particular restriction at issue. In the context of restrictions on commercial advertising, Justice O'Connor has also written narrow opinions, carefully tailored to particular facts. 16 The same is true in the context of the Establishment Clause, where her jurisprudence has a noteworthy minimalist 17 dimension. 
To be sure, no one believes that all details are relevant. No one contends that judges should attend to the astrological sign of the litigants, or the second letter of their last names, or the hour of the day on which certiorari was sought. But in many cases, Justice O'Connor has shown an unquestionable preference for decisions that are narrowly tailored, that leave a great deal undecided, and that preserve flexibility for the future. In these respects, Justice O'Connor has twaiktehnthaeniratpepnrdoeancchy toto wcoanrdstiitnuctiroenmaelnltaawldtehvaetlobpumildenst.o1n8 common law processes, 
My purpose here is to raise questions about that preference. I begin with the suggestion that in the context of affirmative action, Justice O'Connor's interest in case-by-case judgment has led her to a puzzling and probably indefensible conclusion. It is hardly clear that the Constitution should be taken to require a procedure that sacrifices transparency, predictability, and equal treatment-and that does so while imposing significant burdens on officials who must evaluate particular applications for admission. This objection leads to a much more general point. Any defense of minimalist adjudication is essentially the same in principle as a defense of standards over rules-and there is no reason to think that such a defense can be made convincing in all of the contexts in which Justice O'Connor has ruled narrowly. In short, the choice between narrow and wide rulings cannot itself be resolved by rule. 
Ironically, Justice O'Connor's own practice suggested a kind of presumption in favor of minimalism. As a first approximation, the better approach rejects any such presumption and calls instead for a case-by-case inquiry into whether case-by-case decisions are desirable. This point serves as a challenge to minimalism as a general project, but it also helps to produce a reconstruction and defense of the claim that seems to me to animate much of Justice O'Connor's work: in the hardest cases, at the frontiers of constitutional law, the Court usually does best if it proceeds narrowly and if it avoids steps that might be confounded by unanticipated circumstances. The arguments that support minimalism in particular cases also support this general use of minimalism. As we shall see, there is a democratic argument on behalf of the same position. 
I. AFFIRMATIVE ACTION, RULES, AND TRANSPARENCY 
Begin with affirmative action. We can imagine an admissions program that operates on the basis of a simple rule, as in the idea that anyone with a certain score on the LSAT will be admitted, and anyone with a score below that level will be rejected. If affirmative action is to be introduced, we could imagine an equally simple rule-saying, for example, that African-American candidates will be admitted even if their LSAT scores are below the ordinary requirement so long as the scores are also above a specified level. We could also imagine an admissions program that operates on the basis of a complex rule. The University of Michigan undergraduate program is an example; it offered a range of factors listed in advance, with each being given a specified weight. Rules can certainly make reference to a large number of factors and, in that sense, incorporate a high degree of particularity. The identifying feature of a rule-bound system is not that it makes one or two features central, but that it involves full or nearly full specification, before the fact, of the factors that are relevant and also of the weight that will be assigned to them. 19 
Such a system contrasts with rule-free systems, which involve no such before-the-fact specification, and which require officials to specify, in individual cases, either the relevant factors or the weight to be given to each (or both). We could imagine an entirely open-ended admissions process, in which admissions officers are asked to identify the governing criteria as they see fit. Under such a process, individual officers could decide whether to consider, and how much to consider, LSAT scores, extracurricular activities, background, geography, essays, race, religion, point of view, musical tastes, and so forth; their exercise of discretion would be unmonitored and unconstrained. 
DUKE L.J. 557 ( legal commands as rules or standards). 
Alternatively, we could imagine a process that rested on factors that were specified but unweighted-saying, for example, that officers must consider academic achievement, extracurricular activities, race, geography, athletic achievement, familial connections to the institutions, and so forth. This process would be "holistic," not in the sense that admissions officers could consider such factors as they saw fit, but in the sense that it would not offer anything like a specification of the weight to be assigned to the different variables. The weight of each factor would be decided individually in particular cases. If admissions officers wanted to give great weight to extracurricular activities but little weight to familial connections, they would be permitted to do exactly that; the opposite preference would be acceptable as well as far as the institution is concerned. Indeed, a near-zero weight would seem to be permissible. Perhaps admissions officers could even give no weight to race, or some other factor, if that is what they wanted to do. 
The key point is that to the extent that institutions do not specify admissions criteria in advance, or permit officers to weigh those criteria in individual cases, they ensure that the content of the governing "law" will be made on the spot, in the process of assessing applications. This is the essential difference between a rule-bound and a rule-free system. Of course there is a continuum here, not a sharp dichotomy. We could imagine admissions systems that specify a great deal but still leave some discretion for on-the-spot decisions. We could imagine systems that specify little but do not leave admissions officers utterly unconstrained. 
A court could coherently say that the constitutional issue turns on the degree of the racial preference. Perhaps a constitutional distinction should be drawn between different sorts of rules: those that give excessive weight to race and those that do not. On this view, an admissions office may not accord "undue significance" to race, in a kind of analogue to the "undue burden" standard in the law of abortion. Undue significance would be found, for example, in a rule allowing all African-American applicants to be admitted simply by virtue of their skin color. At the same time, an admissions office might be allowed to grant one point, or a few more, to African-American applicants. In other words, there would be no logical difficulty with saying that some weight, but not too much, may be given to race. An "undue significance" standard would itself require a form of individuation at the level of judicial administration, but perhaps it could be specified either in advance or through application. 
But-and this is the key point-undue significance, in the form of excessive weight, is not Justice O'Connor's concern. Rigidity, not weight, is her objection. She does not contend that the University of Michigan accorded too many points to race-an objection that would also apply to rule-free systems if, in actual operation, they gave an extremely strong preference to African-American applicants. Justice O'Connor's complaint is about ruleness as such, in the form of an ex ante specification of the weight to be given to race. Hence the distinction between Program A and Program B is not that Program A gives more attention to race; it is that Program A is more rigid and imposes firm limits on individual discretion. 
But is this objection convincing? Consider the behavior of an admissions officer engaged in review of applications under Program B. So long as affirmative action is in place, Program B will operate, in a sense, as the functional equivalent of a point system; the only difference is that its essential characteristics, above all the weight given to race, are not disclosed or systematized. As Program B is administered, each admissions officer will inevitably be operating with her own informal point system, in the form of a rough sense of how much weight to give to the relevant factors. (If an officer's decisions were recorded, and if there were enough of them, it should be possible to discern, retrospectively at least, the number of "points" given to race in the average case.) Whether the implicit point system will be constant across officers, or across applicants, is anyone's guess. We can go further. Program B can be drawn up so as to require each officer to give some (unspecified) weight to race, or so as to permit officers to give some (unspecified) weight to race if they choose. In this sense, race may or may not matter at all, depending on the judgment of each officer. 
The key point is that under Program B, it is unlikely that any particular officer will be able to give constant, rather than fluctuating, weight to race. It is even less likely that different officers will use the same system, in the sense that they will allocate the same informal points, or weight, to race. The resulting criteria will likely be highly variable across applicants, and they will not be transparent to anyone. It follows that as compared to Program A, Program B sacrifices three important values: predictability, transparency, and equal treatment. It does so while also imposing significant decisionmaking burdens on individual officers, who have to decide how much weight to give to race in individual cases. 
Program B is that the latter is fairer, less mechanical, and more accurate, precisely because it is so highly individualized. In fact Justice O'Connor's opinion in Grutterseems to be based on a judgment to precisely this effect. 20 But the appearance is a kind of optical illusion. Program B also involves something akin to a point system, in the sense that affirmative value is placed on race; the difference is that points are not formally assigned, and hence variability will likely arise from applicant to applicant, with a corresponding lack of transparency and equal treatment. Different African-Americans will be treated differently, as some obtain larger racial bonuses than others. At first glance, it is not at all clear why that procedure should be constitutionally individualized consideration in the context of a race-conscious admissions program is paramount."). preferred. 
Is anything at all gained with Program B? The most obvious point is that under Program B, there will be a greater lack of knowledge, in individual cases, about what role, if any, race played in an admissions decision. For this reason, the lack of transparency may itself be a benefit; perhaps there are expressive reasons for it. With Program B, a successful applicant does not know that he was affected by being the beneficiary of affirmative action. Such an applicant can think, plausibly, that affirmative action played no role in his case. Perhaps Program A imposes a kind of general stigma that Program B does not, simply because under Program A every African-American knows that a certain number of points were awarded (and every white applicant knows that the same points were not awarded). Under Program B, perhaps applicants can think that their own individual characteristics may have been all that mattered, with race playing little or no role. 
But it is reasonable to wonder whether this possibility, a matter of appearance and psychology rather than reality, really distinguishes the two programs, and it is reasonable to doubt that it should make a constitutional difference. Under Program A, it remains possible that African-American applicants, like athletes and children of alumni, did not really "need" the points-that race played no role in their admission. Along this dimension, there is no evident difference between the two programs. 2 1 At least it is not clear that beneficiaries of Program A feel more stigmatized, or are more stigmatized, than beneficiaries of Program B. And even if this is so, it is not clear that Program A is constitutionallyinferior for that reason. Stigmas of various kinds are imposed by many public decisions, such as the decision to admit a child of an alumnus or an especially good football player, and stigmas are not unconstitutional because of their status as such. In any case, Program A, far more than Program B, is likely to yield equal treatment. Why is the latter superior to the former, under a constitutional provision designed to ensure the equal protection of the laws? 
Any judgment on that point requires an interpretation of the Equal Protection Clause, which is not my topic here. Perhaps Justice O'Connor believes that expressive values lie at the heart of the Clause, that Program A is far inferior to Program B on expressive grounds, and that the constitutional issue must turn on that distinction. But at the very least, we should be able to see that it is not simple to explain why Justice O'Connor concluded that Program B is constitutionally acceptable, whereas Program A is not. To be sure, a holistic, particularistic program is less transparent and less predictable than officers could decide, in individual cases, to give no weight to race-if applicants could believe in the possibility that race played no role at all. But it is unclear that this possibility should make a constitutional difference, and in most "holistic" admissions programs, it is generally agreed that race plays a positive role for African-American applicants. one that uses points, but is that a constitutional advantage? 
My discussion of the example is meant not only to indicate a difficulty with an important area of constitutional law, but also to help to identify a series of problems with particularistic judgments. They might well lead to inequality. They impose burdens on subsequent decisionmakers, in a way that might lead to errors and arbitrariness. They increase unpredictability. They reduce transparency. They might increase the number and size of mistakes at the same time that they increase the costs of decisions. These problems beset not only case-by-case admissions decisions, but also what Justice O'Connor frequently prefers: case-by-case rulings from the Supreme Court. 
For the last generation, Justice O'Connor has been the Court's leading minimalist, in a way that has left a large impact on American law. Her minimalism embodies an interest in small steps along two distinct dimensions. 22 First, she favors rulings that are narrow rather than wide. Narrow rulings do not venture far beyond the problem at hand; they attempt to focus on the particulars of the dispute before the Court. When presented with a choice between narrow and wide rulings, Justice O'Connor often opts for the former.23 To be sure, the difference between narrowness and width is one of degree rather than kind; no one favors rulings that are limited to people with the same birthdays as those of the litigants before the Court. But among the reasonable alternatives, Justice O'Connor shows a frequent preference for the narrower options. 
Justice O'Connor also seeks rulings that are shallow rather than deep. Shallow rulings attempt to produce outcomes and rationales on which diverse people can agree, notwithstanding their disagreement on fundamental issues. For example, there are many disputes about the underlying purpose of the free speech guarantee: does the guarantee aim to protect democratic selfgovernment, the marketplace of ideas, or individual autonomy? 24 Minimalists hope not to resolve these disputes.25 They seek judgments and rulings that can attract support from people who are committed to one or another of these foundational understan26diTnghse, inotrerweshtoinareshualnlsouwreneassboruatistehse dfiostuinndctaitvioenisssuoefst,h2e7 free principle. and Isspheaelclhbe exploring narrowness rather than shallowness here. 
The discussion of affirmative action does not demonstrate that minimalism is a mistake or that Justice O'Connor has generally been wrong in seeking narrow rulings and in expressing caution about judgments that extend far beyond the facts of particular cases. To understand the uses and limits of minimalism, it is necessary to broaden the viewscreen, venturing well beyond any particular context. Consider, for example, the following propositions: " The constitutional status of sex segregation in education cannot be settled by rule. Often segregated institutions will be invalid, but it remains possible that they can be adequately justified.28 " The President is required to provide some kind of procedure before detaining suspected terrorists, but the extent of the required procedure should be elaborated in particular cases, not through firm rules laid 29 down in advance. " Commercial advertising is often entitled to protection under the First Amendment, but the protection is far from absolute; the extent of protection, in individual cases, depends on a balancing test.30 " Under the Equal Protection Clause, most classifications face mere rationality review. But for some classifications, rationality review is bmuotrneosteaalrlcfhoirnmg,s ionfadwiscaryimthiantatwioilnl odnootmhe (bamasoisngofostehxeuramloeariseunrteast)iosno. m31e " In the context of statewide recounts held under a single judge, states must develop adequate standards to constrain the exercise of discretion 1733 (1993) (discussing possibility of reaching agreements on practices or decisions amidst disagreements on large theoretical issues). 

statesupported university's policy of denying otherwise qualified males the right to enroll for credit in its nursing school violated the Equal Protection Clause); see also United States v. Virginia, 518 U.S. 515 (1996) (holding that the Commonwealth of Virginia's exclusion of women from a citizen-soldier program at a state military college violated the Equal Protection Clause because Virginia did not show an exceedingly persuasive justification, and the remedial program offered by the Commonwealth was not sufficiently comparable in substance to survive equal protection evaluation). 
by those who are counting votes. 32 But this requirement may not be applicable in other settings. 33 " Restrictions on the right to choose abortion should be evaluated not through the rigid trimester system of Roe v. Wade, but through a more flexible standard, one that forbids any "undue burden" on the right to 34 choose. " Public acknowledgements of the existence of God do not always offend the Establishment Clause; history and context matter a great 35 deal. 
In all of these cases, it would be possible to argue that case-by-case judgments are desirable, not for the obscure reasons that underlie Grutter, but on the ground that the Court lacks the information that would permit it to produce sensible rules. For example, a judge might not want to set out any rule to govern the constitutional status of commercial advertising, on the ground that the relevant situations are hard to foresee in advance, and any simple rule would be confounded by circumstances. Or a judge might believe that the procedural protection to be accorded to enemy combatants cannot easily be specified in an initial encounter with that problem; perhaps it is best to avoid rules and to rely instead on an incompletely specified decision, one that will be given content as particular cases arise. Perhaps public acknowledgements of God are acceptable if they do not amount to an effort to inculcate any particular set of religious convictions, but perhaps that test is itself only a crude gesture toward an appropriate set of governing principles, which must emerge from careful investigation of particular cases. can Ibfealjlutshtiisfieisdsoo,nthethne Jusastmicee gOr'oCuonndnsorth'satgesnuepraplorptresftearnednacredsforomveirnimrualeliss;m36 indeed, her own preference for minimalism is very close, analytically, to a preference for standards over rules. Importantly, the two preferences are not identical. A holding that is governed by a standard is, in a relevant respect, narrow, because the standard needs to be specified in particular cases; but a narrow decision need not be a standard at all. Such a decision may even be a rule, restricted to an unusual set of facts; it may endorse a rule and reject a standard of any kind. (Indeed, a holding is always a rule, at least insofar as it with the minimum procedures necessary to protect the fundamental right of each voter in the special instance of a statewide recount under the authority of a single state judicial officer."). 
counters). 
Webster v. Reprod. Health Servs., 492 U.S. 490, 522-31 (1989) (O'Connor, J., concurring). 
(O'Connor, J., concurring). 
binds the parties, and a rationale for a holding is often a standard.) What I am emphasizing is that by its very nature, a minimalist ruling leaves a great deal undecided, in a way that frees up future decisionmakers but also leaves them to some extent at sea. This is exactly the characteristic that distinguishes standards from rules. Insofar as Justice O'Connor prefers narrow rulings to wide ones, it is for the same kinds of reasons that lead some people to prefer standards to rules. 
That preference makes the most sense when it serves to reduce the number and magnitude of errors and when it serves as well to reduce the aggregate burdens of decision. 37 Suppose that the Supreme Court is attempting to resolve a difficult question involving, say, the constitutional status of segregation on the basis of sex. The Court might think that it lacks the information that would enable it to set out a sensible rule. It may believe that sex segregation is not acceptable when such a practice excludes women from a valuable educational opportunity, but that it is permissible to have sex-segregated sports teams and that the legitimacy of sex-segregated high school education presents a difficult question. The Court might believe that a simple rule-sex segregation yes or sex segregation no-would be outrun by reality. It might also believe that a complex rule, specifying the validity of segregation across diverse contexts, is too difficult to set out during an early encounter with the question. 
Points of this kind might well justify a preference for narrow rulings in many contexts. The problem-a general one for those who favor narrownessis that there is no reason to think that judges should systematically favor standards over rules. Whether standards are desirable, and whether narrowness makes sense, depends on whether the arguments that justify them apply in the particular case. There is no justification for a general presumption in favor of standards or minimalism. To the extent that Justice O'Connor has adopted such a general presumption, 38 she seems to have erred. 
It is possible to go further. In an influential essay, Justice Scalia made a general argument on behalf of rules over case-by-case judgments. 39 Justice Scalia marshals a range of considerations on behalf of rules-considerations that track those that argue against a "holistic" admissions process. Justice 
would require a sustained analysis of her votes and opinions. I contend only that in many prominent cases she has struck a minimalist chord and embraced narrowness. 
(1989). Scalia emphasizes that case-by-case judgments introduce unpredictability. 40 They threaten to deny both the reality and the appearance of equal treatment.4 1 They export decision costs to others-above all, to lower courts and litigants, who must give effective content to the law.4 2 At the same time, they leave judges free to exercise their discretion so as to favor their preferred causes, in a way that43also leaves liberty at risk when the Court faces intense political pressure. 
Rule-bound decisions have the dual virtues of binding judges, thus reducing the exercise of discretion, and also of stiffening the judicial spine when the stakes are highest.44 "The chances that frail men and women will stand up to their unpleasant duty are greatly increased if they can stand behind the solid shield of a firm, clear principle enunciated in earlier cases.' ' 5 This point, made in 1989, has evident power in the face of contemporary conflicts between individual rights and efforts to protect the nation against terror. Justice Scalia is linking a point about method with a point about substance. If judges embrace his method, they are more likely to respect the Constitution's substance when the going gets tough. 
Justice Scalia's argument is strengthened by the fact that the Supreme Court is not an ordinary tribunal, entrusted with deciding particular cases. It is not a trial court or even a court of appeals. One of its principal functions is to provide guidance for numerous other judges, public officials, and private actors potentially involved with the legal system.4 6 When focusing on a particular case, the Court may not readily "see" the large assortment of burdens that it imposes on others by virtue of its reticence. It may be much better for the Court to risk error by venturing more broadly than to leave others at sea about the law's requirements. If the Court does not say much about the obligation to provide procedures to enemy combatants, it may create terrible guessing games for the executive branch and for lower courts, in a way that imposes high costs and burdens. Alert to the Court's role in a hierarchical legal system, Justice Scalia contends that the preference for common law methods, focused on narrow rulings, should be reversed. 
In attacking case-by-case judgments, Justice Scalia is best taken as marshaling a set of considerations that support rules over standards. If we are interested in minimizing the costs of errors and the costs of decisions, we might well conclude that the Court should frequently choose width over narrowness. 
Supreme Court's Limited Resources for Judicial Review of Agency Action, 87 COLUM. L. REv. 1093 (1987). Wide rulings will impose decisional burdens on the Court, to be sure, but those burdens might well be lower than those associated with narrow rulings. Perhaps rules will be overinclusive, but they might well produce lower error costs, on balance, than those emerging from a regime of case-by-case decisions. 
But Justice Scalia does not make the implausible suggestion that the Court should always avoid case-by-case particularism. He acknowledges that a degree of particularism is sometimes not "avoidable. ' '4 7 His claim is only that when rules are "possible," the Court should rely on them. But why does Justice Scalia contend that rules should be chosen unless the rule of law "must... leave off'? 48 It would be more plausible to say that the choice between rules and particularity, or width and narrowness, should depend on which is best, all things considered. Rules and width should not be selected merely because they are possible if they are not optimal. The question whether they are optimal cannot itself be settled by rule. And if this point is right, then at the metalevel-in the choice between rules or width and particularity-we should follow Justice O'Connor's approach and be particularistic. No rule can possibly resolve the meta-question. It follows that a case-by-case inquiry, into the choice between rules and particularity or of the optimal degree of ruleness, is the correct way to make that choice. 
But this conclusion does not mean that Justice O'Connor has been right to opt, so much of the time, for particularity. On the contrary, it immediately raises the possibility that she has been wrong, simply because it is unlikely that a case-by-case inquiry would generally suggest that a case-by-case inquiry is best. But Justice Scalia has not adequately supported his apparent suggestion that there should be a kind of presumption in favor of wide rules, rather than a case-by-case inquiry into whether wide rules make sense in the particular 49 context. 
Is there anything that Justice Scalia might say at this point? Perhaps Justice Scalia believes that there is a real risk that a case-by-case inquiry at the metalevel will misfire-that if we are concerned with decision costs and error costs, we will do best with a presumption in favor of rules, or at least with constant alertness to their virtues. On this view, the problems that beset case-by-case decisions in individual cases also beset such decisions about whether to proceed via rules or instead through particularized decisions. The idea that courts should reject case-by-case judgments unless that course is not 
are always superior to standards, or vice-versa ...." MindGames, Inc. v. W. Pub. Co., 218 F.3d 652, 657 (7th Cir. 2000). "avoidable" might not produce the optimal results that would follow if courts could costlessly engage in accurate case-by-case inquiries. But because they cannot, they might do best with a presumption on behalf of rules. The presumption makes case-by-case decisions less burdensome (at the meta-level, as elsewhere), simply because it is a presumption. And perhaps the presumption pushes judges, and especially Justices, in the right direction, in a way that replicates what would emerge from a system in which case-by-case judgments could be made costlessly and accurately. 
But Justice Scalia does not attempt to defend his position in this way, and for excellent reason. He is best taken to have marshaled a series of considerations against case-by-case judgments, without having demonstrated that those considerations justify a rule or a presumption in favor of wide rulings from the Supreme Court. The weight, or force, of those considerations depends on the same factors that underlie the general choice between rules and standards. To summarize a complex story: 50 When the Court's decision must be applied by numerous actors, dealing with frequent or common situations, the argument for rule-bound decisions has great force. When there is reason to distrust those who would operate free from rules, there is a good argument for rules. When predictability is exceedingly important, case-by-case decisions impose high costs. When the potential rulemaker has the information to produce good rules, there is every reason to produce rules. 
But no general argument in favor of rules would make much sense, even at the meta-level. When the Court lacks relevant information, so that any rule will predictably misfire, rules are hazardous. When there are few decisions in the pertinent domain, the argument for rules is less insistent. When the potential rulemaker cannot be confident about any rule, rules may not be worthwhile. When subsequent decisionmakers can be trusted, because of their competence and lack of bias, the argument for avoiding rules is certainly strengthened. 
Perhaps Justice O'Connor's view is the converse of Justice Scalia's-and similarly vulnerable. Perhaps she believes that for the Supreme Court, it is frequently or typically the case that the case-by-case inquiry, at the meta-level, will produce a judgment in favor of case-by-case decisions. Her implicit judgment to that effect might reflect the application of a case-by-case metarule. But it is difficult to see how such a meta-rule might be defended, especially if we notice that minimalist decisions from the Court do not promote predictability and impose high decisional burdens on fallible actors at later stages. Too much of the time, narrow rulings are properly subject to Justice Scalia's critique. 
It is possible to identify specific domains in which the argument for rules is difficult to resist. Consider the question whether speed limits should involve standards or rules-a question easily resolved in favor of the latter in part supra note 19. because of the need for predictability and the sheer number of decisions that must be made. So too, there are domains in which the argument for standards is strong. For decisions involving parole, for example, a rigidly rule-bound system, forbidding a degree of individualized attention, would risk a high level of error. But for adjudication as a whole, even at the Supreme Court, no general conclusion makes much sense. 
It might be possible, however, to defend Justice O'Connor's approach in narrower terms, and the discussion thus far should enable us to see the form the defense might take. 
In my view, the most promising claim on her behalf involves a particularly important subset of cases on the Supreme Court's docket: the "frontiers" questions in constitutional law. By their very nature, these questions are unlikely to come up often and require answers about which the Court cannot always be confident. In these domains, the argument for wide rulings, offering large and hard-to-reverse steps, is substantially weakened. Two considerations are relevant here. First, predictability is likely to be less important, simply because the relevant questions arise infrequently. Second, the likelihood that a wide ruling will misfire is often high, simply because the Court lacks much familiarity with the context. 
Indeed, this latter point has been emphasized by Justice Breyer in the context of the use of new technologies to compromise privacy, where he insists that courts do not have the experience that would justify width. 5 ' Much of Justice O'Connor's jurisprudence might be taken to have generalized the point. Her largely minimalist opinion in Hamdi can be understood in exactly these terms; 52 in its first decisions involving conflicts between constitutional rights and the war on terror, the Supreme Court had good reason to proceed cautiously. The same point helps to justify the Court's minimalism in Bush v. Gore, where a most unusual controversy with enormous stakes might be taken as a good occasion for particularly small steps. 53 The same can be said in defense of Justice O'Connor's reluctance to embrace a broad ruling during the Court's first encounter with gang-loitering legislation. 54 
None of this means that minimalism is always appropriate in constitutional cases. When the area requires a high degree of predictability, and when the Court has had a great deal of experience with the area, width might well be CONSTITrrION 70-73 (2005). 

concurring). justified. The same conclusion follows if the Court, notwithstanding its lack of experience, has good reason for confidence in a wide ruling. The only point is that in many frontiers cases, the very arguments that justify standards will justify minimalism as well. In short, Justice O'Connor's practice of minimalism is best defended on the ground that it can be found in exactly the areas in which an assessment of the contest between rules and standards argues most powerfully in favor of the latter. 
III. DEMOCRACY 
Thus far my emphasis has been on error costs and decision costs-the conventional foundation for analysis of the choice between rules and standards. But there is another issue, and it has to do with the relationship between Justice O'Connor's preference for minimalism and democratic self-government. There are two different points here. 
First, the Court might choose a narrow ruling precisely because it seeks to retain room for democratic debate and experimentation. Justice Frankfurter's concurring opinion in the Steel Seizure Case offers the most elaborate discussion of the basic claim. 55 He emphasized that "[r]igorous adherence to the narrow scope of the judicial function" is especially important in constitutional cases when national security is at risk, notwithstanding the national "eagerness to settle-preferably forever-a specific problem on the basis of the broadest possible constitutional pronouncements .... "56 In his view, the Court's duty "lies in the opposite direction," through judgments that make it unnecessary to consider "delicate problems of power under the Constitution." 57 Thus, the Court has an obligation "to avoid putting fetters upon the future by needless pronouncements today." 5 8 Justice Frankfurter is naturally read to be emphasizing the costs of error, but his emphasis is also on "putting fetters" on those involved in democratic self-government. 
If the Court were able to invalidate a legally unacceptable decision in a way that nonetheless would maintain flexibility for other institutions, it might do so for that very reason, at least if it cannot be confident that a broader ruling is correct. 59 And indeed, Justice O'Connor's separate opinion in Morales, the gang-loitering case, emphasized the continued authority of state and local governments. 60 Narrow rulings permit a kind of continuing dialogue within the (Frankfurter, J., concurring). 

(contrasting the Court's institutional deficiencies with Congress's flexibility in setting terms for suspension of writ ofhabeas corpus). 
polity, and between the Court and other institutions, in a way that promotes learning.6 1 As between wide and narrow invalidations, the latter have significant advantages for just that reason. 
This, in fact, is a large part of Justice Breyer's claim in the context of privacy, 62 where narrowness ensures that the legislature will not be foreclosed by an inadequately informed decision from the Court. The same point emphatically holds amidst the war on terror. In Hamdi, the Court pointedly declined to say anything about the President's power as Commander in Chief, relying instead on statutory authorization. 63 At least where the issue has a great deal of novelty, and where the Court is unsure how to handle it, a great deal is to be gained by allowing the democratic process continuing room for experimentation. 
Second, some minimalist rulings are democracy-forcing, in the particular sense that they work to ensure that decisions are made by the democratically preferred institution of government. The most prominent example is the Avoidance Canon-the idea that intrusions on constitutionally sensitive interests must be authorized by Congress and may not be made by the executive alone. 64 Justice O'Connor has attempted to vindicate this idea in prominent cases.6 5 The simple idea is that when a reasonable constitutional objection is raised, the Court does best, if it possibly can, to construe the relevant statute so as to make it unnecessary to address that objection, and so as to force the national legislature to confront it directly and first. The Avoidance Canon is minimalist in the simple sense that it ensures narrower rulings than constitutional invalidations. But it also has the distinctive property of forcing national legislators to deal with the question squarely and unambiguously. 
The same general idea helps to explain Justice O'Connor's controversial opinion in the Brown & Williamson case. 66 There the Court struck down the FDA's assertion of regulatory jurisdiction over tobacco and tobacco products, notwithstanding the ambiguity of the statutory language and the ordinary rule that agencies are permitted to interpret ambiguous statutes as they see fit.67 Early on, Justice O'Connor's opinion suggests that "we must be guided to a degree by common sense as to the manner in which Congress is likely to delegate a policy decision of such economic and political magnitude to an concurring) 
such authority, however, because . . . Congress has in fact authorized Hamdi's detention ....). 
administrative agency. ' 68 This is a clear suggestion that for certain questions of great "economic and political magnitude," a legislative resolution, rather than a delegation to an agency, will be anticipated. And in closing, her opinion insists "that Congress could not have intended to delegate a decision of such economic and political significance to an agency in so cryptic a fashion." 69 We might well take this passage to suggest a kind of democracy-forcing minimalism, in the form of a suggestion that if the United States government is going to assert jurisdiction over the tobacco industry, it must be because of a clear judgment to that effect from Congress and not because of the construction of ambiguous terms by an administrative agency. 
A general effort to defend Justice O'Connor's approach, focusing on frontiers cases, might invoke these points as well. When minimalism is defensible in democratic terms, it is in precisely these areas, simply because it is there that the Supreme Court should be reluctant to foreclose democratic experimentation, and there too that the Court might evince a preference for judgments by the nation's lawmakers. At the very least, I suggest that ideas of this kind play a significant role in Justice O'Connor's work on the Court. 
CONCLUSION 
One of my central goals in this Article has been to raise some questions about minimalism-to suggest that any general preference for narrow, case-bycase rulings would be too crude and reflexive, and insufficiently attuned to the frequent advantages of width. In Justice O'Connor's opinions, the most vivid example of this problem can be found in the context of affirmative action, where she approves "holistic" admissions decisions but disapproves of point systems. The difficulty is that in key respects, holistic decisions involve the functional equivalent of a point system-but in a way that is not transparent, that eliminates predictability, and that almost certainly ensures unequal treatment of the similarly situated. The idea that the Constitution requires "holistic" judgment neglects the many benefits of clarity and width, not least from the standpoint of equality itself. 
None of this means that width is always preferable, or even that courts should adopt a presumption in its favor. The choice between narrow and wide rulings must itself be made on a case-by-case basis; no rule is adequate to the task. Where the Court's decision must be applied in many contexts, and when the issue frequently recurs, the argument for width may well be irresistible. But where the issue arises infrequently, and when the Court lacks the information that would enable it to produce a wide rule in which it has much confidence, the argument for narrowness is quite strong. 

In the context of the war on terror, there is good reason to avoid wide rules, simply because they are so likely to misfire. But in the context of sex discrimination, the Court has earned the right to create strong presumptions that are also wide. Ideas of this kind follow from a conventional inquiry into the costs of decisions and the costs of errors. They are also supported by reference to democratic considerations. I have emphasized that narrow rulings often leave democracy room to maneuver; an additional point is that some forms of minimalism turn out to be democracy-forcing. 
With respect to method, a sympathetic understanding of Justice O'Connor's legacy might therefore take the following form. In the most difficult and sensitive cases, at the frontiers of constitutional law, the Court usually does best if it proceeds narrowly. It does so not because there is any general reason to prefer narrowness to width, but because there are identifiable settings in which width is likely to misfire, and in which it is best to preserve a large space for public dialogue and debate. Understood in these (narrow) terms, Justice O'Connor's preference for narrowness is likely to leave an enduring mark on constitutional law. 
Follow this and additional works at: https://chicagounbound.uchicago.edu/law_and_economics Recommended Citation 
RANKING LAW SCHOOLS: A MARKET TEST?  
Cass R. Sunstein                
T H E   L A W   S C H O O L   T H E   U N I V E R S I T Y   O F   C H I C A G O   
  
July 2005  This paper can be downloaded without charge at the John M. Olin Program in Law and Economics  Working Paper Series: http://www.law.uchicago.edu/Lawecon/index.html and  
The Social Science Research Network Electronic Paper Collection:  
http://ssrn.com/abstract_id=703282   Preliminary draft 11/18/04 All rights reserved 
Cass R. Sunstein* 
Instead of ranking law schools through statistical aggregations of expert judgments, or by combining a list of heterogeneous factors, it would be possible to rely on a market test, simply by examining student choices. This tournament-type approach would have the large advantage of relying on the widely dispersed information that students actually have; it would also reduce reliance on factors that can be manipulated (and whose manipulation does no good other than to increase rankings). On the other hand, a market test has several problems as a measure of law school quality, partly because cognitive biases and social influences may lead some or many students to make bad choices and thus to participate in the production of inaccurate rankings. 
I. 
Law school rankings are notoriously contentious. Many such rankings are obtained in the following way. Much of the analysis comes from the judgments of a group of experts, who are asked to assess law school quality, in general and along particular dimensions. For example, forty percent of the US News and World Report rankings are generated in this way. Academic reputation, measured by responses of law school deans and faculty, counts for 25% of the total score; reputation among lawyers and judges counts for 15% of the total score. In Brian Leiter’s rankings, faculty quality is measured solely through aggregating surveys, in a way that makes expert judgments crucial.1 
This method is controversial, but there is a great deal of logic to relying on statistical aggregations of expert opinions. When experts are available, it makes sense to obtain a statistical answer from them, rather than to select one or a few. If experts are likely to have relevant information, a statistical group of experts should have a significant advantage over individual experts, just as a statistical group of ordinary people has a * Karl N. Llewellyn Distinguished Service Professor, Law School and Department of Political Science, University of Chicago. 1 See http://www.utexas.edu/law/faculty/bleiter/rankings/rankings03.html significant advantage over ordinary individuals.2 If the goal is to rank baseball players, or football teams, a large set of informed people might well be asked, instead of relying on a small set. 
This claim is hard to test in the contest of rankings, simply because we lack objective evidence that would permit us to test the question whether the experts got it right or wrong. But where the claim can be tested, that is in the domain of predictive tests, a great deal of evidence supports the use of statistical aggregations of expert judgments.3 In a series of thirty comparisons, statistical groups of experts had significantly fewer errors than individual experts on forecasting tasks involving such diverse issues as company earnings, cattle and chicken prices, real and nominal GNP, survival of patients, and housing starts.4 Statistical groups of experts significantly outperformed individual experts in predicting the annual earnings of firms; changes in the American economy; and annual peek rainfall runoff in eight different countries.5 
Or consider political polling, where it has become standard practice to combine a set of poll results and to rely on the mean or median, rather than to select one or two.6 The most sophisticated treatment here involves “Polly,” a program designed to predict the 2004 election.7 Polly make her predictions after combining a large set of sources: polls, computer models, expert panels, and information markets. In eight months, she continued to predict that President Bush would receive at least 50% of the vote; her final forecast was that President Bush would receive 51.5% of the vote, a number that was stunningly close to precisely right.8 2 See James Suriewicki, The Wisdom of Crowds (2004). 3 See J. Scott Armstrong, Combining Forecasts, in Principles of Forecasting 416 (2001). 4 Id. at 428. 5 Id. at 430-31. 6 See, e.g., Sam Wang, Electoral College Meta-Analysis (2004), available at http://synapse.princeton.edu/~sam/pollcalc.html 7 See http://pollyvote.com 8 Id.: “POLLY WAS RIGHT! Who would win in November - George W. Bush or John F. Kerry? This question consumed Polly since March, when her page was launched. She heard from many sources, including 268 polls, 10 forecasting models, three surveys over as many months of a select panel of American politics experts, and the Iowa Electronic Markets (IEM). All of it she dutifully reported in her tables and graphs. As well as passing along what she found, Polly performed a simple computation, something she calls “pollynizing.” Averaging across methods, which Armstrong (2001) shows reduces forecasting error, Polly calculated the Pollyvote, the share of the two-party vote (that is, omitting third-parties) that Bush was predicted to win. Once or twice a week our parrot would post the latest value of the Pollyvote on this page and, invariably, even as Bush’s standing in the polls sank in July (see Pollygraph 2) or plummeted in the IEM in August (see Pollygraph 3), this variable showed Bush would win on election day. Not once during the past eight months did the value of the Pollyvote dip below 50 percent (see Pollygraph 4). On the morning of November 2, after pollynizing the latest information gleaned from her sources (the polls, the models, the experts’ panel, and the IEM), Polly 
If groups of experts do well for predictive judgments, they are likely to do well for judgments involving ranking as well. But no one doubts that errors and perhaps biases might well play a role in any set of assessments, including those by groups. If individual experts are likely to make mistakes, then statistical aggregations of expert judgments will be mistaken too.9 Suppose, for example, that a law school has been exceedingly good for many decades, but that its quality has sharply diminished in the last years. Some experts might fall to update their judgments; hearing the name of the school, they might rank it exceedingly high. Perhaps it would be said that such experts are not so expert. And if we define expertise to require people to have a great deal of information on the issue at hand, experts are unlikely to blunder by definition. But law school deans, law school faculty, and (even more) practicing lawyers might not know all that they need to know to rank schools correctly. 
Note in this connection that the US News and World Report also attends to other factors that do not depend at all on expert judgments. These include median LSAT (12.5% of overall score), employment rate nine months after graduation (12%), median GPA (10%), average per capita expenditures (11%, divided into 9.5% for instruction, library, and support services, and 1.5% for all else, such as utilities and financial aid), employment rate after graduation (6%), student-teacher ratio (3%), acceptance rate for students (2.5%), bar pass rate (2%), and number of volumes in the library (.75%). But this is a diverse list of factors, to say the least, and there is no assurance that the aggregation produces sense rather than nonsense. How can we be assured that a combination of expert judgments with factors of this kind lead to a sensible ranking of institutions? 
Consider another method. Perhaps law schools should be ranked through a market test, one that relies on the choices of the people directly involved: Students who apply to law schools. Such students have every incentive to make good decisions about where to enroll. They have every incentive, moreover, to identify and to care about what factors are relevant. The number of volumes in the library, and the employment rate after graduation, are likely to receive the attention they deserve, at least if numerous student choices are aggregated. For most students, the choice of law school counts among the most important decisions of their lives. They are most unlikely to decide cavalierly or foolishly. On the contrary, they are likely to acquire a great deal of information about the various options. And if students pick Yale over Harvard, or Stanford over Berkeley, or New York University over the University of Southern California, or the University of Illinois over the University of Utah, then we have strong presumptive reason to believe that their choices are correct. issued her final forecast: President Bush would take 51.5 percent of the two-party vote. According to the following Friday's New York Times, with 99.8 percent of precincts reporting, this is exactly what he received. Polly was right all along.” 9 See Cass R. Sunstein, Group Judgments, NYU L Rev (forthcoming 2005). 
Here, then, is a simple proposal: To rank law schools, information should be obtained by student choices, head-to-head, among relevant schools. The results of the resulting “tournaments” would produce an overall ranking. 
Does this idea seem exotic, or quixotic? In fact Christopher Avery and his colleagues have carried out exactly this task for colleges and universities.10 Avery et al. are particularly concerned that many currently available measures of quality, such as admissions rate and matriculation rate, can be manipulated by schools that want to act strategically to improve their rankings. A particular problem here is that many schools would prefer not to have to manipulate these factors, but the current system of ranking strongly pressures them to do so. If schools do not engage in manipulation, but their competitors do, then they will lose students – and eventually much else as well. Avery and his coauthors contend that a “revealed preference ranking,” based on actual student choices, would provide important information without also distorting admissions processes.11 Their own ranking of undergraduate programs is both plausible and interesting, with a top fifteen of Harvard, Yale, Stanford, California Institute of Technology, MIT, Princeton, Brown, Columbia, Amherst, and Dartmoth, Wellesley, the University of Pennsylvania, Notre Dame, and Swarthmore. 
For law school, a ranking of this sort would have many advantages. Most modestly, it could be taken simply as a measure of what it reflects: student preferences. So taken, it might be a complement to existing rankings, offering information that many people might want to have. Somewhat more ambitiously, a revealed preference ranking might be used as an input into aggregate rankings, taking its place alongside other factors. Most ambitiously of all, a revealed preference ranking could be seen as the most reliable one of all – as a figure that, all by itself, tells people all or almost all that they need to know to rank schools. 
Consider in this connection Friedrich Hayek’s greatest contribution to social thought, that is, his emphasis on the diffusion of information within society, and on the great difficulty of aggregating that information through any mechanism other than the price signal.12 In Hayek’s view, market choices, based on the preferences and judgments of countless consumers, reflect more information than could possibly be obtained by any central planner, even one who is both well-motivated and expert. In the same vein, student choices might be thought to produce a kind of “price,” at least in the form of rankings that reflect those choices. Perhaps revealed preferences ranking have greater accuracy than would emerge from the judgments of a group of experts. (I will raise questions about this proposition shortly.) 10 See Christopher Avery et al., A Revealed Preference Ranking of U.S. Colleges and Universities, available on ssrn.com 11 Id. at 2. 12 See F.A. Hayek, The Use of Knowledge in Society, 35 Am Econ Rev 519 (1945). 
Whether or not this is so, a revealed preference ranking would eliminate or reduce the need to consult experts, with their potential biases and their personal hobbyhorses. It would also eliminate strategic behavior on the part of law schools -- in the form, for example, of efforts to increase applications simply in order to have an apparently more selective admissions rate. Casual empiricism suggests that this form of strategic behavior is common; and no one seems to like it. Perhaps best of all, and in good Hayekian fashion, a revealed preference ranking would take advantage of the extraordinary amount of information held by the numerous students who choose where to go. It should also prove able to incorporate new information as it emerges over time. If one school has significant gains in terms of faculty, and another shows significant losses, student judgments are likely to respond to that fact. 
The simplest point is that all by itself, a revealed preference ranking would provide important information to law students. When choosing among schools, students would probably care what other students choose, and for two different reasons. That information is intrinsically valuable; it provides a clue to which schools have the most desirable student. In addition, that information provides a good heuristic for what school is best. If School A is regularly chosen over School B, there is reason to believe that School A is, in fact, better than School B. 
What might be wrong with this idea? Might errors be expected? Might a revealed preference ranking increase, rather than reduce, the distortions contained in, and produced by, current ranking systems? 
preferences simply to provide a ranking of student choices, without believing that the ranking is a reliable heuristic for quality or anything else. Perhaps applicants and others would overweight the revealed preference ranking, treating it as decisive when it should be merely informative. Students might easily think: “I like the Northwestern University Law School; all things considered, I like it much more than New York University Law School. But a strong majority of students choose the latter over the former, and so my inclination to go to Northwestern must be odd or foolish.” Thoughts of this kind may or may not lead students in good directions; some of those who follow the decisions of most people will undoubtedly err by their own lights. On the other hand, many applicants will treat a revealed preference ranking for what it is, neither more nor less, and the mere risk of overweighting is not usually a reason to fail to provide relevant information. The optimistic scenario is that a revealed preference ranking will inform choices without distorting them. 
not merely for what it shows, but as the best available evidence of real rankings. If so, oddities might emerge, and they might give us reason to doubt the rankings that emerge from student choices. From the rankings offered by Avery et al., several anomalies are immediately apparent. Notre Dame is certainly a superb college, but should it really be ranked above Swarthmore, Cornell, the University of Chicago, and the University of Michigan? Georgia Tech is excellent, but is it really better than Wesleyan, Carnegie Mellon, and Johns Hopkins? I am hardly an expert on these questions, but there is reason for doubt. It is sensible to speculate that Notre Dame has a particularly enthusiastic applicant pool, intensely drawn to the institution because of its distinctive characteristics. The same is plausibly true of Georgia Tech. If those who apply to Notre Dame and Georgia Tech are especially likely to go there if admitted, even if also admitted at (say) Swarthmore and Johns Hopkins, their choices may tell us little or nothing about relative quality. What choices may be reflecting instead is a kind of intense loyalty on the part of applicants to particular institutions. 
The same problem might materialize for law schools. Consider a few possibilities. It is conceivable that the University of Chicago Law School would be ranked below Stanford Law School, purely in terms of student choices. (I don’t have the data, but this result would not be a shocker.) Nonetheless I am firmly convinced that as wonderful as Stanford Law School is, the University of Chicago Law School is even more wonderful. I am undoubtedly biased on this count. But I predict, with some confidence. that the following law schools would be “underranked” by the market process I am describing: the University of Southern California Law School, the University of San Diego Law School, George Mason Law School, and Chicago-Kent Law School. The first of these has long had a first-rate faculty, and an excellent culture, but it probably does not draw students in a way that is quite commensurate with its overall quality. The University of San Diego Law School is known by insiders to have an extremely strong faculty, but outsiders may not have similar knowledge. George Mason and Chicago-Kent have become extremely good more recently, and I speculate that their overall quality would not show up in student choices. Why not? What is going on here? 
colleges, it bears on the particularly high rankings of Notre Dame and Georgia Tech as well). Law schools are not only giving students an education. They are also giving students at least two other things: an experience and a credential. Students might prefer a school that is slightly worse on educational grounds if it provides a much better experience. For example, schools in lovely locations might be preferred on that ground. But the credential point is even more important. For law schools as for colleges, the credential – the “signal” given to relevant others by the degree -- will matter a great deal. When choosing among institutions, students are selecting signals, not simply educations. For many choosers, the signal may loom largest of all. It follows that that students might prefer schools that are slightly worse, on educational grounds, if they provide a significantly better credential. If students choose Notre Dame over Swarthmore, it may be because Notre Dame admittees believe that relevant people, in their community, will be enthusiastic about a Notre Dame degree, and less so about a Swarthmore degree. If the University of Southern California Law School, the University of San Diego Law School, George Mason, and Chicago-Kent do worse than their actual educational quality suggests, the “credential factor” might be one reason. 
For those who emphasize revealed preferences, another complication involves tuition and scholarships (and cost-of-living as well). In deciding where to attend law school, most students engage in an informal cost-benefit analysis; and for this reason, education, experience, and credential are not all that matter. If the costs of attending one school are particularly low, they might attend that school even if the benefits are relatively low as well. For some students, a tuition-free degree from Boston College might be preferable to a fully-funded degree from Stanford. Or suppose that a student is admitted to the University of Utah but also to the University of Virginia; suppose too that the costs of the former are far lower than the costs of the latter. A choice in favor of Utah does not reflect a judgment about the quality of the two schools. 
It follows that we should expect some distortion, from the revealed preference ranking, in the direction of those schools that are most able to give their admittees an economic incentive to come. This point provides a reason to discount the resulting rankings if they are taken as a heuristic for overall quality. It even provides a reason to discount the rankings if they are taken as fully informative evidence, for individual applicants, about relevant student choices. They are choices, to be sure, but for an applicant who has a distinctive pattern of economic inducements, the choices of the average or median student may not be decisive or even terribly informative. 
on revealed preferences. In the law school context, student choices might well be an artifact, at least in part, of other ranking systems – for example, the ranking from the US News and World Report. If so, then the information provided by student choices is endogenous to ranking systems that are, by hypothesis, biased or not entirely reliable. Suppose, for example, that students make their choices partly on the basis of rankings that are produced partly as a result of manipulative, ranking-focused admissions decisions. If so, revealed preferences will not do what Avery et al. hope, which is to correct the effects of those decisions. On the contrary, they will reflect their influence. What – it might be asked – if the gain of relying on a market process that is itself infected by artificial rankings systems? Isn’t there a kind of vicious circle here? 
The objection is valid, but its importance should not be overstated. Students have access to the standard rankings, to be sure, but they have access to much other information as well. At least in principle, their decisions should incorporate relevant considerations that the official rankings omit. Many students do not go by the standard rankings. Their own knowledge about quality, and about “fit,” can lead them to choose schools that are ranked somewhat lower than others to which they have been admitted. Of course more empirical work would be helpful here; if student choices are tightly correlated with US News and World Report rankings, then those choices would seem to be less informative. But if there is some divergence, a revealed preference ranking would provide information that is, in an important respect, more valuable than the rankings that are now used. And if there is no divergence, we have reason to substitute revealed preference rankings for existing ones, with the hope that over time, student choices will not be distorted by ranking systems that we have reason to distrust. 
Of course it would be possible to object that revealed preference rankings would become a self-fulfilling prophecy, or a kind of closed circle, unable to incorporate new information. If students emphasize the choices of previous students, then how can law schools disrupt previous patterns? The answer is the same as in any market. In one year, a certain brand of sneakers is popular, and it is popular partly because it is popular; but if better sneakers appear on the market, they will eventually find their customers. Some law schools are more popular now than they were twenty years ago, and some are less so. Revealed preference rankings would not be frozen in time. 
interesting problem casts doubt on the Hayek-inspired claim that when taken in the aggregate , student choices properly pool information in a way that results in something like an efficient market. This claim is closely connected with current arguments over the efficiency of the stock market.13 For a long period, it has been argued that stock markets are “efficient,” in the sense that multiple investors produce prices that are more or less accurate. Of course there is mounting evidence that people depart from rational actor models of human behavior.14 But in the face of such evidence, many people, still convinced of the efficient capital markets hypothesis, contend that even if individual investors are prone to err, market prices have a high degree of reliability as a measure of value, at least when compared to other available measures. This contention seems implausible to many who suggest that cognitive biases and social influences can lead not merely individuals but market prices in the wrong direction.15 
Arguments of this kind might be enlisted by those who distrust revealed preferences as a measure of law school quality. The question is whether cognitive biases and social influences might lead both individual students and aggregate patterns in unfortunate directions. In this context, one bias is readily identifiable: There is reason to think that people use name recognition as a proxy, or heuristic, for quality.16 The “name recognition” heuristic surely works well most of the time, but like other heuristics, it can lead to severe and systematic errors. For example, Harvard, Notre Dame, and Duke University are undoubtedly helped to this heuristic, whereas the University of Southern California, Chicago-Kent, and George Mason are undoubtedly hurt by it. If the name recognition heuristic is widely shared, then biased results should be seen at the systemic level, and not just in some individual decisions. Of course this heuristic will not produce errors if students care about name recognition as such – if, for example, signaling is what most motivates them. But if educational quality is what most interests students, then name recognition will produce some real blunders. 
Social influences can aggravate the problem. Law school applicants do not make choices independently; they are much affected by the influences of their peers. One result 13 See Andrei Shleifer, Inefficient Markets (2002). 14 See, e.g., Richard Thaler, Quasi-Rational Economics (1993). 15 See id.; Robert Shiller, Irrational Exuberance (2001). 16 See the treatment of the “recognition heuristic” in Gerd Gigerenzer & Peter Todd, Simple heuristics that make us smart (1999). can be cascade effects, in which a number of students favor (say) New York University, and reject (say) Michigan, not because they are relying on their own independent information, but because they are responding to the signals given by the choices of other students.17 When cascades are at work, participants amplify the very signals by which they are influenced. One difficulty is that the cascade may seem far more informative than it actually is. If many students in one area are choosing NYU over Michigan, the reason may lie in the decisions of a few “early movers,” who started the cascade; other choosers may be adding no new information but simply responding to the early signal. Undoubtedly some law schools benefit from cascade effects, whereas others suffer from them. Sometimes a small “shock” can lead a large numbers of students in the direction of one choice rather than another, and not for especially good reasons. 
With respect to educational institutions, there can be fads and fashions as elsewhere. I would predict, for example, that one or another American law school can become extremely popular in a particular country (Israel, Australia, Taiwan) not because it is better than imaginable alternatives, but because it has benefited from a cascade. Within the United States, similar processes can occur, with students in some regions showing otherwise inexplicable enthusiasm for one or a few colleges and law schools. Because it is so easy to obtain relevant information, especially bad cascades are unlikely to persist. But when name recognition interacts with peer influences, revealed preferences can be “locked” into persistent patterns that do not perfectly correlate with law school quality.18 
A related problem involves the process of group polarization,19 by which likeminded people, engaged in deliberation with one another, typically end up thinking a more extreme version of what they thought before they began to talk. When group members are inclined in a certain direction, arguments that go in that direction are likely to be repeated most often, and hence to be most persuasive. It is easy to see how this process might affect student choices. Suppose that students talk with one another about their decisions; suppose too that there is a predisposition, among those able to make the choice, to rank Fordham Law School relatively low, and to rank Cardozo Law School relatively high. Arguments in favor of Cardozo will be heard more often; and peer influences will strongly favor Cardozo. Participants in a group discussion will be less likely to select Fordham if group members think that Cardozo is better, partly because of the arguments offered within the group, partly because of simple peer pressure. In sum, internal deliberations will aggravate the initial tendency not because of sensible aggregation of information, but instead through the dynamics of group interaction.20 Inferior judgments are a likely result.21 17 See Cass R. Sunstein, Why Societies Need Dissent (2003). 18 See the analogous processes discussed in Timur Kuran and Cass R. Sunstein, Availability Cascades and Risk Regulation, 51 Stan. L. Rev. 683 (1999). 19 See id. 20 See Sunstein, Group Judgments, supra note. 21 See the treatment of “hidden profiles” in Garold Stasser and William Titus, Hidden Profiles: A Brief History, 14 Psych Inquiry 304 (2003). 
If cognitive biases are interacting with social processes, large blunders should be expected.22 If, for example, name recognition is distorting judgments, group interactions should be expected to amplify, rather than to reduce, the distortion. For polarization as for cascade effects, the availability of relevant information should provide at least a partial corrective. Most students are unlikely to select a plainly inferior school, unless that school can offer inducements that make it a reasonable choice. But undoubtedly many students are affected by processes of this kind, in a way that raises legitimate doubts about any effort to rank law school by reference to market processes. 
These points should be sufficient to show that the revealed preferences of law school students are a highly imperfect proxy for law school quality. But those preferences are of considerable interest in themselves. If students generally choose the University of Illinois over the University of Iowa, that fact is relevant for other students to consider – partly because it is good to go to schools chosen by the strongest students, and partly because student choices convey relevant information. The choices of students also have some advantages over alternative rankings, such as those that stress faculty quality or a heterogeneous range of variables. Students are in a good position to give the various factors the weight that they deserve. A revealed preference ranking also reduces the incentive to manipulate relevant variables, not to improve quality, but to obtain a better ranking. 
I have not suggested, and do not believe, that revealed preference rankings would give a fully accurate account of law school quality, or even that they would be superior to real and imagined alternative ranking systems. But I do believe that they would provide valuable information and might well provide a helpful complement to other rankings systems. 22 See id.; William P. Bottom et al., Propagation of Individual Bias Through Group Judgment: Error in the Treatment of Asymmetrically Informative Signals, 25 J Risk and Uncertainty 147 (2002).                                                                 Readers with comments should address them to: Professor Cass Sunstein University of Chicago Law School 1111 East 60th Street Chicago, IL 60637 
csunstei@uchicago.edu    1.  38.  39.  40.  41.  
Follow this and additional works at: https://chicagounbound.uchicago.edu/law_and_economics Part of the Law Commons Recommended Citation 
The Availability Heuristic, Intuitive Cost‐Benefit Analysis,  and Climate Change  
Cass R. Sunstein  
T H E   L A W   S C H O O L   T H E   U N I V E R S I T Y  O F   C H I C A G O   
  November 2005  
  
This paper can be downloaded without charge at:  The Chicago Working Paper Series Index: http://www.law.uchicago.edu/Lawecon/index.html  and at the Social Science Research Network Electronic Paper Collection:  
 http://ssrn.com/abstract_id=844444   
The Availability Heuristic, Intuitive Cost-Benefit Analysis, and Climate Change 
773-702-9498 csunstei@uchicago.edu 
Abstract. Because risks are on all sides of social situations, it is not possible to be “precautionary” in general. The availability heuristic ensures that some risks stand out as particularly salient, whatever their actual magnitude. Taken together with intuitive costbenefit balancing, the availability heuristic helps to explain differences across groups, cultures, and even nations in the assessment of precautions to reduce the risks associated with climate change. There are complex links among availability, social processes for the spreading of information, and predispositions. If the United States is to take a stronger stand against climate change, it is likely to be a result of available incidents that seem to show that climate change produces serious and tangible harm. 
“Many Germans believe that drinking water after eating cherries is deadly; they also believe that putting ice in soft drinks is unhealthy. The English, however, rather enjoy a cold drink of water after some cherries; and Americans love icy refreshments” 
safety and environmental regulation in Europe has been a series of regulatory failures and crises that placed new regulatory issues on the political agenda and pressured policy makers to adopt more risk averse or precautionary policies. . . . The regulatory failure associated with BSE significantly affected the attitude of the European public toward GM foods. . . . Consumer and environmental regulations are likely to become more innovative, comprehensive and risk averse as a response to a widespread public perception of regulatory failures” 
It has become standard to say that with respect to risks, Europe and the United States can be distinguished along a single axis: Europe accepts the Precautionary Principle, and the United States does not. On this view, Europeans attempt to build a “margin of safety” into public decisions, taking care to protect citizens against risks that cannot be established with certainty. By contrast, Americans are reluctant to take precautions, requiring clear evidence of harm in order to justify regulation. These claims seem plausible in light of the fact that the United States appears comparatively unconcerned about the risks associated with climate change and genetic modification of food; in those contexts, Europeans favor precautions, whereas Americans seem to require something akin of proof of danger. To be sure, the matter is quite different in the context of threats to national security. For the war in Iraq, the United States (and England) followed a kind of Precautionary Principle, whereas other nations (most notably France and Germany) wanted clearer proof of danger. But for most threats to safety and health, and for climate change in particular, many people believe that Europe is precautionary and the United States is not. 
and Rogers, 2002) precautionary than Americans. As an empirical matter, neither is “more precautionary.” Europeans are not more averse to risks than Americans. They are more averse to particular risks, perhaps most prominently the risks associated with climate change; but Americans have their own preoccupations as well. No nation can, even in principle, commit itself to precaution as such with the Precautionary Principle, at least in its strongest forms, is that it is incoherent. It purports to give guidance, but it fails to do so, because it condemns the very steps that it requires. The reason is simple: Precautions always give rise to risks of their own. 
be based on a form of intuitive cost-benefit balancing. If the costs of precautions are high, they are less likely to be appealing; so too if the benefit are low. This point applies to climate change as to all other problems, and it helps to explain the massive differences between the United States and Europe with respect to that topic. In addition, the availability heuristic is often the source of people’s fears about certain risks and Renn, 2000) then people will have a heightened fear of the risk in question. If people in one nation fear the risks associated with climate change, and people in another nation fear the risks associated with terrorism, the availability heuristic is likely to be the reason. But this point misses some complexities, about intuitive cost-benefit balancing, social influences and cultural predispositions; I shall turn to these in due course. The availability heuristic does not operate in a social or cultural vacuum. 
In short, I suggest that cross-cultural differences in both risk perception and in precautions are produced, in large part, by availability, which operates in the context of social influences and intuitive attention to both costs and benefits. In the context of climate change, many Americans believe that far more would be lost than gained by extensive precautions; in Europe, the opposite is true. It is important, for example, that the risks associated with climate change are not salient to most Americans; it is important as well that efforts to control greenhouse gases would impose unusually high burdens on the United States. These points bear directly on cross-cultural differences with respect to climate change. If the United States will ultimately show more concern about the risks associated with climate change, it is likely to be a product of a shift in intuitive costbenefit balancing—with available incidents, apparently linking climate change to serious harm, playing a large role. 
are not “more precautionary” than the United States. Simply as a logical matter, societies, like individuals, cannot be highly precautionary with respect to all risks. Each society and each person must select certain risks for special attention. In these respects, the selectivity of precautions is not merely an empirical fact; it is a conceptual inevitability. Comparing Europe to the United States, empirically. In the early twenty-first century, for example, the United States appears to 1 Undoubtedly a great deal can be learned from use of the psychometric paradigm, stressed in Rohrmann and Renn (2000, p. 17-18). I stress the availability heuristic here because of its comparative simplicity, but the heuristic interacts in complex ways with psychometrics and with culture; I try at least to scratch some of the surfaces. take a highly precautionary approach to the risks associated with abandoned hazardous waste dumps and terrorism, but not to take a highly precautionary approach to the risks associated with climate change, indoor air pollution, poverty, poor diet, and obesity. It would be most valuable to attempt to see which nations are especially precautionary with respect to which risks, and also to explore changes over time. 
Agency goes so far as to conclude that there are two separate camps in the industrialized world: “precaution countries” (Germany, Sweden, the Netherlands, and the United States) and “protection countries” (Japan, France, and the United Kingdom) p. 448) large to permit categorizations of this kind. The most general point is that no nation is precautionary in general and costly precautions are inevitably taken against only those hazards that seem especially salient or insistent. The problem with the idea of precaution, and any general Precautionary Principle, is that it wrongly suggests that nations can and should adopt a general form of risk aversion 
I suggest that the Precautionary Principle becomes operational if and only if those who apply it wear blinders—only, that is, if they focus on some aspects of the regulatory situation but downplay or disregard others. But this suggestion simply raises an additional question: What accounts for the particular blinders that underlie applications of the Precautionary Principle? What people’s attention is selective, why is it selective in the way that it is? Part of the answer, I contend, lies in an understanding of behavioral economics and cognitive psychology, which provide important clues to cross-cultural differences in risk perception, in a way that much bears on social judgments about climate change. The availability heuristic is the place to start. 
people rely on certain heuristics, or rules of thumb, which serve to simplify their inquiry substitution,” in which people answer a hard question by substituting an easier one (Kahneman and Frederick, p. 53). Should we be fearful of climate change? When people use the availability heuristic, they assess the magnitude of risks by asking whether examples of harm can readily be brought to mind 1114) than if they cannot. The availability heuristic illuminates the operation of the Precautionary Principle, by showing why some hazards will be on-screen and why others will be neglected. The availability heuristic also tells us a great deal about differences in risk perceptions across groups, cultures, and even nations. 
numerous than a class of equal frequency whose instances are less retrievable” and Kahneman, 2002, p. 11) wellknown people of both sexes, and asking them whether the list contains more names of women or more names of men. In lists in which the men were especially famous, people thought that they were more names of men, whereas in lists in which the women were the more famous, people thought that there were more names of women Kahneman, 2002) 
that is familiar, like that associated with terrorism, will be seen as more serious than a risk that is less familiar, like that associated with sun-bathing. But salience is important as well. “For example, the impact of seeing a house burning on the subjective probability of such accidents is probably greater than the impact of reading about a fire in the local paper” and space in much risk-related behavior, including decisions to take precautions. Whether people will buy insurance for natural disasters is greatly affected by recent experiences on flood plains are far less likely to purchase insurance. In the aftermath of an earthquake, insurance for earthquakes rises sharply—but it declines steadily from that point, as vivid memories recede. Note that the use of the availability heuristic, in these contexts, is hardly irrational.2 Both insurance and precautionary measures can be 2 and usually effective,” but also that they “lead to systematic and predictable errors.” Gerd Gigerenzer, among others, has emphasized that some heuristics can work extremely well Gigerenzer, 2000) expensive, and what has happened before seems, much of the time, to be the best available guide to what will happen again. The problem is that the availability heuristic can lead to serious errors, in terms of both excessive fear and neglect. 
effects of ease of imagery on perceived judgments of risk study asked subjects to read about an illness (Hyposcenia-B) that “was becoming increasingly prevalent” on the local campus. In one condition, the symptoms were concrete and easy to imagine—involving muscle aches, low energy, and frequent severe headaches. In another condition, the symptoms were vague and hard to imagine, involving an inflamed liver, a malfunctioning nervous system, and a general sense of disorientation. Subjects in both conditions were asked to imagine a three-week period in which they had the disease and to write a detailed description of what they imagined. After doing so, subjects were asked to assess, on a ten-point scale, their likelihood of contracting the disease. The basic finding was that likelihood judgments were very different in the two conditions, with easily-imagined symptoms making people far more inclined to believe that they were likely to get the disease. 
3.2. Availability and cross-national risk perceptions in general. The availability heuristic helps to explains the operation of the Precautionary Principle and cross-national differences for a simple reason: Sometimes a certain risk, said to call for precautions, is cognitively available, whereas other risks, including those associated with regulation itself, are not. In many cases where the Precautionary Principle seems to offer guidance, the reason is that some of the relevant risks are available while others are barely visible. Differences across nations, in the perception of risks, have a great deal to do with the operation of the availability heuristic. I shall turn to climate change shortly; for the moment, let us explore the hypothesis more generally. 
Rohrmann, 2000) establish to be true. What is necessary, and what is lacking, is anything like comprehensive information about cross-cultural risk perceptions, allowing us to test the and biases. I do not mean to take a stand on the resulting debates. Even if many heuristics mostly work well in daily life, a sensible government can do much better than to rely on them. role of availability. And we shall shortly see some complexities that bear on the adequacy of the availability hypothesis. But for now, consider some supportive evidence: • Within the United States, public concern about risks usually does track changes in the actual fluctuations in those risks. But public concern outruns actual fluctuations in the important case of “panics,” bred by vivid illustrations that do not reflect changes in levels of danger certain points in the 1970s and 1980s, there were extreme leaps in concern about teenage suicides, herpes, illegitimacy, and AIDS—leaps that did not correspond to changes in the size of the problem. Availability, produced by “a particularly vivid case or new finding that receives considerable media attention,” played a major role in those leaps in public concern 172) some parents who refused to allow their children to attend classes having students with signs of herpes. • What accounts for people’s perception of their risk of being infected with HIV? Why are some people and some groups largely unconcerned about that risk, while other people and groups are highly focused on with it? A study of rural Kenya and Malawi suggests that availability plays a critical role authors find that risk perception is a product of discussions that “are often provoked by observing or hearing about an illness or death” 2003, p. 10) be prevented,” but they are unclear “about the advisability and effectiveness of the changes in sexual behavior that are recommended by experts” 2003, p. 18) social networks, with pronounced changes in belief and behavior resulting from interactions with other people expressing a high level of concern. The effects of social networks are thus asymmetric, with substantial effects from having “at least one network partner who is perceived to have a great deal of concern about AIDS.” The authors do not refer explicitly to the availability heuristic, but their findings are compatible with the suggestion that with respect to AIDS, risk perceptions are produced by availability. • What accounts for the recent rise of precautionary thinking in Europe? Why have certain environmental and health risk achieved so much salience in England, France, and the European Union generally? A comprehensive study suggests that a few readily available incidents played a large role “wave of crises” involving food safety, above all mad cow disease, led to the deaths of about one hundred people, with especially large effects on public attitudes the “regulatory failure associated with BSE significantly affected the attitude of the European public toward GM foods” “scandal was the apparent failure of French government officials and doctors to protect haemopholiacs from blood contaminated with AIDS” virus, in a way that had large repercussions for public opinion in France The conclusion is that differences between European and American policies are not a product of deep-rooted cultural differences, but instead have a great deal to do with “widespread public perception of regulatory failures,” often based on particular, vivid, and widely salient events 
5.3. Availability, climate change, catastrophe, and long-term risks. These points have particular implications for risks from climate change that, by their very nature, are not likely to cause serious harms in the near future. The problem, a serious one, is that such harms will not be cognitively available to citizens, at least not ordinarily. People will not “see” those harms until it is too late. In this way, the availability heuristic tends to help explain high discount rates, by which people do not take preventive action against even serious harms that will not come about for many years. For potentially catastrophic risks whose prevention requires long-term investment, there are built-in obstacles to serious regulatory efforts. If salient events, such as hurricane activity, can be associated with climate change, the likelihood of a response is increased. But for most people most of the time, these associations seem speculative. 
A real puzzle, in this light, is not that the United States is relatively unconcerned with climate change; it is that European nations are so willing to take action to combat it. How do we explain this puzzle? I suggest that the availability heuristic operates as an important “input” into a form of intuitive cost-benefit balancing, and that when the balance favors regulation, people will seek regulatory solutions even if social harms are not clearly “available.” The availability heuristic does help people to assess the magnitude of a risk; people’s judgments about magnitude are greatly affected by use of the availability heuristic. But availability is not the only factor. If the costs of reducing the risk are also “on screen,” and if they seem high, then people will not be so enthusiastic about extensive precautions. In the United States, intuitive cost-benefit balancing, done with the assistance of the availability heuristic, does not clearly support significant precautions. For Europe, exactly the opposite is true. 
Of course cost-benefit analysis is often done by technocrats in and out of government, usually without close reference to the availability heuristic. But for both intuitive and expert practitioners of cost-benefit analysis, the evaluation of global warming is very different in the United States from what it is in Europe. 
from aggressive regulation than European nation do. For the United States, the likely costs of the Kyoto Protocol, for example, seem to exceed its likely benefits, at least on the latest numbers whole is far more mixed, with Europe anticipated to be a net gainer, and with Russia likely to gain an especially large amount those nations that favor aggressive controls on greenhouse gases are responding in large part to the fact that they are likely to gain more than they lose. In such circumstances, regulation will seem attractive if the risks of climate change are even mildly “available” to leaders and citizens. Compare in this regard the assessment of ozone depletion. In the end, the United States was highly supportive of extensive precautions, largely because a study from the Council of Economic Advisers suggested that the benefits of precautions, for Americans, greatly outweighed the costs. The reason for this conclusion is that reductions in skin cancer and cataracts, once monetized, suggested that the decreasing costs of precaution would be well-justified. Hence President Reagan himself, no enthusiast for extensive regulation, strongly supported American involvement in the Montreal Protocol. 
If the costs of reducing greenhouse gases were perceived as very low, the likelihood of American involvement in precautionary efforts would dramatically increase; so too if it were generally perceived that the United States had a great deal to gain from such reductions. But at the present time, many people believe that the United States will be able to handle the costs of climate change, and hence that expensive precautions are hard to justify simply from the standpoint of national self-interest. If this is so, then intuitive cost-benefit balancing, undertaken without readily available incidents of harm, is the source of the official position of the United States; it also helps to explain Europe’s greater willingness to engage in precautionary measures. 
climate change in the late 1990s. About 63 percent of Americans agreed with the following statement: “Protecting the environment is so important that requirements and standards cannot be too high and continuing environmental improvements must be made regardless of cost.”3 In the same general vein, 59 percent supported the Kyoto Treaty on global warming, with only 21 percent opposed. But in the same period, 52 percent of Americans said that they would refuse to support the Kyoto Treaty on global warming if “it would cost an extra $50 per month for an average American household.” In fact only 11 percent of Americans would support the Kyoto Treaty if the monthly expense were $100 or more. How can we explain strong majority support for “environmental improvements . . . regardless of cost” and strong majority rejection of environmental improvements when the cost is high? The answer lies in the fact that people are not, in fact, willing to spend an infinite amount for environmental improvements. When the costs are squarely placed “on screen,” people begin to weigh both costs and benefits. Surveys in Europe suggests that significant numbers of citizens there are willing to pay a considerable amount to reduce the risks of global warming; but even there, the amount is not extremely high assessment of costs and benefits plays a large role in determining the level of precautions actually sought. 
But let us simply stipulate (without arguing) that the United States ought to be doing more to control greenhouse gases than it is currently willing to do. If so, what can be done by way of response? A clue comes from President Bush’s efforts to activate 3 See The Program on International Policy Attitudes, Americans on the Global Warming Treaty, available at http://www.pipa.org/OnlineReports/GlobalWarming/glob_warm_treaty.html at Box 15. public concern about the catastrophic risks associated with terrorism: Conjure up vivid images of what might happen when the relevant risks come to fruition. In this way, the availability heuristic might be enlisted on behalf of regulatory controls. In connection with the Iraq war, the Patriot Act, and many other terrorism-related initiatives, vivid images of the Sept. 11 attacks helped to ensure that Americans would be willing to “invest” in initiatives that would cost a great deal. Of course it is true that the most serious harms associated with climate change are not likely to occur in the near-term, a contrast with the risks of terrorism, where a catastrophe could be around the corner. 
This fact makes it difficult to capture people’s attention with vivid images of harm—difficult, but not impossible, at least if those images are combined with moral appeals (involving obligations to future generations, whose members can be concrete, as in, “your children and your grandchildren”) and with efforts to quell fears about the high costs of regulatory controls. In other words, availability and salience are a promising way of promoting public attention to risks that will not materialize for a long time. If current hurricane activity can be associated with climate change, citizens and officials will be more likely to favor aggressive action. To see this point, it is necessary to shift from individual judgments to social ones. 
Thus far my emphasis has been on individual cognition. But to say the least, the availability heuristic does not operate in a social vacuum. What is readily “available” to some individuals, groups, cultures, and nations will not be available to all. In the context of climate change, environmentalists, in and out of government, often attempt to focus public attention on potentially catastrophic harms. Well-organized private groups play a central role in activating public concern. The “social amplification of risk” is a wellknown phenomenon (Pigeon et al., 2003). When social amplification occurs, it is often a result of the availability heuristic, operating alongside social processes. 
The question suggests the need to attend to the social and cultural dimensions of fear and risk perception. In many cases of high-visibility, low-probability dangers, the sources of availability are not obscure. The mass media focus on those risks; people communicate their fear and concern to one another; the widespread fact of fear and concern increases media attention; and the spiral continues until people move on. Hence the “risk of the month” syndrome, familiar in many societies, stems from the interaction between availability and social influences. Much of the time, however, what is available and salient to some is not available and salient to all. 
In any case people and cultures have different predispositions. These predispositions play a large role in determining which, of the numerous possibilities, is salient. Those who are predisposed to believe that most media scares are false or trumped-up will find cases in which public fears have been proved baseless. This is an example of an individual predispositions, but undoubtedly cultural forces, some deep and some less so, help account for differences across nations. Availability helps to determine beliefs, to be sure; but beliefs help to determine availability as well. Both beliefs and availability are endogenous to one another. When social and cultural forces interact with salience, to produce concern about one set of problems but not another, predispositions are crucial. Fears about the risks of climate change, and dismissal of those fears, can both be explained in this way. It is in this sense that availability can be a product of forces that must be explained independently. But let us now turn to how availability spreads. 
cascades, in which apparently representative anecdotes and gripping examples move rapidly from one person to another example. Andrew hears of a dangerous event, which he finds to be revealing or illustrative. (The event might involve a harmful effect produced by climate change.) Andrew tells Barry, who would be inclined to see the event as not terribly informative, but who, learning Andrew’s reaction, comes to believe that the event does indeed reveal a great deal, and that a serious threat exists. Carol would tend to discount the risk, but once she hears the shared opinion of Andrew and Barry, she is frightened as well. Deborah will have to have a great deal of private information to reject what has become the shared opinion of Andrew, Barry, and Carol is, the example shows that once several people start to take an example as probative, many people may come to be influenced by their opinion, giving rise to cascade effects. Cultural and even national differences can be explained partly in this way. 
A distinctive feature of social cascades is that the people who participate in them are simultaneously amplifying the very social signal by which they are being influenced. By their very participation, those who join the cascade increase its size, making it more likely that others will join too. 
many social beliefs it is available, tends to be repeated, leading to cascade effects, as the event becomes available to increasingly large numbers of people. The point is amplified by the fact that fear-inducing accounts, with high emotional valence, are especially likely to spread influences can be found in different communities, local variations are inevitable, with different examples becoming salient in each. Hence such variations—between say England and the United States, or between Germany and France—might involve coincidence or small or random factors, rather than large-scale cultural differences. Different judgments within different social groups, with different “available” examples, owe their origin to social processes of this sort. Return to my epigraph: “Many Germans believe that drinking water after eating cherries is deadly; they also believe that putting ice in soft drinks is unhealthy. The English, however, rather enjoy a cold drink of water after some cherries; and Americans love icy refreshments” 353354) 
one another, they typically end up accepting a more extreme version of the views with which they began Consider a few examples: • 
its intentions with respect to economic aid • A group of moderately profeminist women becomes more strongly profeminist after discussion • After discussion, whites predisposed to show racial prejudice offer more negative responses to the question whether white racism is responsible for conditions faced by African-Americans in American cities • After discussion, whites predisposed not to show racial prejudice offer more positive responses to the same question, that is, they are more likely to find white prejudice to be the source of conditions faced by African-Americans in American cities • Juries inclined to award punitive damages typically produce awards that are significantly higher than the awards chosen, before deliberation, by their median member 
Group polarization will inevitably occur in the context of perceptions of risk; and hence group polarization helps to account for cultural and even national differences. If several people fear climate change, and speak to one another, their fear is likely to increase as a result of internal discussions. If some groups seem hysterical about certain risks, and other groups treat those risks as nonexistent, group polarization is likely to be a reason. Hence group polarization provides another explanation for the different fears of groups, localities, and even nations. 
Group polarization undoubtedly occurs in connection with climate change; indeed, it helps explain cross-cultural differences. An initial predisposition toward fear is likely to be aggravated as a result of collective deliberations. Within groups, a tendency toward fear or neglect breeds its own amplification. In the United States, group polarization has played a large role within groups concerned or less concerned about climate change. Those who believe that the risks are trivial, or not worth addressing, often speak largely with one another, intensifying their antecedent belief. 
It should be clear that in the real world, some voices are more important than others, especially when availability and salience are involved. In particular, the behavior and preoccupations of the media play a large role. Knowing the importance of media coverage, well-organized private groups work extremely hard to promote public attention to particular risks. Some of these groups are altruistic; others are entirely self-interested. 
The common tactic is to publicize an incident that might trigger both availability and salience. Showing at least a working knowledge of the availability heuristic, private groups seize on selected incidents, even ones expected to occur in the future, and publicize them to make them generally salient to the public. In all of these examples, the use of particular instances might be necessary to move the public, and legislatures, in the right directions. Certainly the social processes that interact with salience and availability can promote reform where it is needed. 
influential politician comes with amplifiers. When public officials bring an incident before the public, a seemingly illustrative example is likely to spread far and wide. A legal enactment can itself promote availability; if the law responds to the problems associated with climate change, people might well come to see those problems as readily available. The terrorist attacks of September 11, 2001 would inevitably loom large no matter what President George W. Bush chose to emphasize. But the President, and his White House generally, referred to the attacks on countless occasions, frequently as a way of emphasizing the reality of seemingly distant threats and the need to incur significant costs to counteract them presidential speeches including vivid narratives of catastrophic harm) doubt that the salience of these attacks played a large role in affecting political behavior—and that this role cannot be understood without reference to social influences. The implications for cultural differences and for climate change should be clear. If leaders in different nations draw attention to different risks, there will be large-scale differences in risk perceptions. 
But all this does not provide the full picture. Beliefs and orientations are a product of availability, and social influences ensure both availability and salience. But as I have suggested, what is available is also a product of antecedent beliefs and orientations, both individual and social. In other words, availability is endogenous to, or a product of, predispositions, individual, cultural, and national.4 
precautions led to serious environmental harm? A likely reason is that they are predisposed to favor environmental protection. And why do some people recall and emphasize incidents in which environmental protection led to huge costs for little gain? A likely reason is that they are predisposed to oppose environmental controls. Here is an interaction between the availability heuristic and confirmation bias—”the tendency to seek information to confirm our original hypotheses and beliefs” Confirmation bias plays a large role in different risk perceptions across individuals and groups. If members of a culturally distinct group are predisposed to believe that climate change contains serious risks, apparently supportive illustrations will be memorable, and contrary ones will be discounted. 
They have sources. Among their sources are availability and salience. After incidents of mad cow disease in England, many Europeans lost trust in the relevant authorities and acquired a predisposition to fear, and to take and urge precautions against, associated and analogous threats. In Europe, the growth of precautionary thinking, across certain domains, had a great deal to do with particular salient incidents to combat climate change was spurred in this way. Hence there is complex set of interactions, with heuristics helping to constitute predispositions, which are in turn responsible for the real-world operation of heuristics. All this happens socially, not merely individually; and predispositions are not static. When people are in a group that is predisposed in a particular direction, the salient examples will be quite different from those that are salient in a group with an opposite predisposition. Here group polarization is especially important. What is sometimes described as “culture,” or as “deep-rooted cultural differences,” may be no such thing. Cascade effects and polarization, interacting with availability, can be responsible for inclinations and variations that might well have taken another form. 
and the work of those inspired by them is that availability is a product of cultural orientations, rather than vice versa. But see 
Why are some groups and some nations concerned with the risks associated with climate change, and why are others much less so? A sensible default assumption is that they are motivated by a form of intuitive cost-benefit balancing. Nations usually follow their rational self-interest, and a nation that has relatively less to gain from precautions, and relatively more to lose, will naturally be interested in greater precautions. Contrast here the enthusiasm of the United States for precautionary steps to reduce ozone depletion with the reluctance of the United to endorse such steps to reduce global warming. The difference has a great deal to do with that nation’s assessment of the costs and benefits of precautions. 
I have also suggested that the operation of the Precautionary Principle, and differences in risk perception among nations, have a great deal to do with the availability heuristic, which helps to inform intuitive cost-benefit balancing. For the risks associated with climate change, which are not likely to come to fruition in the near future, it is difficult to promote availability; but vivid images are possible to provide here as well. European nations are more concerned about climate change than the United States in part because certain environmental risks have become more salient in the former than in the later, and in part because both intuitive and formal cost-benefit analysis suggest that with expensive preventive measures, the United States is more likely to be a net loser. If that analysis shifted, through declining costs of control or through more vivid incidents of tangible harm, American participation in international agreements would be far more probable. 
Of course availability is a product of social influences. Cascade effects and group polarization play substantial roles in making one or another incident available to many or most. There are multiple equilibria here: Single incidents and small shocks can make an extraordinary difference. Moreover, what is available to some will not be available to all, in part because of social influences, and in part because of individual, cultural, and national predispositions. It follows that some cultures will find risks of climate change “available” not because of simple facts about what citizens have to gain and to lose, but also because the relevant citizens are predisposed to focus on some risks but not on others. But even across cultural differences, intuitive cost-benefit balancing can be altered by available incidents; if vivid incidents become salient, aggressive regulation is far more likely to be forthcoming.     Readers with comments should address them to: Professor Cass Sunstein University of Chicago Law School 1111 East 60th Street Chicago, IL 60637 csunstei@uchicago.edu  177.  Chicago Working Papers in Law and Economics  (Second Series)    For a listing of papers 1–174 please go to Working Papers at http://www.law.uchicago.edu/Lawecon/index.html    175.  176.  Richard A. Epstein, Disparities and Discrimination in Health Care Coverage; A Critique of the  Institute of Medicine Study (March 2004)  Richard A. Epstein and Bruce N. Kuhlik, Navigating the Anticommons for Pharmaceutical Patents:  Steady the Course on Hatch‐Waxman (March 2004)  Richard A. Esptein, The Optimal Complexity of Legal Rules (April 2004)  Eric A. Posner and Alan O. Sykes, Optimal War and Jus Ad Bellum (April 2004)  Alan O. Sykes, The Persistent Puzzles of Safeguards: Lessons from the Steel Dispute (May 2004)  Luis Garicano and Thomas N. Hubbard, Specialization, Firms, and Markets: The Division of Labor  within and between Law Firms (April 2004)  Luis Garicano and Thomas N. Hubbard, Hierarchies, Specialization, and the Utilization of  Knowledge: Theory and Evidence from the Legal Services Industry (April 2004)  James C. Spindler, Conflict or Credibility: Analyst Conflicts of Interest and the Market for  Underwriting Business (July 2004)  Alan O. Sykes, The Economics of Public International Law (July 2004)  Douglas Lichtman and Eric Posner, Holding Internet Service Providers Accountable (July 2004)  Shlomo Benartzi, Richard H. Thaler, Stephen P. Utkus, and Cass R. Sunstein, Company Stock,  Market Rationality, and Legal Reform (July 2004)  Cass R. Sunstein, Group Judgments: Deliberation, Statistical Means, and Information Markets  (August 2004, revised October 2004)  Cass R. Sunstein, Precautions against What? The Availability Heuristic and Cross‐Cultural Risk  Perceptions (August 2004)  M. Todd Henderson and James C. Spindler, Corporate Heroin: A Defense of Perks (August 2004)  Eric A. Posner and Cass R. Sunstein, Dollars and Death (August 2004)  Randal C. Picker, Cyber Security: Of Heterogeneity and Autarky (August 2004)  Randal C. Picker, Unbundling Scope‐of‐Permission Goods: When Should We Invest in Reducing  Entry Barriers? (September 2004)  Christine Jolls and Cass R. Sunstein, Debiasing through Law (September 2004)  Richard A. Posner, An Economic Analysis of the Use of Citations in the Law (2000)  Cass R. Sunstein, Cost‐Benefit Analysis and the Environment (October 2004)  Kenneth W. Dam, Cordell Hull, the Reciprocal Trade Agreement Act, and the WTO (October 2004)  Richard A. Posner, The Law and Economics of Contract Interpretation (November 2004)  Lior Jacob Strahilevitz, A Social Networks Theory of Privacy (December 2004)  Cass R. Sunstein, Minimalism at War (December 2004)  Douglas Lichtman, How the Law Responds to Self‐Help (December 2004)  Eric A. Posner, The Decline of the International Court of Justice (December 2004)  Eric A. Posner, Is the International Court of Justice Biased? (December 2004)  Alan O. Sykes, Public vs. Private Enforcement of International Economic Law: Of Standing and  Remedy (February 2005)  Douglas G. Baird and Edward R. Morrison, Serial Entrepreneurs and Small Business Bankruptcies  (March 2005)  Eric A. Posner, There Are No Penalty Default Rules in Contract Law (March 2005)  Randal C. Picker, Copyright and the DMCA: Market Locks and Technological Contracts (March  2005)  Cass R. Sunstein and Adrian Vermeule, Is Capital Punishment Morally Required? The Relevance of  Life‐Life Tradeoffs (March 2005)  Alan O. Sykes, Trade Remedy Laws (March 2005)  Randal C. Picker, Rewinding Sony: The Evolving Product, Phoning Home, and the Duty of  Ongoing Design (March 2005)  Cass R. Sunstein, Irreversible and Catastrophic (April 2005)   James C. Spindler, IPO Liability and Entrepreneurial Response (May 2005)  Douglas Lichtman, Substitutes for the Doctrine of Equivalents: A Response to Meurer and Nard  (May 2005)  Cass R. Sunstein, A New Progressivism (May 2005)  246.   253.  254.  255.  256.  257.  258.  259.  260.  261.  
Douglas G. Baird, Property, Natural Monopoly, and the Uneasy Legacy of INS v. AP (May 2005)  Douglas G. Baird and Robert K. Rasmussen, Private Debt and the Missing Lever of Corporate Governance (May 2005) Cass R. Sunstein, Administrative Law Goes to War (May 2005) Cass R. Sunstein, Chevron Step Zero (May 2005) Lior Jacob Strahilevitz, Exclusionary Amenities in Residential Communities (July 2005)  Joseph Bankman and David A. Weisbach, The Superiority of an Ideal Consumption Tax over an  Ideal Income Tax (July 2005)  Cass R. Sunstein and Arden Rowell, On Discounting Regulatory Benefits: Risk, Money, and  Ingergenerational Equity (July 2005)  Cass R. Sunstein, Boundedly Rational Borrowing: A Consumer’s Guide (July 2005)  Cass R. Sunstein, Ranking Law Schools: A Market Test? (July 2005)  David A. Weisbach, Paretian Intergenerational Discounting (August 2005)  Eric A. Posner, International Law: A Welfarist Approach (September 2005)  Adrian Vermeule, Absolute Voting Rules (August 2005)  Eric Posner and Adrian Vermeule, Emergencies and Democratic Failure (August 2005)  Douglas G. Baird and Donald S. Bernstein, Absolute Priority, Valuation Uncertainty, and the  Reorganization Bargain (September 2005)  Adrian Vermeule, Reparations as Rough Justice (September 2005)  Arthur J. Jacobson and John P. McCormick, The Business of Business Is Democracy (September  2005)  Adrian Vermeule, Political Constraints on Supreme Court Reform (October 2005)  Cass R. Sunstein, The Availability Heuristic, Intuitive Cost‐Benefit Analysis, and Climate Change  (November 2005)  
