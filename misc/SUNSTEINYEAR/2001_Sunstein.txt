Follow this and additional works at: https://chicagounbound.uchicago.edu/law_and_economics Part of the Law Commons Recommended Citation Cass R. Sunstein, "Academic Fads and Fashions (with Special Reference to Law)" ( John M. Olin Program in Law and Economics Working Paper No. 118, 2001). 
ACADEMIC FADS AND FASHIONS (WITH SPECIAL REFERENCE TO LAW) 
C a s s R . S u n s t s e i n This paper can be downloaded without charge at: 
The Chicago Working Paper Series Index: http://www.law.uchicago.edu/Publications/Working/index.html The Social Science Research Network Electronic Paper Collection: http://papers.ssrn.com/paper.taf?abstract_id=262331 Preliminary draft 3/2/01 Forthcoming Michigan Law Review, foreword to book review issue All rights reserved 
Cass R. Sunstein* 
Like everyone else, academics are susceptible to informational and reputational signals. Sometimes academics lack confidence in their methods and beliefs, and they pay a great deal of attention to the methods and beliefs of others. The academic study of law is particularly subject to cascade effects, as people follow signals that they participate in amplifying. Some of these effects run their course quickly, whereas others last a long time. Leaders can play a special role in starting and stopping cascades; external shocks play a special role in the academic study of law; sometimes like-minded people within academia move one another to extremes. This informal essay, the Foreword to the forthcoming annual book review issue of the Michigan Law Review, discusses these points in a tentative and impressionistic way, with brief comparisons to other fields. 
Why did critical legal studies disappear? Will it reappear? Why does the Federalist Society prosper? Why, and when, do people write books on constitutional law, rather than tort law or antitrust? Why did people laugh at the notion of “animal rights,” and why do they now laugh less? Why do law professors seem increasingly respectful of "textualism" and "originalism," ideas that produced ridicule and contempt just two decades ago? How do book reviewers choose what books to review? Why has law and economics had such staying power? 
Academics are generally committed to truth, and they are drawn to ideas that can be shown to be good ones. Hence the most optimistic answer to these questions is that ideas survive because and to the extent that they are true or good. On this view, law and economics has outlasted critical legal studies because it has much more to offer. * Karl N. Llewellyn Distinguished Service Professor of Jurisprudence, University of Chicago, Law School and Department of Political Science. I am grateful to Jack Balkin, Jack Goldsmith, Tracey Meares, Eric Posner, and Richard Posner for helpful comments on an earlier draft. 
Textualism and originalism have had a resurgence because much can be said on their behalf. Book reviewers, in the academic domain, tend to choose to review the best books. 
In my view, these claims contain some truth, but they are far too optimistic. Academics, like everyone else, are subject to cascade effects. They start, join, and accelerate bandwagons. More particularly, they are subject to the informational signals sent by the acts and statements of others. They participate in creating the very signals to which they respond. Academics, like everyone else, are also susceptible to the reputational pressures imposed by the (perceived) beliefs of others. They respond to these pressures, and by so doing, they help to amplify them. It is for these reasons that fads, fashions, and bandwagon effects can be found in academia, emphatically including the academic study of law. Fortunately, the underlying forces can spark creativity and give new ideas a chance to prosper. Unfortunately, these same forces can also produce error and confusion. 
Sometimes cascades have enduring effects; but in many fields, academic cascades are fragile, with numerous people focussing on issues and adopting methods that disappear in short order. Some cascades produce unpredictable and seemingly random movements, as external shocks sometimes lead in dramatic directions. In social life, small sparks cause wildfires; it is for this reason, among others, that we cannot easily predict future academic trends, or foresee new movements in the academic study of law. (In 1985, would it have been possible to predict the resurgence, in the 1990s, of interest in the study of social norms1? Or the rise of interest in the Second Amendment?) There is even a tipping point phenomenon here, in which a certain pressure, from the perceived views of others, can produce a kind of sudden “rush” toward a methodology or point of view.2 
In this essay, I attempt to cast some preliminary light on the general topic of academic bandwagons and cascades, with particular reference to law. Several caveats are in order at the outset. First, my focus here is on trends in academic law, but informational and reputational signals are of course ubiquitous. The same forces discussed here help explain many social movements, including reactions to environmental risks, the rise and fall of communism, the success or failure of students and job candidates, the creation of ethnic identifications, and the rise and partial fall of 1 Fueled by Robert Ellickson, Order Without Law (1991). 2 For a popular treatment, see Malcolm Gladwell, The Tipping Point (2000). affirmative action.3 Second, I do not mean to present any criticism of legal scholarship in general, or to depict those who produce it as especially prone to informational and reputational influences. A general attack on legal scholarship would be senseless, if only because so much of it is obviously excellent. Third, I aim only to establish the existence of cascade effects, not to give a clear test for distinguishing cascades from cases in which approaches and arguments have spread because of their merits or independent appeal (though some of my remarks will bear on that issue). Fourth, my treatment will be informal and anecdotal, offering examples that will, I hope, be intuitive and familiar. With respect to the underlying phenomena, I draw on some more systematic and formal treatments,4 both general and particular, and try to apply the central ideas to the academic context. Obviously a great deal might be said about this topic; the brief treatment here is intended to be a start. 
To make progress on this topic, it is necessary to have some sense of what academics care about, and also to know something about the nature of the market for academic ideas. On these subjects, I hope not to say anything controversial. But because some account is implicit in any description of cascades, I offer some brief notations. 
Most academics care about what most people care about. They seek to retain their jobs and to have the good opinion of (relevant) others. Few of them are indifferent to status. But they also care, more than most, about ideas, and they are willing to forego benefits of various sorts in order to be able to think and talk about issues suitable for teaching and academic research. Many academics are interested in pursuing truth as such; those who do or use empirical work often fall in this category. The same is true for 3 See, e.g., David Hirschleifer, The Blind Leading the Blind: Social Influence, Fads, and Informational Cascades, in The New Economics of Human Behavior 188, 189 (Mariano Tommasi and Kathryn Ierulli eds 1995); Timur Kuran, Public Lies, Private Truths (1997). 4 See note supra. A helpful overview is Sushil Bikchandani et al., Learning From the Behavior of Others: Conformity, Fads, and Informational Cascades, 12 J Econ Persp 151 (1998). In the social sciences, the analytical literature on cascades begins with Magoroh Maruyama, The Second Cybernetics: DeviationAmplifying Mutual Causal Processes, 51 American Scientist 164 (1963); Thomas C. Schelling, Micromotives and Macrobehavior (1978); and Mark Granovetter, Threshold Models of Collective Behavior, 83 American Journal of Sociology 1420 (1978). For analysis of purely informational cascades, see Sushil Bikchandani, David Hirshleifer, and Ivo Welch, A Theory of Fads, Fashion, Custom, and Cultural Change as Informational Cascades, 100 Journal of Political Economy 992 (1992); Lisa Anderrson and Charles Holt, Information Cascades in the Laboratory, 87 Am. Ec. Rev. 847 (1997); Abhiijit Banerjee, A Simple Model of Herd Behavior, 107 Q. J. Econ. 797 (1992). See also B. Douglas Bernheim, A Theory of Conformity, 102 J. Polit. Economy 841 (1994) (discussing similar mechanisms). those whose basic goal is to help produce clarity and coherence in the law. In the particular context of law, there is an additional point: Many academics would like to contribute to improvements in law and society by helping to make law better in the domains of (for example) antitrust law, race or sex equality, and freedom of speech. Of course academics are a diverse lot on these counts. For some, reputation matters a great deal, whereas for others, the pursuit of truth, or justice, is what is really important. 
There is also a market for academic ideas, and this market will have significant effects on what academics do. In the market for academic law, law professors are the producers, while consumers include other law professors, students, government officials, judges, and law clerks. The extent of interest from these various groups will of course vary with the material; some academic work is of direct interest only to other academics. The market here is unusual in many ways, above all because no one pays directly for what academics produce. Of course law reviews usually do not compensate people for articles and essays, and the same is true for other journals (in economics and philosophy, for example) in which law professors might publish. Boo publishers will pay for the right to publish books, and professors receive royalties, but for academic books, little money is usually involved, and hence the motivation for writing books is rarely material. 
On the other hand, indirect compensation, monetary and nonmonetary, is omnipresent. Job opportunities are a direct function of what academics produce, and at many schools, salary is partly a reflection of quality and quantity of publications. Invitations to conferences and the like – dreaded by some, welcomed by most – are also affected by the perceived quality of academic work. It is here, above all, that the market disciplines academic activity. 
In a well-functioning market, only or mostly valuable ideas will be produced – although of course some ideas will be valuable even if they are misleading or incorrect. But it is not at all clear that an ordinary economic market, based on the willingness to pay criterion, is a good way to produce valuable ideas in law or elsewhere. Such a market might well cater unduly to existing tastes or to the interests of those with a great deal of money to pay. (Research funding by groups with a large financial stake in outcomes is therefore a problem.) A large point of academic work, in many fields, is to challenge and transform current preferences and values, not to reinforce them. 
In fact the complex system of indirect compensation, alongside the tenure system, is commonly defended as a way of insulating the production of ideas from ordinary markets. And if this complex system works extremely well (by the appropriate criteria), it will lack “bad cascades,” that is, cascades in which valueless ideas travel not because they are valuable, but because of the mechanisms that I will be discussing here. But I will suggest that in many contexts, an absence of private information, together with a concern for reputation and various features of human condition, can produce academic cascades that are bad as well as good. 
III. 
Academic cascades take two forms: informational and reputational. Let us begin with the role of information. 
Most people, in most domains, lack reliable information about what is true and what is right. For this reason, they are interested in the signals of others. The point holds for the selection of movies, restaurants, and carpets; it holds for ideas as well. If you are unsure whether textualism is a sensible or instead pernicious approach to constitutional interpretation, you might care a great deal about other people's views. Of course academics, especially older ones, are sometimes settled in their beliefs. Often they are confident that they know what to think, and to the extent that this is so, they are not terribly susceptible to the views of others. (Notice here that the precondition for immunity to informational influences is confidence about one’s preexisting views, not adequate or accurate information.) But among some groups, and in some fields, any settlement is provisional and somewhat fragile. In many fields, including law, young people in particular can both influence and be influenced by informational signals. If many people are susceptible to influence, cascades can readily develop. The significant swings in legal scholarship over the last decades suggest that academic lawyers are vulnerable to them. 
Suppose, for example, that John, a young academic, does not know whether textualism is a sensible approach to constitutional interpretation, but that Mary, a slightly older academic, is in favor of it.5 If John is otherwise in equipoise, but attaches value to what Mary thinks because she seems wise or has often been right in the past, it is easy to see how John might come to Mary's view. If John and Mary believe that textualism makes sense, Sally, a contemporary of John, might be moved to agree, at least if she lacks reasons to be confident that they are wrong. And once John, Mary, and Sally come to a certain view, David, a recently hired faculty member, will likely agree with them unless he has enough private knowledge, or (more precisely) confidence about his antecedent view, to stand in their way. At some point one or more of these people might even produce an article or book in defense of textualism. 5 The example adapts from the treatment in Bikchandani, supra note. 
As stylized as the example seems, I believe that it captures a great deal about academic movements, in law and elsewhere. Consider, for example, the spread of feminism within American law schools, starting roughly in the late 1980s. In many places, feminism appears to have succeeded through a kind of informational cascade, as people who would otherwise be skeptical, or unsure, came to think that feminist approaches had something to offer -- not (in many cases) because they carefully investigated the underlying claims and believed that they were illuminating or right, but because the beliefs of others seemed hard to resist for those lacking a great deal of confidence in their own (skeptical) judgments. If so many people seemed to think feminist approaches to law are valuable, mustn’t they be right? 6 
Or consider the life and apparent death of the critical legal studies movement, which flourished (again speaking roughly) from 1977 to 1989. When I was a visiting professor at Harvard Law School in 1987, critical legal studies was having a truly extraordinary influence on both students and younger faculty. A significant number of students seemed to have a sense of what critical legal studies was about; and they seemed to agree with it. A significant number of assistant professors (some of them now professors, with apparently little continuing interest in critical legal studies) were in the same category. Within both groups, the informational signals sent by the large number of critical legal studies members were extremely important. 
At the University of Chicago Law School, much the same can be said, then and now, about the economic analysis of law. Many faculty members engage in economic analysis of law, and a strong majority of the faculty shows considerable interest in the basic approach. As a general rule, younger faculty members are especially interested in the informational signals sent by their colleagues, and at Chicago, many of them end up doing work that is influenced by economics. Cascade effects are even easier to observe within the student body, as certain concepts (involving, for example, the value of efficiency, the implications of the Coase theorem, the futility of redistributive regulation) spread as if by contagion. Of course it is true that many students, and some faculty members, show no interest in economic analysis of law. But mere exposure to economic thinking, voiced in many settings (including workshops, lunch discussions, and comments on drafts of articles), lead in the expected directions. 
As informational cascades develop, people end up amplifying the very informational signals to which they have responded. Scholarship, including the production of articles and books, is much affected by processes of this kind. If this is so, 6 Of course a possible answer is: They might be wrong, especially if they are participating in a cascade, rather than acting independently. it is possible to predict, with some confidence, that the publication of books on various topics or with various methodologies will often be highly concentrated over time, showing that fads and fashions play a role in the academic world as elsewhere. 
In making these claims, I do not mean to make any normative claims about feminism, critical legal studies, or economic analysis of law. Nor do I mean to suggest that those who are vulnerable to informational pressures are weak or irrational. People who know that they have limited information certainly should pay attention to the signals given by others. And whether pressures of this kind will lead in desirable directions cannot be decided in the abstract. Certainly a field that is susceptible to cascade effects will not be unduly settled or complacent. All that can be said is that the underlying mechanisms give little reason for confidence that academic "movements" will be good ones. Ideas can spread, even among people with some expertise, despite the fact that little is to be said on their behalf. 
From the examples that I have given, we can also see the possibility of purely or mostly local informational cascades. Outside of the academic world, some communities come to believe that abandoned hazardous waste dumps are extremely dangerous, whereas others think that they pose no hazard at all. So too, some law schools might come to embrace the economic analysis of law, whereas others might see enthusiasm for traditional doctrinal analysis – not because of a large number of independent judgments, but because of mutual interactions and influences. 
It should be clear that something important is missing from the picture: people's concerns about their reputations.7 Generally people care about what others think of them, and most academics are, on this count, like most other people. 
At many law schools, and in many economics departments, an effort to show that centralized planning really can work well, or to vindicate socialism, or to show that people are irrational, would be very risky, no matter the quality of the relevant work. People might be ridiculed. They might well jeopardize their careers. In many law schools, the same would be true for people who attempted to show that current differences between men and women are biological rather than social, and to bring evidence to that effect to bear on legal issues. 7 On reputation and signalling in general, see Eric Posner, Law and Social Norms (2000). 
Reputational considerations influence the public behavior of most people, not excluding academics. This is not because people lack integrity, or are sycophantic, or are unwilling to follow their own paths. It is simply because most people, most of the time, want others to think well, or at least not ill, of them. Of course people have varying susceptibility to reputational pressures. Some people can withstand a great deal; others will be inclined to take the safe course, showing reluctance to say, or especially to publish, anything that could create trouble for them in the future. And of course sometimes those who incur reputational sanctions in one place (the nonacademic world, for example) will reap reputational benefits elsewhere (perhaps their local academic community). Thus those who seem to be venturing out on their own, and to be “brave,” might in fact be motivated by the goal of gaining status within a particular group. 
Because most people care about the views of others, and because people have varying rather than uniform susceptibility to reputational pressures, it is easy to imagine reputational cascades with respect to actions or stated beliefs.8 Suppose, for example, that A and B would think ill of anyone who argues that the minimum wage should be significantly increased. C, who is not sure what to think about a higher minimum wage, might be unmoved privately by the views of A and B, but might nonetheless not want to incur the wrath of A and B, or to seem ignorant of basic economic principles, or to appear indifferent to economic efficiency. If so, C might show no enthusiasm for an increase in the minimum wage, or might even agree publicly with A and B that an increase would be a bad idea. If D is otherwise in equipoise, she might be most reluctant to oppose A, B, and C publicly. Mounting reputational pressures might well lead E, F, G, and H, and many more, to join the bandwagon. Eventually a large number of people might speak as if the minimum wage should not be increased. The result would be to affect academic discussion of government’s role in the labor market, including the treatment of this topic in articles and books. 
Here too, a highly stylized example seems to help account for many shifts in the academic world, including the field of law. The rise of feminism within legal academia undoubtedly has a great deal to do with reputational as well as informational incentives. In the early 1980s, those who expressed contempt for feminist scholarship were rarely punished for doing so, and were sometimes rewarded. Currently those who express contempt for feminist scholarship generally (not always) put their reputation in considerable danger. If a young academic chooses to write on certain topics, or from certain points of view, the reputational sanctions might be quite severe. At the University of Chicago Law School, I cannot recall many faculty members expressing 8 See Kuran, supra note. public support for a substantial increase in the minimum wage, though I would not be surprised if more than one faculty member actually believes that such an increase would be a good idea. Five years ago, those who borrowed from behavioral economics were viewed with considerable suspicion inside the world of law and economics; through a kind of cascade effect, this is decreasingly true. 
It follows that “political correctness” is hardly a narrow phenomenon involving the practices of left-leaning academics. Wherever reputational pressures are in place, a form of “political correctness” will discipline action and public statements. Reputational cascades are a possible consequence. 
A closely related phenomenon helps explain the initial growth of academic fashions, and also gives some guidance on how to create, and how not to create, an academic "school." The phenomenon is that of group polarization.9 In brief, the claim of group polarization is that when a group of people engages in deliberation, group members will move toward a more extreme position in line with their predeliberation inclinations. This is the typical pattern among deliberating bodies. Thus, for example, a group of Federalist Society members, inclined to support originalism, is likely to be extremely enthusiastic about originalism after discussing it with one another. So too, a semi-formal organization of law professors, meeting one a month, is likely to emerge with a stronger commitment to critical race theory if its members are inclined, before discussion, to be favorably disposed toward critical race theory. It would be easy to multiply examples. 
Why does group polarization occur? Though no cascade need be involved,10 the two principal explanations are close to the explanations for informational and reputational cascades. The first involves informational influences. In a deliberating group with an initial tendency in favor of X and against Y, there will be a disproportionate number of arguments in favor of X, simply because most people will speak out on behalf of X. Group members will have thought of some, but not all, of the arguments in that direction. After deliberating, the arguments for X will seem stronger, to individual members, and the arguments for Y will seem even weaker. Hence it is to be expected 9 See Roger Brown, Social Psychology (2d ed. 1986); Cass R. Sunstein, Deliberative Trouble: Why Groups Go To Extremes, 10 This is because group polarization can result from simultaneous independent influences on group members. that discussion will move people to a more extreme form of their original enthusiasm for X. 
The second explanation for group polarization points to social influences. Most people, emphatically including professors of law, care about their reputations and their self-conception. Suppose, for example, that you are inclined to think that affirmative action does not offend the Constitution, but you are not entirely sure; suppose too that you find yourself in a group which also rejects the idea that affirmative action offends the Constitution. If you think of yourself as the sort of person who is, more than most, inclined to support the constitutionality of affirmative action programs, you might move a bit, if only to maintain your reputation within the group and your selfconception on the issue at hand. The evidence strongly supports the proposition that this happens.11 
In the academic context, the lesson is simple. A group of like-minded people, thinking about some issue or topic, is highly likely to move toward a more extreme position, not merely fortifying but amplifying their predeliberation inclinations. Through this route, it is possible to make some progress in understanding the creation and effects of academic "schools." In the early 1980s, for example, the critical legal studies movement flourished at Harvard Law School in particular, no doubt in part because of the presence of members who talked a great deal with one another and fueled their predeliberation inclinations. Several influential books emerged from these discussions.12 In roughly the same period, the Federalist Society was created at Chicago and Yale, and the existence of a group of like-minded people undoubtedly helped to fuel certain commitments. In fact it is reasonable to speculate that the growth of conservative legal thought, within both faculties and students, has had a great deal to do with the existence of a group of people who are relatively well-organized and who are able to ensure that like-minded people can find some kind of home. 
Informational and reputational influences, and group polarization, play a significant role in academic life. Cascade effects are present here as elsewhere. If the account here is correct, we should expect a large number of fads and fashions in the academic study of law. I would predict, for example, that a citation analysis would 11 See Brown, supra note. 12 See, e.g., Roberto Unger, The Critical Legal Studies Movement (1985). show many academic “bubbles”: rapid rises and declines in references to certain ideas and people.13 But this basic sketch omits important parts of the overall picture. 
With respect to both informational signals and reputational pressures, all people are not created equal. Some people carry more weight than others. For example, the signals sent by well-known academics, and academics at well-known schools, are likely to be especially loud. If faculty members at Yale end up endorsing a new method for understanding law, there might seem to be particularly good reason to take that method seriously; and it is less likely that people who do so will face the kind of reputational sanction that would be imposed if the method was being used at a littleknown school. Those who are in a position to start cascades operate as leaders, above all because of the social amplification of their voices.14 Note that this amplification can occur independently of the merits of the argument being made. In listening especially carefully to well-known people, or to people at well-known schools, followers are generally behaving rationally, because such people are unusually likely to be interesting or correct, simply as a statistical matter. But there are no guarantees here, and hence arguments can be amplified even if they are meritless. (Perhaps the resulting bubble will eventually pop, as discussed below.) 
Some of the relevant leaders are simply saying what they think to be true; others affirmatively want followers, perhaps because they seek status, perhaps because they want to ensure that their ideas are disseminated. Such people take steps self-consciously to promote cascade effects, perhaps by organizing conferences, reading groups, or even journals. More specifically, we can describe as "polarization entrepreneurs" those people who foster deliberative groups of like-minded people, and to ensure that participants share a common methodology or point of view. Exclusion of outsiders, and inclusion of a large number of insiders, is an important component of this strategy. 
An obvious implication is that if the goal is to spread ideas, it is probably best to begin by promoting discussion among groups of like-minded people. If members of such groups speak mostly or only to one another, views might become entrenched, and the entrenchment among the views of increasingly large groups might initiate a cascade effect. A much worse strategy - often a doomed strategy - is to ensure that people with 13 Some support can be found in Robert Ellickson, Trends in Legal Scholarship: A Statistical Study, 29 J Legal Stud 517, 527 (2000). 14 Cf. Gary Becker and Kevin Murphy, Social Economics 140-43 (2001). new ideas are placed in heterogeneous groups, where their ideas are unlikely to travel, or might be squelched, or might even be subject to self-silencing. 
In fact the forces here are compounded by another: the availability heuristic.15 It is well known that certain facts and ideas are cognitively “available,” or highly salient, and that this cognitive availability can exert a large influence on beliefs and decisions.16 If a leader, or an idea, ends up widely known, through independent decisions or through cascade effects, dramatic changes in scholarly paths can be expected. Hence availability cascades occur, within academia, when a particular person or idea becomes salient, and when through the informational and reputational mechanisms discussed here, a certain point of view becomes widely known and widely held. 
Some people are relatively immune to the influences explored thus far. As I have suggested, people who are confident about their views are especially likely to resist informational and reputational incentives. The point suggests that in some arenas, cascades are likely to arise quite infrequently. Academic areas are highly variable on this count, and some domains, most academics have a great deal of confidence, thus immunizing themselves from cascade effects. In fields with well-established methods and goals, we should expect cascades to be uncommon. In the sciences, for example, large-scale shifts certainly occur, but the existence of settled methods makes cascade effects unlikely17 – far less probable than in, for example, comparative literature. Law, economics, and psychology are perhaps intermediate cases. 
This point raises an important question: When and why do academic cascades start and stop? A crucial reason has to do with external shocks. Suppose, for example, that a group of people believes some fact. Suppose that evidence shows that the belief is false. The belief will fade because it has been demonstrated to be wrong. 
But external shocks can take many different forms. Sometimes academic trends, perhaps especially in law, have nothing to do with demonstrated fact, but are greatly affected by what happens outside of the academic domain. For example, the selection of Antonin Scalia to be a member of the Supreme Court undoubtedly had a great deal to do with the legitimation of originalism and textualism, methods favored by Justice 15 See Timur Kuran and Cass R. Sunstein, Availability Cascades and Risk Regulation, 51 Stan. L. Rev. 683 (1999). 16 See Jonathan Baron, Thinking and Deciding (3d ed. 2000). 17 This is a possible reading of Thomas Kuhn, The Structure of Scientific Revolution (1969). Scalia. This is partly because Justice Scalia’s opinions provided a kind of focal point for academic debate; it is also because his office conferred a kind of legitimacy on arguments that might otherwise have been easy to dismiss. Nor is it irrelevant that some of Justice Scalia’s law clerks became academics. In fact a significant source of informational and reputational influences will come, directly and indirectly, from the selection of Supreme Court clerks, and from the choice, among clerks of particular justices, to become law professors. In a previous generation, the law clerks of Felix Frankfurter, much influence d by Frankfurter, became influential academics; the same appears to be true of Scalia clerks today. 
More generally, the 1980 election of President Reagan made it most unlikely that the Supreme Court would continue to use the equal protection and due process clauses as a basis for announcing a series of new rights for disadvantaged people; hence academics interested in social reform showed decreasing interest in elaborating legal doctrine for that purpose. Perhaps the rise of interest in constitutional deliberation outside of the courtroom had something to do with the Court’s lack of receptivity to the professors’ arguments. Perhaps decreasing interest in judicial review had something to do with the changing composition of the Court.18 Highly visible public events with legal dimensions, such as the 1998 Clinton impeachment and the 2000 post-election struggle between George W. Bush and Al Gore, will inevitably affect people’s choice of what to write about. (Perhaps Bush v. Gore will inaugurate a new era of neo-realism, questioning the division between law and politics. Perhaps Bush v. Gore will lead to an outpouring of work on the law of elections.) Academics may or may not follow the election returns, but in law, the election returns can set the academic agenda. 
Other external shocks can come from developments in adjacent fields. If, for example, economists show a great deal of interest in the idea of spontaneous ordering, academic lawyers are eventually likely to show an interest in that topic too. Part of the reason is informational: The fact that a certain topic interests economists is likely to be important to academic lawyers, who care about what economists think. If there is a resurgence of interest in utilitarianism within philosophy, law professors are likely to write about utilitarianism; the extraordinary interest in the work of John Rawls helps to confirm the point. Critical theory provides another case in point, with Jurgen Habermas and Michel Foucault, for example, exerting a significant influence on legal scholarship by virtue of their prominence within closely related fields. Of course developments within adjacent fields might well be a product of the kinds of influences discussed here. 18 See Mark Tushnet, Taking the Constitution Away From the Court (2000). 
There is a final issue, perhaps in tension with the general argument offered here. It is useful to distinguish between ideas and methods on which multiple people can build for a long time, and ideas and methods that do not and cannot lead to much in the way of further work .19 The notion that people are rational, self-interested profit maximizers is fertile, in the sense that it has applications to many domains of law, helping to produce predictions that can be tested and used. Though it is too early to say, I believe that the same is true for the notion that people are boundedly rational, and also for the claim that people are not only self-interested.20 The idea that law is pervasively basis on male practices and understandings is also easily used as a basis for assessing, or reassessing, many domains of law. But some claims tend to “burn out,” in the sense that once they have been voiced, there is little that can be done with them, even if they are true. Perhaps this is the case for the contention that law is “political,” an important and illuminating partial truth, but one with which it is not easy to do a great deal of illuminating further work. 
Academics, like everyone else, are susceptible to informational and reputational influences, and cascade effects are likely to be found in the academic domain as elsewhere. Notwithstanding the expertise and confidence of many academics, academic life has its own fads and fashions, and the factors discussed here play a role in their development. I believe that the factors discussed here have played a role in many trends in legal theory, including critical legal studies, economic analysis of law, feminism, textualism and originalism in constitutional law, critical race theory, rights-based accounts associated with Ronald Dworkin and others (many at New York University), law and literature, and (more recently) behavioral law and economics. 
both a prescription and a cautionary note. The prescription is that those who seek to promote ideas will do best to ensure, above all, that those ideas have an opportunity to develop through frequent discussions among like-minded people. Most would-be “schools” fail. But those that succeed often transform the field;, and when they do so, group polarization is part of the reason. 19 Cf. the discussion of progressive and degenerate research programs in Imre Lakatos, Falsification and the Methodology of Scientific Research Programmes, in Criticism and the Growth of Knowledge (Imre Lakatos & Alan Musgrave eds., 1970). 20 See Richard Thaler, Quasi-Rational Economics (1993); Behavioral Law and Economics ed. 2000) 
The cautionary note is that in law and many other academic fields, ideas may spread and prosper, not because they are good, but because dozens, hundreds, or even thousands of imperfectly informed people have fortified the very signals by which they have been influenced. Whether bad ideas can prosper for a long time is another matter. Frequently good arguments and good evidence will puncture them, at least when there is agreement about the underlying criteria. But if the account here is correct, longevity, even for bad ideas, is hardly out of the question. 
Readers with comments should address them to: Chicago Working Papers in Law and Economics (Second Series) 
William M. Landes, Copyright Protection of Letters, Diaries and Other Unpublished Works: An Economic Approach (July 1991). 
Richard A. Epstein, The Path to The T. J. Hooper: The Theory and History of Custom in the Law of Tort (August 1991). 
Cass R. Sunstein, On Property and Constitutionalism (September 1991). 
Richard A. Posner, Blackmail, Privacy, and Freedom of Contract (February 1992). Randal C. Picker, Security Interests, Misbehavior, and Common Pools (February 1992). Tomas J. Philipson & Richard A. Posner, Optimal Regulation of AIDS (April 1992). Douglas G. Baird, Revisiting Auctions in Chapter 11 (April 1992). 
William M. Landes, Sequential versus Unitary Trials: An Economic Analysis (July 1992). William M. Landes & Richard A. Posner, The Influence of Economics on Law: A Quantitative Study (August 1992). 
Alan O. Sykes, The Welfare Economics of Immigration Law: A Theoretical Survey With An Analysis of U.S. Policy (September 1992). 
Douglas G. Baird, 1992 Katz Lecture: Reconstructing Contracts (November 1992). Gary S. Becker, The Economic Way of Looking at Life (January 1993). 
J. Mark Ramseyer, Credibly Committing to Efficiency Wages: Cotton Spinning Cartels in Imperial Japan (March 1993). 
Cass R. Sunstein, Endogenous Preferences, Environmental Law (April 1993). Richard A. Posner, What Do Judges and Justices Maximize? (The Same Thing Everyone Else Does) (April 1993). 
Lucian Arye Bebchuk and Randal C. Picker, Bankruptcy Rules, Managerial Entrenchment, and Firm-Specific Human Capital (August 1993). 
J. Mark Ramseyer, Explicit Reasons for Implicit Contracts: The Legal Logic to the Japanese Main Bank System (August 1993). 
William M. Landes and Richard A. Posner, The Economics of Anticipatory Adjudication (September 1993). 
Kenneth W. Dam, The Economic Underpinnings of Patent Law (September 1993). Alan O. Sykes, An Introduction to Regression Analysis (October 1993). 
Richard A. Epstein, The Ubiquity of the Benefit Principle (March 1994). 
Randal C. Picker, An Introduction to Game Theory and the Law (June 1994). 
William M. Landes, Counterclaims: An Economic Analysis (June 1994). 
J. Mark Ramseyer, The Market for Children: Evidence from Early Modern Japan (August 1994). 
Robert H. Gertner and Geoffrey P. Miller, Settlement Escrows (August 1994). Kenneth W. Dam, Some Economic Considerations in the Intellectual Property Protection of Software (August 1994). 
Cass R. Sunstein, Rules and Rulelessness, (October 1994). 
David Friedman, More Justice for Less Money: A Step Beyond Cimino (December 1994). 
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at . http://www.jstor.org/page/info/about/policies/terms.jsp . JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. http://www.jstor.org 
The Michigan Law Review Association is collaborating with JSTOR to digitize, preserve and extend access to Michigan Law Review. 
This content downloaded from 62.122.73.17 on Fri, 20 Jun 2014 00:21:00 AM 
All use subject to JSTOR Terms and Conditions 
INTRODUCTION 
Why did critical legal studies disappear? Will it reappear? Why does the Federalist Society prosper? Why, and when, do people write books on constitutionallaw, ratherthan tort law or antitrust?Why did people laugh at the notion of "animal rights," and why do they now laugh less? Why do law professors seem increasingly respectful of "textualism"and "originalism,"ideas that produced ridicule and contempt just two decades ago? How do book reviewers choose what books to review? Why has law and economics had such stayingpower? 
Academics are generally committed to truth, and they are drawn to ideas that can be shown to be good ones. The most optimistic answer to these questions is that ideas survive because and to the extent that they are true or good. On this view, law and economics has outlasted critical legal studies because it has much more to offer. Textualism and originalismhave had a resurgencebecause much can be said on their behalf. Book reviewers, in the academic domain, tend to choose to review the best books. 
In my view, these claims contain some truth, but they are far too optimistic. Academics, like everyone else, are subject to cascade effects. They start, join, and accelerate bandwagons. More particularly, they are subject to the informational signals sent by the acts and statements of others. They participate in creating the very signals to which they respond. Academics, like everyone else, are also susceptible to the reputationalpressuresimposed by the (perceived) beliefs of others. They respond to these pressures, and by so doing, they help to amplify them. It is for these reasons that fads, fashions, and bandwagon effects can be found in academia, including the academic study of law. Fortunately,the underlyingforces can sparkcreativity and give new ideas a chance to prosper. Unfortunately, these same forces can also produce error and confusion. 
* Karl N. Llewellyn Distinguished Service Professor of Jurisprudence, University of Chicago, Law School and Department of Political Science. - Ed. I am grateful to Jack Goldsmith, Tracey Meares, Eric Posner, and Richard Posner for helpful comments on an earlier draft. 
1251 
Sometimes cascades have enduring effects. But in many fields academic cascades are fragile, with numerous people focusing on issues and adopting methods that disappear in short order. Some cascades produce unpredictable and seemingly random movements, as external shocks lead in dramatic directions. In social life, small sparks cause wildfires; it is for this reason, among others, that we cannot easily predict future academic trends, or foresee new movements in the academic study of law. (In 1985, would it have been possible to predict the resurgence, in the 1990s, of interest in the study of social norms?1 Or the rise of interest in cyberspace? In the Second Amendment?) There is even a tipping point phenomenon here, in which a certain pressure, from the perceived views of others, can produce a sudden "rush" toward a particular methodology or point of view.2 
In this Essay, I attempt to cast light on the general topic of academic bandwagons and cascades, with particular reference to law. 
Several caveats are in order. First, my focus here is on trends in academic law, but informational and reputational signals are ubiquitous. 
The same forces discussed here help explain many social movements, including reactions to environmental risks, the rise and fall of communism, the success or failure of students and job candidates, the creation of ethnic identifications, and the rise and partial fall of affirmative action.3 Second, I do not mean to present any criticism of legal scholarship in general, or to depict those who produce it as especially prone to informational and reputational influences. A general attack on legal scholarship would be senseless, if only because so much of it is obviously excellent. Third, I aim only to establish the existence of cascade effects, not to give a clear test for distinguishing cascades from cases in which approaches and arguments have spread because of their merits (though some of my remarks will bear on that issue). Fourth, my treatment will be informal and anecdotal, offering examples that will, I hope, be intuitive and familiar. With respect to the underlying phenomena, I draw on some more systematic and formal treatments,4 both general and particular, and try to apply the central ideas to the academic context. Obviously a great deal might be said about this topic; my brief treatment here is intended only to be a start. 
II. 
A NOTE ON ACADEMIC UTILITY FUNCTIONS AND THE MARKET 
To make progress on this topic, it is necessary to have some sense of what academics care about, and also to know something about the nature of the market for academic ideas. On these subjects, I hope not to say anything controversial. But because some account is implicit in any description of cascades, I offer some brief notations.5 
Most academics care about what most people care about. They seek to retain their jobs and to have the good opinion of (relevant) others. Few of them are indifferent to status. But they also care, more than most, about ideas, and they are willing to forego various benefits in order to be able to think and talk about issues suitable for teaching and academic research. Many academics are interested in pursuing truth as such. Those who do or use empirical work often fall in this category, and the same is true for those whose basic goal is to help produce clarity and coherence in the law. In the context of law, there is an additional point: Many academics would like to contribute to improvements in law and society by helping to make law better in the domains of, for example, antitrust law, race or sex equality, and freedom of speech. Of course academics are a diverse lot on these counts. 
For some, reputation matters a great deal; for others, the pursuit of truth or justice is especially important. 
There is also a market for academic ideas, and this market will have significant effects on what academics do. In this market, academics are the producers, while consumers include other academics, students, government officials, judges, and law clerks. The extent of interest from these groups will of course vary with the material; some academic work, for instance, is of direct interest only to other academics. The market here is unusual in many ways, above all because no one pays directly for what academics produce. Law reviews usually do not compensate contributors for articles and essays, and the same is true for other journals (in economics and philosophy, for example) in Lisa Anderrson & Charles Holt, Information Cascades in the Laboratory, 87 AM. ECON. 
REV. 847 (1997); Abhiijit Banerjee, A Simple Model of Herd Behavior, 107 Q. J. ECON.797 (1992). See also B. Douglas Berheim, A Theory of Conformity, 102 J. POL. ECON. 841 (1994) (discussing similar mechanisms). 
several commentators on an earlier draft, who urged an elaboration of the utility function of law professors and of the market for academic ideas. Obviously I think that the commentators are right, but the fact that these topics are discussed here is itself an illustration of the forces I discuss in this Essay. which law professors might publish.Publisherswill pay for the right to publish books, and professors receive royalties, but little money is usually involved, and hence the motivation for writing books is rarely material for academics. On the other hand, indirect compensation monetary and nonmonetary- is omnipresent.Job opportunitiesare a direct function of what academics produce, and at many schools, salary is partlya reflection of quality and quantityof publications.Invitations to conferences and the like - dreaded by some, welcomed by many - are also affected by the perceived quality of academic work. 
It is here, above all, that the marketdisciplinesacademicactivity. 
In a well-functioningmarket, only or mostly valuable ideas will be produced - although of course some ideas will be valuable even if they are misleading or incorrect.But it is not at all clear that an ordinary economic market, based on the willingness-to-paycriterion, is a good way to produce valuable ideas in law or elsewhere. Such a market might well cater unduly to existing tastes or to the interests of those with a great deal of money to pay; research funding by groups with a large financial stake in outcomes is therefore a problem. The complex system of indirect compensation, alongside the tenure system, is commonly defended as a way of insulating the production of ideas from ordinarymarkets. If this complex system works extremely well (by the appropriatecriteria),it will lack "badcascades"- that is, cascades in which valueless ideas travel not because they are valuable, but because of the mechanismsthat I will be discussinghere. But I will suggest that in many contexts, an absence of private information, together with a concern for reputation and various features of human cognition, can produce academiccascades that are bad as well as good. 
III. INFORMATION-INDUCED ACADEMIC CASCADES 
Academic cascades take two forms: informational and reputational. Let us begin with the role of information. 
Most people, in most domains, lack reliable information about what is true and what is right. For this reason, they are interested in the signals of others. The point holds for the selection of movies and restaurants and carpets; it holds for ideas as well. If you are unsure whether textualism is a sensible or pernicious approach to constitutional interpretation,you might care a great deal about other people's views. Of course academics, especially older ones, are sometimes settled in their views. Often they are confident that they know what to think, and to that extent, they are not terriblysusceptible to the views of others. (Notice here that the precondition for immunityto informational influences is confidence about one's preexisting views, not adequate or accurateinformation.) But among some groups, and in some fields, any settlement is provisional and somewhat fragile. In many fields, includinglaw, young people in particularcan both influence and be influenced by informationalsignals. If many people are susceptible to influence, cascades can readily develop. The significant swings in legal scholarship over the last decades suggest that academic lawyers are indeed susceptible. 
Suppose, for example, that John, a young academic, does not know whether textualism is a sensible approach to constitutional interpretation, but that Mary, a slightly older academic, is in favor of it.6If John is otherwise in equipoise, but attaches value to what Mary thinks because she seems wise or has often been right in the past, it is easy to see how John might come to Mary's view. If John and Mary believe that textualism makes sense, Sally, a contemporary of John, might be moved to agree, at least if she lacks any reason to be confident that they are wrong. And once John, Mary, and Sally come to a certain view, David, a recently hired faculty member, will likely agree with them unless he has enough private knowledge - or, more precisely, confidence - about his antecedent view to stand in their way. At some point one or more of these people might even produce an article or book in defense of textualism. 
As stylized as this example seems, I believe that it captures a great deal about academic movements, in law and elsewhere. Consider, for example, the rise of feminism within the law schools, starting roughly in the mid-1980s.In many places, feminism appears to have succeeded through a kind of informational cascade, as people who would otherwise be skeptical or unsure came to think that feminist approacheshad something to offer - not (in many cases) because they carefullyinvestigated the underlyingclaims and believed that they were illuminating or right, but because the beliefs of others seemed hard to resist for those lacking a great deal of confidence in their own (skeptical) judgments. If so many people seemed to think feminist approaches to law were valuable, mustn't they be right?7 
Or consider the life and apparent death of the critical legal studies movement, which flourished (again speaking roughly) from 1977 to critical legal studies powerfully influenced both students and younger faculty. A significantnumber of students appeared to sense what critical legal studies was about, and they seemed to agree with it. A significant number of assistant professors (some of them now professors, with apparentlylittle continuing interest in critical legal studies) were in the same category. Within both groups, the informational signals sent by the large number of critical legal studies members were extremely important. 

participating in a cascade, rather than acting independently. 
At the University of Chicago Law School, much the same can be said, then and now, about the economic analysis of law. Many faculty members engage in economic analysis of law, and a majority of the faculty shows considerableinterest in the basic approach.As a general rule, younger faculty members are especially interested in the informational signals sent by their colleagues, and at Chicago, many of them end up doing work that is influenced by economics. Cascade effects are even easier to observe within the student body, as certain concepts (involving, for example, the value of efficiency, the implications of the Coase theorem, and the futility of redistributive regulation) spread as if by contagion. Of course it is true that many students, and some faculty members, show no interest in the economic analysis of law. But mere exposure to economic thinking, voiced in many settings (including workshops, lunch discussions, and comments on articles), leads in the expected directions. 
As informational cascades develop, people end up amplifying the very informationalsignals to which they have responded. Scholarship, including the production of articles and books, is much affected by processes of this kind. If this is so, it is possible to predict, with some confidence, that the publication of books on various topics or with various methodologies often will be highly concentrated over time, showing that fads and fashions play a role in the academic world as elsewhere. 
In making these claims, I do not mean to make any normative claims about feminism, critical legal studies, or economic analysis of law. Nor do I mean to suggest that those who are vulnerable to informational pressures are weak or irrational.People who know that they have limited information certainly should pay attention to the signals given by others. And whether pressures of this kind will lead in desirable directions cannot be decided in the abstract.All that can be said is that the underlyingmechanismsgive little reason for confidence that academic "movements" will be good ones. Ideas can spread, even among people with some expertise, despite the fact that little is to be said on their behalf. 
From these examples, we can also see the possibility of purely or mostly local informational cascades. Outside the academic world, some communities come to believe that abandoned hazardous waste dumps are extremely dangerous, whereas others think that they pose no hazardat all. So, too, some law schools might come to embrace the economic analysis of law, whereas others might see enthusiasm for traditional doctrinal analysis - not because of a large number of independent judgments, but because of mutual interactions and influences. 
IV. REPUTATION AND REPUTATIONAL CASCADES 
It should be clear that something importantis missingfrom the picture:people's concerns about their reputations.8Generally people care about what others think of them, and most academics are, on this count, like most other people. 
In many law schools and economics departments,an effort to show that centralized planning really can work well, or to vindicate socialism, or to show that people are irrational,would be very risky,no matter the quality of the relevant work. People might be ridiculed. They might well jeopardize their careers. At many law schools, the same would be true for people who attempted to show that current differences between men and women are biological rather than social, and to bringevidence to that effect to bear on legal issues. 
Reputational considerations influence the public behavior of most people, not excluding academics. This is not because people lack integrity, or are sycophantic, or are unwilling to follow their own paths. 
It is simply because most people, most of the time, want others to think well or at least not ill 
of them. Of course, people have varyingsusceptibilityto reputationalpressures.Some people can withstand a great deal; others will be inclined to take the safe course, showing reluctance to say, or especially to publish, anythingthat could create trouble for them in the future. And of course sometimes those who incur reputationalsanctions in one place (the nonacademicworld, for example) will reap reputational benefits elsewhere (perhaps their local academic community). Those who seem to be venturing out on their own, and to be "brave,"might in fact be motivated by the goal of gainingstatus within a particulargroup. 
Because most people care about the views of others, and because people have varying, rather than uniform, susceptibility to reputational pressures, it is easy to imagine reputational cascades with respect to actions or stated beliefs.9Suppose, for example, that A and B would think ill of anyone who argues that the minimum wage should be significantly increased. C, who is not sure what to think about a higher minimumwage, might be unmoved privately by the views of A and B, but nonetheless might not want to incur the wrath of A and B, or to seem ignorant of basic economic principles, or to appear indifferent to economic efficiency. If so, C might show no enthusiasm for an increase in the minimum wage, or might even agree with A and B that an increase would be a bad idea. If D is otherwise in equipoise, she might be most reluctantto oppose A, B, and C publicly.Mounting reputationalpressuresmight well lead E, F, G, and H, and many more, 
(2000). 
to join the bandwagon. Eventually a large number of people might speak as if the minimum wage should not be increased. The result would be to affect academic discussion of government's role in the labor market, includingthe treatmentof this topic in articlesand books. 
Here, too, a highly stylized example seems to help account for many shifts in the academic world. The rise of feminism within legal academia undoubtedly has a great deal to do with reputationaland informational incentives. In the early 1980s, those who expressed contempt for feminist scholarshipwere rarely punished for doing so, and were sometimes rewarded. Currentlythose who express contempt for feminist scholarshipgenerally (of course not always) put their reputation in considerable danger. If a young academic chooses to write on certain topics, or from certain points of view, the reputational sanctions might be quite severe. At the University of Chicago Law School, I cannot recall many faculty members expressing public support for a substantialincrease in the minimumwage, though I would not be surprised if more than one faculty member actually believes that such an increase would be a good idea. Five years ago, those who borrowed from behavioral economics were viewed with considerable suspicion inside the world of law and economics; through a cascade effect, this is decreasinglytrue. 
It follows that "politicalcorrectness"is hardly a narrowphenomenon involving the practices of left-leaning academics. Wherever reputational pressures are in place, a form of "political correctness" will discipline action and public statements. Reputational cascades are a possible consequence. 
V. 
GROUP POLARIZATION AND ACADEMIC "SCHOOLS" 
A closely related phenomenon helps explain the initial growth of academic fashions and gives some guidance on how to create, and how not to create, an academic "school."The phenomenon is that of group polarization.'?In brief, the idea behind group polarizationis that when a group of people engages in deliberation, group members will move toward a more extremeposition in line with theirpredeliberationinclinations. This is the typical pattern among deliberating bodies. Thus, for example, a group of Federalist Society members, inclined to support originalism, is likely to be extremely enthusiastic about originalism after discussing it with one another. So, too, a semiformal organization of law professors,meeting once a month, is likely to emerge with a stronger commitment to critical race theory if its members are inclined, before discussion, to be favorably disposed toward critical race theory. It would be easy to multiplyexamples. 
Deliberative Trouble: Why Groups Go to Extremes, 110 YALEL.J. 71 (2000). 
Massive evidence, from many different countries, supports the basic prediction.Why does group polarizationoccur?Though no cascade need be involved,1lthe two principalexplanations are close to the explanations for informational and reputational cascades. The first involves informationalinfluences. In a deliberatinggroup with an initial tendency in favor of X and against Y, there will be a disproportionate number of arguments in favor of X, simply because most people will speak out on behalf of X. Group members will have thought of some, but not all, of the argumentsin that direction. After deliberating, the argumentsfor X will seem stronger to individualmembers, and the arguments for Y will seem even weaker. It is to be expected that discussion will move people to a more extreme form of their original enthusiasm for X. 
The second explanation for group polarization points to social influences. Most people, emphatically including professors of law, care about their reputations and their self-conception. Suppose, for example, that you are inclined to think that affirmativeaction does not offend the Constitution, but you are not entirely sure; suppose too that you find yourself in a group that also rejects the idea that affirmative action offends the Constitution. If you think of yourself as the sort of person who is, more than most, inclined to support the constitutionality of affirmative action programs, you might move a bit, if only to maintainyour reputationwithin the group and your self-conception on the issue at hand. The evidence strongly supports the proposition that this happens.12 
In the academic context, the lesson is simple. A group of likeminded people, thinking about some issue or topic, is highly likely to move toward a more extreme position, not merely fortifying but amplifyingtheir predeliberationinclinations.Throughthis route, it is possible to make some progress in understandingthe creation and effects of academic "schools."In the early 1980s, for example, the critical legal studies movement flourished at HarvardLaw School in particular, no doubt in part because of the presence of members who talked a great deal with one another and fueled their predeliberation inclinations. Several influential books emerged from these discussions.13In roughly the same period, the Federalist Society was created at Chicago and Yale, and the existence of a group of like-minded people undoubtedly helped to fuel certain commitments. In fact, it is reasonable to speculate that the growth of conservative legal thought, within both faculties and student groups, has had a great deal to do with the exis 
influences on group members. 
tence of a collection of people who are relatively well-organized and who are able to ensure that like-minded people can find some kind of home. 
VI. QUALIFICATIONS, EXTENSIONS, IMPLICATIONS 
Informationaland reputationalinfluences, as well as group polarization, play a significantrole in academic life. Cascade effects are present here as elsewhere. For this reason, we should expect a large number of fads and fashions in the academic study of law. I would predict, for example, that a citation analysiswould show many academic "bubbles" - rapid rises and declines in references to certain ideas and people.14But this basic sketch omits importantparts of the overall picture. 
With respect to both informational signals and reputational pressures, all people are not created equal. Some carry more weight than others. For example, the signals sent by well-known academics, and academics at well-known schools, are likely to be especially loud. If faculty members at Yale end up endorsing a new method for understanding law, there might seem to be particularlygood reason to take that method seriously. And it is less likely that people who embrace the method will face the kind of reputational sanction that could be imposed if the method were being used at a little-known school. Those who are in a position to start cascades operate as leaders, above all because of the social amplificationof their voices.15Note that this amplification can occur independently of the merits of the argumentbeing made. In listening carefullyto well-known people, or to people at wellknown schools, followers are probably behaving rationally, because such people are unusuallylikely to be interesting or correct, simply as a statistical matter. But there are no guarantees here, and hence arguments can be amplified even if they are meritless. (Perhaps the resulting bubble will eventually pop, as discussedbelow.) 
Some of the relevant leaders are simply saying what they think to be true; others affirmatively want followers, perhaps because they seek status, or perhaps because they want to ensure that their ideas are disseminated. Such people take steps self-consciously to promote cascade effects, perhaps by organizingconferences, reading groups, or even journals. More specifically, we can describe as "polarizationentrepreneurs" those people who foster deliberative groups of like14. Some support can be found in Robert Ellickson, Trends in Legal Scholarship: A Statistical Study, 29 J. LEGAL STUD. 517, 527 (2000). 
minded people and ensure that participantsshare a common methodology or point of view. Exclusion of outsiders, and inclusion of a large number of insiders,is an importantcomponent of this strategy. 
An obvious implication is that if the goal is to spread ideas, it is probably best to begin by promoting discussion among groups of likeminded people. If members of such groups speak mostly or only to one another, views might become entrenched, and the entrenchment among the views of increasinglylarge groups might initiate a cascade effect. A much worse strategy - often a doomed strategy - is to ensure that people with new ideas are placed in heterogeneous groups, where their ideas are unlikely to travel, or might be squelched, or might even be subjectto self-silencing. 
In fact, the forces here are compounded by another: the availability heuristic. It is well known that certain facts and ideas are cognitively "available,"or highly salient, and that this cognitive availability can exert a large influence on beliefs and decisions.16If a leader, or an idea, ends up widely known, through independent decisions or through cascade effects, dramatic changes in scholarly paths can be expected. 
Some people are relatively immune to the influences discussed here. As I have suggested, people who are confident about their views are especially likely to resist informationaland reputationalincentives. The point suggests that in some arenas, cascades are likely to arise quite infrequently. Academic areas are highly variable on this count, and academicsin some domains have a great deal of confidence, which immunizes themselves from cascade effects. In fields with wellestablished methods and goals, we should expect cascades to be uncommon. In the sciences, for example, large-scale shifts certainly occur, but the existence of settled methods makes cascade effects unlikely7 - far less probable than in, for example, comparative literature. Law, economics, and psychology are perhaps intermediate cases. 
This point raises an important question: When and why do academic cascades start and stop? A crucialreason has to do with external shocks. Suppose, for example, that a group of people believes some fact. Suppose that evidence shows that the belief is false. The belief will fade because it has been demonstratedto be wrong. 
But external shocks can take many different forms. Sometimes academic trends, especially in law, have nothing to do with demon16. See JONATHANBARON,THINKINGANDDECIDING(3d ed. 2000). 
REVOLUTION(1970). strated fact, but are greatly affected by what happens outside of the academic domain. For example, the selection of Antonin Scalia to be a member of the Supreme Court undoubtedly had a great deal to do with the legitimation of originalism and textualism, methods favored by Justice Scalia. This is partly because Justice Scalia's opinions provided a kind of focal point for academic debate; it is also because his office conferred a kind of legitimacy on arguments that might otherwise be easy to dismiss.Nor is it irrelevantthat some of Justice Scalia's law clerks became academics. In fact, a significant source of informational and reputational influences will come, directly and indirectly, from the selection of Supreme Court clerks, and from the choice, among clerks of particularjustices, to become law professors. In a previous generation, the law clerks of Felix Frankfurter, greatly influenced by Frankfurter,became influential academics;the same appears to be true of Scaliaclerks today. 
More generally, the 1980 election of President Reagan made it most unlikely that the Supreme Court would continue to use the Equal Protection and Due Process Clauses as bases for announcing a series of new rights for disadvantaged people. Sensibly enough, academics interested in social reform showed decreasinginterest in elaborating legal doctrine for that purpose. Perhaps the rise of interest in constitutional deliberation outside of the courtroom had something to do with the Court's lack of receptivity to the professors' arguments. Perhaps diminished interest in judicial review had something to do with the changing composition of the Court.18Highly visible public events with legal dimensions, such as the 1998 Clinton impeachment and the 2000 postelection struggle between George W. Bush and Al Gore, will inevitably affect people's choice of what to write about. (Perhaps Bush v. Gore will inaugurate a new era of neorealism, questioning the division between law and politics.) Academics may or may not follow the election returns,but in law, the election returnscan set the academic agenda. 
Other external shocks can come from developments in adjacent fields. If, for example, economists show a great deal of interest in the idea of spontaneous ordering, academic lawyers are likely to show an interest in that topic, too. Part of the reason is informational:the fact that a certain topic interests economists is likely to be important to academic lawyers,many of whom care about what economists think. If there is a resurgence of interest in utilitarianismwithin philosophy, law professors are likely to write about utilitarianism.The extraordinary interest in the work of John Rawls confirms this point. Critical theory provides another case in point, with Jurgen Habermas and Michel Foucault, for example, exerting a significantinfluence on legal scholarshipby virtue of their prominence within closely related fields. Of course, developments within adjacent fields might well be a product of the kinds of influences discussed here. 
There is a final issue, perhaps in tension with the general argument offered thus far. It is useful to distinguishbetween ideas and methods on which multiple people can build for a long time, and ideas and methods that do not lead to much in the way of further work.19The notion that people are rational,self-interested profit maximizersis fertile, in the sense that it has applications to many domains of law, helping to produce predictions that can be tested and used. Though it is too early to say, I believe that the same is true for the notion that people are boundedly rational, and also for the claim that people are not only self-interested.20The idea that law is pervasively based on male practices and understandingsis also easily used as a basis for assessing, or reassessing, many domains of law. But some claims tend to "burnout," in the sense that once they have been voiced, there is little that can be done with them, even if they are true. Perhaps this is the case for the contention that law is "political,"an importantand illuminating partial truth,but one with which it is not easy, in the aftermath of legal realism, to do a great deal of illuminatingfurtherwork. 
CONCLUSION:THE MARKETPLACEOFIDEAS 
Academics, like everyone else, are susceptible to informational and reputational influences, and cascade effects are as likely to be found in the academic domain as elsewhere. Notwithstanding the expertise and confidence of many academics, academic life has its own fads and fashions, and the factors discussed here play a role in their development. I believe that these factors have played a role in many trends in legal theory, including critical legal studies, economic analysis of law, feminism, textualism and originalism in constitutional law, critical race theory, rights-based accounts associated with Ronald Dworkin and others (many at New York University), law and literature, and (more recently) behaviorallaw and economics. 
By way of conclusion, it is worth emphasizing that the basic account contains both a prescription and a cautionary note. The prescription is that those who seek to promote ideas will do best to ensure, above all, that those ideas have an opportunity to develop through frequent discussions among like-minded people. Most would19. Cf. the discussion of progressive and degenerate research programs in Imre Lakatos, Falsification and the Methodology of Scientific Research Programmes, in CRITICISMAND THE GROWTHOFKNOWLEDGE(Imre Lakatos & Alan Musgrave eds., 1970). 
ANDECONOMIC(SCass R. Sunstein ed., 2000). be "schools"fail, but those that succeed often transformthe field; and when they do so, grouppolarizationis part of the reason. 
The cautionarynote is that in law and many other academic fields, ideas may spread and prosper,not because they are good, but because dozens, hundreds, or even thousands of imperfectly informed people have fortified the very signals by which they have been influenced. Whether bad ideas can prosper for a long time is another matter. Frequently good arguments and good evidence will puncture them, at least when there is agreement about the underlyingcriteria.But if the account here is correct, longevity, even for bad ideas, is hardly out of the question. 
The legal culture contains countless discussions of the vices and virtues of cost-benefit analysis.' But it has hardly any explorations of the real world of cost-benefit analysis-of what agencies do, concretely, when they do costbenefit analysis. We know very little about the interactions among law, science, and economics in the actual, rather than theoretical, world of cost-benefit balancing. 
The Arithmetic of Arsenic2 represents a modest effort to begin to fill this gap. My goal was to explore not whether cost-benefit is a good idea in the abstract, but what it means in practice. In fact my major purposes were descriptive and conceptual, not normative. I wanted to highlight a topic about which lawyers and judges know too little: the specific questions whose answers can drive "benefits numbers" in one or another direction. For low levels of arsenic, we lack clear knowledge of the likely health benefits, and any extrapolation from the data will depend on some disputable judgments, involving above all: (1) the dose-response curve, (2) the use of data from other nations, and (3) the decision whether to quantify certain risks on the basis of highly speculative evidence. Monetization of benefits raises further problems involving the valuation of both life and health and the appropriate discount rate for cancers to be averted in the future. With an understanding of these problems, we can see exactly why there are reasonable disputes about both nonmonetized and monetized benefits-why some estimates are high and others are lower. We can also isolate the issues on which more progress, both conceptual and empirical, might be made in the future. 
In identifying those issues, I did not seek to resolve them, much less say how the arsenic controversy should be handled (though in a few places I offered some tentative judgments). I meant instead to provide a kind of lawyer's primer on the real world of cost-benefit analysis, in the process showing where legal objections might be raised, explaining why a deferential judicial role might make sense, and indicating why a "benefits range," and a wide one at that, is sometimes the most that can be offered. 
I am extremely grateful to Lisa Heinzerling and Thomas McGarity for their valuable comments and criticisms. Their objections seem to me to fall into two * Karl N. Llewellyn Distinguished Service Professor of Jurisprudence, Law School and Department of Political Science, University of Chicago. 
PERSPECTIVES (Matthew D. Adler & Eric A. Posner eds., 2001). 

in knowledge. Thomas 0. McGarity, ProfessorSunstein's Fuzzy Math, 90 GEO. L.J. 2341 (2002). 
2379 categories-the first involving the assessment of benefits (my major theme, and a topic emphasized by Heinzerling), the second involving the assessment of cost-benefit analysis itself (which both of them dislike). In this brief reply, I take up these two kinds of criticisms in sequence. 
I. ASSESSING BENEFITS 
Much of Heinzerling's response urges that the EPA should have placed a higher monetary value on life. She emphasizes, for example, that the value-oflife data is quite old (from the 1970s). In her view, per capita growth in economic activity requires a significant upward adjustment.4 She also contends that the EPA should have used a cancer premium on the ground that cancer risks receive a higher willingness to pay than other deaths. These are entirely plausible claims and well worth further investigation. 
But some of Heinzerling's other suggestions seem to me more doubtful. She contends, for example, that because cancer is involved, the EPA was wrong to have concluded that the benefits of the arsenic rule should be discounted because cancer arises decades after exposure to its cause.6 To be sure, it is hard to know how the discounting problem should be handled. But isn't it much worse to get cancer tomorrow than to get cancer twenty years from tomorrow? Isn't it much worse to die in a plane crash tomorrow than to die from cancer twenty years from tomorrow? Heinzerling is right to emphasize the existence of a cancer premium. But even with that premium, it seems reasonable to discount risks that will not turn into harms for many decades. 7 
I also wonder about Heinzerling's suggestion that the EPA should have adjustments, I agree with Heinzerling that poor people's lives should not be valued less than the lives of wealthy people. The complexity here is that poor people are willing, because of their poverty, to spend less to reduce a statistical risk than wealthy people and that it is not clear that government does poor people any favor by forcing them to spend more than they are willing to pay. (This assumes that people are adequately informed and that individuals are being required to pay for risk reduction, which is true in the case of arsenic but not, fortunately, in every case.). 

meaning the lag between a reduction in exposure and a reduction in risk. See ARSENIC RULE BENEFITS REVIEW PANEL, U.S. ENVTL. PROT. AGENCY, ARSENIC RULE BENEFITS ANALYSIS: AN SAB REVIEW 6-7 (2001), available at http://www.epa.gov.sab/ecO1008.pdf. Imagine that those now being exposed to 45 parts-per-billion (ppb) of arsenic are exposed in 2006 to 10 ppb. If so, it will take a while until cancer risks will be equivalent to what they would be if everyone, all of the time, was exposed to 10 ppb or less. Because the EPA did not take account of the cessation lag, the Science Advisory Board suggests that it presented an upper bound on mortality and morbidity savings. Id. at 5. 
involuntarily should be valued especially highly. See Heinzerling, supra note 4 at 2327. To be sure, the line between voluntarily and involuntarily run risks is one of degree, not of kind. See Cass R. Sunstein, The Laws of Fear, 115 HARv. L. REV. 1119, 1154-55 (2002) (book review). When we say that a risk is involuntarily run, we typically mean that those who run it are unaware of it, or that it is very costly for people to avoid it. See id. at 1154. But when the costs of risk-avoidance are especially high, it makes sense to devote extra resources to reducing it. Of course Heinzerling is right to say that there is no market for risks that are involuntarily run in the strong sense. assessed the benefits of reducing arsenic in drinking water by examining the market for bottled water.8 As Heinzerling notes, many people buy bottled water for the taste, not for the increase in safety. In any case, most people do not buy bottled water. It would be extremely odd for government to use the Safe Drinking Water Act to do the equivalent of forcing people to buy bottled water when this option is rejected by most people. (Should the National Highway Safety Traffic Administration require all car manufacturers to spend the amount spent on safety by Volvo owners?) 
Heinzerling also suggests that instead of using a number for statistical lives, the EPA might have done better to survey consumers, asking them how much they would be willing to pay to reduce arsenic levels in drinking water.9 She thinks that such a survey would reveal that people would be willing to pay more than enough for significant reductions. As it happens, I conducted just such a survey with University of Chicago law students. The survey produced a median willingness to pay $100 to eliminate a 1/100,000 risk and a median willingness to pay $50 to eliminate a 1/1,000,000 risk.' ° These numbers, among a fairly affluent group, do not suggest that the EPA's choice was as easy as Heinzerling indicates, because the mean cost per household, for the 10 ppb standard, ranged from under $1 to over $300.11 Of course there are serious doubts about the meaningfulness of people's answers to survey questions of this kind, especially when low probabilities are involved. 12 Hence it was reasonable, it seems to me, for the EPA (assuming that it was interested in willingness to pay) to use its standard dollar amount per life saved, rather than to rely on surveys. 
McGarity emphasizes the EPA's legitimate difficulties in handling adverse effects that it considered nonquantifiable, including certain cancers, hypertension, diabetes, and reproductive effects. He is concerned that these effects "fade in and out of' my analysis "like an aircraft warning beacon"-and also that 
Law, 112 YALE L.J. 61 (2002). 
analysis have strengthened the case for reduction below 20 ppb. See SUMCOMM. ON ARSENIC IN DRINKING WATER, NAT'L RESEARCH COUNCIL, ARSENIC IN DRINKING WATER: 2001 UPDATE (2001). 
Heinzerling engages in some clever framing to make the issue seem easy. She compares a 20 ppb standard to the more protective alternatives-an illuminating exercise to be sure, but one that does not adequately describe the EPA's choice, which was whether and how to depart from the 50 ppb standard. She also calculates the amount that consumers would be willing to spend per day, finding the very low number conclusive on the question. But this kind of framing proves too much. See Sunstein, supra note 2, at 2295. There are countless worthy causes that might reasonably ask Americans to spend, say, a dollar a day. Assuming that 250 million Americans participate, each dollar-a-day program could raise $91.5 billion. Is it worth spending $91.5 billion to clean up hazardous waste dumps? To reduce benzene emissions? To reduce greenhouse gases? To immunize children? To improve education in poor neighborhoods? To house people who need housing? To improve national defense? To increase the number of police officers on the streets? To do all of these eight dollar-a-day programs, raising over $720 billion? To do more? once all of them are included, my low-end estimates "are too low" and my high-end estimates "may only be slightly too high or perhaps not high enough."13 All of these points are reasonable. My basic suggestion is that benefits should be quantified if this is at all possible.' 4 In general, the EPA should assign numbers to adverse effects whenever it can, if only to provide a range, and it should have done more in this vein here. In this case, as in many others, efforts at quantification illuminate the analysis, even when there is uncertainty.' 5 
Heinzerling dislikes the willingness-to-pay criterion. Her most interesting claim is that market behavior is likely to be inadequately informed, not least because people are "intuitive toxicologists," acting in the market domain as they do in politics. 16 For this reason she doubts that market evidence reveals people's informed tradeoffs between dollars and statistical risks. Heinzerling is right to say that cognitive problems and simple ignorance play a role in the market domain; her Perrier example might well be a case in point. The real question, an empirical one, is whether some correctives might not be provided by the sheer number of people involved in market decisions. Many of us do not know whether products we buy will do what they are supposed to do; yet the knowledge of some is enough to ensure, most of the time at least, that there is a correlation between quality and cost. In the domain of risks, it is possible that market processes will work in the same way, ensuring, much of the time, that risky products will cost more than less risky ones, and that other things being equal, employers will have to pay people a wage premium for higher risks. The evidence here is suggestive if not conclusive, indicating that in markets, at least, people will not pay infinite or even huge amounts to eliminate low-level cancer risks. Indeed the amounts they are willing to pay fall within a certain range, one that is wide but far from unbounded.' 7 Of course, Heinzerling is right to question whether an examination of market behavior can uncover informed judgments about the price of a statistical life. 
II. ASSESSING COST-BENEFIT ANALYSIS 
To say the least, Heinzerling and McGarity are skeptics about cost-benefit analysis. I do not share their skepticism. In my view, cost-benefit analysis should be seen as a valuable tool, partly as a way to counteract the errors that we all make in thinking about risk, partly as a mechanism for ensuring that government addresses serious problems rather than trivial ones, partly as a way 
EPA. See ARSENIC RULE BENEFITS REVIEW PANEL, supra note 6, at 3. 
rate, $4.5 million is a sensible value for statistical lives in this context. McGarity, supra note 3, at 2355-56. While discounting seems to me sensible here, I am not at all wedded to the $4.5 million number and would welcome alternative suggestions. 
of getting a sense of the consequences in front of decisionmakers before they act. 8 Is this so terribly controversial? Is it not revealing that President Clinton, no less than Presidents Reagan and Bush, required agencies to produce costbenefit analyses of major rules? 9 
To be sure, cost-benefit analysis may be unhelpful or misused in practice, simply because of the indeterminacy in the data. A lurking question, pressed by both Heinzerling and McGarity, is whether the arsenic controversy supports or undermines the argument for cost-benefit analysis. 20 I think that the analysis was helpful, simply because it showed the range of potential effects from low levels of arsenic exposure. The analysis greatly weakened the efforts of interest groups and others to treat the old 50 ppb standard as unexceptionable. After the EPA did its work, no one could responsibly contend that it would be silly to rethink the old standard. Indeed, the analysis--especially taken in light of the 2001 report from the National Research Council 2 '-helped show that the 10 ppb standard would not be exceedingly costly and likely would be a real improvement, in terms of public health, over the 50 ppb standard. At the same time, the analysis showed why reasonable people might be uncertain whether there would be very large public health gains from a standard below (say) 20 ppb. (Note in this regard that Canada, a nation not unconcerned with the health of its citizens, has a standard of 25 ppb.2 2 ) 
Heinzerling's attack on cost-benefit analysis seems to be based not on a belief that costs and benefits are irrelevant, but on the willingness-to-pay criterion, which she identifies with cost-benefit analysis. The identification is understandable: Those who do cost-benefit analysis generally use that criterion. But to engage in cost-benefit analysis, we can value benefits however we like. In the 1960s and 1970s, it was popular to use the "human capital" approach, which values life by looking at lost earnings.23 This method, still popular within the courts, produced amounts that were a mere fraction of those elicited by willingness-to-pay methods. Or suppose that a contemporary analyst rejected willingness to pay and instead chose, for her own reasons, $15 million as the presumptive amount to spend per statistical life-with the presumptive amount subject to increase or decrease if the circumstances warranted either.24 Such an analyst could still compare costs and benefits. We might even start with the amounts that emerge from labor market studies, not because we have a deep 179 (1999); CASS R. SUNSTEIN, RISK AND REASON (forthcoming 2002). 

people were mostly at risk. Of course the life-years criterion could automatically incorporate this variable. Note that the Science Advisory Board sensibly asked the EPA to consider the "age distribution of cases avoided whenever possible." See ARSENIC RULE BENEFITS REVIEW PANEL, supra note 6, at 5. commitment to willingness to pay, but because of a democratic judgment that those amounts provide a reasonable place to begin. What seems to me most important is not to be dogmatic about willingness to pay, but to ensure against uninformed stabs in the dark and to promote coherence and sense in regulation-so that we are not spending small sums on large problems and large sums on small problems. 
McGarity thinks that the largest lesson of the arsenic controversy involves the "daunting scientific uncertainties" that plague cost-benefit analysis.25 In view of these uncertainties, he thinks that cost-benefit analysis is a matter of "frequently preposterous and always manipulable number spinning.' 26 He has a point. But it need not be a matter of "spinning." We might try instead to identify the likely range of effects, and when the range is large, we might want to know why the benefits might be small and why they might be big. 27 If we are choosing among several levels of protection, what should we do instead? Guess? Flip a coin? Does it really make sense to conclude (as both Heinzerling and McGarity seem to do) that whenever people are being exposed to a carcinogen (any carcinogen?), the government should regulate to the point that is "feasible" for industry28-- even if the cost is (say) $900 billion and the health benefits are likely.to 29 be trivial? 
I suspect that both Heinzerling and McGarity think that cost-benefit analysis fails cost-benefit analysis. They believe that all things considered, we would be better off with some other standard-for example, basing standards on feasibility. They might be right! If cost-benefit analysis simply makes it harder for agencies to protect the public, and mostly increases the power of regulated groups to block desirable regulation, it is hard to celebrate cost-benefit analysis. But there are reasons to believe that cost-benefit analysis is not simply an antiregulatory tool. Indeed cost-benefit analysis helped spur the removal of lead from gasoline and dramatic steps, pushed by the United States, to eliminate 
federal law. See, e.g., Clean Air Act § 108, 42 U.S.C. § 7408 (2000). Often a margin of safety makes sense to prevent risks that are possible but not demonstrable. But sometimes there are risks and costs on all sides of the equation, and when a margin of safety itself creates risks and costs, a high margin of safety can be a bad idea. See INDUR M. GOKLANY, THE PRECAUTIONARY PRINCIPLE: A CRITICAL APPRAISAL OF RISK ASSESSMENT (2001). 
to do 'the best we can' to protect human health from environmental contaminants, especially those that result from profit-making human activities." McGarity, supra note 3, at 2374. The difficulty arises if the costs imposed on "profit-making human activities" turn out to be significant human costs, in the form of lower wages, decreased employment, higher prices, or even poverty. If the costs merely mean reduced profits, there is much less to worry about. 
that are based on an inquiry into technology. See, e.g., 33 U.S.C. § 1314(b)(2)(B) (2000). The problem is that benefits are not assessed and that there is no direct balancing of costs and benefits. chlorofluorocarbons, which contribute to depletion of the ozone layer.30 More recently, the Office of Information and Regulatory Affairs (OIRA) has pioneered the idea of a "prompt letter"-letters designed to promote agencies to act in cases in which the benefits of action seem to outweigh the coStS. 3 ' Inspired by tentative cost-benefit analysis, OIRA has asked the Occupational Safety and Health Administration to consider requiring the placement of automatic defibrillators in workplaces, has urged the Food and Drug Administration to issue a final rule requiring disclosure of the level of trans fatty acids in foods, and has asked the Department of Transportation to take steps to improve automobile safety by establishing a high-speed, frontal offset crash test. In any case, cost-benefit analysis has often driven policy in more sensible directions, by showing the best means of achieving regulatory goals.3 2 
It remains important to ask whether cost-benefit analysis can be defended on normative grounds. But for better or for worse, cost-benefit analysis is here to stay. We need to be asking a number of more concrete questions. Where does science leave gaps, and what gaps does it leave? What are the sources of high benefit judgments and low benefit judgments? How does the dose-response curve matter? How should we think about harms that will not occur for decades? Are there systematic distortions on the cost side, and if so why? An investigation of the arsenic controversy casts light on these questions. But there is a lot more to do. 
1997). 

IS COST-BENEFIT ANALYSIS FOR EVERYONE? 
INTRODUCTION 
Sometimes an initiative in law and policy receives near-universal support, on the ground that all will be helped and none will be hurt-or at least that most will be helped and few will be hurt, and those who are hurt lack a reasonable ground for complaint. 
Something of this sort happened in the 1970s and 1980s, when a consensus developed around an idea that had been extremely controversial: the airline, trucking, and railroad industries should be deregulated. For the most part, deregulation has been a spectacular success, producing lower costs and better performance for consumers, while at the same time increasing jobs and raising wages.' Something similar happened in the 1990s, when a consensus developed around a formerly controversial view: that in many domains, regulation via economic incentives, such as emissions trading systems, should replace regulation via national commandand-control. Here too the evidence has been exceptionally encouraging, 2 justifying a kind of presumption, rebuttable to be sure, in favor of incentive-based approaches. 
Might the same thing happen for cost-benefit analysis (CBA)? In a sense, it already has. Presidents Reagan, Bush, and Clinton have all issued executive orders requiring agencies to pay close attention to the costs and benefits of regulation, and it is highly likely that future American presidents, regardless of party, will continue on this path. Congress has shown great interest in requiring accounts of the costs and benefits of regulation, * Karl N. Llewellyn Distinguished Service Professor of Jurisprudence, University of Chicago. 
(unpublished manuscript, on file with author) (discussing implications of deregulation versus prior monopolistic practices). 
PROGRAM 3-4 (2000) (describing increased usage of emissions trading and related approaches and growing disfavor toward centralized command-and-control pollution control methods). [53:1 and new legislation attests to this interest? While Congress has not enacted a proposed "supermandate" that would require all agencies to make costbenefit analysis the basis for decision, statutory law seems to be moving in this direction. Federal courts are questioning regulatory absolutism of any kind, and both permitting and requiring agencies to engage in careful balancing of both sides of the ledger. A set of "cost-benefit default rules" is now in place in federal administrative law.5 
Nonetheless, it would be premature to say that CBA has received the kind of social consensus now commanded by economic incentives and deregulation of airlines, trucking and railroads. Like Judge Williams, I believe that CBA should command such a consensus, at least as a presumption, and that the presumption in favor of CBA should operate regardless of political commitments. In short, a suitably devised system of CBA is for everyone--committed environmentalists, as well as those who think of environmentalism as a form of hysteria; people who believe that markets generally succeed, as well as those who believe that markets frequently fail; and people who think that workers deserve much more protection, as well as those who think that worker protection programs have gone much too far. Note in this connection that it was CBA that helped legitimate one of the largest environmental programs in American history-the complete phase-out of lead in gasoline. CBA also played a key role in federal protection of the ozone layer, through the phase-out of CFC's, which was imposed when the Council of Economic Advisors, under President Reagan, concluded that in light of the likelihood of numerous additional skin cancers and cataracts, severe, even draconian regulatory steps would be a complete bargain. 
But while I think that a presumption in favor of CBA shouldcommand a consensus, I am not sure that it will, and I aim here to identify the reasonable grounds to resistance. In the end I am broadly in agreement with Judge Williams' remarks. I think that for government regulation in many domains, there should be a firm rule in favor of engaging in CBA, and a presumption in favor of making CBA the basis for decision. At the same time, I believe that Judge Williams has not sufficiently engaged certain objections to CBA-institutional in character-which might rebut the presumption. The basic problem is that CBA will sometimes fail CBA. My aim here is both to extend Judge Williams' claims by making some additional arguments in their favor, and also to uncover some of the strongest grounds for objecting to CBA in some contexts. Here I suggest that we are ending a "first generation" debate about whether to do CBA, with general victory for the proponents. The "second generation" debate involves not whether to do CBA but how, and it is here that crucial issues remain. 
These remarks are organized into six parts. Part I briefly sketches a simple argument for CBA, grounded less in neoclassical economics than in common sense, with an assist from cognitive psychology and democratic theory. Part II deals with some competing approaches, urging that they are inferior to CBA. Part III explores some theoretical objections to CBA, showing how the economic approach has serious limitations, but also that a reasonable form of CBA can defuse those objections. Part IV sets out more pragmatic concerns. Part V is a conclusion. 
Many of the most popular defenses of CBA come from neoclassical economics. 6 For economists, goods, including the goods to be provided by regulation, should be measured in accordance with private "willingness to pay." It follows that we can assess the value of proposed regulations by comparing their aggregated costs to their aggregated benefits, thus measured. Studies suggest, for example, that the value of a statistical life, measured in terms of private willingness to pay, is between $3 million and $7 million.7 If an air quality regulation will save twenty lives per year, it will produce between $60 million and $140 million in benefits. Suppose that the regulation would also produce $40 million in other benefits, because of the morbidity and aesthetic gains that it would generate. If the same regulation would cost $200 million, it would fail CBA. 
But there are some problems with using private willingness to pay as the basis for assessing regulatory benefits. Some of these problems are internal to the economic framework; some of them amount to a rejection of that framework. I will spell them out in some detail in Part III below. For the moment, let us simply notice that Judge Williams says little about how to value the goods at stake in regulation, and rests on the (plausible but unexplained) intuition that valuation will be possible and perhaps not terribly controversial. 
and attempted response, see generally Matthew D. Adler & Eric A. Posner, Rethinking CostBenefit Analysis, 109 YALE L.J. 165, 167-69 (1999) (defending usage of CBA by arguing that CBA (i) is a decision procedure, not a moral compass; (ii) must be tailored to fit each agency's needs; and (iii) when properly applied, is consistent with government policy that seeks to protect the overall welfare of its citizens). 
RISK 20 (1992). [53:1 
I do not believe that Judge Williams is wrong, but there are some hard issues in the background. In my view, the strongest argument for CBA rests not with neoclassical economics but with common sense, informed by behavioral economics and cognitive psychology! The basic idea is that it is exceedingly difficult to choose the appropriate level of regulation without looking at both the benefit and cost sides. Without a full accounting, ordinary thinking about risks and their control is likely to go badly wrong, in such a way as to lead individuals, and governments, to favor policies that do less good than they might, or even more harm than good. Consider, for example, the remarkable finding that if we reallocated current spending to devote our resources to the most serious problems, rather than the less serious ones, we could save over 60,000 more lives each year, without spending a single penny more.9 
Why do ordinary people make mistakes about risks? The most obvious problem is that most of us, most of the time, are inadequately informed. But there are more interesting points in the background. In thinking about risk, people tend to rely on heuristic devices, or mental short cuts, that often work well in daily life, but that also lead to systematic errors. For example, we tend to think that an event is more probable if an example is cognitively "available," in the sense that it comes easily to mind. It is for this reason that some people exaggerate small risks and neglect large ones. It is also for this reason that a highly publicized problem can produce a kind of rush to judgment (and regulation); it is for this reason too that people tend to think that more people die from accidents than diseases, whereas exactly the opposite is true. 
The problem is compounded by social influences, through which false information can be spread rapidly, and even produce mass panics. If many people are starting to believe that genetic modification of food is dangerous, others may well be led to agree, not because they have reliable information, but because without that information, they tend to accept the views of others. This is perfectly rational at the individual level; if I know little, I might as well rely on what others think. But social influences can lead to grave public errors. These often take the form of "availability cascades," involving mass concern over small risks, with a strong call for governmental response.'0 Precisely because it draws attention to the actual risk, and to the costs of addressing it, CBA can serve as a corrective here, ensuring that a governmental response will occur only if public fear is rooted in reality. CBA can also promote attention to problems that, while serious, are not producing much public attention; consider the phase-out of lead and the efforts to control indoor air pollution, a relatively serious danger. 
Ordinary people also fail to see that health risks are on many sides of the problem, and that some regulations designed to protect health-for example, regulation of genetically engineered food-might harm health too. If young children are required to have their own seats in airplanes, parents might drive instead, and because driving is more dangerous than flying, the mandatory seat for children may actually kill children on balance. "Healthhealth tradeoffs" are omnipresent in regulation. Analysis of those tradeoffs is important in its own right and a significant step in the direction of CBA, which puts the adverse health effects of regulation on the public view screen. 
There is a related but more general problem. Sometimes people are alert to the dangers at issue but fail to see the problems, economic and otherwise, with eliminating or reducing those dangers. CBA has the advantage of putting both sides of the picture before the public and relevant officials. And if people's emotions are getting in the way-as they sometimes do in the domain of risk--CBA can have a salutary "cooling effect" by showing that the emotional reaction is excessive. Of course, sometimes CBA will show that public concern is anything but excessive, or even that concerns and emotions need to be far more intense than they now are. 
In these various ways CBA is admirably well suited to overcoming cognitive problems faced by ordinary people in thinking about risks. At the same time, CBA might well have significant democratic advantages. Of course, interest groups can manipulate CBA, a point to which I will return. But when there is no public accounting, interest- group control is especially likely. CBA can promote public attention to what is really at stake, in a way that increases both accountability and transparency. Well-organized private groups very often exploit the cognitive mechanisms just described-pushing regulation, or non-regulation, in their preferred directions. They deploy poignant anecdotes to suggest an approach that promotes their parochial interests. And all too often, citizens and their representatives do not attend to the serious questions at stake: the actual consequences of competing approaches. As Judge Williams suggests, CBA can improve the process and substance of decisions, by allowing people to evaluate agency decisions in an informed way, unclouded by avoidance of the central issues. There is thus a strong democraticcase for CBA, one that whelm governments and that leaders should create disincentives against efforts to instigate widespread panic which occurs before all information is fully disclosed). 
[53:1 does not depend on controversial claims from neoclassical economics. 
II. COMPETING, BUT WORSE, IDEAS 
The argument for CBA is strengthened by comparing it with some alternatives. I will examine several such alternatives in Part IV below. For the moment consider three popular ideas which tend to compete with CBA: pollution prevention, the precautionary principle (well discussed by Judge Williams), and sustainable development. 
The idea of "pollution prevention" is designed to ensure that regulators prevent pollution before it enters the system, and do not rest content with "end of the pipe" controls imposed on polluting technologies. As examples of pollution prevention, consider the phase-out of lead in gasoline, the use of solar power, and the substitution of electric cars for cars powered by gasoline. Advocates of pollution prevention tend to think that this is by far the most effective way to deal with pollution problems, partly because it promises larger and more dramatic pollution reductions, and partly because it does not rely on after-the-fact technological "fixes." 
Pollution prevention often makes a great deal of sense; but sometimes it does not. Consider two examples: automobile emissions and fossil fuels. The best way to "prevent" automobile pollution would be to eliminate the internal combustion engines that power most trucks and cars. The best way to "prevent" pollution from current power sources would be to stop relying on fossil fuels, now used by utility power plants. Should the EPA be told, today, to ban internal combustion engines and coal combustion? If this would be a ludicrous conclusion-as I think it would be-it is because the costs of the ban would dwarf the benefits. Sometimes pollution prevention might even cause health problems, if it leads to unsafe substitutes. Sometimes pollution prevention just isn't worthwhile. 
It seems to me that the case for pollution prevention rests, at bottom, on cost-benefit balancing-and that where the balance does not support regulation, pollution prevention is a mistake. None of this is of course to deny that sometimes projections of the future will involve a degree of guesswork and speculation. But when this is so, good CBA calls not for an artificially and deceptively definite number, but for a range of possibilities. What is not justified is to "prevent" pollution without an inquiry into the conse11. See BARRY COMMONER, MAKING PEACE WITH THE PLANET 41-46 (1990) (arguing that prevention is most effective anti-pollution mechanism, and that pollution controls only provide temporary solutions to long-term problems). quences, good and bad, of prevention. 
There is some important truth in the precautionary principle. The truth rests in the acknowledgement that a small probability (say, one in 100,000) of serious harm (say, 100,000 deaths) deserves serious attention. The fact that a danger is unlikely, even very unlikely, is hardly a decisive point against regulatory controls. But as Judge Williams illustrates, dangers are often on both sides of the equation, as the case of genetic modification of food suggests. Suppose, as some people believe, that genetic modification of food runs afoul of the precautionary principle. It is also true that a failure to allow genetic modification might well result in many deaths, and a small probability of many more-hence the failure to allow genetic modification seems to run afoul of the precautionary principle (unless we foolishly take existing practice, and what will emerge from it, as desirable). The precautionary principle, taken for all that it is worth, is paralyzing: it stands as an obstacle to regulation and non-regulation, and to everything in between. 
A competent CBA takes especially good account of what makes sense in the precautionary principle, by incorporating low-probability risks of significant harms. CBA subsumes this risk, as it does all others, into the overall assessment. Of course nothing in CBA precludes a policymaker from concluding that a 1/10,000 risk of 100,000 deaths is worse, or less bad, than a 1/1000 risk of 10,000 deaths. This is a political judgment, not a technical one to be decided by mechanical use of the numbers. 
influence in the environmental debates, so much as that it now serves as a kind of symbol for serious commitment to environmental protection. And that influence has undoubtedly been salutary, at least in part, by drawing attention to the future consequences of current actions. But the notion of sustainable development is highly ambiguous. Sometimes sustainable development is said to refer to "development that occurs on a scale that does not exceed the carrying capacity of the biosphere.' 2 To the extent that endorsement of sustainable development is meant as a criticism of approaches that are literally "unsustainable" in the sense that future generations will lack environmental goods--clean air and water, for example-everyone POLICY 1182 [53:1 should support sustainable development. Any minimally sensible policy will ensure decent lives and options for future generations. Who could be opposed to that? 
But outside of the easy cases for environmental protection, the real question is not "sustainable" development or "unsustainable" development; it involves what level of resources to commit to environmental protection. Often there is no simple line to divide the sustainable from the unsustainable. If certain regulatory steps would increase "sustainability" but cause a great deal of suffering and misery, simply by virtue of their expense, a sensible administrator will take that point into account-and in the international context, perhaps ask for financial help from wealthier countries, help that is probably required from the standpoint of justice. In short, the goal of sustainable development is in no conflict with CBA. CBA fully supports the idea that sustainability is a desirable goal, and helps give content to the harder question, which is how much should be done to improve environmental quality in poor as well as wealthy nations. 
Like deregulation and economic incentives, CBA has been pressed principally by those interested in economic approaches to regulation. But Judge Williams does not defend the economic approach to valuation; he says little on that question. In refusing to endorse the "willingness to pay" approach in an unqualified way, I think that he is on firm ground, for that approach has many problems. 
My aim in this section is twofold. I attempt to disaggregate some of the objections that have been launched against economic approaches to CBA. I also aim to show, in Judge Williams' spirit, how it is possible to accommodate the most reasonable objections through a suitable form of CBA. Since some kind of CBA seems to make undeniable sense, the question is how to use the objections to make CBA work better, not to jettison it entirely. Ultimately I suggest, also in Judge Williams' spirit, that it should be possible to reach an "incompletely theorized agreement" on CBA-that is, an agreement that rests on a variety of theoretical positions, and that does not require acceptance of any especially controversial commitments, theoretical or otherwise. 
A qualification: Some of the issues discussed here are exceptionally complex, and in a brief treatment of this kind, I will be able only to skim the surfaces. My hope is that even a brief and perhaps reckless treatment will help suggest the contours of an approach to CBA that does not accept the economist's most controversial claims about valuation. 
their actual choices, we might occasionally come up with bad numbers, since people are not always well informed when they take risks. If people are not well informed, we will have a hard time figuring out, from people's behavior and statements, how much they are really willing to pay to avoid risks. It follows that any use of private willingness to pay, to generate values, must be defended by showing that people who take risks have adequate information about what they are doing. A general concern about the lack of adequate information would justify some skepticism about using actual behavior as the basis for numbers, at least in settings where people are unlikely to know about the relevant risks. 
skewed against the poor. Poor people do not have much ability to pay, and hence they are not willing to pay much for goods from which they would greatly benefit. In principle, poor people might "count" little or not at all in a CBA based on private willingness to pay. Rich people have a great deal of ability to pay, and hence they might be willing to pay a great deal for risk reduction. But this problem can be corrected, perhaps by using a uniform number of valuing benefits, a number that will not go down when poor people are being counted, or up when rich people are being counted.13 In any case it is perfectly possible to design a system of CBA that does not count rich people more or poor people less. 
calculus; some "goods" should not be provided even if people would be willing to pay for them. For example, some people would be willing to pay a fair bit to harm other people, and to harm animals and the environment. These preferences should not be recognized by law. Even utilitarians believe that some preferences, such as sadistic and malicious ones, deserve no recognition in ethics or politics. In fact, regulatory agencies committed to CBA do not take int4o account sadistic or malicious preferences, and here they are quite right.' 
necessary to know who bears those costs and enjoys those benefits, and the simple notion of CBA, as understood in neoclassical economics, seems indifferent to that question. Suppose, for example, that an occupational safety and health regulation would have a total cost of $600 million, and that the monetized benefits would be $400 million (including, let us say, forty lives saved per year, and hence $200 million in monetized savings from fatalities averted). Is it clear that this regulation should not go for[53:1 ward? For various reasons it is not. If the people who are saved are children or teenagers, the uniform "lives saved" number might undervalue the relevant benefits. (Perhaps we should look to "life years saved," not merely lives saved.) If the 40 people are predominantly poor, we might want to give extra weight to the risk imposed on them. And what does the $600 million mean, concretely? Does it mean that prices will increase, by a little, for many people? That cost might be worth incurring. So, too, if the consequence of the $600 million expenditure would be a reduction in annual profits for companies that already make billions. Or does the cost mean that people, including poor people, will lose their jobs? 
An ideal cost-benefit analysis would tell us something about the incidence of both costs and benefits. It might well make sense to say that the "bottom line" numbers will not be decisive when an "incidence analysis" shows that those numbers should be adjusted to take account of the identify of the winners and losers. One way to handle this problem would be to give distributional weights to certain effects. It is not clear whether we are now able to produce detailed analysis of the incidence of regulatory costs and benefits; this is an area for much further inquiry. 
public values, or altruistic goals; and sometimes private preferences, as expressed in markets, will inadequately capture those values and goals. It is not clear that people's market valuations should be taken as decisive for purposes of regulatory policy. The best way to handle this would be to adjust the market values, or to use some other number, when there is good reason to do this. For example, it might be thought that pristine areas, such as national parks, should be valued more highly than market valuation would suggest. An articulated agency judgment to this effect, based on reasonable arguments, could support an unusually high valuation of aesthetic and related goals. If the market number, for a statistical life, is somewhere between $3 million and $7 million, perhaps that can be the starting point, to be modified where there is reason for modification. It is important here that this range appears to be about the median of agency practice as well; the same starting point is therefore defensible by reference to both market behavior and agency practice. 
"willingness to pay" numbers entirely disregard the fact that people care partly about their relative economic position, and not only about their absolute economic position. This is a serious mistake. If asked to give money in isolation, people will offer less than they would if they are asked how much money they would give, assuming that everyone else is giving the same amount too. It follows that people will be willing to pay significantly more for regulatory benefits if everyone else is paying the same amount. Because all or most people do pay for many regulatory benefits, an approach based on private willingness to pay will substantially understate actual values. The best response to this point is not to abandon CBA, but to adjust the numbers upward in accordance with this point-an entirely feasible enterprise. 
"commensurable" and should be assessed only in terms of dollars. It is important to know what the numbers mean-what it is that they are meant to capture. If, for example, members of an endangered species are at risk, the CBA should identify that point, and not rest content with the monetized value of their lives. This objection suggests, very reasonably, that a good CBA should present a qualitative as well as quantitative account of consequences, as in fact is conventional agency practice today. The "bottom line" numbers should be accompanied by an understanding of what they are meant to capture (also conventional agency practice today). 
My overall conclusion is that the notion of CBA can be specified in many different ways and that a purely economic approach, based on private willingness to pay, faces serious objections. But it is possible to undertake CBA in a way that takes account of the best of these objections from a theoretical point of view. The result would be to produce numbers that are cautious about private willingness to pay-but that are numbers nonetheless, taken not as a replacement for a full inquiry into what matters, but as an informative part of that inquiry. 
With respect to agency practice, the strongest objections to CBA do not seem to be ones of basic principle. CBA can be adjusted in multiple different ways (as Judge Williams appears to suggest). The strongest objections to CBA involve institutional issues. Judge Williams does not seriously engage these objections. The basic point is that in some settings-probably not the usual ones--CBA is, on balance, inferior to one or more alternatives, not in principle, but because a consideration of institutional points shows that CBA will not work as well. 
This essentially pragmatic claim might be explained through the suggestion that in some circumstances, CBA itself fails CBA. Consider two possible examples. 
[53:1 
National ambient air quality standards are set by the EPA. As a matter of principle, it seems to make sense to ask the EPA to consider both the costs and the benefits of such standards--especially because the benefits of regulation usually continue to increase with regulatory stringency, and there is rarely a point at which exposure to air pollution suddenly switches from "safe" to "unsafe." If more stringent regulation would prevent five to fifteen deaths, surely more stringent regulation makes sense if the cost would be (say) $5 million to $15 million. But perhaps the same would not be true if the cost of doing so would be $900 million to $1.2 billion. This simple point seems to suggest that CBA should be the basis for choosing ambient standards. Small gains are worth producing at low cost, but not at a cost of billions. Of course hard questions of valuation would remain. But at first glance, national ambient standards should be based on CBA, not on an inquiry into benefits alone. Hence it is now being argued, in the Supreme Court, that the Clean Air Act is not reasonably read to require EPA to produce national standards only on the basis of health considerations. 
But in fact things are not so clear. Under the Clean Air Act, national standards, once set, do not automatically bring all states of the nation into immediate compliance. National standards merely initiate a process, one of whose components include the development of state implementation plans. Cost turns out to be highly relevant both in the design of such plans and in their implementation. The point is confirmed by the fact that several decades after the promulgation of national standards, many areas of the country are still not in compliance. Of course noncompliance reflects many things, including public and private intransigence and government vulnerability to the power of well-organized private groups. But one of the things that noncompliance reflects is simple attention to costs. Costs emphatically do play a role in the Clean Air Act-not in the issuance of national standards, which are based largely on health, 16 but during implementation and in the development of compliance dates. 
In these circumstances, it is possible to make a highly pragmatic defense of the current situation under the Clean Air Act. The EPA sets out healthbased standards, which operate not as ordinary law, but instead as goals or aspirations, establishing a level of ambient air quality that EPA thinks all even if the EPA purports to rely solely on health considerations. If costs are playing a role, but that role is not disclosed, the standard-setting process has a democratic deficit. See Cass R. Sunstein, Is the Clean Air Act Unconstitutional?,98 MICH. L. R.Ev. 303, 308-09 (1999) (arguing that EPA decision-making process on clean air standards may have been affected by costs, but that CBA process has been isolated from review by the public). Americans should enjoy. States are encouraged to meet those goals. But if costs turn out to be extremely high, that will be relevant, and deadlines will be understood in light of the need to be reasonable under the circumstances. At least in theory, health-based national standards, alongside cost-sensitive implementation, might be better than an approach that would call for CBAbased national standards. Nothing in the basic defense of CBA shows why this is false. 
I am not sure that the pragmatic defense is convincing. The system of aspirations-and-implementation-delays has serious problems of its own, not least from the democratic point of view. The idea that there is a "safe" level is an evasion of the real question-what level of risk is acceptableand serious rule of law problems are created by delaying deadlines for compliance. All I mean to show is that in principle, it is possible that the best outcomes are produced by coupling health-based standard setting with consideration of costs at a later date. 
A popular alternative to CBA, one that I have not discussed thus far, is to require that regulation be "feasible" or "achievable." At first glance, this standard appears somewhat mysterious. These terms are far from transparent. But as generally understood, such statutes put the focus not on benefits but on costs, and on costs in a particular way: they forbid an agency from regulating to a point that is neither (a) technically feasible, because the relevant control technology does not exist, nor (b) economically feasible, because the industry cannot bear the cost without significant or massive business failures.1 7 If this is the understanding, there is a large difference between CBA and standards of feasibility or achievability. What I am going to suggest here is that such standards might be preferred, not in principle, but on the ground that they greatly ease the agency's task, and in a way that makes people far better off on balance. 
Suppose, for example, that a regulation would cost $800 million and that it would save twenty lives per year. It is easy to imagine that this regula17. See Am. Textile Mfrs. Inst., Inc. v. Donovan, 452 U.S. 490, 508-09 (1981) (holding that Congress mandated that the "benefit" for workers take the highest priority unless compliance makes benefit unfeasible); AFL-CIO v. OSHA, 965 F.2d 962, 980-82 (11 th Cir. 1992) (describing guidelines to determine technical and economic feasibility and stating that an agency must prove technical and economic feasibility by marshalling "substantial" evidence and by demonstrating its usefulness to impacted sectors). tion would be entirely feasible-in the sense that the industry would face no technical problems in meeting it, and also in the sense that it would be practicable for industry to bear the cost. But it is also easy to imagine that such a regulation would fail cost-benefit analysis, in the sense that $800 million expense would not be justified by the monetized savings. If a statistical life is valued at $5 million, for example, the benefit ($100 million) would be only one-eighth the cost. It is also easy to imagine that a regulation might not be feasible, but that it might satisfy any requirement of costbenefit balancing. Suppose, for example, that a regulation would cost $2 billion, that industry could not bear that cost without massive dislocations, but that the regulation would save 5,000 lives. In some cases, the costbenefit requirement is more protective, not less protective, of intended beneficiaries of regulatory programs. 
So far, perhaps, things are clear enough. But there is a problem. Feasibility is not an on-off switch. Any significant increase in costs is likely to prove "not feasible" for at least some companies. As the costs increase, the number of companies for whom the regulation proves "not feasible" will increase too. Perhaps there is a set point showing a large-scale increase in the number of companies who cannot bear the cost while continuing in business. But it is more likely that as the costs grow, the number of companies who cannot bear the cost grows too. In these circumstances, what sense is made by a "feasibility" or "achievability" constraint? Perhaps the motivating idea here is that for most regulations, companies must comply, unless a large number of them can show that they cannot comply and continue. And certainly this is a relatively simple inquiry in most cases. What makes little sense is the suggestion that agencies can pick a point that is "feasible," and not go beyond that point. In these circumstances, CBA seems both different from feasibility and superior to it, because it asks the right question-not whether controls are "feasible" (neither a sufficient nor a necessary condition for good regulation), but whether they are justified, all things considered. 
What, then, accounts for the evident popularity of requirements that regulation be "feasible" or "achievable"? The best defense is institutional. From the standpoint of those concerned with safety and the environment, a cost-benefit standard might be thought to introduce undue opportunities for industry to stall the process, partly because of the prospect and actuality of judicial review. While a feasibility standard seems worse in principle, it might be better in practice, if and to the extent that it allows the agency to accomplish tasks that would otherwise be undermined by well-armed private litigants. A key point here is that as compared with CBA, a requirement that regulation must be "feasible" greatly improves an agency's chances in court-a conclusion that is well-supported by the record of agencies on appeal. Under feasibility requirements, agencies generally win. Under cost-benefit requirements, agencies frequently lose. Agencies and their lawyers know this in advance, as do statute writers. In these circumstances, it is possible that the nation is better off with legally valid feasibility-based regulations, which agencies actually can impose, than with CBA-based regulations, if these will be invalidated in court. 
In the real world, this may be a convincing defense of an approach to regulation that departs from CBA. Ironically, its justification rests on a form of CBA: the benefits of a feasibility standard outweigh the costs, all things considered. Whether the argument is convincing depends on the real-world operation of the competing standards. And perhaps the best response to the argument is not to abandon CBA, but to ensure a deferential form of judicial review of agency action when CBA is required-a form that is not so deferential as to turn CBA into a carte blanche to agencies, but one that is deferential enough not to make those fearful of underregulation think that if anything at all is to be done, CBA must be avoided like the plague. 
Note here that lawyers opposing costly regulation are likely to be both talented and well-funded, and such lawyers are likely to be able to find holes in any agency decision that is rooted in CBA, even if the agency has done its job quite well. What is necessary is for those who endorse CBA to find a judicial role that does not make people reasonably fear that whatever CBA means in principle, it will guarantee agency inaction in practice. 
CONCLUSION: COSTS AND BENEFITS FOR EVERYONE 
My suggestion here has been that properly understood, CBA deserves wide approval-no less than the deregulation movement of the 1970s and 1980s and the1990s shift to economic incentives as opposed to national command-and-control. Indeed, I believe that with respect to regulatory policy, CBA should have the same relationship to the first decade of the twenty-first century as its predecessors had to the closing decades of the twentieth. 
The best defense of CBA relies not on controversial claims from neoclassical economics, but on a simple appreciation of how we all make mistakes in thinking about risks--and on an understanding that when people err, governments will err too. Properly understood, CBA should help us save lives, not only money. This is not at all a claim that in the areas of health, safety, and the environment, American government suffers from a systematic problem of "overregulation." It is far more accurate to say that we have both overregulation and underregulation, or what has been called a situation of simultaneous "paranoia and neglect."' 8 The best starting point for assessing the situation involves a collection of the effects of regulation and, to the extent possible, an attempt to quantify those effects, at least as a tool for attacking the most serious problems, and for ensuring that we do not spend substantial resources on problems that are small or even trivial. 
It is time to go well beyond "1970s environmentalism," with its emphasis on the existence of serious problems and the need for prompt action, to an approach that attempts to assess the magnitude of problems and the need for good priority-setting. At the same time, I have suggested that the bottom-line numbers should not be decisive, and that a more qualitative understanding of the interests at stake might well justify regulation, even if an assessment of the raw numbers suggests otherwise. I have also suggested that in some circumstances, institutional considerations will argue against cost-benefit analysis, largely for pragmatic reasons. What is most important here is to see that the case for cost-benefit balancing does not rest only or even mostly on economic grounds-and that people of widely divergent views can support a suitably specified form of CBA. The emerging questions involve not whether to do CBA but how; it is to those questions that we should now be turning. 
COSTS, AND LIVES SAVED: GETTING BETTER RESULTS FROM REGULATIONS 183, 183 (Robert W. Hahn ed., 1996) (noting that resources are being misappropriated because of social attention on certain issues and that phenomenon can be averted by "embracing a risk-analysis approach to public decision making"). 
Follow this and additional works at: http://chicagounbound.uchicago.edu/roundtable Recommended Citation 
LEGAL REASONING AND ARTIFICIAL INTELLIGENCE: How 
KEVIN ASHLEY, KARL BRANTING, HOWARD MARGOLIS, CASS R. 
SUNSTEIN 
HOWARD MARGOLIS: I look forward to what I'm going to learn this afternoon [November 3, 2000]. Artificial intelligence and the law has its roots about twenty years ago. It has been going rather strong for the last decade, in particular in the hands of two of our guests. 
Karl Branting, who is a professor at the University of Wyoming, is going to speak on some of the philosophical and broader issues. Kevin Ashley has been much more concerned'with the practical problems of creating modules of encapsulated legal judgments that actually work. Cass Sunstein, who as you all know knows everything, will comment. [laughter] 
So we will proceed in a logical fashion. Kevin is going to take twenty minutes because we want to get some concrete examples on the table so we really know what we are talking about. He will take twenty minutes to talk about three concrete examples. Then, Karl will talk in a more philosophical way about how research such as Kevin's links to other things going on in the area of artificial intelligence and scientific discovery and how well it's doing within law. Cass, then, will comment on whatever they say. I am allowed to say as few declarative sentences as possible and to ask questions. [laughte] And so we can proceed, beginning with Kevin Ashley of the University of Pittsburgh School of Law. [applause] 
KEVIN ASHLEY: Thank you very much, it's a pleasure to be here. I'm pretty sure I could not get a roomful of people on a Friday afternoon at the University of Pittsburgh Law School to discuss Af1 and law, but the free booze would help. 
I want to provide three examples of computational models of legal reason 
ing and show what you can do with them. Computational models of analytical legal reasoning are comprised of a knowledge representation and an inference mechanism. The knowledge representation captures some important aspects of legal knowledge. And the inference mechanisms are algorithms that enable a program to use those elements of legal knowledge that are represented in order to solve problems. 
II I! i 1! H ii ii 
"~ Legd inml 
Iun 
Algoidns to. 
om~aireces -, 
M&P,ar w I C~ (a-~) 
C 
D~~3UIIAI 
This is an idealized illustration of a computational model of legal reasoning. It comprises a knowledge representation and an inference mechanism. The knowledge representation is a conceptual hierarchy of legal information dealing with some particular type of legal claim. At the bottom level it relates cases and their facts to the elements of some legal claim and, ideally, ultimately to the legal policies and principles that underlie that legal claim. The algorithms of the inference mechanism use that information. For instance, the inference mechanism may take a problem situation and compare it to other cases in light of the other cases' analyses, draw inferences about how that problem should be decided, and generate arguments using the information in the computational model. 
Now there are three general issues in designing these computational models. One is: How does one connect the facts of cases to the statutory elements of a legal claim? My approach in two programs that I have worked on, HYPO2 and CATO, 3 was to introduce an intermediate level of factors. These are stereotypical patterns of facts that tend to strengthen or weaken a plaintiff's argument in support of its claim. In my work, when the program compares a problem to cases, it is comparing them in terms of these factors. In Karl Branting's GREBE program, 4 a different approach was used. 
Secondly, notice that the case texts are not in the model, not in the knowledge representation. This is because AI programs can't read yet. They can't understand natural language text in general. So someone has to represent the facts of the case manually in such a way that the program can know what the facts are and can determine how to analyze the facts. 
The third general problem is how does one implement, how does one represent, the underlying legal principles and policies of a legal claim? How does one implement their roles in analyzing cases? And what about the dialectical role of the cases in filling out the meanings of these abstract legal principles and policies? This is a problem that I have not solved in any program, but I have some ideas and I hope that I can show you a couple of them. 
misappropriation law. See Kevin Ashley, Modeh'ngLgalA gumenk Reasoningwith CasesandHpotheicals(MIT 1990). 
Througb a Model anedExarples(1997) (PhD dissertation, University of Pittsburgh Graduate Program in Intelligent Systems), available at http://www.cs.emu.edu/-aleven/dissertation.htm. See also Kevin D. Ashley, DedgnigEledroni CasebooksThat Talk Back"The CATO Prgram,40JuimetricsJ 275 (2000). 
integrates legal precedents with statutory and common sense rules for legal analysis. See L Karl Branting, Reasoning with Rules andPrecedentrA ComputationalModelofLegalAna#ysis (Kluwer 1999); L. Karl Branting, BuiJngExplanation, with Rules andStrectured Cases, 34 Intl J Man-Machine Studies 1 (1991); L Karl Branting and Bruce W. Porter, Rules andPrecedents as Compkmentay Warrant, Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI-91), Anaheim, California, July 14-19, 1991; L Karl Branting, Reasoning with Poions of Precedents, Proceedings of the Third International Conference on Artificial Intelligence and Law, Oxford, EnglandJune 25-28, 1991. 
Slide 2 5 CATO FaCtOr IE ar 
Y (.AeuHt 97 i~&~w 97 + ++ ++ 
[ -vi( +~ Mhaiht~d + + _ __ _._._. 
Now that's an idealized model. This is about as close as I've come to realizing that type of computational model. This is a Factor Hierarchy that my student, Vincent Aleven, designed for the claim of trade secret misappropriation. At the top level are the elements of the claim. For example, is the information a trade secret? Is there a confidential relationship? Were improper means used? And for each of those there would be a Factor Hierarchy. I'm just showing you the Factor Hierarchy for one: Is the information a trade secret? 
At the bottom level are the various factors, the stereotypical patterns of facts that strengthen or weaken a claim. For instance, here's Fl 5, factor 15: "unique product." It stands for a stereotypical fact pattern that one often sees in trade secret cases. The plaintiff's claim is stronger to the extent that its product is unique in the industry. It's relevant to the issue of whether that information is a trade secret in two different ways. It shows that the information is valuable. It also suggests that the information is not known in the industry. The Factor Hierarchy is a graph, that is to say any given factor on it can have more than one (cited in note 3). parent and we'll come back to the significance of that in just a second. 
Factor Hierarchy =>Multiple Intepretations 
nmsures tol ep its infrmaticu secret [F6], ard def. entered into an agement not to coopete vith pltf. [F13], plff. should in a claimof trade secrets misapptiation, as inthe ElcorCase. !Ecoris distinguishable. It is strngr for ptff. than is the current problem In E/cor,pltf.'s prouct was uniqu or the ramdkt [15] and thef= e substantial sinrilarities beteenpltf.'s and def.'s products [F18]. Not so in MBL ... [FI5] is not an inpotantdistinction. InMBL def.'s access to pl.'s info enabled it to develop its product in less firm or atloer cos[t8]. It follosthat inboth cases, pl.'s info was valuable for pl.'sbusiness [P104]. [P15] is an-aceddistincticn Shos that inEco, the info appaently was not knowv outside pl's business [F106], x~Ndeas inMBL, pL's info was known outside pl.'s business [F106]: P. disclosed its prauct info to outsiders [Flo] and pl.'s info wasgenerally known in the industry [MO0]. 
LC-0&RM 
QfM :DAie 4 
One can use a computational model like this to generate legal arguments and even to generate alternative interpretations of cases. This is a sample argument that the CATO program generates for a problem called the MBL case.6 I don't want to get into the details here, but I just want you to see the structure of it. It starts with an argument by analogy (first row of Slide 3). CATO argues that the MBL case should have the same result as the Elcor case,7 that is, the plaintiff should win. And it elaborates an analogy in terms of the factors that the two cases share. Then the program switches hats and argues that Elcor is distinguishable from the MBL case (second row of Slide 3). It's now arguing on behalf of the defendant. It points out, among other things, that the product was unique in Elcor, that is to say that factor F15 applies. But that was a strength that one doesn't find in the MBL problem. 
I would also like you to focus on the last two parts. Here, CATO downplays 
the significance of that distinction on behalf of the plaintiff (third row of Slide 3). It argues that the fact that the product was unique in Elcor is not an important distinction. It argues that the reason that that factor matters is that it shows that the product is valuable, that it has value. That was abstract factor 104 in the Factor Hierarchy. CATO points out that in the MBL case, there was other evidence that the product was valuable. In other words, CATO is arguing, given the reason why the distinction matters, that these two cases are basically the same. In this last argument, CATO responds for the defendant, it switches hats again, and now it's emphasizing the significance of this distinction (fourth row of Slide 3). It's saying here that the reason that that factor matters is that it shows that the information was not known outside of the plaintiffs business. And it goes on to say that in MBL there's evidence that the information was generally known, and it points to some other factors in the MBL case. In other words, CATO is arguing that, given the real reason why the distinction matters, the two cases are really quite different. In other words, it's doing analogical reasoning here. 
In downplaying and emphasizing this distinction, from where does the knowledge come about why these differences matter from a legal point of view? Well, it comes from the Factor Hierarchy (Slide 2). Let me just show you that again. In making this argument, downplaying and then emphasizing the distinction, CATO is working up from factor F15 on two alternative paths through that Factor Hierarchy. In arguing for the plaintiff that the cases are similar, it's drawing an analogy at the level of that abstract factor 104 that this information is valuable. In arguing that the cases are actually quite different, it's following a different path from F15 through 106 and making a connection to other facts in the MBL case, factor F10 and F20, and using that information to argue at an abstract level that these cases are really quite different. Now, CATO has algorithms that enable it to decide which paths to follow in this Factor Hierarchy and how high up to go in selecting an abstract way of characterizing the significance of the differences. Those are the algorithms. That's the inference mechanism. 
We use this feature in a program that teaches law students basic argumentation skills. And my student Vincent Aleven, in his dissertation, 8 evaluated the CATO program in a controlled experiment involving first-year legal writing students with some good results. Another practical application of it might be as a kind of brief writer's assistant. One could imagine having a specialized Factor Hierarchy and case database that's updated periodically with good coverage of a particular kind of claim and that would help associates in a law firm analyze claims and make arguments. So that's my first example of a computational model. 
My student, Steffi Brdininghaus, is using a similar kind of computational model in a different way. This time it's an attempt to harness the model to help another computer program called SMILE 9 to learn to classify new texts automatically. This SMILE program learns how to assign factors to the raw text of new trade secret cases based on a corpus of manually marked-up texts that we've prepared for the CATO program. So this time, we're trying to bring those case texts into the computational model. 
! Automated Case Indexing 
Algorithm to: *examples • imc text classifiers Cbse am 
SMILE learns from a set of training instances, sentences that are positive or negative instances of a factor in a given case. The four sentences [on the left side of Slide 5] are positive instances of a factor that you've seen before, factor F15: "unique product." It's on the basis of sentences like these that a human reader might conclude that a particular factor applies to the case from which the sentences come. All the other sentences in the case are treated as negative instances of F15. 
Knowkge IoLearningAgorthmsfor lndeng Legal Cares,in Proceedings of the Seventh International Conference on Artificial Intelligence and Law (ACM 1999), available athttp://www.pittedu/-steffi/papers/icai99.ps. 
SMILE uses a learning algorithm called ID310 to learn a decision tree for distinguishing the positive instances of sentences from the negative instances of a particular factor. On the right [of Slide 5] is a part of the decision tree that SMILE actually learned from the positive instances. It corresponds to a rule for classifying sentences. If the text of the sentence includes the term "unique," then conclude that factor F15 applies. Otherwise, if it contains "known" and "general," conclude factor F15 applies, and so forth. It may seem like a naive rule, but in an evaluation we showed that it does a pretty good job of distinguishing the sentences that are positive instances of a factor from those that are not. 
To refine the decision trees we need to get more mileage out of the examples. Steffi has focused on trying to make the examples more general. For instance, take a look at that first sentence [in Slide 5]. It actually said in the original "Innovative." That's the name of the plaintiff. "Innovative introduced evidence that Parl Brick," that's the name of the plaintiff's product, "was a unique prod10. See Ross Quinlan, C4.5: ProgramsforMaehineLeaming(Morgan Kaufmann 1993). 
To Assign Facton 
Viv F15 al:e 
pat 9-:A 
R info rut gffle n 
priie F15 ceoies: chis rit c to mT2e Puxid epatle 
F15 lies: aa \ 7 [Plaintiff] intoduc evidence that [plaintiff's product] wasa unkiie product in the induryr. lnmtne s:Bowen "It appeaim that onexmvld notorda [plairtitrs prouct inany I l other tha that of the plaintiff."tan YJa&LmielU Xtiay 'qheinfonmtion in the diagnrn is not generally knm to the public nor to any of [the plaintiffs] conipetitor" Th-Tron PVetto 'Sewralfentures of the proo.s ,mere entirdy unique in [product-type] trnufctnurinSI' y RandYv.R&htn (L0 , 31JYntdt.2 
C(li3h. Xs,.D MAy 
Symposium: Legal ReasoningandArfidal ntel'gence uct in the industry." Obviously, the terms "Innovative" and "Panl Brick" don't appear very often in trade secret cases. I made the substitutions of "plaintiff' and "plaintiff's product" manually to make the examples more general. If we can get SMILE, as a program, to make these kinds of substitutions automatically, then it can generalize the instances itself and learn more powerful rules from them. For instance, a rule might be: "If plaintiff's product is unique, then conlude factor F15." We're using information extraction to create better examples for the program to generate more powerful rules. 
Where would this be useful? Well, for one thing, if you've got a program like CATO, SMILE would automatically add new cases to the database. For another, if you are using Westlaw, you know that Westlaw retrieves texts and orders them by statistical criteria. A program like SMILE could process those cases that are retrieved and highlight the important stereotypical facts and weaknesses in the text of the case. 
Okay. Now I'm down to my last example. And the final example is a computational model for practical ethical, rather than legal, reasoning. My former student Bruce McLaren studied a set of more than four hundred decisions of the Board of Ethical Review of the National Society of Professional Engineers. He created a program called SIROCCO" which, given a problem situation, retrieves past ethics cases and ethics code provisions that are relevant to the analysis of the problem. We used that program to investigate empirically an interesting feature of ethical reasoning that also applies to law. 
Normative principles in professional ethics are very abstract rules. For instance, here's an example: "Engineers shall recognize that their primary obligation is to protect the safety, health, and welfare of the public." Now, how does one know how to apply that abstract principle? In ethics, as in law, principles cannot be defined intentionally and applied deductively. There are no readily available sources of authoritative rules that bridge the gap between the high-level principles of the top and the low-level factual scenarios at the bottom. Nevertheless, we observed that, like judges, the Board, in deciding cases, cited relevant ethics code provisions and also cited relevant past cases. And we hypothesized that the decided cases and their explanations of how the principles apply, in effect, flesh out a meaning for those very abstract principles at the top level. We say that the cases "operationalize" the principles. We believed that these casebased extensional definitions of the principles could be represented and also used for improving the program's retrieval ability. Now, notice that this time I have not used factors as a way of bridging the gap between the case facts and the higher level principles. Instead, I've used something called instantiations, Mcar1e1n.,ASsIsReOsiCngCtOhe R(Seylesvteamce ofofCraIsnestealnlidgPendtndRpeletrsieUvsailngoOfpeOrpaieornaatihoznaa6liiozneTdecCniaqsueess(a1n9d99)C O(PdheDs). diSseseertBatriuocne, University of Pittsburgh Graduate Program in Intelligent Systems), available at http://www.pitt.edu/-bmclaren/publications.html. which are the result of the Board's exercise of these operationalization techniques, in effect, to bridge the gap between case facts and abstract principles. 
()peratonaliziig Ethical IP~inciples 
Retrieve relevant cases / ethics codes ~Th 
fl MSe~ 
Let me show you a little about how operationalizations link facts and abstract principles. [Slide 7] contains a list of the operationalizations that Bruce cataloged during his analysis. These are the Board's techniques for bridging the gap between the abstract codes and the specific fact situations. For instance, code instantiation is a kind of more concrete interpretation of a code in the context of a case. In essence, the Board links combinations of selected facts in the case to a code provision when they instantiate it. For instance, in one case, the Board instantiates that public safety code provision that I cited to you before. ["Engineers shall recognize that their primary obligation is to protect the safety, health, and welfare of the public.'] It was a case involving a building inspection. Basically, the Board said that once an engineer discovers that an apartment building, which he has been hired to inspect, presents a danger and he knows that the building authority should be informed, then he has an obligation to warn of the danger even though his client instructs him to withhold the informarion. Well, that's a little concrete set of facts with which we annotate the broad principle and then we can use that information in improving the program's retrieval ability. I'll just close by showing you how that works. 
Slide 7 
Operatonalizaflon Techniques Code O ramizaiom 
in NSPE BER Case Set: (1)CadeI1n9sa 
t (e.g ofILl.a. fom90-5-1: "...It appars that gIn eerA, having becorm aware of the irint danger to the stuure, had an obligation to rmke absolutely cera that the teats and public authorities ae ne inmdivately aware ofthe dangers thatexisted ) (2)Apply Codeto HLothdic Scenaio (3)Rewitea Code (4)Gro 
Codes (5) esignaleSipiorCodein Contrt (e.g., .l.a. over ILI.c.finm90-5-1: "...in cases where the public health and safety is endangered, engimers not nly have the righit lht also the ethical responsibility to reveal such fhcts to the ppr persons.') 
We compared SIROCCO to five other methods, including an oblated version of SIROCCO, that is to say, a version of SIROCCO that doesn't use the operationalization knowledge. We called that "Non-Op SIROCCO. 1' (his is a nice feature of a computational model. One can turn a knowledge source off or on at will for purposes of experimentation.) We performed the tests using a case base of 184 foundational cases and fifty-eight trial or test cases. Historically, the fifty-eight test cases came after the foundational cases. For each trial case, we compared the cases and code provisions that SIROCCO, the program, recommended as being relevant with the cases and code provisions that the Board actually said were relevant. And we compared the overlap using a number called the F-measure. It's a combination of precision and recall. The graph [in Slide 8] in note 11). shows the mean F-measure per method over all fifty-eight trial cases. We used two different kinds of experiments here. 
The point I want to make to you now is that in both instances SIROCCO outperformed Non-Op SIROCCO and the differences were statistically significant. This difference is the contribution to retrieval effectiveness that comes from the Board's operationalizations. We were able to sort of bottle it, if you will, and take advantage of it in improving the program's ability to retrieve cases. 
Slide 813 I IICoxtrbution of prfe mict .5 S 0.4-/ II 0.30.2 0 0.1-/ 
So, in conclusion, those are my three examples of computational models and what you can do with them. You can generate arguments with them and even generate alternative interpretations of the significance of important similarities or differences. You can use them to help a program learn to index cases automatically. And you can use them to conduct interesting empirical investigations of such phenomena as operationalizing principles. 
Thank you. [applause] Figure 4-5 (cited in note 11). 
KARL BRANTING: Thank you, Howard. 
So Kevin has presented several computational models of legal reasoning and how they can be used for analysis and for tutoring. It's my opinion that these models can be useful in jurisprudence for helping to evaluate alternative jurisprudential theories by actually implementing them and testing on examples. But rather than pursuing that question, what I'd like to talk to you about, in the few minutes that I have for my presentation, is to try to make the case that this field is relevant to all of you. I think that, in the long term, it is going to change the character of the American legal system. I am going to make my case slightly more emphatically than I really believe it, just to engender a little bit of discussion about this. 
But anyway, my claim is that the development of computational models of legal reasoning that can actually be used for problem solving-systems that I'll call legal expert systems-are really going to change the practice of law and the American legal system, and that this is going to happen during your careers. To substantiate this claim, I am going to appeal to five separate factors, which I'll enumerate, and then I'll say a little bit about each one of them. 
Here's factor number one, the one I'll talk about the most, the claim that legal expert systems, adequate for many routine legal problems, already exist and new computational models will continue to be developed. That's claim number one. Claim number two is that there is a vast, unmet demand for legal services by the public. Richard Susskind, in his book The Future ofLawp,14 terms this the latent market-all those people out there who can't afford lawyers. Three, the World Wide Web constitutes a kind of electronic infrastructure for the distribution of legal services. Four, legal expert systems constitute a new vehicle for marketing and distributing the expertise of lawyers. And five, funding limitations on governmental bodies, which we know are always short of money, are inevitably going to drive them to automate a larger and larger proportion of the services that they deliver. 
So, in summary, I am suggesting that legal services can be viewed as a kind of commodity, for which the public is the consumer, attorneys are the producers, the World Wide Web is the highway, and legal expert systems are the vehides. And by decreasing distribution costs and increasing economies of scale, legal expert systems will inevitably lead to increased consumption of this commodity. Less efficient producers will inevitably be priced out of the market. So that's my claim. 
Let me consider each of these factors in turn. The one that I'll talk about the most is number one, the claim that legal expert systems adequate for many routine legal tasks already exist. To substantiate this claim actually would take quite a bit of work, maybe a whole semester class would get it started, so I am only going to be able to make a few comments in support of it. 
Let me start by making the observation that there are quite a few different participants in the legal system and, in my view, for each one of these participants there may be various different legal tasks each requiring a separate computational model. By the participants, I mean we have the members of the public, clients, attorneys, judges, clerks, legislators, all people who are participants in the overall process. Just for simplicity, let's hone in maybe on the most typical garden variety sort of legal problem solving episode that we might imagine, which is when an individual comes to consult an attorney about some legal problem. 
What kinds of separate tasks does an attorney perform? Well, I claim that there is a whole series of them. The first one is what I call problem formulation. When a client explains a problem to an attorney, the attorney has to elicit the legally relevant facts, steer the client away from the legally irrelevant stuff (indignation and so forth), and the attorney needs to formulate the problem that is being posed by the client in terms of legally relevant concepts. The second step is retrieval. The attorney, the legal problem solver, needs to think of some legal authorities that are relevant to the problem that has been formulated in the first step. Next is what we might call problem analysis. This is determining what sort of legal consequences might follow from the application of the legal authorities to the facts as elicited by the attorney. Next is the task of prediction. That is, for each of the possible outcomes of some legal action, litigation for example, estimating the probability of those outcomes. What is the expected return on them? How much would it cost to go to trial on a certain issue? How much might you win? How likely are you to win? Other tasks are planning, deciding what sort of actions should be taken on behalf of the client's goals, document drafting, and others. I won't enumerate them all. 
The character of each of these tasks is rather different from the others. Not all of these tasks are amenable to modeling, only a subset. But the subset that, in my view, is most amenable includes the task of retrieving authorities, the analysis task, the prediction task, and the document drafting task. I'll just hone in on those. So far, I am still trying to support my contention that there are models of legal reasoning that are adequate for routine tasks. Number one, retrieval, I'm not going to talk about. There is a long history of AI models doing it. But right now the dominant models, e.g., the LEXIS/Westlaw kind of model, don't use much in the way of Al. I won't talk about that one. Instead I'll start with analysis. 
Analysis. That's the idea that if I have some well-defined facts and I have some well-defined authorities, can I derive some arguments from them? Well, I think that if the problem is sufficiently well posed, then there are a variety of different models for finding arguments for and against a different legal conclusion. The simplest and historically the oldest one simply maps legal rules onto computational rules or logical rules. Such systems are useful if the case facts are relatively stereotyped and clear-cut and it's the legal rules themselves that give rise to the complexity of case analysis, rather than the vagueness, ambiguity, or context dependency of the legal concepts in those rules. So, in other words, something like the UCC15 is much more appropriate than a legal problem involving reasonable care, let's say. 
Computational models based on these rules were first developed in the 1970s. They are nothing new. They, in turn, were based on logical models developed notably by Layman Allen in the 1950s. 16 So they have been studied for quite a long time. People have also been familiar with the weaknesses of these models for quite a long time, because we know that lawyers don't treat legal rules as a static body of legal formulations but rather legal rules are the tools that lawyers use for achieving their goals. Now the kinds of models that Kevin has been showing you are more sophisticated. They involve reasoning by analogy. As it happens, the earliest analogical models were developed in the 1970s and the early 1980s. These analogical models tend to involve a much larger knowledge acquisition effort. In other words, constructing these systems tends to be a more involved process. 
The second task that has been intensively studied is prediction. As I mentioned, after the analysis, if we are thinking of our interaction between a client and an attorney, there is the legal analysis, but then prediction is an important part as well. Now the largest consumers of predictive systems are insurance companies, which formalize the expertise of claims adjusters and attorneys through a lengthy process of interview and observation to produce systems that predict the settlement value of insurance claims. So there are a large number of such systems, but they are almost always proprietary. Oddly enough, there is not a large amount of literature on predictive systems. 
But in my view, there is reason to believe that all of the participants in the legal system could profit from predictive systems. Psychological studies, notably by Elizabeth E. Loftus and W. Wagenaar,17 have shown that attorneys systematically overestimate their likelihood of success at trial. [kaughtu Why is that? Well, there is a reason for it. Optimism is rewarded. In fact, the most successful trial lawyers are those whose estimates are least realistic, that is, are most overly optimistic. So what does this mean? This means that as an institution, courts are rewarding behavior that isn't optimally beneficial to the system as a whole. In other words, the best strategy for an attorney is not necessarily the best for the client. And it is almost certainly not best for a society as a whole that has to pay for lawsuits that would never take place if people had a realistic estimate of their 
J 833 (1957). 
1988). probability of success or, more precisely, the expected return on the lawsuit. 
A third task is document drafting. As we all know, there is already a large commercial market for the very simplest document drafting systems. There are lots of sophisticated models of document drafting, including some based on state of the art linguistics and speech-act theory. 
So in summary, I think that there is, at least for these three tasks-for document drafting, prediction, and analysis-a history of computational models. They vary widely in their flexibility and explanatory power and development costs. And moreover, the relative merits of these models are, of course, a matter of dispute among computer scientists and scholars of jurisprudence. But the fact is that, at least at the low end, executable models-legal expert systems-already exist. 
So that was all on claim number one. My other claims will be briefer. Factor one, the claim that models exist. Two, the unmet need for legal services. I think I can just appeal to the familiar experience that we all have that attorney fees are quite expensive. As a result, individuals are frequently unable to afford answers to basic legal questions. The cost of getting an answer to a legal question is often greater than the value of the claim that the question applies to. Of course, that is not true of large institutions, but for ordinary citizens this tends to be the case. And these costs are exacerbated by uncertainty in the legal system, which is the result of the fact that the law is in a state of evolution and therefore unsettled, the delays of litigation, which are worse in some places than in others, and the overall lack of predictability in the process. 
So far I've said there are good models. There is also an unmet demand for legal services. A third factor is the World Wide Web. The World Wide Web is a wonderful development for people in my area of study because it provides a uniform computer interface familiar to a very high proportion of litigants and attorneys. It largely eliminates the problem that we used to have in software distribution, that is, hardware inconsistencies and interface inconsistency and unfamiliarity. So even the most technology averse lawyer is likely to be familiar with web browsers, if only because he or she has seen his or her children using them. [Iaughter] 
Claim number four, legal expert systems as a distribution mechanism for legal expertise. It is my observation that the economic motivation for the law firms that are most active in development of web-based legal expert systemssuch as London-based Linklaters, Sidney's Blake, Dawson & Waldren, Ernst & Young, and others 18 -is that, well, first of all, that legal expert systems perform work that wouldn't otherwise be done by the firm. So the idea is that clearly you don't want to make an expert system that you market for less than the amount it would cost one of your own attorneys to perform the same work. But the motivation is that the legal expertise can be marketed to a larger number of consum18. See Alan Cohen, LegalAdice IWithott the Layers, New York LJ (Nov 15, 1999). ers if it is formalized as a computer program that is then delivered over the web. And the second factor is that users of the expert systems may become customers for more complicated and more lucrative personal services. People who pay less to get some advice may then have an incentive to say, "Hey, I want to know some more than this program can deliver to me." The bottom line is that these law firms are betting that legal expert systems can improve the profits that they obtain from the marketing of their legal expertise. 
And finally, the last comment about government services. There is an immense demand for routine legal information from state, local, and federal governments. This has already given rise to quite a few web-based legal expert systems produced by government agencies. Right now there are legal expert systems for the Advisors on Employment Standards Administration (ESA), the Mine Safety and Health Administration (MSHA), the Occupational Safety and Health Administration (OSHA), the Pension and Welfare Benefits Administration (PWBA), and the Veterans' Employment and Training Service (VETS).19 
Web-based delivery of legal services is also a promising strategy for addressing the needs of pro se litigants. Of course, there are actually two arguments about that There is one view that "Geez, if you can't afford a lawyer, how can you afford a computer or how likely are you to be able to understand how to use a computer?" But it is my surmise that familiarity with computers is becoming quite ubiquitous in our society. 
So I've made this claim about the growing economic importance of legal expert systems. Let me end by adding a couple of provisos. There are some countervailing factors. One of them is technical. You may have been struck during Kevin's presentation by the fact that this system that he was showing to you seemed quite elaborate. And it is true that the expertise of these systemswe can imagine them as embodying expertise-is hard to come by. It is a very laborious process to take someone's expertise and formalize it in a manner that is executable on a computer. So the development costs of legal expert systems are very high and, in my view, they are only going to come down when there are significant improvements in knowledge acquisition, that is, the process of automating the formalization of expert knowledge. And then in particular, that improved natural language processing is going to be key. But once again, Kevin's student is a typical researcher engaged in improving those techniques. 
There are also institutional and cultural barriers. A really major one is time billing. Lawyers are apt to be reluctant to make use of legal expert systems to perform some of their work if they bill by the hour and such systems reduce the amount of time it takes them to solve a problem. So time-based billing is antithetical to the acceptance of these techniques. But task-based billing, on the other hand, creates an economic incentive to the automation of the more routine things that you can automate. Partnership promotion practices discourage activities that are not billable. In the firms that I mentioned earlier that have invested a great deal of effort in creating legal expert systems, for internal institutional reasons there is not a lot of internal pressure for the people involved to be billing hours constantly because there is this huge upfront cost in the time of legal experts that is only amortized over the lifetime of the use of the program. 
And the last one is the rather difficult institutional barrier, the ill-defined standards for the unauthorized practice of law. I think that there is going to be litigation on this subject in increasing amounts, because there are many attorneys at the low end of the food chain that are going to be directly threatened, already are threatened, by these systems, who are going to find daims of unauthorized practice of law as a way of attempting to stanch this flood. 
Let me conclude with some predictions concerning the effects on the legal community. I think that legal expert systems aren't going to reduce demand for high-end legal services. In fact, I think, to the contrary, it is going to improve the delivery of high-end legal services by automating some of the more routine aspects. As an example, there is a new product that uses natural language processing to do proofreading pretty effectively. And it is marketed by saying that this is a mechanism to retain associates who would otherwise get so discouraged at being kept up late proofreading documents over and over again that they would move to some other firm. 20 Plausible or not, I don't know. 
On the other hand, the providers of routine legal services are going to face increasing competition from legal expert systems. And I think, as I said, that solo practitioners are already under such pressure. It may be that a new field of legal information engineers is going to develop consisting of attorneys whose job is to organize information for electronic mass distribution. And finally, I think that interactions with low-level government functionaries will increasingly be replaced by simple web-based legal expert systems. Am I out of time? MARGOLIS: Yes. [laughte] BRANTING: Thank you. [applause] 
CASS SUNSTEIN: This is extremely interesting material. A major question is: What can we learn about artificial intelligence and what can we learn about legal reasoning from bringing them into contact? That's what I'm going to try to say something about. 
There's a weak version of the enthusiasm for artificial intelligence in lawweak meaning less ambitious-and that is that this is like really upscale LEXIS and Westlaw. It bears the same relationship to LEXIS and Westlaw as LEXIS and Westlaw bear to Shepard's. In this view, it's extremely helpful for lawyers, who can find a lot of cases quickly. Plug in a problem and they'll see lots of cases like it and potential similarities and differences. That seems to me a convincing claim. Kevin Ashley has demonstrated it. That's the weak version and I'm all for that. 
2001] 
Sympomium: LegalReasoningandArtfiaallntelligence 
The strong version, which both speakers actually endorsed, is that artificial intelligence as we now have it can engage in analogical reasoning or does engage in analogical reasoning. To phrase it a little more polemically than is probably fair, I'll say that's just a mistake because at the present state of the art artificial intelligence cannot engage in analogical reasoning or legal reasoning. They can't do it. And the view that they can do it, or are doing it, is based on a misunderstanding of what analogical reasoning is, one that disregards the inescapably evaluative or normative dimension to my claim that one case is "like" another case. To engage in analogical reasoning, to do it, there has to be an evaluative argument showing that this case is like that case. There has to be a principle, and at the current state of the art, artificial intelligence can't generate good principles, or principles at all. I'm hoping this will be helpful. 
Suppose you have someone who's been fired by an employer, a copilot, say, for refusing to fly an airplane on the ground that it's not safe. The employer has fired the copilot, and the copilot wants his job back or wants some money. Let's suppose, to make it very simple, that we're in a jurisdiction in which one court has held that you can't fire someone for refusing to commit a crime and another court has held that you can fire someone for reporting that the bank for which he works hasn't engaged in advertising activity in low-income communities. This is a world with just three cases: the case at hand, one case the employee wins, another case the employee loses. What's to be done? This is a problem in analogical reasoning. 
A going account of analogical reasoning is by Edward Levi,21 and the title of this subsection of my talk is "Levi's Mistake." What Levi suggested was, in engaging in analogical reasoning, judges ask which case is more similar to the case at hand or which case has more similarities to the case at hand. Is it dear that that's not a very helpful way of doing analogical reasoning in our pilot case? Is the pilot case more similar to the bank case or is it more similar to the crime case? To figure that out you can count similarities. But is that what you're going to do? It's not an exercise in counting. You have to do something else, and let's make a little amendment to Levi and say you can search for relevant similarities. Now that's helpful, or at least more helpful than counting for "more." You need to find relevant similarities and HYPO, the computer program, can do that, but that's not helpful enough. To know whether a similarity is relevant, you need to figure out the principle for which the first case stands, and the first case doesn't tell you that. Is the idea in the crime case that you can't fire someone for refusing to inflict harm on third parties? If so, then our pilot maybe is going to be okay. Or is the principle instead you can't fire someone for refusing to commit a crime? If so, then our pilot's in trouble. 
The ideas of "relevant" similarities and "more" similarities are pretty much non-starters. You need to figure out what the principle is that links or separates the various cases. Ronald Dworkin,22 maybe the subsequent generation's Levi, gave some help on this, a kind of clue. He says what you do when you're engaging in legal reasoning is you put the previous decision in the best constructive light. You try to make the best sense out of it. So Dworkin says analogy without theory is blind. An analogy is a way of stating a conclusion, not reaching one, and theory must do the real work, where theory is the principle that links cases or that separates them. The upshot of this is that in any case that's a real case, to figure out whether something's analogous to something else, you have to generate a principle by which the two cases get linked or separated. Lists of factors will be a start, better than Westlaw and LEXIS, but they won't be analogical reasoning. That's not what analogical reasoning is. 
To make progress here, we shouldn't give up on artificial intelligence and its potential. A lot more can be done. Good reasoners are going to deal with our copilot case. Can our copilot be fired? Probably anyone in this room, given ten minutes, could figure out ways of thinking the copilot should win or ways of thinking the copilot should lose by reference to the previous cases, reporting on the bank's violations on one hand and the person refusing to commit a crime on the other hand. But how can we make better progress? One thing we might consider is empirical: What are the consequences if you give copilots a right not to fly planes that they see as dangerous? If they can't lose their jobs for that, you might ask, is that going to make people safer? If so, that's a point for the copilot. If copilots do get this right, if the right is given to them, is this going to make it very much harder to run airplanes? Is this going to decrease convenience and order for airplanes and passengers? Those are empirical questions, which Judge Posner, in the relevant case,23 thought relevant. He's surely right on that. To do the analogical job well, one thing to pursue is these empirical questionsnot empirical in the sense that Professor Ashley suggested, not about collecting cases and factors, but an empirical inquiry into the real world effects of one or another legal rule. There's no reason in principle that a computer can't be helpful with that. 
If we're not going to get empirical, then what we'd want to do is square our judgment of principle about whether the copilot should win with the rest of the things we think in imaginable cases. Then we'd have to be very creative and go beyond the cases at hand, the precedents, and hypothesize lots of analogies and think what makes best sense of our system of labor law insofar as it bears on this. A lot of really good judges go that route. So far as I can tell from Professor Ashley's really quite outstanding book,24 HYPO isn't able to do that. What HYPO can do is come up with cases, and it can be pretty exhaustive in that, telling how they might be similar and how they might be different, but in a kind 2001] 
Symposium: LegalReasoning andArfidalIntelligence of blind fashion, one that is not alert to the need for a guiding principle that might justify a claim of similarity. HYPO can't do what needs to be done. 
The upshot of all of this is that artificial intelligence in the current state of the art can be a wonderful advance over LEXIS and Westlaw. What Professor Branting suggested seems to me quite convincing-that this can be a real aid. It's not so much different in the analogical domain from a computer program that can just tell you what the rule or law is. That's very good. What can't be done yet is to do analogical reasoning-to do what lawyers, at least decent lawyers or judges, actually do. 
Two qualifications with which I'll end. It may be that in some domains, and I bet that trade secrets is one, you can generate cases that are so sharply hemmed in by precedents that if you look at the precedents in even a kind of crude way, without any principles, you're going to know all you need to know. In some trade secrets cases the fact pattern in question will be one which is not plausibly distinguishable from the precedents. In a case like that, HYPO, or your computer program, is going to do all of the work. In a case like that, by the way, Westlaw and LEXIS are going to do all the work. It's just going to take a little more time with Westlaw and LEXIS than it would with HYPO. 
The second qualification seems to me more interesting for the future. There's no reason, so far as I know, in principle to think that in the long run computers won't be able to make the empirical and principled judgments that a good analogizer has to make. The co-panelists would know a lot more about that. To ask whether the social consequences of one or another rule would be A or B, why can't a computer do that? No reason not. So too for generating good normative principles. If a computer can win chess games against pretty good chess players, why couldn't they do that too? If they're doing that, then they're engaging in legal reasoning. Not yet. [applause] 
MARGOLIS: I'm allowed to ask a question and it happens to have a certain kinship to what Cass was talking about, so I'll ask it and then we'll ask the two other speakers to comment. I have occasion to caution my students against what I call logical democracy. And logical democracy is you just list the arguments on one side and you list the arguments on the other and you count them up and majority wins. And the reason why that's so pernicious is sometimes some arguments are really good and there's a large number'of bad arguments on the other side. And so I share Cass's uneasiness at when the AI systems will be ready for that kind of judgment. Why don't you comment? 
ASHLEY: Well, I'd like to first respond to something that Cass said and that is that although HYPO's arguments might look like lists of factors, in my work and in the work of other people in AI law, we have been moving in a direction that Cass, I think, would approve. For instance, in my example with the argument that CATO generated, it wasn't just factors but it was reasons why the factors mattered to the legal claim. So I am connecting factors to reasons, and the arguments are working with those reasons. In work that's being done by colleagues in Europe, Henry Prakken and Giovanni Sartor, they are representing values, principles that are at stake in cases, and those are being worked into the arguments as well. So at least these concepts, these normative concepts, are being worked into the arguments. 
Now whether judgment is being applied is another question. But I will opt in favor of the weak AI approach. My game is, I think, to try to find ways in which representing knowledge drawn from our models of how we reason in law, how can that knowledge be applied to do a better job of doing those weak tasks, like retrieval of the right cases at the right time or, for the cases that are retrieved, highlighting what's interesting about them, what's useful about them in the context of an argument. My game is not to try to reproduce the hard tasks of legal reasoning so much as to try to use the knowledge of how we do the hard tasks of legal reasoning to try to build better tools to support those tasks. 
We, I think, have been careful not to compare strengths of arguments in terms of numbers. In HYPO there was no comparison of numbers. It was comparisons of sets in terms of set overlap, which is quite a different thing. And we're also, many of us, very sensitive to any attempts to assign numerical weights to anything like principles or values or factors or whatever and to collapse pluses and minuses in that way. We tend to eschew that kind of thing. So I think that we are sensitive to these concerns. We're just gradually working our way upward into the more complicated kinds of arguments that Cass and Howard are talking about. 
BRANTING: I guess I'd have several responses. First of all, the legal expert systems that I was describing would clearly all fall into the weak category. They are useful tools. They may be performing some functions other than just weighing arguments. For example, document drafting and prediction are somewhat different tasks. But I guess I would want to emphasize also that people who work in the field of artificial intelligence and law by and large are sensitive to the fact that analogical legal reasoning is not a mechanical process and that the current computational models don't do an adequate job including the evaluative factors that Professor Sunstein pointed to. 
On the other hand, in my view, what artificial intelligence is about is really two things. One is making useful artifacts. But a second thing is self-knowledge. That is to say, for example, not that this is what you said, but to say that certain kinds of problem solving or certain kinds of analysis cannot be modeled is kind of a way of saying we can't know ourselves, we can't understand how we solve certain problems well enough to define it with a specificity that's required of a computer. So from my point of view, one of the benefits of this field is that the exercise of trying to formalize legal knowledge in a computer-executable fashion is it forces you to make explicit every piece of knowledge that goes into that decisionmaking and can sometimes make obvious the gaps. For example, some 
Syposium: LegalReasoningandArifidalIntelligence of the more naive views of legal problem solving are that it is very rule-driven. One of the ways to demonstrate the inadequacy of this naive view is to code up some rules and observe that the resulting system doesn't reason anything like a lawyer. I guess my last comment is that I think that the AI and law field progresses through criticisms of the sort that Professor Sunstein just made. The process we'd like to see is: "Here's the argument that's generated by my system and what's wrong with it?" 
MARGOLIS: You wanted to add to that? 
ASHLEY: I had one thing to add to what Karl just said, and that is that once you have taken the trouble to build a computational model of some interesting phenomenon of legal reasoning, you have a program that works on a range of examples and you can use that as a framework for investigating what that program can't do. So if Cass comes forward with an example of a kind of reasoning that it cannot do, I'm in a position to start playing with the model, to tweak it, change it, see how it has to be revamped or modified. Thus, AI is a kind of empirical methodology for making progress on modeling the phenomena that we all think are so interesting and important. 
MARGOLIS: Let me offer Cass just a moment for a comment on a comment on the comments if he wishes, and then open it to the floor. 
SUNSTEIN: I want to hear what the audience has to say. There's a computer in the back also that I know has a question. [laugbt] 
MARGOLIS: Do we have any questions? Yes, please. 
AUDIENCE: I have a question or a suggestion, and I wanted to get your response, primarily from Mr. Branting and Mr. Ashley. What would you say to the charge that what you are proposing would be the worst thing for legal reasoning, with LEXIS and Westlaw being the second worst, because whatever you produce will ratify the weaknesses of the person doing the inputs. Here is what I mean. With LEXIS and Westlaw, one of the problems, I think, that's happened is that while you can retrieve large numbers of cases based on the contexts or words that you enter, what happens is that you only produce those cases that happen to match the concepts or terms that you were smart enough to pick. And so you get enough case law to produce the set of precedents or a legal reasoning argument that will end up being pretty good. The only dilemma being that you get nothing else that might have made some kind of analogical reasoning that might have given you additional terms you should have looked for. 
It seems like the same kind of thing could happen here, because whether you are using factors or another means of deconstructing a case before inputting into the computer, someone has to make a judgment about what factors are worth mentioning. And somebody conceivably could input ten personal injury cases and put in all the facts, and they could all be the same slip and fall case and never think that it was worth mentioning whether the plaintiff was black or white. What would happen if, in the end, the case was that all of the black plain25. See, for example, Theodore Eisenberg and James A. Henderson, Jr., Inside d/eQuietRevolution in Products Liabiliy,39 UCLA L Rev 731 (1992). 
Syposium:Legal ReasoningandArfidalIntelligence far as the quality of advice or analysis that a system is able to produce, the question about describing the facts to a computer system, I think that that is a separate issue, one that I did not have time to get to. 
I listed a number of the tasks that go on in even the most garden variety interaction between an attorney and a client One of them, the very first one, was problem formulation, and when I was listing the things for which we have good computer models, I didn't include that one. I think we don't have a good computer model of this process of taking a sort of undirected narrative by someone who isn't familiar with legal concepts and reformulating it into a fashion that can be manipulated by one of these legal models. What that means is that the consumers of legal expert systems are initially going to fall into two categories, it seems to me. One category is comprised of lawyers who are able to perform this problem formulation themselves. And the second includes people whose problems are extremely stereotyped and for whom the problem formulation is extremely simple. For example, people who want to know advice about, let's say, social security benefits or, maybe, domestic relations. How do I get a divorce? Can I get a protection order? 
AUDIENCE: Computers are progressing very quickly of course, and I was just wondering if you could say very briefly, not what the goal is in the next five years, but what the goal is in the next thirty years, the next fifty years, and the goals for computers in the future? 
BRANTING: The long range goal is for computers to become more like the ones in the movies. That's the goal. 
AUDIENCE: I'd like to take the question that arose before and point it in the other direction. I agree that there's always a problem of choosing input, and therefore we will have limited output. Now, if we go back to the strong AI argument, which is let's let these computers take over at least part of what lawyers do, then what Professor Sunstein said, if I understood correctly, is that there are some things which simply cannot be done with computers. But, are there things which most lawyers, and I'm not talking about the best lawyers, I'm not talking about professors who have the best knowledge and some quantum leaps of thinlking, but most lawyers ... Most lawyers have a very regular way of approaching things. Most lawyers deal with a limited practice of law. They deal with limited case law of which they are aware of or are willing to search for. Therefore these programs, maybe not today, but in the near future, will probably be able to outperform at least the low end of what lawyers do today. If you take that with the economic arguments of if I'm going to buy from the low end of the food chain, then I'm going to get some of these results, wouldn't it be better to pay less, get a computer program which will probably not do the best but will do at least as well as the lawyer I would probably go to anyway and save money? 
SUNSTEIN: Clearly. The issue, I think, is even more interesting than we've gotten a hold of so far. Some of what you say, and some of what's been said, raises interesting issues about what artificial intelligence can ultimately do and also about what legal reasoning really is. You know, there's a joke among the faculty that you could imagine a computer program-I bet someone could do it-that could write a law and economics article about any topic. Choose a topic and promptly it's the case that you take your favorite or least favorite methodology, and you could imagine a computer program that could do that. I think you're absolutely right that these programs can perform as well as or better than really busy people who don't have time to think a whole lot about what's the best principle to construct for an area of law. 
One of the great parts of Professor Ashley's book talks about the relationship between HYPO's performance-HYPO is the computer program-and judicial performance. They're pretty close. That tells us a lot, actually, about the legal system. It shows us that in daily legal reasoning often what does happen is seizing on one or two relevant differences that have been established, not terribly reflectively, as being super salient. That tells us a lot about what our judges are doing. Posner's own opinion in the copilot case26 was pretty brief. My hunch is that a computer program could improve on it a lot, along one dimension certainly, and maybe in a couple of others. The dimension it could certainly improve on is that it would have access to and use a much larger universe of precedents. Judge Posner just used a couple. The computer could give him a whole lot more. And maybe it could refine the principle by forcing him to grapple with the analogies. 
There are intuitive leaps that are involved in chess and in driving. My research assistant, who may be in the room here, found what some of you may not know, that a computer program can drive extremely long distances across the United States at sixty-three miles an hour on average while being able to navigate all but a very small percentage of miles. Now that small percentage is really important if you want to be safe, but it's a small percentage, and what ordinary people would call intuitive leaps for driving, computers can do that. 
A lot of creativity in the law, even by people who are very busy, consists of giving a meaning to a case or series of cases that nobody has seen before. This is much more mundane than it sounds, but really thrilling moments in a lawyer's life are when you can create a new pattern out of preexisting materials. That's where creativity lies and in the last few years alone long established cases have been given exceedingly new meanings. Judge Posner, not so recently, but not ages ago, understood the common law as about promoting economic efficiency2. 7 No one thought that way before, and it gave a whole new meaning to a tremendous pattern of cases. For artificial intelligence really to take off in law, and this probably isn't short term, it would have to become capable of being a little like a literary critic reading a poem, not in the sense of making nonsense 
2001] 
Siposium: LegalReasoningandArifldal nteligence out of it but in creating a new pattern to it that you didn't see before. That's what even daily lawyers, not the people that are just trying to find out what the law is, but people who actually litigate, that's what they do. 
MARGOLIS: We have time for one last question. 
AUDIENCE: You were talking about models of analogical reasoning. I was wondering if anything has been done with, say, models of statutory interpretation or constitutional interpretation? 
ASHLEY: There has been a lot of work and a lot of progress in representing bodies of statutory rules in a computable way. There has not been very much progress in what we would call statutory interpretation. So, for instance, one sees in civil law jurisdictions people drawing inferences from the structure of the code, for instance, about the meaning of a statutory predicate. We haven't come close to that, I don't think, in AI and law. Also, we have not succeeded in representing the alternative policies that the legislature must have had in mind for a particular statutory provision and trying to look at a problem situation through the statute in light of these alternative policies, actually modeling that. We'd like to, but I don't believe we have yet. 
OF ARTIFICIAL INTELLIGENCE AND LEGAL REASONING Cass R. Sunstein All rights reserved 
Can computers, or artificial intelligence, reason by analogy? This essay urges that they cannot, because they are unable to engage in the crucial task of identifying the normative principle that links or separates cases. Current claims, about the ability of artificial intelligence to reason analogically, rest on an inadequate picture of what legal reasoning actually is. For the most part, artificial intelligence now operates as a kind of advanced version of LEXIS, offering research assistance rather than analogical reasoning. But this is a claim about current technology, not about inevitable limitations of artificial intelligence; things might change in the future. 
The computer on which I am now writing is capable of many impressive feats. Sometimes it talks to me. It can recognize spelling errors and point them out to me. It is astonishing how many words it seems to know. My computer can also find (some) bad writing, and it lets me know when I should rewrite (some) bad sentences. Everyone also knows that the best computer chess player can beat the best human chess player. Fewer people know that an onboard computer system from Carnegie Mellon University has driven a ban almost all of the 2849 miles from Washington, DC to San Diego, California, both day and night, in the rain, and with an average of 63 miles per hour.1 But this is only the barest tip of the iceberg. * Karl N. Llewellyn Distinguished Service Professor, University of Chicago, Law School and Department of Political Science. 1 See David Waltz, Artificial Intelligence: Realizing the Ultimate Promises of Computing (2000), http://www.cs.washington.edu/homes/lazowska/cra/ai.html. 
Can computers engage in legal reasoning too? Can they do it well? Even better than people? Some grounds for an affirmative answer might emerge from the simple observation that much of legal reasoning is analogical in nature.2 In ordinary life, analogical reasoning often takes the form, White House is to President as X is to Congress, with the solution consisting of a judgment that X is the Capitol Building. The task of identifying good analogies - the kind of task imposed on high school students - seems to be the sort of thing on which computers can excel. If this is right, perhaps computers can do well in law too, simply because legal reasoning is pervasively analogical and based on close attention to past cases. An understanding of the relationship between artificial intelligence and legal reasoning might well illuminate both of these endeavors. 
It is best to anchor the discussion in an illustration. Suppose that the rule in state A is that employers can discharge employees “at will,” that is, for any reason or for no reason at all. Suppose that an airline then discharges a copilot for refusing to fly a plane that the copilot believes to be unsafe to fly.3 Is the latter discharge lawful? 
Let us assume that there are many analogies in the relevant jurisdiction. Suppose that the courts in state A have created a series of public policy exceptions to the at will rule – that they have said that an employer cannot be discharged for refusing to commit a crime, or for obtaining workers’ compensation benefits, or for cooperating with the police about potential criminal activity on the part of the employer. Suppose too that courts have limited by the reach of the public policy argument by allowing employers to discharge employees for smoking on the premises, for reporting to the Community Credit Bureau about possible regulatory violations by a bank, and for engaging in political activity, outside of the workplace, on behalf of candidates of whom the employers disapproves. Might it be possible for a computer to find, or show, which cases are “most” analogous to the discharge of the copilot, and which cases are “least” analogous to it? 
A number of people have attempted to answer this question in the affirmative -- to show the potential role of artificial intelligence in assisting lawyers, and perhaps even in engaging in legal reasoning. I will use as an illustration an extremely interesting book by Kevin Ashley, which makes some striking claims about the role of computer programs in analogical reasoning in 2 See Edward Levi, An Introduction to Legal Reasoning (1949). 3 See Buethe v. Britt Airlines, 787 F.2d 1194 (7th Cir 1986). law.4 Ashley has created a computer program, HYPO, which appears to excel at providing assistance in trade secrets cases. If a HYPO is told about a case, HYPO will, among other things, draw up a set of analogous cases; tell you how they are similar and how they might be distinguished; rank them in order of analogousness; and even give you arguments about how to meet the claim that the cases are different from the case at hand, with citations. Ashley suggests that HYPO is far more useful, in many ways, that Lexis and Westlaw, insofar as the latter simply rely on “keywords” in past cases. 
More strikingly, he shows that HYPO’s performance, when confronted with a fact pattern, is not so different from the performance of actual judges. HYPO tends to refer to the same cases and to make the same arguments about how they are similar and different; HYPO even make similar responses to claims that cases are similar and different. But Ashley’s conclusion is still more ambitious: “If lawyers argue with precedents precisely because it is not feasible to prove the right answer by deductive logic, then the goal of a theory of analogical legal argument should not be to explain what the right answer is. Precedential reasoning is interesting precisely because, even without logical necessity, there still may be an ordering to the persuasiveness of arguments. The appropriate goal for a theory of arguing from precedents is to describe that order accurately. . . . Hypo is a step toward such a theory.”5 
How does HYPO provide “a step” toward a theory of accurately describing the “order” of the persuasiveness of arguments? How would we know if artificial intelligence is actually engaging in legal reasoning? 
What I am going to urge here is that there is a weak and strong version of the claims for artificial intelligence in legal reasoning; that we should accept the weak version; and that we should reject the strong version, because it is based on an inadequate account of what legal reasoning is. We should reject the strong version not because artificial intelligence is, in principle, incapable of doing what the strong version requires (there is no way to answer that question, in principle), but because there is no evidence that, at the present time, any computer program is in a position to do what is necessary. To the question, can computer programs engage in legal reasoning, the best answer is therefore: Not yet. 
According to the weak version, artificial intelligence can serve as a large improvement on existing computerized services such as Lexis and Westlaw, because well-designed programs are able to assemble an array of relevant cases, to suggest similarities and differences, and to sketch arguments and counterarguments. This is a true and important point. On the strong version, artificial intelligence can now engage in legal reasoning, because a well-designed program can tell a lawyer, or even a judge, what cases are really closest to the case at hand, and what cases are properly distinguished from it. I believe that the strong version is wrong, because it misses a central point about analogical reasoning: its inevitably evaluative, value-driven character. 
What is legal reasoning? Let us agree that it is often analogical. In his classic discussion of legal reasoning, Edward Levi rightly emphasizes this point.6 But in doing so, Levi makes a serious mistake: He suggests that when engaged in reasoning by example, courts ask what case is “more” similar to the case at hand.7 It is much more accurate to say that analogizers in law have to ask which case has relevant similarities to the case at hand. It is more accurate still to say that whether a case has relevant similarities to the case at hand depends on the principle for which the initial case is said, on reflection, to stand. It follows that the crucial step in analogical reasoning consists, not in a finding of “more” similarities, not in establishing “many” distinctions, and not even showing “relevant” similarities and differences, but instead in the identification of a principle that justifies a claim of similarity or difference. Because the identification of that principle is a matter of evaluation, and not of finding or counting something, artificial intelligence is able to engage in analogical reasoning only to the extent that it is capable of making good evaluative judgments. 
The point is illuminated by Ronald Dworkin’s influential work on legal reasoning.8 Dworkin says that “analogy without theory is blind. An analogy is a way of stating a conclusion, not a way of reaching one, and theory must do the 6 See Levi, supra note 1. 7 Id. at 3, note 8. 8 Ronald Dworkin, Law’s Empire (1985). real work.”9 I think that this view is too simple; an analogy is partly a way of reaching a conclusion, because it helps people to understand and to assess the principles to which they are actually committed. But Dworkin is right to say that analogical thinking cannot get off the ground without some kind of theory or principle, helping to unify or divide the case at hand and the cases that have come before. 
We can therefore venture a hypothesis: Since HYPO can only retrieve cases, and identify similarities and differences, HYPO cannot really reason analogically. The reason is that HYPO has no special expertise is making good evaluative judgments. Indeed, there is no reason to think that HYPO can make evaluative judgments at all. 
Consider the problem with which I began. Is an airline permitted to discharge a copilot who refuses to fly a plane on the ground that it is unsafe to fly? Let us see how HYPO might be helpful on this question. In a way, HYPO might show, this case like a case in which an employee discharges someone for refusing to commit perjury. In both cases, the employer’s action threatens to injure third parties. On the other hand, HYPO might add, the cases are distinguishable: The discharge by the airplane does not threaten to produce a crime, and in any case the airplane seems to have a legitimate interest in ensuring that safety judgments are made by pilots rather than copilots. Perhaps HYPO will note that in a way, the airplane case is “most” like the decision allowing employees to be fired for reporting possible regulatory violations by a bank. In the airplane case, however, the discharge would have more serious consequences, including massive deaths. Doesn’t this distinction make a difference? 
The only way to answer these questions, and to come to terms with the universe of analogies, is to settle on a principle that explains why the case at hand should fall on one or another side of the line. We might say, for example, that an employer is never permitted to discharge an employee as a result of an objectively reasonable judgment, by the employee, that a certain course of action is necessary to save lives. This principle does not conflict with any of the precedents. Or we might say that an employer is always permitted to discharge an employee when the employee has refused to accept a reasonable order from a 9 Ronald Dworkin, In Praise of Theory, Ariz. State L. J. (1997). hierarchical superior, if that order (a) is job-related and (b) would not require the employee to commit a crime. This principle does not conflict with any of the precedents. 
How should a court choose among the two possible principles? How should a lawyer persuade a court to make that choice? It is not helpful to say that the question is which precedent is “closer” to the case at hand. Whether a precedent is closer depends not on a factual inquiry, but on identification of a (normative) principle by which “closeness” can be established. It is more helpful to proceed by asking which principle is actually better. How can we figure that out? An important question is whether the pro-employee principle, in the airline case, would actually improve safety on balance (or instead perhaps impair, as the court of appeals suggested in the case). Another important question is whether the pro-employee principle would disrupt airplane operations, by giving copilots a right to veto flights when safety is not much of an issue. It is worthwhile to note that these are empirical issues. Judges may not know how to answer them. But my guess is that HYPO, with its admittedly excellent database, knows even less. 
There is yet another avenue for progress, involving an assessment of the proposed principle by seeing if it is inconsistent, from the normative point of view, with anything else that we believe, or to which the legal system would likely commit itself. Here HYPO is not entirely unhelpful, but it can hardly do what needs to be done. I think that Dworkin is correct to suggest that legal reasoning often consists of an effort to make best constructive sense out of past legal events.10 If analogical reasoning is understood in this light, the analogizer attempts to make best constructive sense out of a past decision by generating a principle that best justifies it, and by bringing that principle to bear on the case at hand. Why should we think that HYPO has any skill at that endeavor? 
My conclusion is that artificial intelligence is, in the domain of legal reasoning, a kind of upscale LEXIS or WESTLAW—bearing, perhaps, the same relationship to these services as LEXIS and WESTLAW have to Shepherd’s. A terrific advantage is that the relevant programs can assemble a wide range of relevant cases without turning up so much that does not bear on the problem at hand. But the more extravagant claims on behalf of artificial intelligence in law are based on a crude picture of legal reasoning, one that disregards the need to 10 See id. root judgments of analogousness, or disanalogousness, in judgments of principle and policy. 
There are three qualifications to what I have said thus far. First, precedents will sometimes sharply constrain the law’s room to maneuver. Assume, for example, that an employee alleges that she was discharged for cooperating with the authorities about apparent tax fraud by her employer, and that a previous case says that an employer may not discharge an employee for cooperating with the authorities about apparent drug use by her employer. Sometimes the case at hand cannot plausibly be distinguished from previous cases, because there is no principle that can support the precedent without also producing a certain result in the case at hand. An upscale version of LEXIS, one that has a full stock of precedents on hand, should be able to identify and resolve problems of this kind. 
The second qualification is that we cannot exclude the possibility that eventually, computer programs will be able both to generate competing principles for analogical reasoning and to give grounds for thinking that one or another principle is best. Perhaps computers will be able to engage in the kind of empirical testing that is often a crucial (though overlooked) basis for good legal outcomes. Perhaps computers will be able to say whether a particular normative principle fits well with the normative commitments of most people in the relevant community. I have hardly suggested that these are unimaginable possibilities. The possibilities for growth, in the domain of artificial intelligence, cannot be predicted at this exceptionally early stage. 
The third qualification is that the weak and strong versions of the claims for artificial intelligence in law, as I have described them, are really poles on a continuum, not a dichotomy, and there is reason to hope for movement from the weak in the direction of the strong. In fact Ashley moves in this direction insofar as he attempts to order cases by determining the strength, or weakness, of one or another connection between the case at hand and the analogies. An effort to specify relevant factors, and to order their importance, is a step in the direction of producing analogy-warranting principles.11 If artificial intelligence is not now able to engage in legal reasoning, it does not follow that it cannot get closer to 11 See Kevin Ashley, An AI Model of Case-Based Legal Argument from a Jurisprudential Viewpoint (forthcoming). doing exactly that. At this stage, there are promising experiments, ones that could be quite helpful to lawyers. 
I have emphasized that those who cannot make evaluative arguments cannot engage in analogical reasoning as it occurs in law. Computer programs do not yet reason analogically. But this proposition should not be confused with the suggestion that in the nature of things, evaluative arguments are uniquely the province of human beings, or that computer programs will never be able to help human beings with it, or even to engage on it on their own. 
Readers with comments should address them to: Cass R. Sunstein University of Chicago Law School 1111 East 60th Street Chicago, IL 60637 773.702.9498 csunstei@midway.uchicago.edu 
INTRODUCTION 
Why did critical legal studies disappear? Will it reappear? Why does the Federalist Society prosper? Why, and when, do people write books on constitutional law, rather than tort law or antitrust? Why did people laugh at the notion of "animal rights," and why do they now laugh less? Why do law professors seem increasingly respectful of "textualism" and "originalism," ideas that produced ridicule and contempt just two decades ago? How do book reviewers choose what books to review? Why has law and economics had such staying power? 
Academics are generally committed to truth, and they are drawn to ideas that can be shown to be good ones. The most optimistic answer to these questions is that ideas survive because and to the extent that they are true or good. On this view, law and economics has outlasted critical legal studies because it has much more to offer. Textualism and originalism have had a resurgence because much can be said on their behalf. Book reviewers, in the academic domain, tend to choose to review the best books. 
In my view, these claims contain some truth, but they are far too optimistic. Academics, like everyone else, are subject to cascade effects. They start, join, and accelerate bandwagons. More particularly, they are subject to the informational signals sent by the acts and statements of others. They participate in creating the very signals to which they respond. Academics, like everyone else, are also susceptible to the reputationalpressures imposed by the (perceived) beliefs of others. They respond to these pressures, and by so doing, they help to amplify them. It is for these reasons that fads, fashions, and bandwagon effects can be found in academia, including the academic study of law. Fortunately, the underlying forces can spark creativity and give new ideas a chance to prosper. Unfortunately, these same forces can also produce error and confusion. 
* Karl N. Llewellyn Distinguished Service Professor of Jurisprudence, University of Chicago, Law School and Department of Political Science. - Ed. I am grateful to Jack Goldsmith, Tracey Meares, Eric Posner, and Richard Posner for helpful comments on an earlier draft. 
1251 
Sometimes cascades have enduring effects. But in many fields academic cascades are fragile, with numerous people focusing on issues and adopting methods that disappear in short order. Some cascades produce unpredictable and seemingly random movements, as external shocks lead in dramatic directions. In social life, small sparks cause wildfires; it is for this reason, among others, that we cannot easily predict future academic trends, or foresee new movements in the academic study of law. (In 1985, would it have been possible to predict the resurgence, in the 1990s, of interest in the study of social norms?1 Or the rise of interest in cyberspace? In the Second Amendment?) There is even a tipping point phenomenon here, in which a certain pressure, from the perceived views of others, can produce a sudden "rush" toward a particular methodology or point of view.2 
In this Essay, I attempt to cast light on the general topic of academic bandwagons and cascades, with particular reference to law. Several caveats are in order. First, my focus here is on trends in academic law, but informational and reputational signals are ubiquitous. The same forces discussed here help explain many social movements, including reactions to environmental risks, the rise and fall of communism, the success or failure of students and job candidates, the creation of ethnic identifications, and the rise and partial fall of affirmative action.3 Second, I do not mean to present any criticism of legal scholarship in general, or to depict those who produce it as especially prone to informational and reputational influences. A general attack on legal scholarship would be senseless, if only because so much of it is obviously excellent. Third, I aim only to establish the existence of cascade effects, not to give a clear test for distinguishing cascades from cases in which approaches and arguments have spread because of their merits (though some of my remarks will bear on that issue). Fourth, my treatment will be informal and anecdotal, offering examples that will, I hope, be intuitive and familiar. With respect to the underlying phenomena, I draw on some more systematic and formal treatments,4 Informational Cascades,in THE NEW ECONOMICS OF HUMAN BEHAVIOR 188, 189 (Mariano Tommasi & Kathryn Ierulli eds., 1995); TIMUR KURAN, PUBLIC LIES, PRIVATE TRUTHS (1997). 
Behavior of Others: Conformity, Fads,and Informational Cascades, 12 J. ECON. PERSP. 151 (1998). In the social sciences, the analytical literature on cascades begins with Magoroh Maruyama, The Second Cybernetics: Deviation-Amplifying Mutual CausalProcesses, 51 AM. SCIENTIST 164 (1963); THOMAS C. SCHELLING, MICROMOTIVES AND MACROBEHAVIOR (1978); and Mark Granovetter, Threshold Models of Collective Behavior, 83 AMER. J. OF SOC. 1420 (1978). For analysis of purely informational cascades, see Sushil Bikchandani, David Hirshleifer, & Ivo Welch, A Theory of Fads,Fashion, Custom, and CulturalChange as Informational Cascades, 100 J. OF POL. ECON. 992 (1992); David Hirschleifer, supra note 3; both general and particular, and try to apply the central ideas to the academic context. Obviously a great deal might be said about this topic; my brief treatment here is intended only to be a start. II. 
To make progress on this topic, it is necessary to have some sense of what academics care about, and also to know something about the nature of the market for academic ideas. On these subjects, I hope not to say anything controversial. But because some account is implicit in any description of cascades, I offer some brief notations.' 
Most academics care about what most people care about. They seek to retain their jobs and to have the good opinion of (relevant) others. Few of them are indifferent to status. But they also care, more than most, about ideas, and they are willing to forego various benefits in order to be able to think and talk about issues suitable for teaching and academic research. Many academics are interested in pursuing truth as such. Those who do or use empirical work often fall in this category, and the same is true for those whose basic goal is to help produce clarity and coherence in the law. In the context of law, there is an additional point: Many academics would like to contribute to improvements in law and society by helping to make law better in the domains of, for example, antitrust law, race or sex equality, and freedom of speech. Of course academics are a diverse lot on these counts. For some, reputation matters a great deal; for others, the pursuit of truth or justice is especially important. 
There is also a market for academic ideas, and this market will have significant effects on what academics do. In this market, academics are the producers, while consumers include other academics, students, government officials, judges, and law clerks. The extent of interest from these groups will of course vary with the material; some academic work, for instance, is of direct interest only to other academics. The market here is unusual in many ways, above all because no one pays directly for what academics produce. Law reviews usually do not compensate contributors for articles and essays, and the same is true for other journals (in economics and philosophy, for example) in Lisa Anderrson & Charles Holt, Information Cascades in the Laboratory, 87 AM. ECON. REV. 847 (1997); Abhiijit Banerjee, A Simple Model of Herd Behavior, 107 Q. J. ECON. 797 (1992). See also B. Douglas Bernheim, A Theory of Conformity, 102 J. POL. ECON. 841 (1994) (discussing similar mechanisms). 
several commentators on an earlier draft, who urged an elaboration of the utility function of law professors and of the market for academic ideas. Obviously I think that the commentators are right, but the fact that these topics are discussed here is itself an illustration of the forces I discuss in this Essay. which law professors might publish. Publishers will pay for the right to publish books, and professors receive royalties, but little money is usually involved, and hence the motivation for writing books is rarely material for academics. On the other hand, indirect compensation monetary and nonmonetary - is omnipresent. Job opportunities are a direct function of what academics produce, and at many schools, salary is partly a reflection of quality and quantity of publications. Invitations to conferences and the like - dreaded by some, welcomed by many - are also affected by the perceived quality of academic work. It is here, above all, that the market disciplines academic activity. 
In a well-functioning market, only or mostly valuable ideas will be produced - although of course some ideas will be valuable even if they are misleading or incorrect. But it is not at all clear that an ordinary economic market, based on the willingness-to-pay criterion, is a good way to produce valuable ideas in law or elsewhere. Such a market might well cater unduly to existing tastes or to the interests of those with a great deal of money to pay; research funding by groups with a large financial stake in outcomes is therefore a problem. The complex system of indirect compensation, alongside the tenure system, is commonly defended as a way of insulating the production of ideas from ordinary markets. If this complex system works extremely well (by the appropriate criteria), it will lack "bad cascades" - that is, cascades in which valueless ideas travel not because they are valuable, but because of the mechanisms that I will be discussing here. But I will suggest that in many contexts, an absence of private information, together with a concern for reputation and various features of human cognition, can produce academic cascades that are bad as well as good. 
III. INFORMATION-INDUCED ACADEMIC CASCADES 
Academic cascades take two forms: informational and reputational. Let us begin with the role of information. 
Most people, in most domains, lack reliable information about what is true and what is right. For this reason, they are interested in the signals of others. The point holds for the selection of movies and restaurants and carpets; it holds for ideas as well. If you are unsure whether textualism is a sensible or pernicious approach to constitutional interpretation, you might care a great deal about other people's views. Of course academics, especially older ones, are sometimes settled in their views. Often they are confident that they know what to think, and to that extent, they are not terribly susceptible to the views of others. (Notice here that the precondition for immunity to informational influences is confidence about one's preexisting views, not adequate or accurate information.) But among some groups, and in some fields, any settlement is provisional and somewhat fragile. In many fields, including law, young people in particular can both influence and be influenced by informational signals. If many people are susceptible to influence, cascades can readily develop. The significant swings in legal scholarship over the last decades suggest that academic lawyers are indeed susceptible. 
Suppose, for example, that John, a young academic, does not know whether textualism is a sensible approach to constitutional interpretation, but that Mary, a slightly older academic, is in favor of it.6 If John is otherwise in equipoise, but attaches value to what Mary thinks because she seems wise or has often been right in the past, it is easy to see how John might come to Mary's view. If John and Mary believe that textualism makes sense, Sally, a contemporary of John, might be moved to agree, at least if she lacks any reason to be confident that they are wrong. And once John, Mary, and Sally come to a certain view, David, a recently hired faculty member, will likely agree with them unless he has enough private knowledge - or, more precisely, confidence - about his antecedent view to stand in their way. At some point one or more of these people might even produce an article or book in defense of textualism. 
As stylized as this example seems, I believe that it captures a great deal about academic movements, in law and elsewhere. Consider, for example, the rise of feminism within the law schools, starting roughly in the mid-1980s. In many places, feminism appears to have succeeded through a kind of informational cascade, as people who would otherwise be skeptical or unsure came to think that feminist approaches had something to offer - not (in many cases) because they carefully investigated the underlying claims and believed that they were illuminating or right, but because the beliefs of others seemed hard to resist for those lacking a great deal of confidence in their own (skeptical) judgments. If so many people seemed to think feminist approaches to law were valuable, mustn't they be right?7 
Or consider the life and apparent death of the critical legal studies movement, which flourished (again speaking roughly) from 1977 to critical legal studies powerfully influenced both students and younger faculty. A significant number of students appeared to sense what critical legal studies was about, and they seemed to agree with it. A significant number of assistant professors (some of them now professors, with apparently little continuing interest in critical legal studies) were in the same category. Within both groups, the informational signals sent by the large number of critical legal studies members were extremely important. 

participating in a cascade, rather than acting independently. 
At the University of Chicago Law School, much the same can be said, then and now, about the economic analysis of law. Many faculty members engage in economic analysis of law, and a majority of the faculty shows considerable interest in the basic approach. As a general rule, younger faculty members are especially interested in the informational signals sent by their colleagues, and at Chicago, many of them end up doing work that is influenced by economics. Cascade effects are even easier to observe within the student body, as certain concepts (involving, for example, the value of efficiency, the implications of the Coase theorem, and the futility of redistributive regulation) spread as if by contagion. Of course it is true that many students, and some faculty members, show no interest in the economic analysis of law. But mere exposure to economic thinking, voiced in many settings (including workshops, lunch discussions, and comments on articles), leads in the expected directions. 
As informational cascades develop, people end up amplifying the very informational signals to which they have responded. Scholarship, including the production of articles and books, is much affected by processes of this kind. If this is so, it is possible to predict, with some confidence, that the publication of books on various topics or with various methodologies often will be highly concentrated over time, showing that fads and fashions play a role in the academic world as elsewhere. 
In making these claims, I do not mean to make any normative claims about feminism, critical legal studies, or economic analysis of law. Nor do I mean to suggest that those who are vulnerable to informational pressures are weak or irrational. People who know that they have limited information certainly should pay attention to the signals given by others. And whether pressures of this kind will lead in desirable directions cannot be decided in the abstract. All that can be said is that the underlying mechanisms give little reason for confidence that academic "movements" will be good ones. Ideas can spread, even among people with some expertise, despite the fact that little is to be said on their behalf. 
From these examples, we can also see the possibility of purely or mostly local informational cascades. Outside the academic world, some communities come to believe that abandoned hazardous waste dumps are extremely dangerous, whereas others think that they pose no hazard at all. So, too, some law schools might come to embrace the economic analysis of law, whereas others might see enthusiasm for traditional doctrinal analysis - not because of a large number of independent judgments, but because of mutual interactions and influences. 
It should be clear that something important is missing from the picture: people's concerns about their reputations.' Generally people care about what others think of them, and most academics are, on this count, like most other people. 
In many law schools and economics departments, an effort to show that centralized planning really can work well, or to vindicate socialism, or to show that people are irrational, would be very risky, no matter the quality of the relevant work. People might be ridiculed. They might well jeopardize their careers. At many law schools, the same would be true for people who attempted to show that current differences between men and women are biological rather than social, and to bring evidence to that effect to bear on legal issues. 
Reputational considerations influence the public behavior of most people, not excluding academics. This is not because people lack integrity, or are sycophantic, or are unwilling to follow their own paths. It is simply because most people, most of the time, want others to think well - or at least not ill - of them. Of course, people have varying susceptibility to reputational pressures. Some people can withstand a great deal; others will be inclined to take the safe course, showing reluctance to say, or especially to publish, anything that could create trouble for them in the future. And of course sometimes those who incur reputational sanctions in one place (the nonacademic world, for example) will reap reputational benefits elsewhere (perhaps their local academic community). Those who seem to be venturing out on their own, and to be "brave," might in fact be motivated by the goal of gaining status within a particular group. 
Because most people care about the views of others, and because people have varying, rather than uniform, susceptibility to reputational pressures, it is easy to imagine reputational cascades with respect to actions or stated beliefs.9 Suppose, for example, that A and B would think ill of anyone who argues that the minimum wage should be significantly increased. C, who is not sure what to think about a higher minimum wage, might be unmoved privately by the views of A and B, but nonetheless might not want to incur the wrath of A and B, or to seem ignorant of basic economic principles, or to appear indifferent to economic efficiency. If so, C might show no enthusiasm for an increase in the minimum wage, or might even agree with A and B that an increase would be a bad idea. If D is otherwise in equipoise, she might be most reluctant to oppose A, B, and C publicly. Mounting reputational pressures might well lead E, F, G, and H, and many more, (2000). 
to join the bandwagon. Eventually a large number of people might speak as if the minimum wage should not be increased. The result would be to affect academic discussion of government's role in the labor market, including the treatment of this topic in articles and books. 
Here, too, a highly stylized example seems to help account for many shifts in the academic world. The rise of feminism within legal academia undoubtedly has a great deal to do with reputational and informational incentives. In the early 1980s, those who expressed contempt for feminist scholarship were rarely punished for doing so, and were sometimes rewarded. Currently those who express contempt for feminist scholarship generally (of course not always) put their reputation in considerable danger: If a young academic chooses to write on certain topics, or from certain points of view, the reputational sanctions might be quite severe. At the University of Chicago Law School, I cannot recall many faculty members expressing public support for a substantial increase in the minimum wage, though I would not be surprised if more than one faculty member actually believes that such an increase would be a good idea. Five years ago, those who borrowed from behavioral economics were viewed with considerable suspicion inside the world of law and economics; through a cascade effect, this is decreasingly true. 
It follows that "political correctness" is hardly a narrow phenomenon involving the practices of left-leaning academics. Wherever reputational pressures are in place, a form of "political correctness" will discipline action and public statements. Reputational cascades are a possible consequence. 
V. 
GROUP POLARIZATION AND ACADEMIC "SCHOOLS" 
A closely related phenomenon helps explain the initial growth of academic fashions and gives some guidance on how to create, and how not to create, an academic "school." The phenomenon is that of group polarization."In brief, the idea behind group polarization is that when a group of people engages in deliberation, group members will move toward a more extreme position in line with their predeliberationinclinations. This is the typical pattern among deliberating bodies. Thus, for example, a group of Federalist Society members, inclined to support originalism, is likely to be extremely enthusiastic about originalism after discussing it with one another. So, too, a semiformal organization of law professors, meeting once a month, is likely to emerge with a stronger commitment to critical race theory if its members are inclined, before discussion, to be favorably disposed toward critical race theory. It would be easy to multiply examples. 
Massive evidence, from many different countries, supports the basic prediction. Why does group polarization occur? Though no cascade need be involved,1 the two principal explanations are close to the explanations for informational and reputational cascades. The first involves informational influences. In a deliberating group with an initial tendency in favor of X and against Y, there will be a disproportionate number of arguments in favor of X, simply because most people will speak out on behalf of X. Group members will have thought of some, but not all, of the arguments in that direction. After deliberating, the arguments for X will seem stronger to individual members, and the arguments for Y will seem even weaker. It is to be expected that discussion will move people to a more extreme form of their original enthusiasm for X. 
The second explanation for group polarization points to social influences. Most people, emphatically including professors of law, care about their reputations and their self-conception. Suppose, for example, that you are inclined to think that affirmative action does not offend the Constitution, but you are not entirely sure; suppose too that you find yourself in a group that also rejects the idea that affirmative action offends the Constitution. If you think of yourself as the sort of person who is, more than most, inclined to support the constitutionality of affirmative action programs, you might move a bit, if only to maintain your reputation within the group and your self-conception on the issue at hand. The evidence strongly supports the proposition that this happens. 2 
In the academic context, the lesson is simple. A group of likeminded people, thinking about some issue or topic, is highly likely to move toward a more extreme position, not merely fortifying but amplifying their predeliberation inclinations. Through this route, it is possible to make some progress in understanding the creation and effects of academic "schools." In the early 1980s, for example, the critical legal studies movement flourished at Harvard Law School in particular, no doubt in part because of the presence of members who talked a great deal with one another and fueled their predeliberation inclinations. Several influential books emerged from these discussions."3 In roughly the same period, the Federalist Society was created at Chicago and Yale, and the existence of a group of like-minded people undoubtedly helped to fuel certain commitments. In fact, it is reasonable to speculate that the growth of conservative legal thought, within both faculties and student groups, has had a great deal to do with the exis11. This is because group polarization can result from simultaneous independent influences on group members. 
tence of a collection of people who are relatively well-organized and who are able to ensure that like-minded people can find some kind of home. 
VI. QUALIFICATIONS, EXTENSIONS, IMPLICATIONS 
Informational and reputational influences, as well as group polarization, play a significant role in academic life. Cascade effects are present here as elsewhere. For this reason, we should expect a large number of fads and fashions in the academic study of law. I would predict, for example, that a citation analysis would show many academic "bubbles" - rapid rises and declines in references to certain ideas and people.14 But this basic sketch omits important parts of the overall picture. 
With respect to both informational signals and reputational pressures, all people are not created equal. Some carry more weight than others. For example, the signals sent by well-known academics, and academics at well-known schools, are likely to be especially loud. If faculty members at Yale end up endorsing a new method for understanding law, there might seem to be particularly good reason to take that method seriously. And it is less likely that people who embrace the method will face the kind of reputational sanction that could be imposed if the method were being used at a little-known school. Those who are in a position to start cascades operate as leaders, above all because of the social amplification of their voices.15 Note that this amplification can occur independently of the merits of the argument being made. In listening carefully to well-known people, or to people at wellknown schools, followers are probably behaving rationally, because such people are unusually likely to be interesting or correct, simply as a statistical matter. But there are no guarantees here, and hence arguments can be amplified even if they are meritless. (Perhaps the resulting bubble will eventually pop, as discussed below.) 
Some of the relevant leaders are simply saying what they think to be true; others affirmatively want followers, perhaps because they seek status, or perhaps because they want to ensure that their ideas are disseminated. Such people take steps self-consciously to promote cascade effects, perhaps by organizing conferences, reading groups, or even journals. More specifically, we can describe as "polarization entrepreneurs" those people who foster deliberative groups of like14. Some support can be found in Robert Ellickson, Trends in Legal Scholarship:A StatisticalStudy, 29 J. LEGAL STUD. 517, 527 (2000). 
minded people and ensure that participants share a common methodology or point of view. Exclusion of outsiders, and inclusion of a large number of insiders, is an important component of this strategy. 
An obvious implication is that if the goal is to spread ideas, it is probably best to begin by promoting discussion among groups of likeminded people. If members of such groups speak mostly or only to one another, views might become entrenched, and the entrenchment among the views of increasingly large groups might initiate a cascade effect. A much worse strategy - often a doomed strategy - is to ensure that people with new ideas are placed in heterogeneous groups, where their ideas are unlikely to travel, or might be squelched, or might even be subject to self-silencing. 
In fact, the forces here are compounded by another: the availability heuristic. It is well known that certain facts and ideas are cognitively "available," or highly salient, and that this cognitive availability can exert a large influence on beliefs and decisions.16 If a leader, or an idea, ends up widely known, through independent decisions or through cascade effects, dramatic changes in scholarly paths can be expected. 
B. Startingand Stopping Cascades 
Some people are relatively immune to the influences discussed here. As I have suggested, people who are confident about their views are especially likely to resist informational and reputational incentives. The point suggests that in some arenas, cascades are likely to arise quite infrequently. Academic areas are highly variable on this count, and academics in some domains have a great deal of confidence, which immunizes themselves from cascade effects. In fields with wellestablished methods and goals, we should expect cascades to be uncommon. In the sciences, for example, large-scale shifts certainly occur, but the existence of settled methods makes cascade effects unlikely 7 - far less probable than in, for example, comparative literature. Law, economics, and psychology are perhaps intermediate cases. 
This point raises an important question: When and why do academic cascades start and stop? A crucial reason has to do with external shocks. Suppose, for example, that a group of people believes some fact. Suppose that evidence shows that the belief is false. The belief will fade because it has been demonstrated to be wrong. 
But external shocks can take many different forms. Sometimes academic trends, especially in law, have nothing to do with demon16. See JONATHAN BARON, THINKING AND DECIDING (3d ed. 2000). 
REVOLUTION (1970). strated fact, but are greatly affected by what happens outside of the academic domain. For example, the selection of Antonin Scalia to be a member of the Supreme Court undoubtedly had a great deal to do with the legitimation of originalism and textualism, methods favored by Justice Scalia. This is partly because Justice Scalia's opinions provided a kind of focal point for academic debate; it is also because his office conferred a kind of legitimacy on arguments that might otherwise be easy to dismiss. Nor is it irrelevant that some of Justice Scalia's law clerks became academics. In fact, a significant source of informational and reputational influences will come, directly and indirectly, from the selection of Supreme Court clerks, and from the choice, among clerks of particular justices, to become law professors. In a previous generation, the law clerks of Felix Frankfurter, greatly influenced by Frankfurter, became influential academics; the same appears to be true of Scalia clerks today. 
More generally, the 1980 election of President Reagan made it most unlikely that the Supreme Court would continue to use the Equal Protection and Due Process Clauses as bases for announcing a series of new rights for disadvantaged people. Sensibly enough, academics interested in social reform showed decreasing interest in elaborating legal doctrine for that purpose. Perhaps the rise of interest in constitutional deliberation outside of the courtroom had something to do with the Court's lack of receptivity to the professors' arguments. Perhaps diminished interest in judicial review had something to do with the changing composition of the Court.18 Highly visible public events with legal dimensions, such as the 1998 Clinton impeachment and the 2000 postelection struggle between George W. Bush and Al Gore, will inevitably affect people's choice of what to write about. (Perhaps Bush v. Gore will inaugurate a new era of neorealism, questioning the division between law and politics.) Academics may or may not follow the election returns, but in law, the election returns can set the academic agenda. 
Other external shocks can come from developments in adjacent fields. If, for example, economists show a great deal of interest in the idea of spontaneous ordering, academic lawyers are likely to show an interest in that topic, too. Part of the reason is informational: the fact that a certain topic interests economists is likely to be important to academic lawyers, many of whom care about what economists think. If there is a resurgence of interest in utilitarianism within philosophy, law professors are likely to write about utilitarianism. The extraordinary interest in the work of John Rawls confirms this point. Critical theory provides another case in point, with Jurgen Habermas and Michel Foucault, for example, exerting a significant influence on legal scholarship by virtue of their prominence within closely related fields. Of course, developments within adjacent fields might well be a product of the kinds of influences discussed here. 
There is a final issue, perhaps in tension with the general argument offered thus far. It is useful to distinguish between ideas and methods on which multiple people can build for a long time, and ideas and methods that do not lead to much in the way of further work.19 The notion that people are rational, self-interested profit maximizers is fertile, in the sense that it has applications to many domains of law, helping to produce predictions that can be tested and used. Though it is too early to say, I believe that the same is true for the notion that people are boundedly rational, and also for the claim that people are not only self-interested. 20 The idea that law is pervasively based on male practices and understandings is also easily used as a basis for assessing, or reassessing, many domains of law. But some claims tend to "burn out," in the sense that once they have been voiced, there is little that can be done with them, even if they are true. Perhaps this is the case for the contention that law is "political," an important and illuminating partial truth, but one with which it is not easy, in the aftermath of legal realism, to do a great deal of illuminating further work. 
CONCLUSION: THE MARKETPLACE OF IDEAS 
Academics, like everyone else, are susceptible to informational and reputational influences, and cascade effects are as likely to be found in the academic domain as elsewhere. Notwithstanding the expertise and confidence of many academics, academic life has its own fads and fashions, and the factors discussed here play a role in their development. I believe that these factors have played a role in many trends in legal theory, including critical legal studies, economic analysis of law, feminism, textualism and originalism in constitutional law, critical race theory, rights-based accounts associated with Ronald Dworkin and others (many at New York University), law and literature, and (more recently) behavioral law and economics. 
By way of conclusion, it is worth emphasizing that the basic account contains both a prescription and a cautionary note. The prescription is that those who seek to promote ideas will do best to ensure, above all, that those ideas have an opportunity to develop through frequent discussions among like-minded people. Most would19. Cf.the discussion of progressive and degenerate research programs in Imre Lakatos, Falsificationand the Methodology of Scientific Research Programmes,in CRITICISM AND THE GROWTH OF KNOWLEDGE (Imre Lakatos & Alan Musgrave eds., 1970). 
AND ECONOMICS (Cass R. Sunstein ed., 2000). be "schools" fail, but those that succeed often transform the field; and when they do so, group polarization is part of the reason. 
The cautionary note is that in law and many other academic fields, ideas may spread and prosper, not because they are good, but because dozens, hundreds, or even thousands of imperfectly informed people have fortified the very signals by which they have been influenced. Whether bad ideas can prosper for a long time is another matter. Frequently good arguments and good evidence will puncture them, at least when there is agreement about the underlying criteria. But if the account here is correct, longevity, even for bad ideas, is hardly out of the question. 
Under the leadership of Chief Justice William Rehnquist, the Supreme Court of the United States has generally been minimalist, in the sense that it has attempted to say no more than is necessary to decide the case at hand, without venturing anything large or ambitious.' To some extent, the Court's minimalism appears to have been a product of some of the justices' conception of the appropriately limited role of the judiciary in American political life. To some extent, the tendency toward minimalism has been a product of the simple need to assemble a majority vote. If five or more votes are sought, the opinion might well tend in the direction of minimalism, reflecting judgments and commitments that can command agreement from diverse people. 
To be sure, the Court has been willing, on occasion, to be extremely aggressive. In a number of cases, the Court has asserted its own, highly contestable vision of the Constitution against the democratic process. This aggressive strand has been most evident in a set of decisions involving federalism; it can be found elsewhere as well.' But generally these decisions have been minimalist too. Notwithstanding their aggressiveness, they tend to decide the case at hand, without making many commitments for the future. Sometimes those decisions have even been "subminimalist," in the sense that they have said less than is required to justify the particular outcome.4 
t Karl N. Llewellyn Distinguished Service Professor, Law School and Department of Political Science, The University of Chicago. I am grateful to Richard Posner and Mary Anne Case for helpful comments on an earlier draft. Apologies for the title choice to Professor Robert C. Ellickson. See his superb (and unrelated) Order Without Law: How Neighbors Settle Disputes (Harvard 1991). 
1 For a general discussion of judicial minimalism, see Cass R. Sunstein, One Case at a Time: JudicialMinimalism on the Supreme Court(Harvard 1999). 
2 United States v Morrison,120 S Ct 1740,1759 (2000) (striking down the Violence Against Women Act as beyond the power of Congress under the Commerce Clause); City of Boerne v Flores,521 US 507, 536 (1997) (holding that the Religious Freedom Restoration Act exceeded Congress's remedial powers under the Fourteenth Amendment); United States v Lopez, 514 US 549, 567-68 (1995) (holding that the prohibition of firearm possession near schools was outside the power of Congress). 
3 Justices Antonin Scalia and Clarence Thomas are fairly consistent maximalists, on the ground that they favor rule-bound decisions. See Antonin Scalia, The Rule of Law as a Law of Rules, 56 U Chi L Rev 1175, 1178-86 (1989) (discussing reasons to prefer rules over judicial discretion). 
4 See Romer v Evans, 517 US 620,635 (1996) (holding law forbidding special government protections for homosexuals to be invalid under the Equal Protection Clause, without fully explaining its consistency with earlier decisions). 
In the Court's two decisions involving the 2000 presidential election, minimalism was on full display. The Court's unanimous decision in Bush v Palm Beach County Canvassing Board was firmly in the minimalist camp. Here the Court refused to resolve the most fundamental issues and merely remanded to the Florida Supreme Court for clarification. The Court's 5-4 decision in Bush v Gore' was also minimalist in its own way, for it purported to resolve the case without doing anything for the future. But here the Court effectively ended the presidential election. It. did so with rulings, on the merits and (especially) on the question of remedy, that combined hubris with minimalism. 
The Court's decision in Bush v Gore did have two fundamental virtues. First, it produced a prompt and decisive conclusion to the chaotic post-election period of 2000. Indeed, it probably did so in a way that carried more simplicity and authority than anything that might have been expected from the United States Congress. The Court might even have avoided a genuine constitutional crisis. Second, the Court's equal protection holding carries considerable appeal. On its face, that holding has the potential to create the most expansive, and perhaps sensible, protection for voting rights since the Court's oneperson, one-vote decisions of mid-century.' In the fullness of time, that promise might conceivably be realized within the federal courts, policing various inequalities with respect to voting and voting technology. But it is far more likely that the Court's decision, alongside the evident problems in the Florida presidential vote, will help to spur corrective action from Congress and state legislatures. 
The Court's decision also had two large vices. First, the Court effectively resolved the presidential election not unanimously, but by a 5-4 vote, with the majority consisting entirely of the Court's most conservative justices. Second, the Court's rationale was not only exceedingly ambitious but also embarrassingly weak. However appealing, its equal protection holding had no basis in precedent or in history. It also raises a host of puzzles for the future, which the Court appeared to try to resolve with its minimalist cry of "here, but nowhere else." Far more problematic, as a matter of law, was the majority's subminimalist decision on the issue of remedy. By terminating the manual recount in 5 121 S Ct 471 (2000) (per curiam). 6 121 S Ct 525 (2000) (per curiam). 
7 See Reynolds v Sims, 377 US 533,568 (1964) (holding an apportionment decision violative of the Equal Protection Clause because of the different weight given to different votes); Baker v Carr, 369 US 186, 208-37 (1962) (holding state apportionment decision not to present nonjusticiable political questions). Florida, the Court resolved what it acknowledged to be a question of Florida law, without giving the Florida courts the chance to offer an interpretation of their own state's law. 
In a case of this degree of political salience, the Court should assure the nation, through its actions and its words, that it is speaking for the law, and not for anything resembling partisan or parochial interests. A unanimous or near-unanimous decision can go a long way toward providing that assurance, because agreement between diverse people suggests that the Court is really speaking for the law. So too for an opinion that is based on reasoning that, whether or not unassailable, is so logical and clear as to dispel any doubt about the legitimacy of the outcome. The Court offered no such opinion. 
From the standpoint of constitutional order, the Court might well have done the nation a service. From the standpoint of legal reasoning, the Court's decision was very bad. In short, the Court's decision produced order without law. 
I. PRELIMINARIES 
States Supreme Court, in the litigation over the outcome of the presidential election in Florida. In sequence, the Court's interventions consisted of the surprising grant of certiorari on November 24, 2000;8 the unanimous, minimalist remand on December 4, 2000;' the grant of a stay, and certiorari, on December 9, 2000;1" and the decisive opinion in Bush v Gore on December 
On November 13, Florida Secretary of State Katherine Harris announced that the statutory deadline of November 14, 2000, was final, and that she would not exercise her discretion so as to allow extensions.2 On November 21, the Florida Supreme Court interpreted state law to require the Secretary of State to extend the statutory deadline for a manual recount." This was a highly controversial inter 
HeinOnline -- 68 U. Chi. L. Rev. 759 2001 pretation of Florida law, and it might well have been wrong. At the time, however, any errors seemed to raise issues of state rather than federal law. 
In seeking certiorari, Bush raised three federal challenges to the decision of the Florida Supreme Court." First, he argued that by changing state law, the Florida Court had violated Article II of the United States Constitution, which provides that states shall appoint electors "in such manner as the Legislature," and not any court, may direct.16 Second, Bush invoked a federal law saying that a state's appointment of electors is "conclusive" if a state provides for the appointment of electors "by laws enacted prior to the day fixed" for the election." According to Bush, the Florida court did not follow, but instead changed, the law "enacted prior" to Election Day, and in his view this change amounted to a violation of federal law.8 Third, Bush argued that the manual recount would violate the Due Process and Equal Protection Clauses, because no clear standards had been established to ensure that similarly situated people would be treated similarly." 
At the time, most observers thought it exceedingly unlikely that the Court would agree to hear the case. Even if the Florida Supreme Court had effectively "changed" state law, it appeared improbable that the United States Supreme Court could be convinced to say so. Whatever the merits, the Court seemed unlikely to intervene into a continuing controversy over the presidential vote in Florida. This was not technically a "political question,""° but it did not seem to be the kind of question that would warrant Supreme Court involvement, certainly not at this preliminary stage. To the general surprise of most observers, the Court agreed to grant certiorari, limited to the first two questions raised by Bush. 
Bush asked the United States Supreme Court to hold that because the Florida Supreme Court had violated the federal Constitu 
HeinOnline -- 68 U. Chi. L. Rev. 760 2001 tion and federal law, Florida's Secretary of State had the authority to certify the vote as of November 14. For his part, Gore wanted the Court to affirm the Florida Supreme Court on the ground that that court had merely interpreted the law.2 The United States Supreme Court refused these invitations and took an exceptionally small step, asking the state supreme court to clarify the basis for its decision.' Did the state court use the Florida Constitution to override the will of the Florida legislature? In the Court's view, that would be a serious problem, because the United States Constitution requires state legislatures, not state constitutions, to determine the manner of appointing electors." The Supreme Court also asked the state court to address the federal law requiring electors to be appointed under state law enacted "prior to" Election Day.7 In its own opinion, the Florida Supreme Court had said nothing about that law. 
This was judicial minimalism in action. Why did the Court proceed in this way? It seems possible that some of the justices refused to settle the merits on principle, thinking that the federal judiciary should insert itself as little as possible into the continuing electoral struggle. But the most likely explanation is that the Court sought unanimity and found, as groups often do, that unanimity is possible only if as little as possible is decided. 
B. 
On December 8, the Florida Supreme Court ruled, by a vote of 4-3, that a manual recount was required by state law, and it thus accepted Gore's contest.26 This decision threw the presidential election into apparent disarray. With the manual recount beginning, it became quite unclear whether Bush or Gore would emerge as the winner. 
On December 9, the Supreme Court issued a stay of the decision of the Florida Supreme Court.7 This was the first genuinely extraordinary action taken by the United States Supreme Court. It was not only extraordinary but also a departure from conventional practice, and one that is difficult to defend on conventional legal grounds-not because Bush lacked a substantial probability of success, but because he had shown no irreparable harm. 
To be sure, some harm would have come to Bush from the continuation of the manual recount. It is entirely possible that the recount would have narrowed the gap between Bush and Gore. This would have been an unquestionable harm to Bush, in the nontrivial sense that it would have raised some questions about the legitimacy of his ensuing presidency, if it had subsequently been determined that the manual recount was unlawful." But the question remains: How serious and irreparable would this "harm" have been? If the manual recount was soon to be deemed unlawful, would the Bush presidency really have been "irreparably" harmed? This is extremely doubtful. 
At the same time, the stay of the manual recount would seem to have worked an irreparable harm to Gore. For Gore, time was very much of the essence, and if the counting was stopped, the difficulty of completing it in the requisite period would become all the more serious. By itself, the Supreme Court's stay of the manual recount did not hand the election to Bush. But it came very close to doing precisely that. 
In these circumstances, can anything be said on behalf of the stay? A reasonable argument is available, at least in retrospect. Suppose that a majority of the Court was entirely convinced that the manual recount was unlawful, perhaps because in the absence of uniform standards, similarly situated voters would not be treated similarly. If the judgment on the merits was clear, why should the voting be allowed to continue, in light of the fact that it would undoubtedly have to be stopped soon in any case, and its continuation in the interim would work some harm to the legitimacy of the next president? The question suggests that if the ultimate judgment on the merits was clear, the stay would not be so hard to defend. If the likelihood of success is overwhelming, the plaintiff should not be required to make the ordinary showing of irreparable harm.29 The problem, then, was less the stay than the Court's ambitious, poorly reasoned judgment on the merits. 
28 As emphasized by Justice Scalia, see id at 512 (Scalia concurring). 
29 Cuomo v United States Nuclear Regulatory Commission, 772 F2d 972, 974 (DC Cir 1985). 
On the merits, there are two especially striking features to the Court's decision. The first is that six justices were unwilling to accept Bush's major submission, to the effect that the Florida Supreme Court had produced an unacceptable change in Florida law." The second is that five members of the Court accepted the adventurous equal protection argument. 
The equal protection claim does have considerable appeal, at least as a matter of common sense. If a vote is not counted in one area when it would be counted in another, something certainly seems to be amiss. Suppose, for example, that in one county, a vote will not count unless the stylus goes all the way through, whereas in another country, a vote counts merely because it contains a highly visible "dimple." If this is the situation, some voters can legitimately object that they are being treated unequally for no good reason. 
In its per curiam opinion, the Court spelled out the equal protection rationale in some detail. "In some cases a piece of the card-a chad -is hanging, say by two corners. In other cases there is no separation at all, just an indentation."'" The disparate treatment of these markings in different counties was unnecessary, because the "search for intent can be confined by specific rules designed to ensure uniform treatment.". In Florida, that search was not so confined, for the record suggested that in Miami-Dade County, different standards had been applied in defining legal votes; and Palm Beach County appeared to go so far as to change its standards during the process of counting. To this, the Court added "further concerns."33 These included an absence of specification of "who would recount the ballots," leading to a situation in which untrained members of "ad hoc teams" would be involved in the process.4 And "while others were permitted to observe, they were prohibited from objecting during the recount."3 Thus the Court concluded that the recount process "i sinconsistent with the minimum 30 I will not discuss that issue here. In brief, I think that the argument becomes less convincing the more one reflects on it. To be sure, a decision by a state court to disregard state law would raise serious questions under Article II. And I do not believe that the Florida Supreme Court correctly interpreted state law. But the majority's view was not so implausible as to amount to a change, rather than an interpretation. See Bush v Gore, 121 S Ct at 543-45 (Souter dissenting); id at 554 (Breyer dissenting). 
31 Id at 530. 32 Id. 33 Id at 532. 34 Id. 35 Id. procedures necessary to protect the fundamental right of each voter in the special instance of a statewide recount under the authority of a single state judicial officer."' 
The Court was well aware that its equal protection holding could have explosive impiications for the future, throwing much of state election law into constitutional doubt. Thus the Court emphasized the limited nature of its ruling: "The question before the Court is not whether local entities, in the exercise of their expertise, may develop different systems for implementing elections. Instead, we are presented with a situation where a state court with the power to assure uniformity has ordered a statewide recount with minimal procedural safeguards."37 B. 
There are three problems with this reasoning. First, the Court's decision lacked any basis in precedent. Second, the Court's effort to cabin the reach of its decision seemed ad hoc and unprincipled-a common risk with minimalism. And third, the system that the Court let stand seemed at least as problematic, from the standpoint of equal protection, as the system that the Court held invalid. 
Nothing in the Court's previous decisions suggested that constitutional questions would be raised by this kind of inequality. The cases that the Court invoked on behalf of the equal protection holdingmostly involving one-person, one-vote and the poll tax' -were entirely far afield. To be sure, the absence of precedential support is not decisive; perhaps the problem had simply never arisen. But manual recounts are far from uncommon, and no one had ever thought that the Constitution requires that they be administered under clear and specific standards. 
To make the problem more vivid, suppose that in 1998, a candidate for statewide office-say, the position of attorney general-lost after a manual recount, and brought a constitutional challenge on equal protection grounds, claiming that county standards for counting votes were unjustifiably variable. Is there any chance that the disappointed candidate would succeed in federal court? In all likelihood 36 Id. 37 Id. 
38 Harperv Virginia Board of Elections,383 US 663,666 (1966); Reynolds v Sims, 377 US 533,568 (1964). the constitutional objection would fail; in most courts, it would not even be taken seriously. The rationale would be predictable, going roughly like this: "No previous decision of any court supports the view that the Constitution requires uniformity in methods for ascertaining the will of the voter. There is no violation here of the principle of oneperson, one-vote. Nor is there any sign of discrimination against poor people or members of any identifiable group. There is no demonstration of fraud or favoritism or self-dealing. In the absence of such evidence, varying local standards, chosen reasonably and in good faith by local officials, do not give rise to a violation of the federal Constitution. In addition, a finding of an equal protection violation would entangle federal courts in what has, for many decades, been seen as a matter for state and local government." 
Of course it is possible to think that this equal protection holding would be wrong. Whether the federal Constitution should be read to cabin local discretion in this way is a difficult question. The problem is that in a case of such great public visibility, the Court embraced the principle with no support in precedent, with little consideration of implications, and as a kind of bolt from the blue. 
It is not at all clear how the rationale of Bush v Gore can be cabined in the way that the Court sought to do. What is missing from the opinion is an explanation of why the situation in the case is distinctive, and hence to be treated differently from countless apparently similar situations involving equal protection problems. The effort to cabin the outcome, without a sense of the principle to justify the cabining, gives the opinion an unprincipled cast. 
Suppose, for example, that a particular area in a state has an old technology, one that misses an unusually high percentage of intended votes. Suppose that many areas in that state have new technology, capable of detecting a far higher percentage of votes. Suppose that voters in that area urge that the Equal Protection Clause is violated by the absence of uniformity in technology. Why doesn't Bush v Gore make that claim quite plausible? Perhaps it can be urged that budget39 One of the real oddities of the majority opinion is that it was joined by two JusticesScalia and Thomas-who have insisted in their commitment to "originalism" as a method of constitutional interpretation. There is no reason to think that by adopting the Equal Protection Clause, the nation thought that it was requiring clear and specific standards in the context of manual recounts in statewide elections. In fact it is controversial to say that the Fourteenth Amendment applies to voting at all. The failure of Justices Scalia and Thomas to suggest the relevance of originalism, their preferred method, raises many puzzles. ary considerations, combined with unobjectionable and longstanding rules of local autonomy, make such disparities legitimate. In the context of a statewide recount administered by a single judge -the situation in Bush v Gore-these considerations appear less relevant. But it is easy to imagine cases in which those considerations do not seem weighty. I will return to these questions below. 
The system that the recount was designed to correct might well have been as arbitrary as the manual recount that the Court struck down-and hence the Court's decision might well have created an even more severe problem of inequality. Consider the multiple inequalities in the certified vote. Under that vote, some machines counted votes that were left uncounted by other machines, simply because of different technology. Where optical scan ballots were used, for example, voters were far more likely to have their votes counted than where punchcard ballots were used. In Florida, fifteen of every one thousand punchcard ballots showed no presidential vote, whereas only three of every optically scanned ballot showed no such vote.4 These disparities might have been reduced with a manual recount. If the broad principle of Bush v Gore is correct, manual recounts might even seem constitutionally compelled. But the Court's decision, forbidding manual recounts, ensured that the relevant inequalities would not be corrected. 
Nor were the machine recounts free from inequality. Some counties merely checked the arithmetic; others put ballots through a tabulating machine. The result is a significant difference in the effect of the machine recount. If the constitutional problem consists of the different treatment of the similarly situated, then it seems entirely possible that the manual recount, under the admittedly vague "intent of the voter" standard, would have made things better rather than worse and that the decision of the United States Supreme Court aggravated the problem of unjustified inequality. 
On the merits, then, the most reasonable conclusion is not that the Court's decision was senseless-it was not- but that it lacked support in precedent or history, that it raised many unaddressed issues with respect to scope, and that it might well have authorized equality 40 Bush v Gore, 121 S Ct at 552. problems as serious as those that it prevented. In these ways, the majority's opinion has some of the most severe vices of judicial minimalism. In fact this was a subminimalist opinion, giving the appearance of having been built for the specific occasion. 
C. 
Now turn to the Court's decision on the issue of remedy. If the manual recount would be unconstitutional without clear standards, what is the appropriate federal response? Should the manual recount be terminated, or should it be continued with clear standards? At first glance, that would appear to be a question of Florida law. If the Florida legislature would want manual recounts to continue, at the expense of losing the federal safe harbor, then manual recounts should continue. If the Florida legislature would want manual recounts to stop, in order to preserve the safe harbor, then manual recounts should stop. 
Why did the Supreme Court nonetheless halt the manual recount? The simple answer is that the Court thought it clear that the Florida Supreme Court would interpret Florida law so as to halt the process. As the Court wrote, 
The Supreme Court of Florida has said that the legislature intended the State's electors to 'participate[ ] fully in the federal electoral process .... Because it is evident that any recount seeking to meet the December 12 date will be unconstitutional for the reasons we have discussed, we reverse the judgment of the Supreme Court of Florida ordering a recount to proceed. 
Thus the Court concluded that as a matter of Florida law, a continuation of the manual recount "could not be part of an 'appropriate' order authorized by" Florida law. 
This was a blunder. It is true that the Florida Supreme Court had emphasized the importance, for the Florida legislature, of the safe harbor provision. But the Florida courts had never been asked to say whether they would interpret Florida law to require a cessation in the counting of votes, if the consequence of the counting would be to extend the choice of electors past December 12. In fact the Florida Court's pervasive emphasis on the need to ensure the inclusion of law41 Id at 533. 42 Id. 
S Ct 525. ful votes" would seem to indicate that if a choice must be made between the safe harbor and the inclusion of votes, the latter might have priority. It is not easy to explain the United States Supreme Court's failure to allow the Florida Supreme Court to consider this issue of Florida law. 
Here, then, is the part of the United States Supreme Court's opinion that is most difficult to defend on conventional legal grounds. 
Might anything unconventional help to defend the Court's conclusion? I have suggested that the Court's decision produced order. In fact it might well have averted chaos. It is worthwhile to spend some time on this question, because it provides the best explanation of the Court's otherwise inexplicable approach. 
Let us briefly imagine what would have happened if the Court had affirmed the Florida Supreme Court, or remanded for continued counting under a constitutionally adequate standard. In the event of an affirmance, manual counting would of course have continued. In the event of a remand, the Florida Supreme Court would have had to sort out the relationship between the legislature's desire to preserve the safe harbor and its desire to ensure an accurate count. That Court had been divided 4-3 on the question whether a manual recount should be required at all. It is reasonable to speculate that the three dissenters would continue to object to the manual recount. The question is whether any of the four members of the majority would conclude that the December 12 deadline took precedence over the continuation of the contest. There is certainly a chance that the Florida Supreme Court would have terminated the election at that point. But if it failed to do so, things would have gotten extremely messy. 
Almost certainly, the Republican-dominated Florida legislature would have promptly sent a slate of electors, thus producing two (identical) slates for Bush-the November 26 certification and the legislatively specified choice. The legislative slate would in turn have been certified by the Secretary of State and the Governor of Florida. In the meantime the counting would, by hypothesis, have continued, well after the expiration of the December 12 safe harbor date. If Bush had won the manual recount, things would be very simple. But suppose Gore had won; what then? Would the Secretary of State have voluntarily certified the new count? Would the Governor of Florida have signed off on the certification? It is not at all clear that Florida's 44 Gore v Harris,772 S2d at 1256-57. executive officials would do what the Florida courts wanted them to do. And if the Secretary of State and the Governor refused, how would the Florida courts have responded? Would they have threatened executive officials with contempt? How would they have responded to the threat? At the very least, there is a risk here of a minor constitutional crisis within Florida itself. 
Suppose that this problem had been solved-and that three certified votes from Florida had come before Congress. At that point, both houses of Congress, acting separately, would have to vote on which certification to accept.45 Almost certainly the Republican-dominated House of Representatives would have accepted a Bush slate. The Senate, split 50-50, would be much harder to call; perhaps some Democrats, in conservative states won by Bush, would have agreed to accept the Bush slate from Florida. But perhaps there would have been an even division within the Senate. If so, Vice President Gore would have been in a position to cast the deciding vote. Suppose that he did-and that he voted for the third Florida slate, and thus for himself, so as to ensure that the House and the Senate would come to different conclusions.At that point, the outcome is supposed to turn on the executive's certification.' But which was that? Here the law provides no clear answers. At this point, a genuine constitutional crisis might have arisen. It is not clear how it would have been settled. No doubt the nation would have survived, but things would have gotten very messy. 
The Court's decision made all of these issues academic. It averted what would have been, at the very least, an intense partisan struggle, lacking a solution that is likely to have been minimally acceptable to all sides. I do not mean to suggest that the Supreme Court majority was correct. The Court owes a duty of fidelity to the law. Pragmatic concerns are certainly relevant in the face of ambiguous law, but there is a reasonable argument that the Court abandoned the law simply because of pragmatic concerns. What I hope to have shown is why the Court might have done the nation a big favor. 
For the future, the most important question involves the scope of the right recognized in Bush v Gore. Notwithstanding the Court's efforts, that right is not at all easy to cabin, at least as a matter of basic principle. On its face, the Court appears to have created the most expansive voting right in many decades. 
45 See 3 USC § 15 (1994). 46 Id. 
At its narrowest, the Court has held that in the context of a statewide recount proceeding overseen by a single judge, the standard for counting votes must be (a) uniform and (b) concrete enough to ensure that similarly situated people will be treated similarly. This holding extends well beyond the context of presidential elections; it applies to statewide offices, not just federal offices. thirtyBystaittseeslfftahilistios sapseucbifsytacnotinaclreretenosvtaantidoanrdosf cfourrrmenatnluaawl, srienccoeunotvs.er4 This does not mean that state legislatures must set down clear standards in advance; a decision by state judges should suffice. But the inevitable effect of the opinion will be to increase the pressure for legislative reform at the state and possibly even the national level. Any state legislature would be well-advised to specify the standard by which votes will be counted in the context of a manual recount. All this should count, by itself, as a gain for sense and rationality in the recount process. 
B. 
It is hard to understand why the principle of Bush v Gore does not extend much further than the case itself, at least in the context of voting. Consider the following easily imaginable cases: 
percent of votes; wealthy counties have newer machinery that successfully counts 99 percent of votes. Those in poor counties mount a constitutional challenge, claiming that the difference in rejection rates is a violation of the Equal Protection Clause. 
does not involve poor and rich counties. It is simply the case that some areas use machines that have a near-perfect counting rate, and others do not. The distribution of machines seems quite random. 
version of the controversial "butterfly ballot"; most do not. It is clear that where the butterfly ballot is used, an unusual number of voters are confused, and do not successfully vote for the candidate of their choice. Does this violate the Equal Protection Clause? 
machinery from that used by citizens in New York. The consequence is that citizens in Alabama are far more likely to have their votes un47 See Bush v Gore, 121 S Ct at 540 n 2 (Stevens dissenting). counted than citizens in New York. Do they have a valid equal protection claim? What if the statistical disparity is very large? 
The Bush Court's suggestion that ordinary voting raises "many complexities" is correct;0 but how do those complexities justify unequal treatment in the cases just given? The best answer would point to two practical points: budgetary considerations and the tradition of local control. In light of these points, it might be difficult for some areas to have the same technology as others. Wealthy counties might prefer to purchase more expensive machinery, whereas poorer communities might devote their limited resources to other problems. Perhaps judicial caution in the cases just given can be justified in this way. But even if this is so, Bush v Gore plainly suggests the legitimacy of both state and national action designed to combat disparities of this kind. It is for this reason that the Court's decision, however narrowly intended, set out a rationale that might well create an extremely important (and appealing) innovation in the law of voting rights. Perhaps legislatures will respond to the invitation if courts refuse to do so. C. 
In fact the Court's rationale might extend more broadly still. Outside of the context of voting, governments do not impose the most severe imaginable constraints on official discretion. Because discretion exists, the similarly situated are treated differently.9 Perhaps the most obvious example is the "beyond a reasonable doubt" standard for criminal conviction, a standard that different juries will inevitably interpret in different ways. Is this unacceptable? 0 
In the abstract, the question might seem fanciful; but analogous constitutional challenges are hardly unfamiliar. In the 1960s and 1970s, there was an effort to use the Due Process and Equal Protection Clauses to try to ensure more rule-bound decisions, in such contexts as licensing and admission to public housing.' Plaintiffs argued that without clear criteria to discipline the exercise of discretion, there was a risk that the similarly situated would not be treated similarly, and that this risk was constitutionally unacceptable. But outside of the 48 Bush v Gore, 121 S Ct at 532. 
49 This is the basic theme of Kenneth Culp Davis, DiscretionaryJustice:A PreliminaryInquiry (LSU 1969). 
50 A possible answer is that no more rule-bound approach would be better, all things considered. This is a difference from Bush v Gore,where it was easy to imagine a rule-bound approach that would add constraints on discretion without sacrificing any important value. 
51 Hornsby v Allen, 326 F2d 605,610 (5th Cir 1964); Holmes v New York City HousingAuthority,398 F2d 262,264-65 (2d Cir 1968). most egregious settings, these efforts failed,2 apparently on the theory that rule-bound decisions produce arbitrariness of their own, and courts are in a poor position to know whether rules are better than discretionary judgments. Does Bush v Gore require courts to extend the limited precedents here? 
Perhaps it could be responded that because the choice between rule-bound and more discretionary judgments is difficult in many cases, judicial deference is generally appropriate-but not when fundamental rights, such as the right to vote, are at risk. If so, Bush v Gore has a limited scope. But does this mean that methods must be in place to ensure against differential treatment of those subject to capital punishment? To life imprisonment? I cannot explore these questions here. But for better or for worse, the rationale in Bush v Gore appears to make it necessary to consider these issues anew. 
CONCLUSION 
If the Supreme Court is asked to intervene in an electoral controversy, especially a presidential election, it should try to avoid even the slightest appearance that the justices are speaking for something other than the law. Unanimity, or near-unanimity, can go a long way toward providing the necessary assurance. Whether or not this is possible, the Court's opinion should be well-reasoned and rooted firmly in the existing legal materials. 
In Bush v Gore, the Court did not succeed on these counts. The 5-4 division was unfortunate enough; it was still worse that the fivemember majority consisted of the most conservative justices. Regrettably, the Court's opinion had no basis in precedent or history. To be sure, the equal protection argument had a certain appeal in common sense. But even if it were correct, the natural remedy would have been to remand to the Florida Supreme Court, to ask that court to say whether Florida law would favor the manual recount over the safe harbor provision, or vice-versa. This remedy seems especially sensible in light of the fact that the inequalities that the Court condemned might well have been less serious than the inequalities that the recount would have corrected. 
Nonetheless, there are two things to be said on behalf of the Court's ruling. First, the Court brought a chaotic situation to an abrupt end. From the standpoint of constitutional order, it is reasonable to 52 For examples of unsuccessful attempts to challenge unconditioned discretion violative of equal protection in these contexts, see Phelps v Housing Authority of Woodruff, 742 F2d 816, 822-23 (4th Cir 1984); Atlanta Bowling Center,Inc v Allen, 389 F2d 713,715-17 (5th Cir 1968). speculate that any other conclusion would have been far worse. In all likelihood, the outcome would have been resolved in Congress, and here political partisanship might well have spiraled out of control. Second, the principle behind the equal protection ruling has considerable appeal. In a statewide recount, it is not easy to explain why votes should count in one area when they would not count elsewhere. In fact the principle has even more appeal if understood broadly, so as to forbid similarly situated voters from being treated differently because their votes are being counted through different technologies. Understood in that broader way, the principle of Bush v Gore should bring a range of questionable practices under fresh constitutional scrutiny. 
Bush v Gore is likely to intensify public concern about unjustifiably aggressive decisions from the Supreme Court, and perhaps that concern will give the Court an incentive to be more cautious about unsupportable intrusions into the democratic arena. Far more important, Bush v Gore might come to stand for a principle, in legislatures if not courts, that greatly outruns the Court's subminimalist holding -a principle that calls for an end to the many unjustified disparities in treatment in voting and perhaps beyond. It would be a nice irony if the Court's weak and unprecedented opinion, properly condemned on democratic grounds, led to significant social improvements from the democratic point of view. 
Predictably Incoherent Judgments Cass R. Sunstein, Daniel Kahneman, David Schkade, and Ilana Ritov 
THE LAW SCHOOL 
THE UNIVERSITY OF CHICAGO This paper can be downloaded without charge at: 
The Chicago Working Paper Series Index: http://www.law.uchicago.edu/Lawecon/index.html The Social Science Research Network Electronic Paper Collection: 
http://papers.ssrn.com/paper.taf?abstract_id=279181 Preliminary draft 7/28/01 All rights reserved Cass R. Sunstein,* Daniel Kahneman,** David Schkade, *** and Ilana Ritov**** 
When people make moral or legal judgments in isolation, they produce a pattern of outcomes that they would themselves reject, if only they could see that pattern as a whole. A major reason is that human thinking is category-bound. When people see a case in isolation, they spontaneously compare it to other cases that are mainly drawn from the same category of harms. When people are required to compare cases that involve different kinds of harms, judgments that appear sensible when the problems are considered separately often appear incoherent and arbitrary in the broader context. Another major source of incoherence is what we call the translation problem: The translation of moral judgments into the relevant metrics of dollars and years is not grounded in either principle or intuition, and produces large differences among people.. The incoherence produced by category-bound thinking is illustrated by an experimental study of punitive damages and contingent valuation. We also show how category-bound thinking and the translation problem combine to produce anomalies in administrative penalties. The underlying phenomena have large implications for many topics in law, including jury behavior, the valuation of public goods, punitive damages, criminal sentencing, and civil fines. We consider institutional reforms that might overcome the problem of predictably incoherent judgments. Connections are also drawn to several issues in legal theory, including valuation of life, incommensurability, and the aspiration to global coherence in adjudication. 
“Why didn’t the Commission sit down and really go and rationalize this thing . . .? The short answer to that is: We couldn’t. . . . Try listing all the crimes that there are in rank order of punishable merit. . . . Then collect results from your friends and see if they all match. I will tell you they don’t.”1 * Karl N. Llewellyn Distinguished Service Professor, University of Chicago. ** Eugene Higgins Professor of Psychology and Professor of Public Affairs, Princeton University *** Herbert D. Kelleher Professor of Business, University of Texas, Austin. **** Associate Professor, Hebrew University of Jerusalem. For helpful comments, we are grateful to Eric Posner, Richard Posner, and participants in workshops at the John F. Kennedy School of Government at Harvard University, the University of Chicago Law School, and Yale Law School. 1 Justice Stephen Breyer, as quoted in The New Republic, June 6, 1994, at p. 12. 
I. Introduction: Coherence and Incoherence 
Coherence in law is a widely shared ideal. Almost everyone hopes for a legal system in which the similarly situated are treated similarly. But there are many obstacles to the achievement of coherence in the law. This Article is concerned with one particular test of coherence, and with two cognitive limitations that help cause many failures of that test in actual legal systems. We believe that these failures are also failures of justice, and that they suggest a pervasive problem in existing legal systems. 
Our test of coherence is straightforward. We ask: When two or more judgments have been made separately, and each seems to make sense on its own, do they still make sense when considered together? When this test fails, and a need is felt to adjust or reverse the judgments that were made separately, we will speak of judgment reversals. The test of coherence can be readily applied to decisions by juries and by judges.2 With suitable modifications, it can also be applied to acts of legislators and regulators. More generally, we ask whether judgments made in isolation fit together in an orderly way when considered as part of the larger whole. 
Our emphasis will be on many categories of harms with which the law is concerned, such as physical injury‚ commercial fraud, and ecological damage. Our first psychological observation is that in law, as in ordinary life, people’s thinking is category-bound. People do not easily cross the boundaries of categories3 of harms in their thinking. When they consider an individual case of physical injury, or commercial fraud, the frame of reference for evaluation is usually a set of instances of the same kind of harm.4 When setting penalties for a category of cases, such as violations of regulations for occupational safety, regulators will naturally focus on instances that belong to that category. They are much less likely to concern themselves with the consistency of their determinations with punishments for other categories of harmful conduct, such as damage to endangered species. Yet, as we will show, simultaneous 2 Note that this is a test of coherence, not of social consensus, which was explored in Cass R. Sunstein, Daniel Kahneman & David Schkade, Assessing Punitive Damages, 107 Yale LJ 2071 (1998). But the absence of consensus on dollar amounts, discussed below, does bear on the possibility of achieving coherence. 3 We say a few words below on the nature of categories. 4 Daniel Kahneman and Dale Miller. Norm Theory: Comparing reality to its alternatives, 93 Psychological Review 136 (1986). consideration of penalties for different kinds of infractions will often reveal that the more severe punishment was assigned to the misconduct which, in context, appears to be the less serious. 
A second significant source of incoherence is what we shall call the translation problem. By this term, we refer to the distinctive problem involved in translating a moral judgment of some kind5 into the terms made relevant by the legal system, such as monetary penalties, civil fines, or criminal punishment.6 We argue that the act of translation causes serious problems, because it is grounded neither in agreed-upon principle nor in widely shared intuitions. Even when people show coherent and consistent moral intuitions, they may show little consistency and coherence in translating those intuitions into numbers, such as dollars of fines or years in jail. Because of the translation problem, coherence fails: there is no guarantee that the relative severity of punishments administered by the system will still appear sensible, just, or fair when several punishments are considered together. The translation problem helps identify the cognitive foundations of current controversies over criminal sentencing, punitive damages, and contingent valuation. It affects the work not only of juries, but also of legislative and regulatory bodies that determine punishments for different kinds of misconduct within a particular category. The result, we will argue, is that the overall level of penalties set by different regulatory agencies may appear sensible when each set of regulations is considered on its own, but fail our test of coherence when several sets are considered at once. 
We consider it self-evident that if it exists, incoherence in punishments is a form of injustice. We shall also assume that when the public would not believe that outcomes fit sensibly together, this is a problem that calls for social response. We will assume that these points are correct, without defending them in any detail. 
The coherence and incoherence of punishments, both civil and criminal, will be the focus of our analysis, and in this domain we will attempt to show considerable reason for concern. Juries typically assess cases in isolation; in fact 5 We use this term because as a matter of fact, moral judgments appear to be the foundation of punishments. On the economic theory of punishment, optimal deterrence is the goal, and optimal deterrence is in conflict with ordinary intuitions, as we discuss in detail below. A. Mitchell Polinsky & Steven Shavell, Punitive Damages: An Economic Analysis, 111 Harv. L. Rev. 869, 870–76 (1998).For interesting reasons, the translation problem is not a serious one from the standpoint of economic analysis of law. We will refer to this point below. 6 We initially discussed the translation problem, under the technical term “scaling without a modulus,” in the particular context of punitive damage awards in Sunstein, et al., supra. We generalize the problem here. lawyers are actually barred from referring to awards in other cases. Administrators, and congressional committees setting up penalties for regulatory misconduct, typically deal with one category of misconduct, and do not attend to problems of other types.7 Criminal sentences are established over time by different legislatures and legislative committees, with little effort to ensure a good fit of penalties to crimes across a broad frame of reference.8 Because people are not inclined to consider the overall pattern – either because it is too difficult to do so or because it does not occur to them to try – the problem of incoherence does not naturally receive attention.9 As things now stand, the structure of those institutions charged with making regulatory and legislative decisions reinforces the effects of category-bound thinking. Scandalously large inconsistencies can therefore persist indefinitely, in the absence of a special effort to impose coherence. 
The fact that coherence cannot be taken for granted has significant implications for institutional design. It suggests, in some domains, a possible reason to favor judicial decisions over jury decisions, because judges are more likely to have a menu of cases before them. Because judges are human,10 they too are susceptible to producing incoherent patterns11; but especially if the risk of incoherence is brought to judicial attention, they might well likely to do better, on this count, than juries. Our claims also suggest the potential value of “coherence commissions,” assigned the explicit mission to ensure that decisions fit together as an orderly whole, or at least to correct the most serious anomalies. We bring the idea of “coherence commissions” in contact with many areas of the law, including civil and criminal penalties, punitive damages, and valuation of statistical lives. We will also attempt to cast new light on some large topics in legal theory, including the aspiration to similar treatment of the similarly 7 Indeed, an administrator or regulator who shows explicit concern with problems that lie outside his or her jurisdiction risks being criticized as improperly “poaching” on someone else’s turf, or even having his decision held unlawful. See Pension Benefit Guaranty Corp. v. LTV Corp., 496 US 633 (1990) (allowing agency entrusted with pension guarantees not to consider other, relevant areas of law, and suggesting that any such consideration would violate the underlying statute). 8 The goals of the United States Sentencing Commission are, on this dimension, quite unambitious. See below. In fact a version of the translation problem appears in the international domain, where criminal sentencing also shows a degree of incoherence. See Allison Marston Danner, Constructing a Hierarchy of Crimes in International Criminal Sentencing, Virginia Law Review (forthcoming 2001). 9 Indeed we believe that this Article is the first to venture even a tentative assessment of the pattern of administrative penalties. See below. 10 Chris Guthrie, Jeffrey Rachlinsk, Andrew Wistrich, Inside the Judicial Mind, 86 Corn. L. Rev. 778 (2001). 11 This was an explicit rationale for the formulation of Sentencing Guidelines situated, the twentieth-century movement toward bureaucracy,12 and the general problem of “incommensurability.” Discussing several different kinds of incoherence, we identify some of the cognitive limits of the aspiration to global coherence in law,13 while also pointing the way toward institutional reforms that could overcome those limits. We urge that “coherence commissions” could do a great deal to reduce existing injustice, in a way that would provide a twenty-first century analogue to important, but less ambitious, institutional developments in the twentieth century. 
In this Article, we will be covering many topics, some of them in considerable detail. For purposes of exposition, it will be useful to give an overview of the specific claims that undergird our general arguments about incoherence • 
The moral intuitions of the public are firmly retributive in character.14 The intensity of what we shall call “punitive intention” – the desire to punish wrongdoing – is influenced both by the outrageousness of an action and by the severity of the harm that the action caused. • It is extremely difficult for people to translate punitive intentions into the terms made relevant by the legal system, such as fines or prison terms. That task is not rooted in shared intuitions, moral or otherwise, and outcomes can be largely arbitrary and unpredictable. Different juries may express the same punitive intention, but come up with quite different dollar awards. The bodies that set administrative punishments for particular categories of misconduct may differ widely in the general range of punishments that they choose, for no principled reason. State legislatures may produce widely varying punishments for the same crime, not because of different moral judgments, but simply because of the translation problem. • If people are asked to assess cases that fall within a particular category of actions and harms, but are not asked to translate their punitive 12 A helpful overview is Price Fishback & Shawn Kantor, A Prelude to the Welfare State: The Origins of Workers” Compensation (1999); see also Jerry Mashaw, Due Process in the Administrative State (1983), for a valuable discussion in the context of social security disability determinations. 13 Associated with Ronald Dworkin, Law’s Empire (1985). 14 As recognized by the Supreme Court in Cooper v Leatherman, 532 U.S. S. Ct. (2001) at footnotes 5 and 11. intentions into dollars or years, people’s judgments tend to be both shared and coherent.15 It follows that if they are evaluating cases within each category, diverse people are likely to agree on how to rank a set of personal injury cases, business fraud cases, sexual harassment cases, or libel cases by their “punishable merit” (Justice Breyer’s term). When asked to evaluate a case separately and hence in isolation, people spontaneously proceed by comparing it against others falling within the same category.16 Thus, for example, people’s responses to a case of business fraud will be generated largely by comparing that case to other cases of business fraud. If the case of business fraud involves extremely egregious misconduct that caused very severe harm in this frame of reference, people will be extremely outraged. The fact that there are other categories of cases, involving actions that people view as more evil and harms that they think more serious, will be effectively neglected. We call this effect “normalization.”17 The requirement directly to compare cases drawn from different categories of harmful actions can cause large shifts in moral evaluations, punitive intentions and actual punishments, relative to the judgments of the same cases in isolation. Such shifts, which we generically label judgment reversals, provide a diagnostic indication of a breakdown of coherence. 
There is less consensus on the ranking of cases in the hierarchy of “punishable merit” when the cases involve different kinds of harms (e.g., personal injury vs. environmental damage) than when they belong to the same category. Harms from different categories may seem “incommensurable,” because they cannot easily be described in terms of the same dimensions: the question of how many animals died, for example, is relevant in one of these situations but not in the other. The difficulty of comparisons across categories of harms may be one of the cognitive sources of the difficulty pointed out by Justice Breyer. 15 The claim is based on results reported by Daniel Kahneman, David Schkade & Cass R. Sunstein, Shared Outrage and Unpredictable Awards, 16 J. Risk & Uncertainty 49 (1998) and Sunstein et al., supra. 16 See Kahneman & Miller, supra; see also Daniel Kahneman, Ilana Ritov & David Schkade. Economic preferences or attitude expressions? An analysis of dollar responses to public issues, 19 Journal of Risk & Uncertainty 220 (1999). 17 Id. 
There is substantial consensus on the ranking of categories, including those created or used by the law. People agree, for example, that murder is worse than rape, than rape is worse than assault, and that assault is worse than libel. We suspect, however, that there is less social agreement on the ranking of categories than the on ranking of cases within categories. • In spite of the difficulties of comparing categories of cases and cases across categories, there is in many cases sufficient consensus to permit a test of coherence, which examine whether judgments made in isolation are still retained when explicitly compared. 
We illustrate these claims by investigating judgments in several kinds of domains – some of them experimental, some involving actual government practice. The first involves punitive damages awards. Here we show that people rank cases within a given category of harms in a consistent and coherent fashion, that their judgments about isolated cases are “normalized” according to the category in which they fall – but that these narrowly-based and easily-made judgments shift when people are compelled to consider the case in a broader context, by forcing a comparison to a case that involves harm of a different kind. The second kind of problem involves contingent valuation – an influential method of valuation by which people assign dollar values to goods that are not ordinarily traded on markets.18 We observe a similar pattern here. The amount that the public will be willing to pay to prevent or correct some harm to a public good will be very different depending on whether the goods and the harms are viewed in isolation or in explicit comparison with harms to goods and harms from another category.19 This point fortifies existing doubts about the rationality and reliability of the contingent valuation method.20 
A third kind of problem involves administrative penalties. Investigating actual statutory practice, we find that such penalties tend to make a great deal of sense within categories, because (for example) the more serious occupational safety and health violations are penalized more severely than the less serious ones. At the same time, administrative practice seems to make little sense as a 18 See, e.g., George Tolley et al., Valuing Health For Policy (1995); Valuing Environmental Preferences (Ian Bateman & K. G. Willis eds., 1999). 19 See Julie Irwin, Paul Slovic, Sarah Lichtenstein & Gary McClelland, Preference reversals and the measurement of environmental values, 6 Journal of Risk & Uncertainty 5 (1993) 20 See Peter Diamond & Jerry Hausman, Contingent Valuation: Is Some Number Better Than No Number, 8 J. Econ. Persp. 45, 49-52 (1994); Note, Ask A Silly Question, 105 Harv L Rev 1981 (1992). whole: Once the practices of diverse agencies are put together, the area appears pervaded by cross-category anomalies. What requires explanation here is both how such anomalies arise and why they persist. We argue that the anomalies arise from the combined effects of category-bound thinking and the arbitrary nature of the translation of moral judgments into punishments. The anomalies persist because of the absence of a strong need, at the individual and social levels, to ensure coherence. Apparently no constituency is seeking to ensure that the system of penalties fit together as a whole. In fact we believe that ours is the first effort even to explore the question of coherence in civil penalties – a point that attests to the fact that people can live (perhaps in ignorant bliss) with patterns that make little general sense. Here as well, underlying sources of the difficulty are category-bound judgments and the translation problem. The result is injustice and arbitrariness. 
While emphasizing the problem of incoherence, we do not suggest that coherence is sufficient to produce good outcomes. Systems that are internally coherent can and should be criticized on independent grounds.21 It is also possible to insist that in some domains, the price of coherence is too steep, perhaps because of the administrative costs of achieving it, perhaps because coherence can be achieved only by altering some institutions, outcomes, and judgments that, on independent normative grounds, turn out to be good. But incoherence of the sort documented here is at the very least a serious problem, because it ensures a set of results that would widely be seen as indefensible and arbitrary. One of our largest goals is to uncover the mechanism that helps to produce this state of affairs, and to see what might be done about it. 
This Article is organized as follows. Part II provides some general background on the concept of coherence. Part III explores the psychology underlying the forms of incoherence that is our focus here. Part IV turns to the basic cases of punitive damages and contingent valuation, offering experimental evidence of judgment reversals. In order to provide a focus for normative work on incoherence in law, Part V discusses the implications of our findings for punitive damages awards. Part VI explores more general implications by showing patterns of administrative penalties that most reasonable people would 21 See Joseph Raz, The Relevance of Coherence, in Ethics in the Public Domain 261 (1994). Raz emphasizes that “Coherence conveys a specific good, the value of which is undeniable. What is incoherence is unintelligible, because it is self-contradictory, fragmented, disjointed.” Id. at 264. Raz raises serious doubts about coherence theories in id. at 265-70; we do not investigate the resulting complexities here. reject as absurdly inconsistent, and that nevertheless persist indefinitely, reflecting the pervasive indifference to issues of global coherence that we have attributed to human cognition. 
Our basic purpose is descriptive, but we also contend that at a minimum, we have uncovered a serious problem, one that infects judgments and penalties in many areas of the law. While emphasizing the difficulty of achieving agreement on the requirements of full coherence, we suggest that many steps can be taken to correct the worst anomalies. In Part VII, we urge that as an ideal, the legal system should attempt to create institutions that would create more in the way of systemic rationality, and that where results do not fit, there is likely to be a problem of injustice. With respect to the budget, the Office of Management and Budget was originally created on just this ground, and the Sentencing Guidelines had similar aspirations (as yet unrealized, partly because of the absence of explicit cross-category comparisons). With respect to regulation, there are some related problems: the existence of large and apparently inexplicable disparities in expenditures per life saved is, in part, testimony to the absence of sustained cross-category comparisons. Of course the jury system raises special problems and concerns, and there are large questions about the extent to which reforms, even dramatic ones, might overcome the problems stressed here. 
In Part VIII, we show that our analysis bears on some larger issues in legal theory, including the debate over the value and possibility of coherence in law and the nature and existence of “incommensurability.” A main theme is that any effort to proceed “one case at a time” will produce serious problems, because of identifiable features of human cognition. We also attempt to show the cognitive basis for the experience of incommensurability. Our closing plea is for institutional changes designed to overcome the problems we identify, replacing predictably incoherent judgments with reforms whose goal is to produce systemic rationality. 
Our emphasis here will be on the particular types of incoherence that stem from the human tendency to make category-bound judgments, and from the arbitrariness of the translation of punitive intent into actual punishments. But this type of incoherence should be understood against a more general background, formed by the broader interest in coherence as a goal and by a continuing debate within economics and other social sciences. 
Ethical and political philosophers have often viewed inconsistencies with concern,22 treating them as local warps in a web of beliefs that must be repaired by adequate reflection, in which specific beliefs are brought in line with each other and with broader principles.23 The search for “reflective equilibrium” is designed to ensure that one’s beliefs, at multiple levels of abstraction, fit together as a sensible whole.24 There is a similar aspiration to coherence within law,25 though a decentralized system, with numerous judges, may well have special difficulty in achieving that goal.26 Our central findings here will show some new difficulties with efforts to achieve anything corresponding to reflective equilibrium within the legal system, or even in moral judgments. 
“Rational agent” theories in social sciences, and much important theorizing in the domain of law, rest on the assumption that human agents are endowed with coherent systems of beliefs and preferences, and define coherence as the principal criterion of rationality.27 An influential definition of rationality avoids any normative evaluation of the specific contents of beliefs and choices, and refers neither to the truth of beliefs nor to the consequences of choices.28 Only internal consistency matters.29 In modern economic thinking, and in the economic analysis of law in particular, coherence is considered a touchstone of rationality.30 The preferences of the idealized rational agent provide a coherent ordering of possible states of affairs, and the beliefs of that agent permit an ordering of events by their probabilities.31 Furthermore, the dispositions to form new beliefs in the light of evidence, or to make choices when new options are offered, are also assumed to belong to the same coherent structure.32 
continuing debate about the “rationality” of human agents, involving 22 See Joseph Raz, The Relevance of Coherence, in Ethics in the Public Domain 261-278 (1994). 23 See Gilbert Harmon, Change in View (1986); David Brink, Moral Realism and the Foundations of Ethics (1989); Germain Grisez et al., Practical Principles, Moral Truth, and Ultimate Ends, 32 Am J. Jurisprudence 231 (1987). 24 See John Rawls, A Theory of Justice 19-22, 46-51 (1971). 25 See Dworkin, supra note. 26 See Cass R. Sunstein, Legal Reasoning and Political Conflict 48-52 (1996). 27 For overviews, see Rational Choice ( Richard A. Posner, Economic Analysis of Law ch. 1 (5th ed 1999). 28 See John Von Neumann & Oscar Morgenstern, Theory of Games and Economic Behavior (2d ed. 1947). 29For an outline and critique, see Amartya Sen, Internal Consistency of Choice, 61 Econometrica 495 (1993). 30 See Von Neumann & Morgenstern, supra. In law, see Richard A. Posner, Economic Analysis of Law ch. 1 (5th ed. 1999). 31 See Gary Becker, Accounting for Tastes ch. 1 (1999); Leonard J. Savage, The Foundations of Statistics. (1954). 32 See id. economists, decision theorists, and psychologists, as well as philosophers and academic lawyers.33 It is now well-known that people are not in fact perfectly rational.34 They are better described, in Herbert Simon’s phrase, as boundedly rational.35 Bounded rationality has both cognitive and motivational aspects.36 Because of the limitations in their ability to process information, boundedly rational agents are not able to maintain a system of beliefs, preferences, and dispositions that is both comprehensive and internally coherent. At best, such agents are locally coherent -- achieving consistency over small regions of the space of possible events and outcomes, but not between more remote regions. Furthermore, boundedly rational agents are also cognitive misers -- they economize on difficult thinking and are not inclined to search for inconsistencies among their ideas, or even to acknowledge inconsistencies, unless they are pressured to do so. In particular, we will develop the claim that people effortlessly achieve local coherence in their rankings of actions and outcomes, but show limited ability and little interest in global coherence. Hence global incoherence can persist for long periods. 
As a practical matter, complete consistency of beliefs and preferences is an unattainable ideal for any individual, and probably for any legal system. Failures of consistency are inevitable, but some are easier to avoid than others. People are normally successful at avoiding immediate inconsistencies between statements they make in the same setting, but it is much harder to prevent remote inconsistencies between a judgment one makes now and judgments made, or accepted in the past. It is harder still to prevent situations in which a judgment that one makes now is inconsistent with a judgment that one would make if one were asked a different question (or the same question in different words). People who are unable to ensure that their current judgments are consistent with other judgments they accept, or with judgments they would make or would have made under different circumstances, inevitably produce a pattern of outcomes that they would themselves consider incoherent and indefensible. We will see this problem in many legal and policy domains. 33 See, e.g., Choices, Values, and Frames (Daniel Kahneman & Amos Tversky eds. 2000); The Rational Foundations of Economic Behavior (Kenneth Arrow et al. eds. 1996); Gerd Gigerenzer et al., Simple Heuristics That Make Us Smart (1999). 34 For an overview, see Christine Jolls, Cass R. Sunstein & Richard Thaler, A Behavioral Approach to Law and Economics, 50 Stan L Rev 1471, 1476-1480 (1998). 35 See Herbert A. Simon, A Behavioral Model of Rational Choice, 69 Q. J. Econ. 99 (1955). 36 For an overview, see Jon Elster, Sour Grapes (1983). 
In this section we review in some detail the psychological underpinnings of the analysis to be presented in this Article. Much of our treatment here is an effort both to extend and generalize our earlier empirical research on punitive damages37 and on contingent valuation,38 and more generally to build on previous theoretical and empirical analyses, not yet applied to law and policy, of intuitive judgment,39 attitudes and emotion,40 and spontaneous categorization.41 We also offer two empirical results that we report for the first time here. The first is an experiment that demonstrates category-bound incoherence in judgments of punitive damages and in contingent valuation. The second is an examination of category-bound incoherence in administrative penalties. In this section, we explore the psychological mechanisms in some depth. 
Our previous studies indicated that the dollar numbers produced by jurors in civil cases involving punitive awards, and by respondents in contingent valuation surveys, can be interpreted as expressions of the intensity of a positive or negative attitude – an emotional evaluation of a defendant, or of a public issue.42 Specifically, we have argued that punitive damages are an expression of indignation or outrage on a scale of dollars, and we identified the problem of translating outrage onto that unfamiliar scale as a critical cause of unpredictability in punitive awards. We extend this conception of punishment here, by adding that (i) reprehensible actions are naturally categorized, and (ii) outrage and its manifestations – including punitive damages – involve a process that situates any particular case in relation to its category. We will show that this manifestation of category-bound thinking yields systematic incoherence in punitive intentions and in punitive awards. 37 Sunstein, Kahneman & Schkade, supra; Daniel Kahneman & Ilana Ritov, Determinants of stated willingness to pay for public goods: A study in the headline method. 9 Journal of Risk & Uncertainty 5 (1994); Cass R. Sunstein, David Schkade & Daniel Kahneman, Do People Want Optimal Deterrence? 29 Journal of Legal Studies 237 (2000); Kahneman, Schkade & Sunstein, supra; David Schkade, Cass R. Sunstein & Daniel Kahneman. Deliberating about dollars: The severity shift, 100 Columbia Law Review 101 (2000); Kahneman, Ritov & Schkade, supra. 38 Kahneman & Ritov, supra.. 39 Daniel Kahneman & Shane Frederick, Representativeness revisited: Attribute substitution in intuitive judgment, to appear in Heuristics of Intuitive Judgment: Extensions and Applications (Thomas Gilovich, Dale Griffin & Daniel Kahneman, eds, 2002). 40 Kahneman, Ritov & Schkade, supra; Kahneman & Frederick, supra; Paul Slovic et al., The Affect Heuristic to appear in Heuristics of Intuitive Judgment: Extensions and Applications (Thomas Gilovich, Dale Griffin & Daniel Kahneman, eds, 2002). 41 Kahneman & Miller, supra. 42 We intend to take no stand on the relationship between emotion and cognition. See Jon Elster, Alchemies of Mind (1999); Martha Nussbaum, Upheavals of Thought (2001). 
We also extend the same analysis of punishment to a task that at first glance seems to involve little emotion: the setting of penalties by legislative or administrative bodies. The leap is not as radical as it may appear at first. We argue that members of a society are in wide agreement on the categorization of reprehensible actions, and on the relative outrageousness of actions within any one category. Individuals and commissions that set penalties are likely to respect this ordering both for psychological reasons – it corresponds to their moral intuitions as well as to the intuitions of the public – and for political reasons: setting punishments that transparently violate the common ranking of “punishable merit” within a category will appear unjust and evoke resistance. In this situation as well, we argue, category-bound thinking and the problem of translating punitive intent into dollars or other penalties combine to produce global incoherence. 
We divide the task of setting punishment into two parts: which locates the appropriate punishment on a subjective scale that ranges from “no punishment at all” to “extremely severe punishment.” the legal system, such as dollars of fine or years in jail. 
As we shall show, some forms of incoherence arise at the level of punitive intent, while others are caused by features of the translation process. 
Social psychologists commonly identify an emotion and a tendency to action as elements of attitudes.43 In our usage, outrage is the emotion and punitive intent is the action tendency. We believe that they are directly related. Outrage and punitive intent both are psychological variables – along with other subjective variables such as brightness, loudness, pain, trust and dislike. Any subjective variable can be expressed in multiple ways. For example, outrage may be expressed by appropriate adjectives, by the choice of a number on a rating scale, by loud screams, or by jail sentences. 
The existence of something like a moral community is assumed in the analysis that follows. When we speak of the outrageousness of an action or the punitive intention that is evoked by it, we have in mind an emotional state and a tendency to act that are widely shared in a relevant community. Consensus in moral attitudes was demonstrated in an earlier study44 in which respondents (drawn from jury-eligible citizens in Travis County, Texas) were asked to evaluate product liability cases involving physical injuries. Different groups of respondents used different scales in these evaluations. Some evaluated the outrageousness of the defendant’s behavior (always a firm, also identified by annual profit as an indication of its size) using a rating scale (from “Completely Acceptable” to “Absolutely Outrageous”); others rated their punitive intent, also on a rating scale (from “No Punishment” to “Extremely Severe Punishment”). A third group assessed the appropriate amount of punitive damages, in dollars. 
We found substantial consensus in these judgments. Thus, on average, rich and poor; educated and less educated; white, Hispanic, and AfricanAmerican; old and young; male and female; and all others are likely to agree on how to rank and rate a set of personal injury cases in terms of their “punishable merit,”45 the outrageousness of the defendant’s conduct, and the punitive damages that are appropriate.46 Note that the agreement is between the average judgments for the various social groups. There is some variability in judgments of each case within each group, and especially for judgments made in dollars. In general, however, people appear to agree on what makes one personal injury worse than another (e.g., amount and duration of pain, disability, vulnerability of victims such as the elderly and the very young); they also agree on elements of the defendant’s behavior (e.g., intentionality, deception) that make one reprehensible action worse than another; and they agree as well on the severity of the harm inflicted on the plaintiff – provided that comparisons are restricted to harms of a particular type, such as physical injury. . 
An analysis of the results of our study showed that the punitive awards assessed for 28 separate scenarios was predicted quite accurately by a very simple formula, which captures the psychological conception of punishment that we apply and extend in this article47: 44 The results are reported in Sunstein, Kahneman & Schkade, supra note, and in Kahneman, Schkade & Sunstein, supra note. 45 See Breyer, supra note 1. 46 See Kahneman Schkade & Sunstein, supra; at 2099. Sunstein et al., supra. 47 The results of this analysis were described by Kahneman & Frederick, supra. The severity of the harm associated with each of the scenarios used by Kahneman, Schkade & Sunstein, supra was rated by a small group of Princeton students, who were not given any details about the cause of the harm. 
What this formula says is that punitive intent is proportional to the outrageousness of the harmful action, and that punitive intent is also proportional to the severity of the harm. The translation factor is required to transform punitive intent – here construed as a state of mind – into actual punishments. 
Punitive intent, as this formula indicates, is firmly retributive, in the sense that both the act and its consequences matter.48 Because of the retributive nature of the underlying intuitions, the public sense of what is just punishment is radically at odds with the idea, popular in some academic circles, that punishment should be grounded largely in its deterrent function, and therefore directly linked to the likelihood that an act will be discovered and punished.49 Punishments that appear just in this society are based on outrage directed at the action and concern with the degree of harm that the action caused.50 
We have described punitive intent as a state of mind – a sense that it is right for a miscreant to suffer some degree of pain. Punitive intent can be expressed in words, such as “severe” or “mild.” To have an effect in the real world, however, the intention to punish must be translated onto a scale that can be used by the legal system, such as dollars of fine or months in jail. The translation factor in the formula above represents this operation of scaling. 
For many classes of harms, people lack shared moral intuitions that might specify the translation factor. Except to the extent that they are familiar with existing practice, people do not have a clear, agreed-upon sense that a grossly reckless action should be punished with a punitive award of $50,000, or $100,000, 48 Jonathan Baron & Ilana Ritov, Intuitions about penalties and compensation in the context of tort law, 7 Journal of Risk & Uncertainty 17 (1993). 49 Sunstein, Schkade & Kahneman, supra, reported that probability of detection, which is central to the law and economics analysis of deterrence, is largely irrelevant to intuitions about just punishment. Punishment that is deliberately calibrated by probability of detection is considered unjust, when it results in similar actions being punished differently in different localities. Baron & Ritov, supra, report that known and explicitly specified future impact of decisions do not substantially affect judgment, even of experts, although regulating future behavior is a critical factor in the theory of law and economics. 50 See the acknowledgment of this point in Cooper v Leatherman, supra. or $1,000,000, or that a case of assault should be punished with a jail term of six months, two years, or five years. As a consequence, the translation factor that is used is often influenced by irrelevant considerations, personal experience, and random circumstances. In the case of punitive damage awards, for example, the plaintiff’s demand matters a great deal, simply because juries often have few other relevant dollar figures from which to begin.51 And because of the influence of these irrelevant considerations and random circumstances, actual punishments for transgressions are often incoherent in our sense. That is, different juries, who agree on punitive intent for a given set of case facts, may impose substantially different punishments solely because of arbitrary differences in their translation factors. These propositions have direct implications for a wide range of tasks relevant to the law. They apply with equal force to the determination of punishment for individual cases, and to the writing of statutes or regulations that specify punishments for particular transgressions. 
Optimal deterrence theory offers a way of calculating the appropriate punishments, again by connecting dollars awards to the harm through the translation factor of “likelihood of compensation.”52 But we have already seen that this analysis violates common intuitions about just punishment.53 For those who seek to come up with dollar amounts that are intuitively appealing, a main difficulty is that harm and punishment do not generally occur in the same or even in commensurable units What is the appropriate fine for hunting an endangered bird? $50? $500? $5000? Should the punitive damages for a case of employment discrimination be set at three times the value of compensatory damages, or at fifteen times that value? 
In an earlier analysis of punitive damages we focused on the arbitrariness of translation factors as a major cause of the notorious unpredictability of punitive damage awards.54 We expected, and found, large variation in the 51 Reid Hastie, David Schkade & John Payne. Juror judgments in civil cases: Effects of plaintiff’s request and plaintiffs identity on punitive damage awards, 23 Law & Human Behavior 445 (1999); also Gretchen B. Chapman & Brian H. Bornstein, The more you ask for, the more you get: Anchoring in personal injury verdicts, 10 Applied Cognitive Psychology 519 (1996) (finding similar effects for plaintiffs’ demand in the context of compensatory awards for personal injury). 52 See A. Mitchell Polinsky & Steven Shavell, Punitive Damages: An Economic Analysis, 111 Harv. L. Rev. 869, 870–76 (1998). 53 See Sunstein, Schkade & Kahneman, supra. Indeed, the Supreme Court has acknowledged that punitive awards are not rooted in deterrent judgments. See Cooper v Leatherman, supra. 54 Kahneman, Schkade & Sunstein, supra at 2106-07. We described the juror’s task as a special case of scaling without a modulus. The term is borrowed from psychological experiments in which observers use numbers to indicate the intensity of sensations such as brightness, loudness or pain. The common practice in such scaling experiments is to specify a particular stimulus (e.g., a level of luminance) as a standard. Observers translation factor that people will apply in judging a particular case, even when their punitive intent is the same. In subsequent research we found that jury deliberations are not a cure for the variability of translation factors.55 The general effect of deliberation was instead a severity shift: it appears that when jurors start off with different translation factors, higher punishments that are proposed in the deliberation are more likely to be adopted, and are sometimes even exceeded by the jury’s award. Even after deliberation, different mock juries considering the same case often reach radically different decisions – punitive damage awards sometimes vary by a factor of 100 or more.56 
It is easy to see that free-floating translation factors will often cause failures of our test of coherence. Imagine two juries that consider separate claims A and B against the same defendant, awarding $1 million for case A and $5 million for case B. Because the translation factor that they apply may be different, it is entirely possible that both juries, if asked to compare the two cases, would agree that case A is higher in “punishable merit” than case B. The inevitable consequence of variability in the translation factor is incoherence in punishments: different individuals and different juries will make judgments that cannot be reconciled, even in the presence of underlying consensus on punitive intent. 
We do not intend to imply that the translation factors that people apply are completely arbitrary. Indeed, data that we have reported demonstrate the opposite. In one of our earlier studies57 of punitive damages each case was presented in two versions which differed only in the annual profits of the defendant firm. For example this value was “$10-20 million” in one version and “$100-200 million” in another. The indication of the size of the defendant firm had a large and systematic effect on punitive awards, which were substantially higher for larger firms. The mechanism that produces this effect is most likely are instructed to assign a particular number (the modulus) to that stimulus, and to assign numbers to other stimuli by comparing them to the subjective intensity associated with the standard. The task may appear meaningless, but in fact it is one that people can carry out with fair agreement – much like the consensus they exhibit in judging the severity of harms or the outrageousness of behaviors. In some experiments, however, a modulus is not supplied, and the observers are requested to assign whatever numbers they feel appropriate to report the subjective intensity of their sensations. In such situations of scaling without a modulus, observers spontaneously adopt their own individual standard and apply it consistently. However, different observers choose different moduli for no apparent reason. Thus, two stimuli may be rated as 10 and 2 by one individual, and as 40 and 8 by another, simply because the second individual picked a modulus that is 5 times larger than the first. 55 See Schkade, Sunstein & Kahneman, supra. 56 Id. at Table 5 57 Kahneman, Schkade & Sunstein, supra. the same that produces anchoring of awards on irrelevant factors. In this instance, however, the effect of firm size on awards corresponds to the sensible intuition that a larger financial punishment is required to inflict the same level of “pain” if the defendant firm is large than if it is small. Other anchoring effects could be defended as well: punitive awards have been found to be strongly correlated with compensatory awards in the same case,58 and perhaps anchoring of punitive on compensatory awards could be justified as appropriate, though this would not be an easy task.59 
In this Article we study two manifestations of category-bound thinking that produce incoherence in punishments: (1) the frame of reference for judging harmful actions is linked to a category of harms, and therefore liable to change when cases that involve harms of different kinds are explicitly compared; (2) the commissions that set penalties for categories of misconduct appear to be unconcerned with the penalties already on the books for other categories. In both situations, we suggest that judgments are sensible and coherent within each category separately, but not when the view screen is expanded to include more than one category. To understand this suggestion, and the mechanisms that produce judgment reversals, it is necessary to know something about what categories are and how they operate. 
cognitive science, where it serves to explain how people use categories and category labels in informal reasoning and in everyday language. 60 This concept of categories and categorization is quite different from an approach in which a category is defined by a set of necessary and sufficient conditions for membership. In that system, the boundary between categories are sharp and membership is all-or-none. In everyday language, in contrast, the boundaries of categories are fuzzy and membership is graded. 61 A chicken is surely a bird, but is not quite as good a bird as a robin, or an eagle. And a whale is not a fish, but it 58 Theodore Eisenberg et al., The Predictability of Punitive Damages, 26 J. Legal Stud. 623, 644 (1997). 59 It would not be easy on any theory of punitive damages. On optimal deterrence theory, a key issue is the multiplier; the compensatory award is in some sense an anchor, but a weak one, because a 100% likelihood of compensation should produce a punitive award of 0, an a 10% likelihood of compensation should produce a punitive award ten times that of the compensatory award. On the retributive view, there is no obvious translation formula. 60 See, e.g., Daniel Reisberg, Cognition: Explorin Cognitive Psychology: An Overview for Cognitive Scientists (1992). 61 Eleanor Rosch et al., Basic Objects in Natural Categories, 8 Cognitive Psychology 382 (1976). 
The last years have seen an intense debate about the aspiration to global coherence in law.127 Much of the debate has involved the appropriate conception of legal reasoning, an issue with both normative and descriptive dimensions. Ronald Dworkin has been the most prominent advocate of ambitious thinking, in which judges do not always restrict themselves to small pockets of problems, but sometimes attempt to ensure that all parts of law fit together as a principled whole.128 Others (including one of the current authors) have argued against this idea, on the ground that it would strain judicial capacities, and perhaps tend to produce errors of its own.129 In a discussion of particular relevance to our topic here, Dworkin discusses the “compartmentalization of law into separate departments” and sees that as a “prominent feature of legal practice.”130 Hercules, Dworkin's idealized judge, makes judgments that “expand out from the immediate case before him in a series of concentric circles,” increasingly far afield from the particular case and category at hand.131 Hercules does not take the law’s compartments for granted, and he is willing to reject any idea of “local priority” where ‘traditional boundaries between departments have become mechanical and arbitrary.”132 It seems clear that Hercules is willing to attempt global coherence, as, for example, through “a new unification of private law that blurs even the long-established and once much firmer boundary between contract and tort.”133 Advocates of less ambitious thinking stress that judges are not in a good position to make global sense of multiple areas of the law, and that the effort to try, especially if it is early, might well overwhelm the cognitive capacities of judges.134 
Our findings here do not resolve these issues, which have arisen in a much broader context than those discussed here. We have assessed judgments within and across categories, but in the relatively simple setting of judgments 127 See Ronald Dworkin, Law’s Empire (1985); Cass R. Sunstein, Legal Reasoning and Political Conflict (1996); Cass R. Sunstein, One Case At A Time (1999); Edward McCaffery, Inside-Out Freedom’s Law, 85 Cal. L. Rev. 1043 (1997). 128 See Ronald Dworkin, In Praise of Theory, 29 Ariz State L J 353 (1997). 129 See Sunstein, supra note. 130 Dworkin, Law’s Empire, at 251. 131 Id. at 250. 132 Id. at 253. 133 Id. at 254. 134 See Sunstein, supra note; see also Edward McCaffery, Inside-Out Freedom’s Law, 85 Cal. L. Rev. 1043 (1997) (arguing that cognitive limitations might justify a less ambitious approach from courts, and that this less ambitious approach is consistent with Dworkin’s general understanding of legal interpretation as integrity). about penalties rendered in terms of dollars or years. We have not explored how the process of category-bound judgments would affect free speech principles in categories involving pornography, commercial speech, and libel, or tort and contract principles involving medical malpractice, building construction, and prescription drug companies. It would be important to know whether (for example) people’s judgments about the appropriate treatment of false commercial speech and libelous speech would be different in isolation from what they would be if the two categories were considered together. We do not have evidence on that issue. Nonetheless, we think that what we have found casts some fresh light on this debate, giving a more detailed account of why it is hard to produce global coherence, but also providing new support for Dworkin's view, on the ground that local pockets of coherence might well produce an overall pattern that is senseless, or that at least contains what everyone would see as senselessness. 
To be sure, global coherence would be a significant strain on judicial capacities, in part because the mental operations involved do not come naturally. Judicial efforts to provide more limited forms of “local coherence,” through relatively unambitious reasoning by analogy, can be understood as a good way of avoiding cognitive overload – by focusing on the cognitively manageable category, and by failing to investigate problems from other categories. But our larger point cuts the other way, giving strong cognitive ammunition to Dworkin's plea for global coherence. We have provided fresh reason to believe that in law, the various categories showing internal coherence will not fit together -- and that the pattern of outcomes, generated by unambitious judges, will contain what they themselves would see as error and confusion. Referring precisely to this risk, Dworkin suggests that we “must strive, so far as we can, not to apply one theory of liability to pharmaceutical companies and a different one to motorists, not to embrace one theory of free speech when we are worried about pornography and another when we are worried about flag burning.”135 If law is to be coherent, Dworkin is entirely correct. Judges who seek only local coherence, or who proceed one case at a time, are highly likely to produce a pattern of outcomes of which they themselves would disapprove. 
Are values or options commensurable?136 In what sense? Those who object 135 Id. 136 See Joseph Raz, The Morality of Freedom (1985); Elizabeth Anderson, Value in Ethics and Economics (1993). to the idea of commensurability claim that people lack a shared metric by which to assess an array of qualitatively distinctive options, and that the use of such a metric can do violence to our considered judgments about how such options should be assessed.137 On this view, ideas like “utility” and “efficiency” are quite inadequate, as a way of capturing the operation of practical reason in law or daily life; these ideas are inadequate because they elide qualitative differences that matter when people reason well.138 
We do not attempt to take a stand on any normative issues here, nor do we attempt to give anything like an account of the operation of practical reason. But we do suggest that our findings here help to establish a cognitive basis for part of the experience of “incommensurability” in both law and ordinary thinking. People lack confident judgments about how to rank cases from different categories, and their judgments on this point are not widely shared. Recall the difficulty of comparing a case of tax fraud with an occupational safety and health violation, or of ranking a case of outrageous commercial fraud with one involving a relatively minor physical injury. For those using ordinary intuitions, there is no readily available metric by which to make the relevant comparisons.139 It is in this particular respect that incommensurability is a concrete psychological phenomenon.140 
We have attempted here to identify some pervasive features of human judgment and cognition – features that, we believe, account for significant anomalies in both private and public law. In making many judgments in law, people must translate a moral judgment into numerical terms, involving dollars or years. Moreover, people’s judgments are insistently category-bound. They do not naturally seek coherence across categories. Their assessment of problems, taken in isolation, are often different from their judgments about problems, taken in the context of cases from other categories. This is largely because any judgment, in isolation, is made against a background of a “natural” comparison set, consisting of problems from the same basic category. Much of the time, people will look at problems from other categories only when forced to do so. When a problem from a different category is introduced, the isolated judgment is 137 See Anderson, supra note. 138 See id. at 203-15. 139 Here economics can overcome the incommensurability problem by using the metric of dollars. 140 See also the discussion of ‘taboo tradeoffs” in Philip E. Tetlock, Coping With Trade-Offs: Psychological Constraints and Political Implications, in Elements of Reason 239 (Arthur Lopia et al. eds 2000). unsettled, and people’s judgments will shift, sometimes quite dramatically. The reason is that the introduction of the new problem alters the set of comparison cases, and shifts in judgment are a common consequence of that alteration. 
The most important implication of this phenomenon is that judgments in isolation will predictably produce incoherence from the standpoint of the very people asked to make those judgments. This is true of judgments about punitive damage awards; it is also true of willingness to pay for public goods. Thus judgment shifts are easy to generate in experimental settings. Outside of those settings, we have seen similar results in the domains of regulatory penalties. The pattern of within-category coherence, and global incoherence, is a nearly inevitable product of adjudication, which is defined by one-shot judgments; but the same pattern is embedded in many domains of law and policy. 
These are descriptive points. It is far less clear what to do about the situation. Introduction of a single problem from one separate category may make things worse rather than better. There is a serious risk of manipulation and strategic behavior here; careful selection of the comparison case can drive judgment in predictable directions. Deaths of buffaloes may seem a relatively small problem when presented alongside deaths of human beings; but deaths of buffaloes may seem a relatively large problem when presented alongside injuries to plants. In these circumstances, the ideal solution, for a legal system committed to obtaining people’s reflective judgments, is to move in the direction of ensuring, not attention to one category or two, but simultaneous appreciation of the large number of categories of cases to which any particular case might be compared. 
Coherence is important; it seems to be a minimum requirement of rationality. But coherence is not a trumping value, and a system displaying incoherence may well be better than one that is coherent but pervasively unjust. An incoherent system in which penalties fit together, but are three times as high, or one-third as high, as they ought to be. Nonetheless, we think that any domain of law should aspire to coherence, at least as a presumption, in order to prevent the kinds of arbitrariness and injustice that we have found in both experimental and real-world settings. At the very least, efforts should be made to correct the most conspicuous anomalies – a goal that can be obtained without thinking that it is easy or even possible for people to agree on what full coherence actually requires. 
We have emphasized throughout that for any single person, or jury, the achievement of coherence is an exceptionally difficult cognitive task. But steps in the direction of coherence are far less difficult at the level of institutional design.141 We close with the suggestion that the practical remedies for predictably incoherent judgments are institutional; they involve the creation of frameworks for decision that ensure a pattern of judgments that, when taken as a whole, reflective people could endorse. Perhaps this seems an unrealistically ambitious aspiration. But something of just this sort underlies many of the most impressive institutional innovations of the twentieth century. It should not be too much to expect twenty-first century institutions to build on these precedents, bringing a measure of rationality and sense to areas of the law that now lack them. 141 This is a lesson of the movement to workers” compensation, as described in Fishback & Kantor, supra note. 
In this part of the study, we would like you to imagine that you are a juror for a legal case in a civil court. Civil law suits involve disputes between private individuals, companies, or individuals and companies, in which the plaintiff alleges that the defendant harmed them in some way. The primary purpose of a civil suit brought by a plaintiff is to seek compensation from the defendant for the alleged harm. 
Civil suits involve two different types of potential damages that a defendant could be could be required to pay. Compensatory damages are intended to fully compensate a plaintiff for the harm suffered as a result of the defendant’s actions. Punitive damages are intended to achieve two purposes: (1) to punish the defendant for unusual misconduct, and (2) to deter the defendant and others from committing similar actions in the future. 
Punitive damages should be awarded if a preponderance of the evidence shows that the defendant acted either maliciously or with reckless disregard for the welfare of others. Defendants are considered to have acted maliciously if they intended to injure or harm someone or their property. Defendants are considered to have acted with reckless disregard for the welfare of others if they were aware of the probable harm to others or their property but disregarded it, and their actions were a gross deviation from the standard of care that a normal person would use. 
The case you will consider involves a special procedure that is sometimes used in civil trials, and which requires two different juries: (1) a trial jury, which decides whether the defendant should pay compensatory damages to the plaintiff, and if so in what amount, and (2) if compensatory damages are awarded, a separate punishment jury decides whether the defendant’s conduct also warrants punitive damages. 
Please imagine that you are a member of the punishment jury for this case. Your job is to decide whether and how much the defendant should be punished, in addition to paying compensatory damages. 
In the case you will consider, the trial jury has already ordered the defendant to pay compensatory damages to the plaintiff. This does not necessarily mean that punitive damages must also be awarded. Whether or not punitive damages should be awarded and if so how large they should be is completely separate from compensatory damages. 
Among his other large investments, William Marking owns Canyon Luxury Leasing, a company that leases luxury cars. He bought 1000 cars from Royal Motors, a manufacturer of high-priced automobiles with annual profits of around $150 million. Royal Motors did not disclose the fact that the paint on all 1000 cars was not the original factory paint: all cars had been repainted from an ugly shade of green to the current colors of red and black. 
One month after the purchase, an employee of Royal Motors reported this fact to a local newspaper, and the repaintings received a great deal of national publicity. Mr. Marking believes that people who lease luxury cars demand perfection, and that this negative publicity caused a sharp decrease in business. He has sued Royal Motors for compensation. 
Internal documents of Royal Motors produced at the trial included a management memo: "Marking will not be happy at all when he finds out that the cars have been repainted. He will feel that we took advantage of him, but it was his responsibility to check the cars. Anyway we run almost no risk: it will not be in Marking’s interest to publicize the problem, so he will not dare sue us, no matter how mad he is. The worst that will happen is that he won’t buy from us again.” The trial jury ordered Royal Motors to pay $500,000 in compensatory damages. 
Joan Glover, a five-year-old child, ingested a large number of pills of a nonprescription allergy medicine called Allerfree. The Allerfree bottle carried a label reading "Childproof Cap,” but it did not meet federal regulations for the use of that label. 
Joan’s parents testified that they had been very careful in ensuring that all of their medications had childproof safety caps. Joan found the pills in a kitchen drawer and ingested most of the bottle. The overdose permanently weakened her respiratory system, which will make her more susceptible to breathing-related diseases such as asthma and emphysema for the rest of her life, and may reduce her life expectancy. Joan’s parents sued the manufacturer of Allerfree, the General Assistance company, a drug manufacturer with annual profits of $150 million. Internal company documents showed that General Assistance chose to ignore federal regulations about standards for using the label "childproof cap.” An internal memo presented at trial says that ‘this stupid, unnecessary federal regulation is a waste of our money"; it acknowledges the risk that Allerfree might be punished for violating the regulation but says ‘the punishments are extremely mild; basically we”d be asked to improve the safety caps in the future." The trial jury ordered General Assistance to pay the Glovers $500,000 in compensatory damages. 
What amount of punitive damages (if any) should Royal Motors be required to pay as punishment and to deter them and others from similar actions in the future? Compensatory damages do not count as part of the punishment. Please write the appropriate amount of punitive damages in the blank below. 
$ _____________________ 
How severely should Royal Motors be punished because of their actions, and to deter them and others from similar actions in the future? Note that the compensatory damages that they must pay do not count as part of the punishment. Please circle the number that best expresses your judgment of the appropriate level of punishment. 
None 0 1 
Mild 2 3 
Substantial 4 5 
Severe 6 7 .4. Response Scales: Comparison We would like you to compare this case (Glover v. General Assistance) to the legal case you saw earlier (Marking v. Royal Motors, which involved repainted cars). 
Less than Royal Motors 
The same as Royal Motors 
More than Royal Motors 
What amount of punitive damages (if any) should General Assistance be required to pay as punishment and to deter them and others from similar actions in the future? Compensatory damages do not count as part of the punishment. Please write the appropriate amount of punitive damages in the blank below. 
$ _____________________ 
OR 
How severely should General Assistance be punished because of their actions, and to deter them and others from similar actions in the future? Note that the compensatory damages that they must pay do not count as part of the punishment. Please circle the number that best expresses your judgment of the appropriate level of punishment. 
None 0 
The next question moves outside the legal context. It involves problems of a more general nature, which concern society as a whole. In this part we are interested in your reaction as a member of society, rather than as a potential juror. 
Dolphins in many breeding locations are threatened by pollution. The threat to breeding locations is expected to result in a decline of the dolphin population. A special fund is needed to clean up and protect dolphin breeding locations. 
Increased funding to provide pollution free breeding locations for dolphins must be supported mostly by private contributions. Consider the possibility of making a voluntary contribution of money to this fund. 
The next question moves outside the legal context. It involves problems of a more general nature, which concern society as a whole. In this part we are interested in your reaction as a member of society, rather than as a potential juror. 
Farm workers, who are exposed to the sun for many hours, have a higher rate of skin cancer than the general population. Frequent medical checkup can reduce the risk. Increased funds are needed to establish programs for more frequent checkups of the threatened groups. 
Increased funding for these programs must be supported mostly by private contributions. Consider the possibility of making a voluntary contribution of money to this fund. 
What amount of money (if any) would you be willing to contribute to the fund to protect dolphins? Please write your amount in the blank below. 
$ ___________ 
OR 
How much personal satisfaction would you expect to get from making a contribution a fund to protect dolphins? (please circle your answer) 
a little 1 less than for most other contributions 2 a great extreme deal satisfaction 5 6 
We would like you to compare this problem (farm workers” skin cancer), to the problem of public concern that you saw earlier (protection of dolphins). Compared to protecting dolphins, how much money would you be willing to contribute to a fund for farm workers” skin cancer? (circle your choice) less than for dolphins the same as for dolphins more than for dolphins 
What amount of money (if any) would you be willing to contribute to a fund to reduce farm workers” skin cancer? Please write your amount in the blank below. $ ___________ 
OR 
How much personal satisfaction would you expect to get from making a contribution a fund to reduce farm workers” skin cancer? (please circle your answer) 
Type of violation Criminal 
Violation of sanitary food transportation act 
Violation of Filled milk act Violation of Filled milk act Violation of a serious nature Violation of non-serious nature Violation of posting requirement Willful violation if non-serious or regulatory Willful violation if serious Repeated violations Violation of record keeping- reporting Fail to make requested forms available Egg products inspection act- violation Above + intent to defraud or distribute adulterated eggs Federal Meat Inspection act violation Above + intent to defraud or distribute adulterated meat Violation of sanitary food transportation act Administrative penalty for CERCLA and EPCRA- class 1 “ “ “ 
class 2 Violation of right to know reporting under EPCRA Frivolous trade secret claim under EPCRA Administrative violation of CWA- class 1 Administrative violation of CWA- class II Hazardous substance discharge violation of 
CWA Agency OSHA FDA EPA 
Civil Civil Civil Civil Civil Civil Civil Civil Up to 1000 Up to 10000 Up to 1000 Up to 10000 Same as hazardous material transport act Same as hazardous material transport act Up to 1000 Up to 1yr prison Up to 25000 a day Up to 75000 a day Up to 10000 a day Up to 25000 Up to 10000 a day 25000 max Up to 10000 a day 25000 max Up to 25000 a day Fish and Wildlife Service Civil Civil Civil Civil Civil Civil Civil 
Above + gross negligence of willful misconduct Violation of FIFRA if for hire applicator or dealer Violation of FIFRA if non hire applicator Knowingly violate FIFRA if registrant or producer Knowingly violate FIFRA if other Knowingly violate FIFRA if private applicator Violation of RCRA Knowingly violate financial disclosure for solid waste disposal See above Violation of compliance requirement for solid waste Knowingly transport, treats or stores haz waste w/o permit See above (except permit) and knows that puts person in imminent danger of death or serious bodily injury See above and defendant is organization Violates requirement of solid waste disposal Owner fails to notify or submits false info about underground storage tanks Violation of Wild Bird Conservation Act Violate WBCA 111 a1 or 2 (import in violation of ban )or permit under 112 ( exemptions for sci research and ltd breeding) Knowingly violate and import under WBCA 11a3 ? If non business related and violate WBCA Civil violation of African Elephant Conservation Act Violation of Bald Eagle Protection Act Violation of ESA if knowingly violate/in business of im/export, taking, possessing etc No less than 100000 Up to 500 1st offense then up to 1000 Max 50,000 Max 25,000 Max 1000 Up to 25000 a day Up to 2500 Up to 1 yr prison Up to 25000 a day Up to 50000 a day or 2 yr prison Up to 250,000 or 15 yr prison Up to 1,000,000 Up to 25000 Up to 10,000 Up to 25000 Up to 12000 Up to 500 Up to 5000 Up to 5000 Up to 25000 EEOC Up to 1 yr prison Reinstate, hire, backpay or other approp equit relief Up to 100 Punitive damages Up to 50,000 Up to 100,000 Up to 200,000 Up to 300,000 Up to 10,000 Up to 6 mo prison Liable for wages and liquidated damages Up to 10,000 Up to 1000 Violation of ESA if knowingly violate or ex/import and not enumerated above Otherwise violate ESA Up to 500 Give public info about unlawful employment Up to 1000 practice under T7 during proceeding See above Intentional engagement in unlawful employment practice under T7 Willful failure to post pertinent provisions of T7 Same remedies available as under T7 Discriminatory practice with malice Total damages under t7 for small business (under 100 employees) Total damages under T7 for medium business (over 100 less than 200 employees) Total damages under T7 for large business (Over 200 less than 500) Total damages under T7 for big business (more than 500) Willful violation of section 15 of equal pay act See above Violation of section 6 or 7 of equal pay act Violation of section 212 (child labor) of equal pay act Repeated or willful violation of section 6 or 7 of EPA 
Regulation Agency Unvented Space Heater Ban Aircraft Cabin Fire Protection Standard Auto Passive Restraint/Seat Belt Standards Steering Column Protection Standard Underground Construction Standards Trihalomethane Drinking Water Standards Aircraft Seat Cushion Flammability Standard Alcohol and Drug Control Standards Auto Fuel-System Integrity Standard Standards for Servicing Auto Wheel Rims Aircraft Floor Emergency Lighting Standard Concrete & Masonry Construction Standards Crane Suspended Personnel Platform Standard Passive Restraints for Trucks & Buses (Proposed) Side-Impact Standards for Autos (Dynamic) Children’s Sleepwear Flammability Ban Auto Side Door Support Standards Low Altitude Windshear Equipment & Training Standards Electrical Equipment Standards (Metal Mines) Trenching and Excavation Standards Traffic Alert and Collision Avoidance (TCAS) Systems Hazard Communication Standard Side-Impact Standards for Trucks, Buses, and MPVs (Proposed) Grain Dust Explosion Prevention Standards Rear Lap/Shoulder Belts for Autos Standards for Radionuclides in Uranium Mines Benzine NESHAP (Original: Fugitive Emissions) Ethylene Dibromide Drinking Water Standard Benzene NESHAP (Revised: Coke Byproducts) CPSC 
FAA NHTSA NHTSA OSHA-S 
EPA FAA 
FRA NHTSA OSHA-S 
FAA OSHA-S OSHA-S NHTSA NHTSA 
CPSC NHTSA 
FAA MSHA OSHA-S 
FAA OSHA-S NHTSA OSHA-S NHTSA 
EPA EPA EPA EPA Cost per premature 
death averted ($ millions 1990) 0.1 0.1 0.1 0.1 0.1 0.2 0.4 0.4 0.4 0.4 0.6 0.6 0.7 0.7 0.8 0.8 0.8 1.3 Asbestos Occupational Exposure Limit Benzene Occupational Exposure Limit Electrical Equipment Standards (Coal Mines) Arsenic Emission Standards for Glass Plants Ethylene Oxide Occupational Exposure Limit Arsenic/Copper NESHAP Hazardous Waste Listing for Petroleum Refining Sludge Cover/Move Uranium Mill Tailings (Inactive Sites) Benzene NESHAP (Revised: Transfer Operations) Cover/Move Uranium Mill Tailings (Active Sites) Acrylonitrile Occupational Exposure Limit Coke Ovens Occupational Exposure Limit Lockout/Tagout Asbestos Occupational Exposure Limit Arsenic Occupational Exposure Limit Asbestos Ban Diethylstilbestrol (DES) Cattlefeed Ban Benzene NESHAP (Revised: Waste Operations) 1,2 Dichloropropane Drinking Water Standard Hazardous Waste Land Disposal Ban (1st 3rd) Municipal Solid Waste Landfill Standards (Proposed) Formaldehyde Occupational Exposure Limit Atrazine/Alachlor Drinking Water Standard Hazardous Waste Listing for Wood-Preserving Chemicals OSHA-H OSHA-H 
MSHA 
EPA OSHA-H 
EPA EPA EPA EPA 
EPA OSHA-H OSHA-H OSHA-S OSHA-H OSHA-H 
EPA FDA EPA EPA EPA 
EPA OSHA-H 
EPA EPA Readers with comments should address them to: Professor Cass R. Sunstein Karl N. Llewellyn Distinguished Service Professor of Jurisprudence University of Chicago Law School 1111 East 60th Street Chicago, IL 60637 773.702.9498 csunstei@midway.uchicago.edu Chicago Working Papers in Law and Economics 
(Second Series) Lisa Bernstein, The Questionable Empirical Basis of Article 2’s Incorporation Strategy: A Preliminary Study (May 1999) Richard A. Epstein, Deconstructing Privacy: and Putting It Back Together Again (May 1999) William M. Landes, Winning the Art Lottery: The Economic Returns to the Ganz Collection (May 1999) Cass R. Sunstein, David Schkade, and Daniel Kahneman, Do People Want Optimal Deterrence? (June 1999) Tomas J. Philipson and Richard A. Posner, The Long-Run Growth in Obesity as a Function of Technological Change (June 1999) David A. Weisbach, Ironing Out the Flat Tax (August 1999) Eric A. Posner, A Theory of Contract Law under Conditions of Radical Judicial Error (August 1999) David Schkade, Cass R. Sunstein, and Daniel Kahneman, Are Juries Less Erratic than Individuals? Deliberation, Polarization, and Punitive Damages (September 1999) Cass R. Sunstein, Nondelegation Canons (September 1999) Richard A. Posner, The Theory and Practice of Citations Analysis, with Special Reference to Law and Economics (September 1999) Randal C. Picker, Regulating Network Industries: A Look at Intel (October 1999) Cass R. Sunstein, Cognition and Cost-Benefit Analysis (October 1999) Douglas G. Baird and Edward R. Morrison, Optimal Timing and Legal Decisionmaking: The Case of the Liquidation Decision in Bankruptcy (October 1999) Gertrud M. Fremling and Richard A. Posner, Market Signaling of Personal Characteristics (November 1999) 
Follow this and additional works at: https://chicagounbound.uchicago.edu/law_and_economics Part of the Law Commons Recommended Citation 
Cass R. Sunstein 
This paper can be downloaded without charge at: 
The Chicago Working Paper Series Index: http://www.law.uchicago.edu/Lawecon/index.html The Social Science Research Network Electronic Paper Collection: http://papers.ssrn.com/paper.taf?abstract_id=292149 
Cass R. Sunstein* 
When strong emotions are triggered by a risk, people show a remarkable tendency to neglect a small probability that the risk will actually come to fruition. Experimental evidence, involving electric shocks and arsenic, supports this claim, as does real-world evidence, involving responses to abandoned hazardous waste dumps, the pesticide Alar, and anthrax. The resulting “probability neglect” has many implications for law and policy. It suggests the need for institutional constraints on policies based on ungrounded fears; it also shows how government might effectively draw attention to risks that warrant special concern. Probability neglect helps to explain the enactment of certain legislation, in which government, no less than ordinary people, suffers from that form of neglect. When people are neglecting the fact that the probability of harm is small, government should generally attempt to inform people, rather than cater to their excessive fear. But when information will not help, government should respond, at least if analysis suggest that the benefits outweigh the costs. The reason is that fear, even if it is excessive, is itself a significant problem, and can create additional significant problems. 
“If someone is predisposed to be worried, degrees of unlikeliness seem to provide no comfort, unless one can prove that harm is absolutely impossible, which itself is not possible.”1 
“[A]ffect-rich probabilities . . . .”2 outcomes yield pronounced overweighting of small “On Sept. 11, Americans entered a new and frightening geography, where the continents of safety and danger seemed forever shifted. Is it safe to fly? Will terrorists wage germ warfare? Where is the line between reasonable precaution and panic? Jittery, uncertain and assuming the worst, many people have answered these questions by forswearing air travel, purchasing gas masks and radiation detectors, placing frantic calls to pediatricians demanding vaccinations against exotic diseases or rushing out to fill prescriptions for Cipro, an antibiotic most experts consider an unnecessary defense against anthrax.”3 *Karl N. Llewellyn Distinguished Service Professor, University of Chicago, Law School and Department of Political Science. I am grateful to Christine Jolls, Eric Posner, Richard Posner, and Adrian Vermeule for helpful comments on a previous draft, and to Martha Nussbaum for valuable discussion. 
1See John Weingart, Waste Is A Terrible Thing To Mind 362 (2001). 
2Yuval Rottenstreich and Christopher Hsee, Money, Kisses, and Electric Shocks: On the Affective Psychology of Risk, 12 Psych Rev. 185, 188 (2001). 
3Erica Goode, Rational and Irrational Fears Combine in Terrorism's Wake, The New York Times, Oct. 2, 2001. 
This is an essay about the role of the emotions in producing responses to risks, and in particular about how strong emotions lead people to neglect the probability that a risk will come to fruition. I started writing the essay a few days before the terrorist attacks on Washington, DC and New York City on September 11, 2001. It goes without saying that in the immediate aftermath of those attacks, people became exceedingly frightened. Many people were saying, “you cannot be safe anywhere.” They were especially afraid to fly. In response to the attacks, new security measures at airports were imposed at airports. There was a significant reduction in flying. One reason was simple fear. Another reason was the increased inconvenience (and to that extent cost, monetary and otherwise) of flying. Many people drove who would have flown; many others stayed at home. 
For those who study the topic of risk regulation, there are many things to say about this state of affairs. First, safety is a matter of degree; it is foolish to worry, as people seemed to be doing, about whether they are “safe” or “not safe.” As a statistical matter, most people, in most places (not excluding airports), were not at significantly more risk after the attacks than they were before. Second, the result of the new security measures might have been to increase deaths on balance.4 It is entirely possible that the new measures saved no lives at all. In addition, any substitution of driving for flying is likely to cause deaths, because driving is a less safe method of transportation.5 Perhaps hundreds of lives, on balance, will ultimately be lost as a result of the new security measures.6 Third, the costs of the new measures might well outweigh the benefits. As noted, there might be no benefits at all. Once monetized, the various costs are likely to be huge, undoubtedly in the billions. The sensible technocrat asks: Should we not, therefore, reject at least some of the new security measures, on the ground that they fail both health-health balancing and cost-benefit balancing? 
It is not at all clear that the sensible technocrat is wrong. But things are not so obvious. People were afraid to fly, and they demanded stringent security measures. Indeed few people publicly objected to them; it is hard to find a single person who was willing to launch a public objection on technocratic grounds. (It is an interesting question whether people objected to them privately and were 4See Robert W. Hahn, The Economics of Airline Safety, 20 Harv. L. & Pub. Policy 791 (1997). 5Id. 6Id. silencing themselves, and if so exactly why.7) Perhaps fewer people would fly, and more people would die, without those measures. This too is a technical question. But the demand for stringent security measures itself reflected the fact that people would have felt distressed, or upset, or fearful if government did not respond. A failure to respond would itself have imposed costs. One point seems clear: An effort to assess the costs of airplane crashes, and terrorism, will be ludicrously incomplete if it does not take account of the consequences of emotional reactions, which are themselves costs, and which will impose significant additional costs. I flew myself, about a week after the attacks, and while I believed that that the probability of danger was very low, I was not at all comfortable about flying, and the day before the flight was pretty miserable. 
On the conventional view of rationality, emotions, as such, are not assessed independently; they are not taken to play a distinctive role.8 Of course people might be risk-averse or risk-inclined. It is possible, for example, that people will be willing to pay $100 to eliminate a 1/1000 risk of losing $900. But analysts do not generally ask, or care, whether these dispositions are a product of emotions or something else. 
What can be said for the conventional view of rationality can also be said for prospect theory, a less conventional view about decision under risk.9 Prospect theory emphasizes a number of anomalies in people’s reactions to risks. For present purposes, what is most important is that prospect theory offers an explanation for simultaneous gambling and insurance.10 When given the choice, many people will reject a certain gain of X in favor of a gamble with an expected value below X, if the gamble involves a small probability of riches. At the same time, people prefer a certain loss of X to a gamble with an expected value less than X, if the gamble involves a small probability of catastrophe.11 If expected utility theory is taken as normative, then people depart from the normative theory of rationality in giving excessive weight to low-probability outcomes when the stakes are high. But in making these descriptive claims, prospect theory does not set out a special role for emotions. Nor is this a puzzling oversight, if it counts as an oversight at all. For many purposes, what matters is what people choose, and it is unimportant to know whether their choices depend on cognition or emotion, whatever may be the difference between these two terms. 
No one doubts, however, that in many domains, emotions have a large effect on judgment and decisionmaking.12 With some low-probability events, anticipated and actual emotions help to determine choice. Those who buy lottery tickets, for example, often fantasize about the goods associated with a lucky outcome.13 With respect to risks of harm, many of our ordinary ways of speaking suggest strong emotions: panic, hysteria, terror. People might refuse to fly, for example, not because they are currently frightened, but because they anticipate their own anxiety, and they want to avoid it. It is well known that people often decide as they do because they anticipate their own regret.14 The same is true for fear. Knowing that they will be afraid, people may refuse to travel to the Israel or South Africa, even if they would much enjoy seeing those nations and even if they believe, on reflection, that their fear is not entirely rational. Recent evidence is quite specific.15 It suggests that people’s aversion to a small probability of an especially bad outcome is heightened when the outcome is “affect rich” – when it involves not simply a serious loss, but one that produces strong emotions, including fear.16 
Drawing on and expanding this evidence, I will emphasize a single phenomenon here. Sometimes people focus on the worst possible case, which triggers strong emotions. When this is so, people fail to inquire into the probability that the worst case will occur. In such cases, emotions lead to what I will call probability neglect, which is properly treated as a form of quasirationality.17 From the normative point of view, it is not fully rational to treat a 1% chance of X as equivalent, or nearly equivalent, to a 99% chance of X, or even a 10% chance of X. Because people suffer from probability neglect, and because neglecting probability is not fully rational, the phenomenon I identify casts additional doubt on the widespread idea that ordinary people have a kind of “richer rationality” superior to that of experts.18 Most of the time, experts are concerned principally with the number of lives at stake,19 and for that reason they will be closely attuned, as ordinary people are not, to the issue of probability. 
It is not at all clear how the law should respond to this form of quasirationality. I will suggest that at a minimum, an understanding of probability neglect will help to understand how government and law might accomplish shared social goals. If government wants to insulate itself from probability neglect, it will create institutions designed to ensure that genuine risks, rather than tiny ones, receive the most concern. Such institutions will not require agencies to discuss the worst-case scenario.20 And if government is attempting to increase public concern with a genuine danger, it should not emphasize statistics and probabilities, but should instead draw attention to the worst case scenario. An understanding of probability neglect will also help us to make better predictions about the public “demand” for law. When a bad outcome is highly salient and triggers strong emotions, government will be asked to do something about it, even if the probability of the bad outcome is low. 
For law, the most difficult questions seem to be normative in character. Should government respond to intense fears that involve statistically remote risks? When people suffer from probability neglect, should law and policy do the same thing? I suggest that if it can, government should attempt to educate and inform people, rather than capitulating to unwarranted public fear. On the other hand, public fear, however unwarranted, may be intractable, in the sense that it is impervious to efforts at reassurance. And if public fear is intractable, it will cause serious problems, because fear is itself extremely unpleasant, and because fear is likely to influence conduct, producing (for example) wasteful and excessive private precautions. If so, a governmental response, via regulatory safeguards, would appear to be justified if the benefits, in terms of fear reduction, justify the costs. 
Many experiments suggest that when it comes to risk, a key question is whether people can imagine or visualize the “worst case” outcome.21 When the worst case produces intense fear, little role is played by the stated probability that that outcome will occur.22 An important function of strong emotions is thus to drive out quantitative judgments, including judgments about probability, by making the best case or the worst case seem highly salient.23 By way of preface, however, it is important to note that a great deal of evidence shows that whether or not emotions are involved, people are relatively insensitive to differences in probabilities, at least when the relevant probabilities are low. 
Do people care about probability at all? Of course they do. But some people, some of the time, show a remarkable unwillingness to attend to that issue. Several studies show that when people are seeking relevant information, they often do not try to learn about the issue of probability. One study, for example, finds that in deciding to purchase warranties for consumer products, people do not spontaneously point to the probability of needing repair as a reason for the purchase.24 Another study finds that those making hypothetical risky managerial decisions rarely ask for data on probabilities.25 Perhaps these findings reflect people’s implicit understanding that in these settings, the relevant probability “low, but not zero,” and that finer distinctions are unhelpful. And indeed, many studies find that significant differences in (low) probabilities have little impact on decisions. This finding is in sharp conflict with the standard view of rationality, which suggests that people’s willingness to pay for small risk reductions ought to be nearly proportional to the size of the reduction.26 In an especially striking study, Kunreuther and his coauthors found that mean willingness to pay insurance premiums did not vary among risks of 1 in 100,000, 1 in 1 million, and 1 in 10 million.27 They also found basically the same willingness to pay for insurance premiums for risks ranging from 1 in 650, to 1 in 6300, to 1 in 68,000.28 
The study just described involved a “between subjects” design; subjects considered only one risk, and the same people were not asked to consider the several risks at the same time. But several studies have a “within subjects” design, and even here, differences in low probabilities have little effect on decisions. An early study examined people’s willingness to pay (WTP) to reduce travel fatality risks. The central finding was that the mean WTP to reduce fatality risk by 7/100,000 was merely 15% higher than the mean WTP to reduce the risk by 4/100,000.29 A later study found that for serious injuries, WTP to reduce the risk by 12/100,000 was only 20% higher than WTP to reduce the same risk by 4/100,000.30 These results are not unusual. Lin and Milon attempted to elicit people’s willingness to pay to reduce the risk of illness from eating oysters.31 There was little sensitivity to variations in probability of illness.32 A similar study found little change in WTP across probability variations involving exposure to pesticides residues on fresh produce.33 A similar anomaly was found in a study involving hazardous wastes, where WTP actually decreased as the stated fatality risk reduction decreased.34 
There is much to say about the general insensitivity to significant variations within the category of low-probability events. It would be difficult to produce a rational explanation for this insensitivity; recall the standard suggestion that WTP for small risk reductions should be roughly proportional to the size of the reduction.35 Why don’t people think in this way? An imaginable explanation is that in the abstract, most people simply do not know how to evaluate low probabilities. A risk of 7/100,000 seems “small”; a risk of 4/100,000 also seems “small.”36 These figures can be evaluated better if they are placed in the context of one another; everyone would prefer a risk of 4/100,000 to a risk of 7/100,000, and joint evaluation improves evaluability.37 But even when the preference is clear, both risks seem “small,” and hence it is not at all clear that a proportional increase in WTP will follow. As noted, most of the studies described above were “within subjects” rather than “between subjects” and hence evaluability was promoted by the process of comparison. As suggested by the findings of Kunreuther and his coauthors, it is likely that in a between-subjects design, WTP to eliminate a risk of 4/100,000 would be about the same as WTP to eliminate a risk of 7/100,000, simply because the small difference would not matter when each risk is taken in isolation. 
Note also that the studies just described involve contingent valuation, not real-world choices. A significant question is whether and when actual behavior, in consumer choice or political judgment, shows a general neglect of differences among low probabilities. In labor markets, for example, are risks of 4/100,000 compensated at about the same level as risks of 7/100,000? This would be a serious market failure. I am aware of no data on the question.38 But we might expect that risk markets will reduce the problem of neglect, if only because some number of people will appreciate the relevant differences, and drive wage and prices in the appropriate direction. Quite apart from market behavior, some imaginative studies attempt to overcome probability neglect through visual aids39 or through providing a great deal of information about comparison scenarios located on a probability scale.40 Without these aids, it is not so surprising that differences in low probabilities do not much matter to people. For most of us, most of the time, the relevant differences—between, say, 1/100,000 and 1/1,000,000—are not pertinent to our decisions, and by experience we are not well-equipped to take those differences into account. 
My topic here, however, is not the general neglect of differences in low probabilities, but the particular role of strong emotions in crowding out any assessment of probability, both low and less low. My central claim is that when strong emotions are involved, large-scale variations in probabilities will matter little – even when the variations unquestionably matter when emotions are not triggered. More generally, probability neglect is dramatically heightened when emotions are involved. The point applies to hope as well as fear; vivid images of good outcomes will crowd out consideration of probability too.41 Lotteries are successful partly for this reason.42 But the subject here is fear rather than hope. 
The basic point has received its clearest empirical confirmation in a striking study of people’s willingness to pay to avoid electric shocks.43 The central purpose of the study was to test the relevance of probability in “affect rich” decisions. The experiment of central importance here attempted to see whether varying the probability of harm would matter more, or less, in settings that trigger strong emotions than in settings that seem relatively emotion-free. In the “strong emotion” setting, participants were asked to imagine that they would participate in an experiment involving some chance of a “short, painful, but not dangerous electric shock.”44 In the relatively emotion-free setting, participants were told that the experiment entailed some chance of a $20 penalty. Participants were asked to say how much they would be willing to pay to avoid participating in the relevant experiment. Some participants were told that there was a 1% chance of receiving the bad outcome (either the $20 loss or the electric shock); others were told that the chance was 99%; and still others were told that the chance was 100%. 
The central result was that variations in probability affected those facing the relatively emotion-free injury, the $20 penalty, far more than they affected people facing the more emotionally evocative outcome of an electric shock. For the cash penalty, the difference between the median payment for a 1% chance and the median payment for a 99% chance was predictably large and indeed consistent with the standard model: $1 to avoid a 1% chance, and $18 to avoid a 99% chance.45 For the electric shock, by contrast, the difference in probability made little difference to median willingness to pay: $7 to avoid a 1% chance, and $10 to avoid a 99% chance!46 Apparently people will pay a significant amount to avoid a small probability of a hazard that is affectively-laden – and the amount that they will pay will not vary greatly with changes in probability. 
To investigate the role of probability and emotions in responses to risk, I conducted an experiment asking about one hundred University of Chicago law students to describe their maximum willingness to pay to reduce levels of arsenic in drinking water. The questions had a high degree of realism. They were based on actual choices confronting the Environmental Protection Agency, involving cost and benefit information within the ballpark of actual figures used by the agency itself.47 
Participants were randomly sorted into four groups, representing the four conditions in the experiment. In the first condition, people were asked to state their maximum willingness to pay to eliminate a cancer risk of one in 1,000,000.48 In the second condition, people were asked to state their maximum willingness to pay to eliminate a cancer risk of one in 100,000. In the third condition, people were asked the same question as in the first, but the cancer was described in vivid terms, as “very gruesome and intensely painful, as the cancer eats away at the internal organs of the body.” In the fourth condition, people were asked the same question as in the second, but the cancer was described in the same terms as in the third condition. In each condition, participants were asked to check off their willingness to pay among the following options: $0, $25, $50, $100, $200, $400, and $800 or more. Notice that the description of the cancer, in the “highly emotional” conditions, was intended to add little information, consisting simply 45Id. 46Id. 47See Cass R. Sunstein, The Arithmetic of Arsenic, Georgetown L.J (forthcoming). 48Note that the phrasing of the question ensures that participants would think of the reduction of the risk to zero, rather than to some fraction of what it was before. People are willing to pay far more to eliminate risks than to reduce them, even if the savings are identical. See Kahneman and Tversky, supra note. of a description of many cancer deaths, though admittedly some participants might well have thought that these were especially horrific deaths. 
The central hypothesis was that the probability variations would matter far less in the highly emotional conditions than in the less emotional conditions. More specifically, it was predicted that differences in probability would make little or no difference in the highly emotional conditions -- and that such variations would have real importance in the less emotional conditions. This prediction was meant to describe a substantial departure from expected utility theory, which predicts that an ordinary, risk-averse person should be willing to pay more than 10X to eliminate a risk that is ten times more likely than a risk that he is willing to pay X to eliminate.49 
Here are the results in tabular form: 1/100,000 1/1,000,000 
$100 ($194.44) $25 ($71.25) 
$100 ($241.30) $100 ($132.95) 
In short, the central hypothesis was confirmed. In the highly emotional condition with a 1/100,000 risk, the median willingness to pay was $100. In the highly emotional condition with a 1/1,000,000 risk, the median WTP was exactly the same. In sharp contrast, variations in probability made a significant difference in the less emotional conditions. For a 1/100,000 risk, the median WTP was $100. But for a 1/1,000,000 risk, it was merely $25. If we investigate the means, the central finding is reduced but in the same direction: The difference in probability had a far greater impact in the less emotional condition (an increase from $71.25 to $194.44, or about 180%, as opposed to an increase from $132.95 to $241.30, or about 80%). 
From this experiment, there are several other noteworthy findings. By itself, the highly emotional description of the cancer had a substantial effect on WTP, holding probability constant. For the 1/1,000,000 risk, the emotional description increased median WTP fourfold, from $25 to $100, and nearly doubled the mean, from $71.25 to $132.95. For the 1/100,000 risk, the median stayed constant ($100), 49See Corso et al., supra. but the mean was significantly affected, rising from $194.44 to $241.30.50 At the same time, and basically consistent with other work on probability neglect, varying the probability had a relatively weak effect on WTP in both conditions. In the emotional conditions, the tenfold increase in the risk had no effect on the median and did not even double the mean, which increased merely from $132.95 to $241.30. In the less emotional conditions, that tenfold increase moved the median from $25 to $100, while moving the mean from $71.25 to $194.44. But note that in this experiment, the relatively sophisticated participants in the study showed far more susceptibility to probability information than in the studies, described above, by Kunreuther et al.; but even so, the susceptibility was far less than conventional (normative) theory would predict.51 
Indeed, the effect of the more emotional description of the outcome was essentially the same as the effect of the tenfold increase in its probability. My principal emphasis, however, is on the fact that when the question was designed to trigger especially strong emotions, variations in probability had little effect on WTP, far less of an effect than when the question was phrased in less emotional terms. This is the kind of probability neglect that I am emphasizing here. 
Probability neglect, when strong emotions are involved, has been confirmed in many studies.52 Consider, for example, experiments designed to test levels of anxiety in anticipation of a painful electric shock of varying intensity, to be administered after a “countdown period” of a stated length. In these studies, the stated intensity of the shock had a significant effect on physiological reactions. But the probability of the shock had no effect. “Evidently, the mere thought of receiving a shock was enough to arouse subjects, and the precise likelihood of being shocked had little impact on their arousal level.”53 A related study asked people to provide their maximum buying prices for risky investments, which contained different stated probabilities of losses and gains of different 50This relatively small effect might be a product of the fact that the less emotional description did, after all, involve a cancer death, which is known to produce strong reactions. See Richard Revesz, Environmental Regulation, Cost-Benefit Analysis, and the Discounting of Human Lives, 99 Col L Rev 941 (1999). A more pronounced effect might be expected if the death was simply described as a death. 
51See id. 
52For an overview see George Loewenstein et al., Risk As Feelings, 127 Psych Bulletin 267, 276 (2001). 
53Id. magnitudes.54 Happily for the standard theory, maximum buying prices were affected by the size of losses and gains and also by probabilities. (Note that for most people, this experiment did not involve an affect-rich environment.) But – and this is the key point -- reported feelings of worry were not much affected by probability levels.55 In this study, then, probability did affect behavior, but it did not affect emotions. In most of the cases dealt with here, intense emotions drive out concern with probability, and hence both behavior and worry are affected. 
It should not be surprising, in this light, that visualization or imagery matters a great deal to people’s reactions to risks.56 When an image of a bad outcome is easily accessible, people will become greatly concerned about a risk, holding probability constant.57 Consider the fact that the fact that when people are asked how much they will pay for flight insurance for losses resulting from “terrorism,” they will pay more than if they are asked how much they will pay for flight insurance from all causes.58 The evident explanation for this peculiar result is that the word “terrorism” evokes vivid images of disaster, thus crowding out probability judgments. Note also that when people discuss a lowprobability risk, their concern rises even if the discussion consists mostly of apparently trustworthy assurances that the likelihood of harm really is infinitesimal.59 The reason is that the discussion makes it easier to visualize the risk and hence to fear it. 
Note that this is not a point about the availability heuristic, which leads not to neglect probability, but answer the question of probability by substituting a hard question (what is the statistical risk?) with an easy question (do salient examples readily come to mind?).60 My point here is not that visualization makes an event seem more probable (though this is also often true), but that visualization makes the issue of probability less relevant or even irrelevant. In theory, the distinction should not be obscure. In practice, of course, it will often be hard to know whether the availability heuristic or probability neglect is driving behavior. 
Emotional reactions to risk, and probability neglect, also account for “alarmist bias.” 61 When presented with competing accounts of danger, people tend to move toward the more alarming account.62 In the key study, W. Kip Viscusi presented subjects with information from two parties, industry and government. Some subjects were given low-risk information from government, and high-risk information from industry; other subjects were given high-risk information from government, and low-risk information from industry. The basic result was that people treated “the high risk information as being more informative.”63 This pattern held regardless of whether the low risk information came from industry or from government. Thus people show “an irrational asymmetry: respondents overweight the value of a high risk judgment.”64 If the discussion here is correct, one reason is that the information, whatever its content, makes people focus on the worst case. There is a lesson for policy here: It might not be helpful to present people with a wide range of information, containing both assuring and less assuring accounts. 
The most sensible conclusion is that with respect to risks of injury of harm, vivid images and concrete pictures of disaster can “crowd out” other kinds of thoughts, including the crucial thought that the probability of disaster is really small.65 “If someone is predisposed to be worried, degrees of unlikeliness seem to provide no comfort, unless one can prove that harm is absolutely impossible, which itself is not possible.”66 With respect to hope, those who operate gambling casinos and state lotteries are well-aware of the underlying mechanisms. They play on people’s emotions in the particular sense that they conjure up palpable pictures of victory and easy living. With respect to risks, insurance companies and environmental groups do exactly the same. The point explains “why societal concerns about hazards such as nuclear power and exposure to extremely small amounts of toxic chemicals fail to recede in response to information about the very small probabilities of the feared consequences from such hazards.”67 61W. Kip Viscusi, Alarmist Decisions With Divergent Risk Information, 107 Ec. Journal 1657, 1657–59 (1997) 62Id. 63Id. at 1666. 64Id at 1668. 
65It would be tempting to venture a sociobiological explanation for probability neglect. While plausible, such an explanation would be highly speculative: We could imagine sociobiological explanations both for probability neglect and for intense concern with probability. I emphasize empirical evidence here, not theoretical accounts. 
66See Weingart, supra note 1, at 362. 
67See Paul Slovic et al., The Affect Heuristic, forthcoming in Intuitive Judgment: Heuristics and Biases (Tom Gilovich et al. eds, forthcoming), unpublished manuscript at 11. 
From what has been said thus far, it should be clear that news sources can do a great deal to trigger fear, simply by offering examples of situations in which the “worst case” has actually come to fruition. For crime, the point is well established.68 Media coverage of highly unusual crimes makes people fearful of risks that they are most unlikely to face.69 When newspapers and magazines are emphasizing deaths from anthrax or mad cow disease, we should expect a significant increase in public concern, not only because of the operation of the availability heuristic, but because people will not naturally make sufficient adjustments from the standpoint of probability. In fact there is a large warning here. If newspapers, magazines, and news programs are stressing certain harms from remote risks, people’s concern is likely to be out of proportion to reality. Significant changes should therefore be expected over time.70 Across nations, it is also easy to imagine substantial differences, in social fear, if small initial differences are magnified as a result of media influences.71 
It is also true that individuals and even societies differ in their susceptibility to probability neglect. Though experiments have not been conducted on the precise point, it is clear that some people take probability information into account even when the context ordinarily engages human emotions. It also seems clear that some people neglect probability information much of the time, focusing insistently on the worst case (or for that matter the best). The arsenic experiment, mentioned above, displays a great deal of individual heterogeneity in taking account of probability.72 Those who are peculiarly insensitive to probability information are likely to do poorly in many domains, including economic markets; those who are unusually attentive to that information are likely to do well for just that reason. Perhaps there are demographic differences here; it is well-known that some groups are less concerned about most risks than are others,73 and that the difference in concern may stem, in part, from the fact that some groups are less likely to neglect probability. 
On the social level, institutions can make a great deal of difference in decreasing or increasing susceptibility to probability/neglect. Highly responsive 68See Joel Best, Random Violence: How We Talk About New Crimes and New Victims (1999). 69Id. 70See the account of fears of criminal violence in id. 71See the discussion of multiple equilibria in Kuran and Sunstein, supra note, at 743–46. 72Unpublished data on file, University of Chicago Law School. 73See Slovic, supra note, at 396–402. democratic institutions, automatically translating public fear into law, will neglect probabilities when emotions are running high. A more deliberative democracy would attempt to create institutions that have a degree of immunity from short-turn public alarm.74 Cost-benefit analysis might, for example, serve as a check on regulation that would accomplish little good, or less good than is justified by the facts.75 The point raises the general question of the relationship between probability neglect and law. 
IV. Law 
If emotionally charged outcomes produce intense reactions even though they are highly unlikely to occur, how might our understanding of law be improved? To answer this question, it is important to separate law’s prescriptive, positive, and normative tasks.76 With prescriptive analysis, we seek to find effective ways to achieve shared goals; positive analysis attempts to explain why law takes the form that it does; normative analysis explores what law should do. I take these up in sequence. 
Suppose that government is seeking to lead people to achieve goals on which there is a social consensus. Government might, for example, want to encourage people to avoid large risks and to worry less over small risks. If so, it would do well to attempt not to provide information about probabilities, but to appeal to people’s emotions and to attend to the worst case. With respect to the risks on which it wants people to focus, government should use vivid images of alarming scenarios. For cigarette smoking, abuse of alcohol, reckless driving, and abuse of drugs, this is exactly what government occasionally attempts to do. It should be no surprise that some of the most effective efforts to control cigarette smoking appeal to people’s emotions, by making them feel that if they smoke, they will be dupes of the tobacco companies or imposing harms on innocent third parties— and they do so especially by providing vivid images of illness or even death.77 
Because of probability neglect, it should not be terribly difficult to trigger public fear (terrorism is effective in part for exactly that reason). But there are serious ethical issues here. Government ought to treat its citizens with respect78; it should not treat them as objects to be channeled in government’s preferred directions. Perhaps government ought not to manipulate or to trick them, by taking advantage of their limitations in thinking about risk. A skeptic might think that the use of worst-case scenarios, or dramatic images of harm, consists of unacceptable manipulation. While I cannot fully resolve the issue here, the charge seems to me unwarranted. So long as the government is democratically accountable, and attempting to discourage people from running genuinely serious risks, there should be no objection in principle. Those who want people to run risks, for economic or other purposes, use similar techniques,79 and government should probably be permitted to meet fire with fire. Democratic accountability is important because it is a check on manipulative behavior; if government is manipulating people in an objectionable way, citizens are likely to rebel. Of course the issue is not simple. In the context of state lotteries, state governments use dramatic images of “easy street” in order to lead people to spend money for tickets whose actuarial value is effectively zero, and this strategy, exploiting probability neglect in the domain of hope, does raise ethical issues.80 My suggestion is only that if government wants people not to run risks, it is likely to do well if it appeals to their emotions. 
There is also a striking asymmetry between increasing fear and decreasing it. If people are now alarmed about a low-probability hazard, is there anything that government can do to provide assurance and to dampen concern? This is an unanswered question. The only clear point is that if government is unlikely to be successful if it simply emphasizes the low probability that the risk will occur. There appears to be no evidence that any particular strategy will succeed. But the best approach seems to be simple: change the subject. We have seen that discussions of low-probability risks tend to heighten public concern, even if those discussions consist largely of reassurance. Perhaps the most effective way of reducing fear of a low-probability risk is simply to discuss something else and to let time do the rest. Of course media attention can undermine this approach. 
78See the discussion of the publicity condition in John Rawls, A Theory of Justice (1971), a condition that raises some questions about any governmental effort to enlist probability neglect in its preferred directions. 
80See Philip Cook and Charles T. Clotfelter, Selling Hope (1991). 
As I have suggested, institutional safeguards might well be the best way of ensuring against the harmful consequences of probability neglect. The Office of Information and Regulatory Affairs, within the Office of Management and Budget, monitors agency action to ensure that it is directed against significant problems.81 A general requirement of cost-benefit balancing should provide a check on regulations that cannot be grounded in objective fact.82 If government wants to protect itself against the pattern of “paranoia and neglect” 83 that now characterizes regulatory policy, analytic requirements and institutional checks will provide a start. 
If probability neglect characterizes individual judgment under certain circumstances, might government and law be neglecting probability under those same circumstances? There is good reason for an affirmative answer. In the domain of risk regulation as elsewhere, public officials are highly responsive to the public demand for law. If people insist on government protection against risk, government is likely to provide that protection. If people show unusually strong reactions to low-probability catastrophes, government is likely to act accordingly. Of course interest groups are involved as well. When their selfinterest is at stake, we should expect them to exploit people’s emotions, in particular by stressing the worst case. In the environmental area, for example, there has been an intense debate about whether the National Environmental Policy Act requires agencies to discuss the worst-case scenario in environmental impact statements.84 Environmental groups sought to ensure discussion of that scenario.85 They did so in part to stimulate public concern, with the knowledge that the worst case might well have a great deal of salience, even if it is highly unlikely. For its part, the government originally required discussion of the worst case, but changed in its mind, with the apparent understanding that people are too likely to overreact. Hence the current approach, upheld by the Supreme Court,86 requires consideration of low-probability events, but only if they are not entirely remote and speculative. 
A good deal of legislation and regulation can be explained partly by reference to probability neglect when emotions are running high. In this space, I cannot demonstrate the point rigorously. But consider a few examples87: ! In the aftermath of the adverse health effects allegedly caused by abandoned hazardous waste in Love Canal, the government responded with an aggressive program for cleaning up abandoned hazardous waste cites, without examining the probability that illness would actually occur. In fact little was accomplished by early efforts to assure people of the low probability of harm.88 When the local health department publicized controlled studies showing little evidence of adverse effects, the publicity did not dampen concern, because the numbers “had no meaning.”89 In fact the numbers seemed to aggravate fear, insofar as they discussed the problem at all: “One woman, divorced and with three sick children, looked at the piece of paper with numbers and started crying hysterically: ‘No wonder my children are sick. Am I doing to die? What’s going to happen to my children?’”90 Questions of this sort contributed to the enactment of new legislation to control abandoned hazardous waste sites, legislation that did not embody careful consideration of the probability of significant health or environmental benefits.91 Even now, the government does not take much account of the probability of significant harm in making clean-up decisions.92 ! During a highly publicized campaign designed to show a connection between Alar, a pesticide, and cancer in children, the public demand for action was not much affected by the EPA’s cautionary notes about the low probability of getting that disease.93 87The catalogue in Aaron Wildavsky, But Is It True (1997), offers many illustrations of inadequately founded health and safety scares, many of which might be analyzed in the terms used here, 
88See Timur Kuran and Cass R. Sunstein, Availability Cascades and Risk Regulation, 51 Stan. L. Rev. 683, 691–98 (1999). 
89 ois Marie Gibbs, Love Canal: The Story Continues 25 (1998). 90Id. 
91See Kuran and Sunstein, supra note; James Hamilton and W. Kip Viscusi, Calculating Risks (2000). 
92See Hamilton and Viscusi, supra note (discussing lack of government interest in size of population affected). 
93See Robert Percival et al., supra note, at 524. ! In the fall of 2001, vivid images of shark attacks created a public outcry about new risks for ocean swimmers.94 Consider the fact that a NEXIS search found no fewer than 940 references to shark attacks between August 4, 2001, and September 4, 2001,95 with 130 references to "the summer of the shark."96 This was so notwithstanding the exceedingly low probability of a shark attack, and the absence of any reliable evidence of an increase in shark attacks in the summer of 2001.97 Predictably, there was considerable discussion of new legislation to control the problem,98 and eventually such legislation was enacted.99 Public fear seemed relatively impervious to the fact that the underlying risk was tiny. ! Jury behavior is not likely to be greatly affected by assurance that the risk was unlikely to come to fruition, even if the issue of probability is legally relevant.100 In cases involving low-probability risks of emotionally gripping harms, it should be relatively easy to convince jurors to offer high damage awards. Litigators therefore do well to try to engage jurors’ emotions by pointing to the worst case. There is a strong implication here for the law of negligence: Even if the law asks the jury to balance the benefits of the defendant’s action against the costs, the jury is likely to disregard the issue of probability if its attention is focused on an outcome that triggers strong emotions. Along the same lines, an understanding of probability neglect helps explain the finding, in both experimental and real-world settings, that juries do not respond favorably to a demonstration that the defendant performed a cost-benefit analysis before proceeding, even if the analysis places a high value on human life.101 The reason is that jurors will be focusing on the badness of the outcome, not the low (ex ante) probability that it would have occurred. ! The anthrax scare of October, 2001 was based on exceedingly few incidents. 
Only four people died of the infection; only about a dozen others fell ill. The probability of being infected was exceedingly low. Nonetheless, fear proliferated, with people focusing their attention on the outcome rather than the low probability of the harm. The government responded accordingly, investing massive resources in ensuring against anthrax infections. Private institutions reacted the same way, asking people to take extraordinary care in opening the mail even though the statistical risks were tiny (see appendix for an example). To say this is not to suggest that extensive precautions were unjustified in this case. Private and public institutions faced an unknown probability of a major health problem, and it was appropriate to respond. My point is that public fear was disproportionate to its cause, and that the level of response was disproportionate too. 
For law, the hardest questions might well be normative ones: How should law and government respond to a quasi-rational public panic, based on an intense emotional reaction to a low-probability risk? Let us distinguish two possible positions. The technocrat would want to ignore public irrationality, and to respond to risks if and to the extent that they are real. The populist would want to respond to public concerns, simply because they are public concerns. In my view, both positions are far too simple, though the populist is closer to the mark. 
Suppose that people are greatly concerned about a risk that has a small or even miniscule probability of occurring -- shark attacks, or anthrax in the mail, or terrorism on airplanes. If government is confident that it knows the facts, and if people are far more concerned than the facts warrant, should the government 101See W. Kip Viscusi, Corporate Risk Analysis: A Reckless Act?, 52 Stan. L. Rev. 547 (2000). respond, via regulation, to their concerns? Or should it ignore them, on the ground that the concerns are irrational? Consider the individual analogy first. Even if people’s fear is itself irrational, it might well be rational for them to take account of that fear in their behavior. If I am afraid to fly, I might decline to do so, on the ground that my fear will make the experience quite dreadful (not only while flying but in anticipating it). At the same time, the fear itself might be irrational, and I might even recognize that fact. If the fear exists, but if I cannot eliminate it, the most rational decision might be not to fly. So too at the social level. Suppose, for example, that people are afraid of arsenic in drinking water, and that they demand steps to provide assurance that arsenic levels will not be hazardous. Suppose too that the risks from existing levels of arsenic are infinitesimal. Is it so clear that government should refuse to do what people want it to do? The fear is by hypothesis real. If people are scared that their drinking water is “not safe,” they are, simply for that reason, experiencing a significant loss. In many domains, widespread fear helps produce an array of additional problems. It may, for example, make people reluctant to engage in certain activities, such as flying on airplanes or eating certain foods. The resulting costs can be extremely high.102 Why shouldn’t government attempt to reduce fear, just as it attempts to produce other gains to people’s well-being? 
people, it should do that instead. It should not waste resources on steps that will do nothing other than to reduce fear. But the simplest answer is too pat. Whether information and education will work is an empirical question on which we lack definitive evidence. If these do not work, government should respond, just as individuals do, to fears that are quasi-rational, but real and by hypothesis difficult to eradicate. Suppose, for example, that government could cheaply undertake a procedure that would reduce a tiny risk to zero – and equally important, be seen to reduce the relevant risk to zero. It seems clear that government should take this step, which may be more effective, and less expensive, than education and information. Recall that fear is a real social cost, and it is likely to lead to other social costs.103 If, for example, people are afraid to 102The mad cow disease scare is an example, producing many millions of dollars in losses. 103My point here is not that all subjective perceptions and losses should be counted in law. Many people, for example, like to discriminate on the basis of race and sex, and they suffer a genuine loss, for which they might be willing to pay, as a result of the legal prohibition on discrimination. I do not believe that their loss should be counted, though I cannot defend the point here. See Matthew Adler and Eric Posner, Implementing Cost-Benefit Analysis when Preferences Are Distorted, in Cost-Benefit Analysis 269 2001) fly, the economy will suffer in multiple ways; so too if people are afraid to send or to receive mail. The reduction of even baseless fear is a social good. 
Even if it is clear that government should respond, many questions remain. How and how much should government respond? The answer must depend in large part on the extent of the fear and the cost of the response. If people are extremely fearful, a substantial response is of course easier to justify; if the cost of response is very high, a refusal to respond might well make sense. With this point, the analysis of appropriate action becomes similar to the analysis in many other settings. We need to know how much good, and how much harm, would be done by the action in question. A special difficulty here consists in the problem of quantifying and monetizing fear and its consequences, a problem that has yet to be seriously engaged in the relevant literature.104 
Thus far I have been writing as if there is a clear distinction between cognition and emotion. But actually the distinction is complex and contested.105 In the domain of risks, and most other places, emotional reactions are usually based on thinking; they are hardly cognition-free. When a negative emotion is associated with a certain risk – pesticides or nuclear power, for example – cognition is playing a central role. In fact there are large debates about whether an emotion is a form of thought, or whether thoughts are necessary and sufficient conditions for emotions, or whether emotions is a sense precede or outrun cognition.106 But it is clear that no simple line can be drawn between emotions and cognition in most social domains. Whatever they are, emotions can lead us astray; but the same is true for math, biology, and animal experiments. 
There are several ways to make progress here. Some research suggests that the brain has special sectors for emotions, and that some types of emotions, including some fear-type reactions (my emphasis here), can be triggered before the more cognitive sectors become involved at all.107 Those who hear sudden, unexplained noises are fearful before they are able to identify the source of the cannot be said to be invidious or vicious, and hence cannot be “impeached” in the same way as discriminatory preferences. 
104For a good overview, see W. Kip Viscusi, Rational Risk Regulation (2000). 
105For varying views, see Ronald deSousa, The Rationality of Emotion (1993); Jon Elster, Alchemies of the Mind (1999); Martha Nussbaum, Upheavals of Thought (2001). 106See id. 107See Joseph LeDoux, The Emotional Brain (1996). noise.108 People who have been given intravenous injections of procaine, which stimulates the amygdala, report panic sensations.109 In research with human beings, electrical stimulation of the amygdala leads to reported feelings of fear and foreboding, even without any reason for these things, leading people to say, for example, that they feel as if someone were chasing them.110 It is not true, however, that fear in human beings is generally pre-cognitive or noncognitive, and even if it is in some cases, it is not clear that noncognitive fear would be triggered by most of the risks faced in everyday human lives. 
For purposes of the analysis here, it is not necessary to say anything especially controversial about the nature of the emotion of fear. I do think that when that emotion is involved, some kind of arousal is necessary; we would not say that someone is really afraid without some kind of arousal.111 But if people have a negative reaction to the prospect of electric shocks, or flying, or anthrax, thinking is emphatically involved. The only suggestion is that when emotions are intense, calculation is less likely to occur, or at least that form of calculation that involves assessment of risks in terms of not only the badness but also the probability of the outcome. That point is sufficient for my arguments here. 
In this Essay, my central claim has been that the probability of harm will be neglected when people’s emotions are activated, especially if people are thinking about the worst-case scenario. If that scenario is vivid and easy to visualize, large-scale changes in thought and behavior are to be expected. The point helps explain public overreaction to highly publicized, low-probability risks, including those posed by abandoned hazardous waste dumps and anthrax. Because rational people focus on the probability as well as the severity of harm, probability neglect is a form of quasi-rationality. 
It follows that if a private or public actor is seeking to produce public attention to a neglected risk, it is best to provide vivid, even visual images of the 108R.B. Zajonc, On the Primacy of Affect, 39 Am Psych 117 (1984); R.B. Zajonc, Feeling and Thinking: Preferences Need No Inferences, 35 Am Psych 151 (1980). 
109Servan-Schreiber and Perlstein, Selective Limbic Activation and its Relevance to Emotional Disorders, 12 Cognition & Emotion 331 (1998). 
1985) 
111See Elster, supra note (urging that emotions and especially fears are accompanied by arousal); but see Nussbaum, supra note (denying that claim). worst that might happen. It also follows that government regulation, affected as it is by the public demand for law, will neglect probability too. At first glance, the government should not respond if the public is demanding attention to a statistically miniscule risk, and doing so simply because people are visualizing the worst that can happen. The best response is information and education. But public fear is itself a concern and sometimes a quite serious one. If that fear cannot be alleviated without risk reduction, then government should engage in risk reduction, at least if the relevant steps are justified by an assessment of costs and benefits.112 112I have not said anything here about the difficult issue of how to monetize public fear. 
The following provides the experimental materials for the study described in part IIIB. 
Assume that you live in an area whose drinking water contains 50 parts per billion of arsenic. Assume also that at this level of arsenic, 1 in 100,000 people who drink this water over a period of years will die of cancer. The Environmental Protection is considering whether to reduce the permissible level of arsenic in drinking water from 50 to 5 parts per billion, which would essentially eliminate the cancer risk. What is the most that you would be willing to pay, in increases in annual water bills, for this reduction? 
Assume that you live in an area whose drinking water is contaminated by 50 parts per billion of arsenic, a known carcinogen. Assume also that this level of arsenic will kill 1 in 100,000 people who drink this water over a period of years. Assume finally that the death from arsenic-induced cancer is very gruesome and intensely painful, as the cancer eats away at internal organs of the body. The Environmental Protection is considering whether to reduce the permissible level of arsenic in drinking water from 50 to 5 parts 1) 0 2) $25 3) $50 4) $100 5) $200 6) $400 7) $800 or more 3) 0 4) $25 3) $50 4) $100 5) $200 6) $400 7) $800 or more 
Assume that you live in an area whose drinking water contains 50 parts per billion of arsenic. Assume also that at this level of arsenic, 1 in 1,000,000 people who drink this water over a period of years will die of cancer. The Environmental Protection is considering whether to reduce the permissible level of arsenic in drinking water from 50 to 5 parts per billion, which would essentially eliminate the cancer risk. What is the most that you would be willing to pay, in increases in annual water bills, for this reduction? per billion, which would essentially eliminate the cancer risk. What is the most that you would be willing to pay, in increases in annual water bills, for this reduction? 5) 0 6) $25 3) $50 4) $100 5) $200 6) $400 7) $800 or more 
Assume that you live in an area whose drinking water is contaminated by 50 parts per billion of arsenic, a known carcinogen. Assume also that this level of arsenic will kill 1 in 1,000,000 people who drink this water over a period of years. Assume finally that the death from arsenic-induced cancer is very gruesome and intensely painful, as the cancer eats away at internal organs of the body. The Environmental Protection is considering whether to reduce the permissible level of arsenic in drinking water from 50 to 5 parts per billion, which would essentially eliminate the cancer risk. What is the most that you would be willing to pay, in increases in annual water bills, for this reduction? 1) 0 2) $25 3) $50 4) $100 5) $200 6) $400 7) $800 or more • • • • • • • • • • • • 
The following memorandum consists of a memorandum sent to a university community in the midst of the anthrax threat. I include it here not because it is idiosyncratic, but because it captures a kind of “best practices” approach at the time. Note the breadth of the concern, calling on people to wear latex gloves while opening mail and including in the category of “suspicious mail” anything from “someone unfamiliar to you” (a large percentage of the mail received by most people), and advising those in receipt of such mail to notify the police, to isolate the mail, and to leave the area. 
Re: 
Updated Procedures for Handling Mail Suspected of Anthrax Contamination 
In light of recent events in which the United States mail was used to distribute anthrax, we recommend that you follow the up-dated guidelines below when handling mail. 
General Mail Handling • • • • • • • • 
Be observant for suspicious envelopes or packages. 
Open all mail with a letter opener or method that is least likely to disturb contents. 
Open packages/envelopes with a minimum amount of movement. 
Do not blow into envelopes. 
Do not shake or pour out contents. 
Keep hands away from nose, and mouth while opening mail. 
Wash hands after handling mail. 
We also recommend that you wear latex gloves when opening mail. 
(If you are allergic to latex, hypoallergenic gloves are available). 
What constitutes suspicious mail or parcels? 
The mail or parcel is considered suspicious when it is/has: a powdery substance on the outside; excessive postage, tape, a handwritten or poorly typed address, incorrect titles, or titles with no names, or misspelling of common words; unexpected or from someone unfamiliar to you; from a foreign country and is not expected; addressed to someone no longer with the organization or otherwise outdated; no return address, or has one that cannot be verified as legitimate; an unusual weight, given its size, or is lopsided or oddly shaped; marked with restrictive language, such as “Personal “ or “Confidential”; protruding wires, strange odors or stains; a postmark that does not match the return address. 
What should I do if I receive suspicious mail or a suspicious parcel? 
Do not handle the mail or parcel. 
Notify your supervisor immediately. 
Make sure that the suspicious mail or parcel remains isolated. 
Leave the area of the suspicious mail or parcel and do not let others into the area until appropriate authorities have indicated that it is safe to do so. 
Ensure that each person who touched the suspicious mail or parcel washes his or her hands with soap and water. 
List all persons who have touched the suspicious mail or parcel. 
Readers with comments should address them to: Professor Cass R. Sustein University of Chicago Law School 1111 East 60th Street Chicago, IL 60637 csunstei@midway.uchicago.edu William M. Landes, Copyright Protection of Letters, Diaries and Other Unpublished Works: An Economic Approach (July 1991). 
Richard A. Epstein, The Path to The T. J. Hooper: The Theory and History of Custom in the Law of Tort (August 1991). 
Cass R. Sunstein, On Property and Constitutionalism (September 1991). Richard A. Posner, Blackmail, Privacy, and Freedom of Contract (February 1992). Randal C. Picker, Security Interests, Misbehavior, and Common Pools (February 1992). 
Tomas J. Philipson & Richard A. Posner, Optimal Regulation of AIDS (April 1992). Douglas G. Baird, Revisiting Auctions in Chapter 11 (April 1992). 
William M. Landes, Sequential versus Unitary Trials: An Economic Analysis (July 1992). 
William M. Landes & Richard A. Posner, The Influence of Economics on Law: A Quantitative Study (August 1992). 
Alan O. Sykes, The Welfare Economics of Immigration Law: A Theoretical Survey With An Analysis of U.S. Policy (September 1992). 
Douglas G. Baird, 1992 Katz Lecture: Reconstructing Contracts (November 1992). Gary S. Becker, The Economic Way of Looking at Life (January 1993). 
J. Mark Ramseyer, Credibly Committing to Efficiency Wages: Cotton Spinning Cartels in Imperial Japan (March 1993). 
Cass R. Sunstein, Endogenous Preferences, Environmental Law (April 1993). Richard A. Posner, What Do Judges and Justices Maximize? (The Same Thing Everyone Else Does) (April 1993). 
Lucian Arye Bebchuk and Randal C. Picker, Bankruptcy Rules, Managerial Entrenchment, and Firm-Specific Human Capital (August 1993). 
J. Mark Ramseyer, Explicit Reasons for Implicit Contracts: The Legal Logic to the Japanese Main Bank System (August 1993). 
William M. Landes and Richard A. Posner, The Economics of Anticipatory Adjudication (September 1993). 
Kenneth W. Dam, The Economic Underpinnings of Patent Law (September 1993). Alan O. Sykes, An Introduction to Regression Analysis (October 1993). 
Richard A. Epstein, The Ubiquity of the Benefit Principle (March 1994). Randal C. Picker, An Introduction to Game Theory and the Law (June 1994). William M. Landes, Counterclaims: An Economic Analysis (June 1994). J. Mark Ramseyer, The Market for Children: Evidence from Early Modern Japan (August 1994). 
Robert H. Gertner and Geoffrey P. Miller, Settlement Escrows (August 1994). Kenneth W. Dam, Some Economic Considerations in the Intellectual Property Protection of Software (August 1994). 
Cass R. Sunstein, Rules and Rulelessness, (October 1994). 
David Friedman, More Justice for Less Money: A Step Beyond Cimino (December 1994). 
Daniel Shaviro, Budget Deficits and the Intergenerational Distribution of Lifetime Consumption (January 1995). 
Douglas G. Baird, The Law and Economics of Contract Damages (February 1995). Daniel Kessler, Thomas Meites, and Geoffrey P. Miller, Explaining Deviations from the Fifty Percent Rule: A Multimodal Approach to the Selection of Cases for Litigation (March 1995). 
Geoffrey P. Miller, Das Kapital: Solvency Regulation of the American Business Enterprise (April 1995). 
Richard Craswell, Freedom of Contract (August 1995). 
J. Mark Ramseyer, Public Choice (November 1995). 
Kenneth W. Dam, Intellectual Property in an Age of Software and Biotechnology (November 1995). 
Cass R. Sunstein, Social Norms and Social Roles (January 1996). 
J. Mark Ramseyer and Eric B. Rasmusen, Judicial Independence in Civil Law Regimes: Econometrics from Japan (January 1996). 
Richard A. Epstein, Transaction Costs and Property Rights: Or Do Good Fences Make Good Neighbors? (March 1996). 
Cass R. Sunstein, The Cost-Benefit State (May 1996). 
William M. Landes and Richard A. Posner, The Economics of Legal Disputes Over the Ownership of Works of Art and Other Collectibles (July 1996). John R. Lott, Jr. and David B. Mustard, Crime, Deterrence, and Right-to-Carry Concealed Handguns (August 1996). 
Cass R. Sunstein, Health-Health Tradeoffs (September 1996). 
G. Baird, The Hidden Virtues of Chapter 11: An Overview of the Law and Economics of Financially Distressed Firms (March 1997). 
Richard A. Posner, Community, Wealth, and Equality (March 1997). 
William M. Landes, The Art of Law and Economics: An Autobiographical Essay (March 1997). 
Cass R. Sunstein, Behavioral Analysis of Law (April 1997). 
John R. Lott, Jr. and Kermit Daniel, Term Limits and Electoral Competitiveness: Evidence from California’s State Legislative Races (May 1997). 
Randal C. Picker, Simple Games in a Complex World: A Generative Approach to the Adoption of Norms (June 1997). 
Richard A. Epstein, Contracts Small and Contracts Large: Contract Law through the Lens of Laissez-Faire (August 1997). 
Cass R. Sunstein, Daniel Kahneman, and David Schkade, Assessing Punitive Damages (with Notes on Cognition and Valuation in Law) (December 1997). William M. Landes, Lawrence Lessig, and Michael E. Solimine, Judicial Influence: A Citation Analysis of Federal Courts of Appeals Judges (January 1998). John R. Lott, Jr., A Simple Explanation for Why Campaign Expenditures are Increasing: The Government is Getting Bigger (February 1998). 
Lisa Bernstein, The Questionable Empirical Basis of Article 2’s Incorporation Strategy: A Preliminary Study (May 1999) Richard A. Epstein, Deconstructing Privacy: and Putting It Back Together Again (May 1999) William M. Landes, Winning the Art Lottery: The Economic Returns to the Ganz Collection (May 1999) Cass R. Sunstein, David Schkade, and Daniel Kahneman, Do People Want Optimal Deterrence? (June 1999) Tomas J. Philipson and Richard A. Posner, The Long-Run Growth in Obesity as a Function of Technological Change (June 1999) David A. Weisbach, Ironing Out the Flat Tax (August 1999) Eric A. Posner, A Theory of Contract Law under Conditions of Radical Judicial Error (August 1999) David Schkade, Cass R. Sunstein, and Daniel Kahneman, Are Juries Less Erratic than Individuals? Deliberation, Polarization, and Punitive Damages (September 1999) Cass R. Sunstein, Nondelegation Canons (September 1999) Richard A. Posner, The Theory and Practice of Citations Analysis, with Special Reference to Law and Economics (September 1999) Randal C. Picker, Regulating Network Industries: A Look at Intel (October 1999) Cass R. Sunstein, Cognition and Cost-Benefit Analysis (October 1999) Douglas G. Baird and Edward R. Morrison, Optimal Timing and Legal Decisionmaking: The Case of the Liquidation Decision in Bankruptcy (October 1999) Gertrud M. Fremling and Richard A. Posner, Market Signaling of Personal Characteristics (November 1999) Matthew D. Adler and Eric A. Posner, Implementing Cost-Benefit Analysis When Preferences Are Distorted (November 1999) Richard A. Posner, Orwell versus Huxley: Economics, Technology, Privacy, and Satire (November 1999) David A. Weisbach, Should the Tax Law Require Current Accrual of Interest on Derivative Financial Instruments? (December 1999) Cass R. Sunstein, The Law of Group Polarization (December 1999) Eric A. Posner, Agency Models in Law and Economics (January 2000) Karen Eggleston, Eric A. Posner, and Richard Zeckhauser, Simplicity and Complexity in Contracts (January 2000) Douglas G. Baird and Robert K. Rasmussen, Boyd’s Legacy and Blackstone’s Ghost (February 2000) David Schkade, Cass R. Sunstein, Daniel Kahneman, Deliberating about Dollars: The Severity Shift (February 2000) Richard A. Posner and Eric B. Rasmusen, Creating and Enforcing Norms, with Special Reference to Sanctions (March 2000) 2000) 2000) 2000) 
Position (August 2000) 
Internet (November 2000) 
System (November 2000) 
Relations: A Rational Choice Perspective (November 2000) 2000) 
Liability, Class Actions and the Patient’s Bill of Rights (December 2000) 
Economic Approach (December 2000) (January 2001) 
Finance (February 2001) (March 2001) 
Political Theory Perspective (April 2001) 
Age (April 2001) the Conceptual Foundations of Corporate Reorganization (April 2001) 
(May 2001) Cass R. Sunstein, Social and Economic Rights? Lessons from South Africa (May 2001) Christopher Avery, Christine Jolls, Richard A. Posner, and Alvin E. Roth, The Market for Federal Judicial Law Clerks (June 2001) Douglas G. Baird and Edward R. Morrison, Bankruptcy Decision Making (June 2001) Cass R. Sunstein, Regulating Risks after ATA (June 2001) Cass R. Sunstein, The Laws of Fear (June 2001) Richard A. Epstein, In and Out of Public Solution: The Hidden Perils of Property Transfer (July 2001) Randal C. Picker, Pursuing a Remedy in Microsoft: The Declining Need for Centralized Coordination in a Networked World (July 2001) Cass R. Sunstein, Daniel Kahneman, David Schkade, and Ilana Ritov, Predictably Incoherent Judgments (July 2001) Eric A. Posner, Courts Should Not Enforce Government Contracts (August 2001) Lisa Bernstein, Private Commercial Law in the Cotton Industry: Creating Cooperation through Rules, Norms, and Institutions (August 2001) Richard A. Epstein, The Allocation of the Commons:Parking and Stopping on the Commons (August 2001) Cass R. Sunstein, The Arithmetic of Arsenic (September 2001) Eric A. Posner, Richard Hynes, and Anup Malani, The Political Economy of Property Exemption Laws (September 2001) Eric A. Posner and George G. Triantis, Covenants Not to Compete from an Incomplete Contracts Perspective (September 2001) Cass R. Sunstein, Probability Neglect: Emotions, Worst Cases, and Law 
Follow this and additional works at: https://chicagounbound.uchicago.edu/law_and_economics Part of the Law Commons Recommended Citation Cass R. Sunstein, "Regulating Risks after ATA" ( John M. Olin Program in Law and Economics Working Paper No. 127, 2001). 
REGULATING RISKS AFTER ATA 
Cass R. Sunstein 
This paper can be downloaded without charge at: 
The Chicago Working Paper Series Index: http://www.law.uchicago.edu/Lawecon/index.html The Social Science Research Network Electronic Paper Collection: http://papers.ssrn.com/paper.taf?abstract_id=274189 
Whitman v. American Trucking Association was one of the most eagerly awaited regulatory decisions in many years. But the Court’s understated, steady, lawyerly opinion was a bit of an anticlimax, representing a return to normalcy and leaving many open questions. The Court was correct to say that the relevant provision of the Clean Air Act forbids consideration of cost; it was also correct to refuse to invoke the nondelegation doctrine. Importantly, the Court left in place a set of important lower court decisions, allowing agencies to consider costs unless Congress expressly concludes otherwise. The Court also raised some new questions about the constitutional status of the Occupational Safety and Health Act. Perhaps most important, the Court has not ruled out nonconstitutional challenges to the particulates and ozone standards, or to ambient air quality standards generally. In suggesting that some of those challenges should succeed, especially against the new ozone regulation, this essay urges that Justice Breyer’s concurring opinion in the case may well be the most influential in the future, because it fits most comfortably with other trends in the law of risk regulation, signaling the emergence of a kind of “cost-benefit state.” 
The last two decades have seen recurring conflicts between two strands in the law of risk regulation. The first strand, captured in what we might call “1970s environmentalism,” places a high premium on immediate responses to evident hazards; rejects claims for quantification of likely benefits; and avoids or downplays consideration of cost. These tendencies play a role in prominent legislation1 and also in the courts.2 The second strand, captured in what we * Karl N. Llewellyn Distinguished Service Professor of Jurisprudence, University of Chicago, Law School and Department of Political Science. I am grateful to Richard A. Posner and Adrian Vermeule for helpful comments on a previous draft. 1 See, e.g., 42 USC 7409(a), as interpreted in Whitman v. American Trucking Associations, 121 S.Ct. 903 (2001). 2 See Lead Industries Association v. EPA, 647 F.2d 1130 (DC Cir 1980); Ethyl Corp. v. EPA, 541 F.2d 1 (DC Cir 1976).. might call “the cost-benefit revolution,” urges a highly quantitative approach to risk control, based on careful specification of both costs and benefits, and on close attention to relevant tradeoffs, including the risks sometimes introduced by regulation. These tendencies can be seen in prominent executive orders,3 in legislation,4 and in courts as well.5 
Some of the most noteworthy conflicts between the two strands have involved efforts by courts to authorize agencies to make cost-benefit comparisons under statutes not explicitly calling for those comparisons.6 Recent presidents and courts have gone so far as to create a series of “cost-benefit default rules,” allowing agencies to disregard trivial risks, requiring agencies to show significant benefits from regulation, allowing agencies to consider the substitute risks introduced by regulation, and authorizing agencies to take costs into account when statutes are not explicit on the point.7 
Within the judiciary, the recent conflicts have pressed two questions in particular: When must agencies, including the Environmental Protection Agency (EPA), consider costs?8 When, if ever, will Congress be required to legislate with particularity?9 The latter question has not often thought to be especially interesting, because courts have not invoked the nondelegation doctrine to invalidate a federal statute since 1935.10 But a number of commentators have suggested that it is past time to revive the doctrine, partly in order to ensure legislative attention to tradeoffs rather than pleasant homilies.11 And in the last decade or so, some courts, concerned about the grant of open-ended discretion to the executive, have shown an unmistakable interest in doing exactly that.12 3 See, e.g., Executive Order 12291, 46 Fed. Reg. 13193 (1980); Executive Order 12498, 50 Fed. Reg. 1036 (1985); Executive Order 12866, 58 Fed. Reg. 51735 (1993). 4 See, e.g., 42 USC 300f et seq. (Safe Drinking Water Act). 5 See Corrosion Proof Fittings v. EPA, 947 F2d 1201 (5th Cir 1991). 6 See State of Michigan v. EPA, 213 F3d 663, 678-79 (DC Cir 2000). 7 See Cass R. Sunstein, Cost-Benefit Default Rules, Mich :L Rev (forthcoming 2001). 8 See, e.g., NRDC v. EPA, 824 F.2d 1146, 1154-1166 (DC Cir 1987); George Warren Corp. V. EPA 159 F3d 616, 623-24 (DC Cir 1998). 9 See, e.g., National Lime Association v. EPA, 233 F3d 625 (DC Cir 2000). 10 Panama Refining Co. v. Ryan, 293 US 388 (1935); Schechter Poultry Corp. v. US, 295 US 495 (1935). 11 See David Schoenbrod, Power Without Responsibility (1992); John Hart Ely, Democracy and Distrust (1981). 12 See American Trucking Assn. v. EPA, 175 F.3d 1027 (DC Cir 1999), reversed in part sub nom. Whitman v. EPA, 121 S. Ct. 903 (2001); International Union, UAW v. OSHA, 938 F.2d 1310 (DC Cir 1991); South Dakota v. Department of Interior, 69 F.2d 878 (8th Cir 1995); Massieu v. Reno, 915 F Supp 681 (DNY 1996). 
Because it promised to help answer both of these two questions, Whitman v. American Trucking Association13 was the most eagerly anticipated case in administrative law in many years. In some circles, a dramatic decision was anticipated, raising the possibility of significant new directions in the law of environmental protection and risk regulation. On the central issues, however, the Court’s unanimous, steady, lawyerly opinion amounted to a return to normalcy – and to an understated but unmistakable rebuke to some attempted innovations on the part of the court of appeals for the District of Columbia Circuit. In its principal ruling, the Supreme Court reaffirmed long-settled law to the effect that in setting national ambient air quality standards, the EPA is not permitted to consider costs.14 The Court also rejected a constitutional attack on the Clean Air Act, reestablishing long-settled law allowing Congress to delegate broad discretionary authority to regulatory agencies.15 And through this return to normalcy, the current Court treated the court of appeals for the District of Columbia in the same way that the 1970s Supreme Court treated that very court of appeals – attacking new judicial innovations and calling for at least a form of “hands off.”16 
The Court ruled quite broadly on both of the key issues, but it also reasoned in an extremely unambitious way, saying invoking statutory language and precedent while saying little about the topic of risk regulation and offering astonishingly little in the way of theoretical ground for its reluctance to invoke the nondelegation doctrine. We know that the Court is not much interested in reviving that doctrine; but we do not know why it lacks that interest. If we consider the importance of the case, the opinion seems a bit of an anticlimax, potentially even a judicial return to 1970s environmentalism. But two more interesting sets of issues lie beneath the dry surface. The first involves the state of the nondelegation doctrine. To be sure, the Court has signaled its lack of enthusiasm for the doctrine – indeed, it has given its clearest signal yet on that 13 121 S. Ct. 903 (2001). 14 For the initial part of the long settlement, see Lead Industries Association v. EPA, 647 F.2d 1130 (DC Cir 1980). 15 See, e.g., Touby v. United States, 500 U.S. 160 (1991). 16 The leading case here is Vermont Yankee Nuclear Power Corp. v. NRDC, 435 US 519 (1978). See Antonin Scalia, Vermont Yankee, The Supreme Court, and the DC Circuit, 1978 Supreme Court Review 345. Professor Scalia’s general (not unqualified) approval of the Supreme Court’s rejection of policy initiatives by the D.C. Circuit finds a kind of echo in his majority opinion in ATA. point. At the same time, the Court’s rejection of the approach of the court of appeals produces new questions about the nondelegation doctrine in some important domains of risk regulation, above all involving the Occupational Safety and Health Act (OSHA). As we shall see, ATA raises anew a serious constitutional question about a key provision of OSHA. 
The second, and more pressing, set of issues stems from the Court’s remarkably thin and unhelpful discussion of the meaning of the Clean Air Act (CAA). As we shall see, the same concerns that led the court of appeals to invoke the nondelegation doctrine might well reemerge on remand in ATA—and in many other contexts involving risk regulation. Above all, the court of appeals sought to require the EPA to attempt to quantify the risks that it was seeking to control, so as to ensure that the agency was attacking large problems rather than small ones.17 After ATA, this goal might be achieved through other, more modest routes, not involving the Constitution at all. While the ATA Court spoke largely in the terms of 1970s environmentalism, it did so in the context of an Article I challenge and a highly adventurous effort to inject cost considerations into an apparently cost-blind statutory provision. As I will urge, the decision should not be seen as an attack on more modest judicial innovations, nonconstitutional in nature, that attempt to increase the sense and rationality of risk regulation. 
In this Article, I suggest reasons to approve of the principal holdings in ATA, arguing that the Court properly interpreted the key statutory provisions and also that it was correct not to invoke the nondelegation doctrine in the case. In a period in which the Court is often criticized for allowing its own political convictions to overwhelm its duty of fidelity to the law, the quiet, lawyer-like analysis in ATA warrants considerable enthusiasm. I also argue on behalf of a large role, even after ATA, for the cost-benefit default principles, suggesting that they deserve a continuing place in the lower courts. In the process, I investigate one of the most interesting features of ATA: the sharply conflicting approaches of the Court’s two specialists in administrative law. Justice Scalia’s opinion for the Court is lawyerly, formalistic, textualist, and apparently indifferent to the topic of consequences. Justice Breyer’s concurring opinion is highly pragmatic, going well beyond the legal materials to try to make sense of the regulatory regime that Congress created. With respect to the law of risk regulation, I believe that Justice Breyer’s opinion, an unambiguous rejection of 1970s environmentalism, is the harbinger of the future, and potentially the most important opinion in the case. Invoking Justice Breyer’s concerns, I suggest that a number of challenges remain 17 See American Trucking Assn. v. EPA, 175 F.3d 1027 (DC Cir 1999). available to national ambient air quality standards – and that the most plausible of these challenges, connected directly with Justice Breyer’s opinion, promise to increase the sense and rationality of national environmental policy. 
This Article comes in four parts. Part I describes the background. Part II discusses the Court’s opinion and offers an evaluation. Part III briefly explores the three concurring opinions in the case, with particular reference to Justice Breyer’s emphasis on recurring issues in risk regulation. Part IV discusses the future. I pay special attention here to the problems not resolved in ATA, including the place of cost-benefit default principles, constitutional status of the Occupational Safety and Health Act (OSHA), and the possibility of nonconstitutional challenges to the EPA’s particulates and ozone regulations, and indeed ambient air quality regulations in general. 
To understand ATA, it is necessary to know something about what happened in the EPA and the court of appeals. For many years, the EPA had refused to issue a new national standard for particulates, notwithstanding scientific evidence apparently indicating grounds for action. Eventually the EPA was spurred to act by a decision of a district court, with an exceedingly rapid timetable for action.18 
So spurred, the EPA issued a new standard for small particulates in 199719; at the same time, the EPA issued a standard for ozone. In both cases, the scientific issues are extremely complex, and reasonable people might well argue about whether the evidence adequately supported the EPA’s decisions. But a general reading of the evidence suggests an important distinction, one that is relevant both to ATA and to the future of risk regulation. Under the EPA’s own data, the particulates standard promises significant health gains, both in terms of mortalities averted and in terms of morbidity.20 Consider the following table: 18 See American Lung Association v. Browner, 884 F. Supp. 345 (D. Ariz. 1994). 19 62 Fed. Reg 38,652 (1997). 20 The clearest evidence comes from U.S. EPA Innovative Strategies & Economics Group, Office of Air Quality Planning and Standards, Regulatory Impact Analyses for the Particulate Matter and Ozone National Ambient Air Quality Standards (1997). For an overview, see appendix to Cass R. Sunstein, Is the Clean Air Act Unconstitutional?, 98 Mich L Rev 303, 384-94 (1999). 
But EPA data suggest a much more mixed picture for ground-level ozone. The evidence is more mixed for two reasons. First, the mortality and health gains from the new standard appear much more modest; it is possible that the ozone standard would save no additional lives per year. Indeed, tighter regulation of particulates, going well beyond the EPA’s rule, would appear to do a great deal more to protect health than would the new regulation of ozone. Consider the following table: 
0.08 4th Max Low- to High-end Est. 
0 – 80 
Second, the evidence for ozone is more mixed because ground-level ozone provides protection against cataracts and skin cancer.21 To be sure, cataracts and skin cancer are not the most serious of health problems, especially in light of the very high cure rate for skin cancer. But if the protective effects are taken into account, it is not entirely clear that the new regulation of ozone will actually produce a net improvement in terms of health. 
The relevant provision of the Clean Air Act requires the EPA to set primary standards “the attainment and maintenance of which . . . are requisite to protect the public health,” with “an adequate margin of safety.”22 The American Trucking Association and others launched a variety of challenges to the particulates and ozone standards. Of these challenges, among the most interesting was based on the evidence just described -- a claim that the EPA was obliged to take into account the health benefits, and not merely the health risks, of ground-level ozone. The court accepted this objection, concluding that the EPA had consider all “identifiable effects” of ozone, both positive and negative.23 The government did not appeal this important aspect of the decision, whose implications for future EPA action remain unclear, in part because of the EPA’s unenthusiastic response to the ruling.24 
But the two major rulings of the court of appeals involved even larger issues. In the court’s view, the statute simply did not allow the agency to consider costs, and hence a cost-blind approach to statutory standard was not merely permissible but required.25 In this way the court rejected American Trucking Association’s imaginative effort to produce a large-scale departure from the longstanding understanding that national ambient air quality standards would be set without regard to costs. But construed as a “benefits only” provision, the court thought that the relevant provision of the CAA raised extremely serious constitutional problems under the nondelegation doctrine, which grows out of the constitutional vesting of “all legislative power” in “a Congress of the United States.”26 The reason for the constitutional difficulty was 21 See Randall Lutter and C. Wolz, UV-B Screening by Tropospheric Ozone: Implications for the National Ambient Air Quality Standard, 31 Env. Science & Tech. 142A (1997); Randall Lutter and Howard Gruenspecht, Assessing Benefits of Ground-Level Ozone: What Role for Science in Setting National Ambient Air Quality Standards (unpublished manuscript 2001). 22 42 USC 7409(b)(1). 23 American Trucking Assns. V. EPA, 175 F.3d 1027 (DC Cir 1999). 24 For an overview, see Lutter and Gruenspecht, supra note. 25 See id. for an outline and for critical discussion. 26 United States Constitution, Article 1, section 1. simple: The relevant provision, as interpreted by EPA, seemed to allow the agency to choose whatever standard it liked best. “The agency rightly recognizes that the question is one of degree, but offers no intelligible principle by which to identify a stopping point.”27 In the court’s view, “EPA’s formulation of its policy judgment leaves it free to pick any point between zero and a hair below the concentrations yielding London’s Killer Fog.”28 The Constitution could not be satisfied without some clearer principle distinguishing permitted from prohibited regulation. Rather than invalidating the statute, however, the court allowed EPA to cure the difficulty. In the court’s view, it remained possible that the EPA could construe the statute in a way that would remove the constitutional doubts.29 Such a narrowing construction would be both necessary and sufficient for validity. 
But what, specifically, did the court have in mind? The question is important both for understanding the Supreme Court’s decision in ATA and for understanding the future of national standards after ATA. Clearly the court of appeals sought a high degree of quantification – indeed a high enough degree of quantification to allow the EPA to explain, with some precision, why one standard would be “requisite” to protect the public health, while other standards would not be. In this way, the court signaled its interest in moving well beyond 1970s environmentalism, toward an approach that would rely on numbers rather than general concern and overall impressions. Above all, the court sought a specification of the kinds of numbers – in terms of likely adverse health effects under the existing standard -- that would call for increased regulation, and the kinds of numbers that the EPA would consider tolerable, and hence would not call for increased regulation. In the key passage, the court wrote: “On the issue of likelihood, for some purposes it might be appropriate to use standards drawn from other areas of the law, such as the familiar ‘more probable than not’ criterion. Of course a one-size-fits-all criterion of probability would make little sense. . . . Nonetheless, an agency wielding the power over American life possessed by the EPA should be capable of developing the rough equivalent of a generic unit of harm that takes into account population affected, severity, and probability. . . .”30 27 I75 F. 3d at 1036. 28 Id. at 1037. 29 Id. at 1039. 30 Id. 
What the court apparently sought was some sort of general measure of adverse effects, one that would give an explanation of why, in any given case, the agency regulated to one “point” rather than another. The agency might, for example, value a statistical death at “100,” treat a case of chronic bronchitis as some fraction of that, and do the same for every other adverse health effect. The universe of harms from exposure might well be aggregated in this way – so that the gains from the chosen level of regulation would be X, whereas the gains from less stringent regulation would be 80% of X, and the gains from more stringent regulation would be 120% of X. With quantification of this sort, it would be possible to know why the agency chose one point rather than another for particulates and ozone, and also to ensure that the agency’s particular choices squared, more or less well, with its particular choices for other pollutants. The result would be to ensure cross-pollutant coherence, and also to provide some coherent explanation, in specific cases, of why industry is wrong to urge less stringent controls, and why environmentalists are wrong to request more stringent controls. 
Perhaps the court suggested this “generic unit of harm” approach partly because of the odd fact that the particulates and ozone regulations were before it in the same case – and because it seemed hard to understand why EPA did not regulate particulates more stringently, and ozone less so. Of course there are many complexities in doing what the court of appeals sought. To give just one illustration, it is not clear if the agency should focus on the probability of harm faced by each individual, or instead on some statistical measure of aggregate harms, faced by the population as a whole.31 But the EPA does have sufficient information to produce some sort of overall measure of harm, in a way that would satisfy the court’s concerns.32 A key question is whether the Constitution, or some other source of law, requires the EPA to be so quantitative, as a way of explaining why it chooses one degree of regulation rather than another, either more or less stringent. 
The government promptly appealed, objecting above all to the nondelegation ruling. In a bit of a surprise, the respondents cross-petitioned, arguing that the Act should be construed to allow the EPA to consider costs in setting ambient standards. The cross-petition was analytically prior, and this was the question that the Supreme Court addressed first. 31 For a plea for looking at the size of the affected population, see James Hamilton and W. Kip Viscusi, Calculating Risks (1999). 32 See Sunstein, supra note, for an overview, with reference to ozone and particulates. 
Let us now turn to the Court’s rulings on the two key issues in the case.33 
The first question was whether national ambient air quality standards should be based on an assessment of benefits alone, or instead on some kind of balancing of benefits and costs. Recall that the relevant provision of the CAA requires the EPA to set primary standards “the attainment and maintenance of which . . . are requisite to protect the public health,” with “an adequate margin of safety.” At first glance, the statute appears to be indifferent to cost. All that matters is that pollution be controlled so as “to protect the public health.” This view of the provision is strengthened by the fact that national standards are supposed to be based on a “criteria” document, which is required, in turn, to discuss all “identifiable effects” of the pollutant on public health and welfare.34 The criteria document is not required to discuss the economic effects of regulation itself. Thus the analysis seemed simple: If the ambient standard is required “to protect the public health” and to be “based” on the criteria document, and if that document is not supposed to discuss costs, then it would seem to follow that standard-setting may not be done with reference to costs. 
In essence, the Court accepted this analysis. Proceeding in textualist manner, with an emphasis on statutory language and structure (but no reference to the legislative history), it held that standard-setting must indeed be costblind.35 The Court said that the key statutory term, “public health,” requires an analysis of the effects of the pollutant, and that costs are irrelevant. But the American Trucking Association and its allies pressed some good questions from common sense: Can the EPA possibly issue national standards on the basis of an inquiry into public health only? If the costs of compliance are extremely high, won’t the EPA inevitably impose more lenient regulations than it would if the costs are extremely low? Isn’t it obviously worthwhile to achieve some small improvement in air quality, if the costs of compliance are (say) $500,000, and obviously less worthwhile to do so if the costs of compliance are (say) $10 billion? These questions seem especially powerful in light of the fact, 33 I do not discuss the third ruling, involving the relationship between the new ozone rule and statutory provisions governing ozone. This third ruling has considerable practical importance, but it does not raise recurring issues. 34 42 USC 7409(b)(1). 35 121 S. Ct. at 911. acknowledged by EPA, that there is no “safe threshold” for many pollutants, including those involving in ATA itself. If there is no safe threshold, a cost-blind analysis would seem to require EPA to eliminate pollutants from the ambient air – a result that would ban automobiles, coal-fired power plants, and much more. Consideration of costs would appear necessary to avert this ludicrous conclusion. And the argument seemed to draw further strength from the apparent fact, urged by credible observers, that the EPA had in fact considered costs, although tacitly and without public supervision.36 
In fact it makes most sense to interpret the “requisite to protect the public health” language to require EPA to make a showing of significant risk, a point that goes some (not all) of the way toward answering these objections.37 But the Supreme Court’s major answer to these points was simple: Tell it to Congress.38 For the Court, the only real question was whether “public health” could be understood to be a reference not only to environmental effects, but also to the adverse effects introduced by regulation. Invoking the dictionary39 as its principal authority, the Court suggested (not so helpfully) that the ordinary meaning “public health” is “the health of the public,” that this is “the most natural of readings,” and that the natural reading is inconsistent with the claim that costs are relevant to the EPA’s decision.40 
To be sure, the cross-petititioners did not rely on policy arguments alone. They also urged (and the Court acknowledged, also with the dictionary’s aid) that “public health” is a not an unambiguous phrase. In their view, “many more factors than air pollution affect public health,” and “a very stringent standard might produce health losses sufficient to offset the health gains achieved in cleaning the air – for example, by closing down whole industries and thereby impoverishing the workers and consumes dependent upon those industries.”41 There is a great deal of evidence to support this claim. Expensive regulation does appear to produce health risks.42 36 See Marc Landy et al., The Environmental Protection Agency: Asking the Wrong Questions (1997). 37 As Justice Breyer emphasized. See below. 38 Id. 39 Use of the dictionary has become a common textualist theme. See Note, Looking It Up, 107 Harv. L. Rev. 1437 (1995); William Eskridge, The New Textualism, 37 UCLA L Rev 621 (1990). 40 121 S. Ct.. at 908. 41 I.d at 909. 42 See Robert Hahn et al. Do Federal Regulations Reduce Mortality? (2000). 
The Court did not reject the empirical claim, but it held that arguments of this kind could not defeat the “natural” interpretation of the Act. Indeed the Court analyzed the statutory structure to give further support to that interpretation. Unlike the apparently cost-blind provision governing national ambient air quality standards, other provisions of the Clean Air Act explicitly permitted cost to be taken into account. For example, a key provision of the Act, for example, asks EPA to consider costs in selecting the “best system” for emissions reductions from new pollution sources.43 The fact that some statutes expressly referred to cost seemed to support the view that the provision at issue in ATA, most naturally read as cost-blind, did in fact have this meaning. To this the Court added that Congress was “unquestionably aware” of the potentially adverse effects of expensive regulation, and hence allowed the EPA to waive the compliance deadline for stationary sources upon a showing, inter alia, that “the continued operation of such sources is essential . . . to the public health or welfare.”44 
All this was enough to suggest that the basic provision of the Act banned consideration of cost. Because that basic provision is “the engine that drives nearly all”45 of the subsequent provisions of the Act, Congress would not lightly be taken to have required cost consideration in other provision.46 Congress “does not alter the fundamental details of a regulatory scheme in vague terms or ancillary provisions – it does not, one might say, hide elephants in mouseholes.”47 Hence the statute’s terms “adequate margin” and “requisite” should not be taken to allow consideration of cost, because it is “implausible that Congress would give to the EPA through these modest words the power to determinate whether implementation costs should moderate national air quality standards.”48 To be sure, a number of provisions of the Act expressly require the EPA to generate information about compliance costs.49 But in the Court’s view, these provisions are designed to allow the EPA to assist the states in finding lowcost strategies for attainment.50 They do not suggest that EPA is allowed to 43 42 USC 7411(b)(1)(B). 44 121 S. Ct. at 909. 45 This is a considerable overstatement, to the point of inaccuracy. A number of provisions of the CAA are affected by the national ambient air quality standards, but most of the (countless) provisions are not affected by national standards. 46 Id. at 910. 47 Id. at 910. 48 Id. at 910. 49 42 USC 7408(b)(l).; 42 USC 7409(d)(2)(C)(iv). 50 121 S. Ct. at 911. consider costs on its own. All these considerations converged on a single conclusion: Ambient standards must be based on considerations of health, and cost simply does not matter. 
If the EPA’s task is to assess public health alone, is the statute an unconstitutional delegation of power? Common sense might suggest that there is a serious problem here. The question of what level is “requisite to protect the public health” seems unaccompanied by statutory standards. If there are no safe thresholds, perhaps EPA could (must?) require some pollutants to be eliminated entirely from the ambient air. But even in the absence of safe thresholds, perhaps EPA could decide that certain risks are to be treated as residual, ordinary, the stuff of everyday life. If the statute does not tell the agency what it must do, perhaps its range of discretion is unacceptable. Note in this regard that many people do not believe that certain imaginable steps are “requisite” to protect their own health: They walk at night in dangerous neighborhoods; they eat peanut butter; they fail to exercise; they gain weight; they drive cars; they own large dogs. In all of these cases, the risks may well be nontrivial as a statistical matter, and perhaps EPA could build on such practices in deciding what level of regulation is “requisite.” But without statutory guidance, perhaps the Act gives the EPA excessive discretion – the discretion essentially to choose the level of risk regulation that it wants, with essentially no legislative guidance. 
It is important to say here that a statute requiring cost-benefit balancing might, on this line of reasoning, raise similar nondelegation issues, at least in the absence of legislative guidance on how to assess both costs and benefits. Without such guidance, is an agency permitted to say that a statistical life is worth $1 million? $10 million? $50 million? Can an agency discount lives saved in the future? By 1%? By 7%? If the statute allows agencies to value statistical lives as they choose, there might seem to be an unacceptably high level of discretion. Thus a constitutional attack on the “requisite to protect the public health” language would doubts about many other provisions, including those that give costs a role to play in regulatory choices.51 
Troubled by the wide range of discretion apparently enjoyed by the EPA, the court of appeals held that the Constitution required the agency to come up with clear standards by which to explain why it would exercise its discretion to 51 See Sunstein, Is the Clean Air Act Unconstitutional?, supra note, for discussion. regulate to one “point” rather than another. But as a constitutional matter, the Supreme Court found the lower court’s approach quite implausible. “In a delegation challenge, the constitutional question is whether the statute has delegated legislative power to the agency.”52 Because this is the question, an agency cannot “cure” an otherwise unconstitutional delegation of power merely by adopting a narrowly construction. “The idea that an agency can cure an unconstitutionally standardless delegation of power by declining to exercise some of that power seems to us internally contradictory.”53 Thus the agency’s “voluntary self-denial”54 has no bearing on the real constitutional question, which is whether Congress has cabined agency discretion with the requisite “intelligible principle.” 
Turning to that question, the Court said that the statute required the EPA to use the latest scientific knowledge to establish standards “at a level that is requisite to protect the public health from the adverse effects of the pollutant in the ambient air.” The statutory idea of ‘requisite-to-protect” meant “sufficient, but not more that necessary,” to provide that protection.55 In the Court’s view, this standard was constitutionally acceptable, because it was “strikingly similar” to limitations found acceptable in other cases. The Court had, for example, upheld a grant of authority to the Attorney General to designate a drug as a controlled substance if this step was “necessary to avoid an imminent hazard to the public health.”56 The Occupational Safety and Health Act contained a quite similar provision, which was also found constitutionally valid.57 The Court acknowledged that “the degree of agency discretion that is acceptable varies according to the scope of the power congressionally conferred.”58 Hence Congress “must provide substantial guidance on setting air standards that affect the entire national economy.”59 But it was nonetheless unnecessary for Congress to provide a “determinate criterion” to establish how much regulation is too much. To be sure, the EPA would be making “judgments of degree.” The decision about what is “requisite” would not run afoul of the nondelegation principle. 52 121 S Ct at 912. 53 Id. 54 Id. 55 Id. at 912. 56 Touhy v. United States, 500 S 160 (1991). 57 Industrial Union Department, AFL-CIO v. API (The Benzene Case), 448 US 607 (1980). 58 121 S. Ct. at 913. 59 Id. 
On both of the key issues, the Court’s unanimity should not be surprising. Indeed, the Court’s fidelity to preexisting law -- even its steady, unimaginative, lawyerly opinion -- gives reason for some confidence in a Court that has been much criticized for allowing its own political convictions to play an excessive role in its interpretation of the law. It is far from unreasonable to speculate that some of the justices lacked much enthusiasm, on political grounds, for a costblind Clean Air Act, and that some of them would like Congress to give narrower delegations of authority to regulatory agencies. Nonetheless, the Court took the existing legal materials, in the form of statutory language and judicial precedents, extremely seriously. The Court was right to say that those materials required both results. The best interpretation of the CAA does not allow the EPA to consider costs when setting national standards. Moreover, there is no good argument for a large-scale revival of the nondelegation doctrine; and if the doctrine is to be used in the most extreme cases, ATA was not such a case. 
urge that it would be impossible to imagine an opinion coming out the other way on either issue. At a minimum, the Court might plausibly have said that in deciding what margin of safety is “adequate,” the EPA is permitted to take account of costs, not merely benefits. In addition, the phrase “requisite to protect the public health” is not altogether transparent. Especially in view of the fact that expensive regulations can have harmful effects on public health – a fact of which Congress was aware – it could be urged that whether a particular regulation qualifies, under that phrase, as “requisite” is a function of cost, not merely benefit. A reasonable court might well be drawn to this interpretation, at least if it were seen as the only way of making sense, rather than nonsense, of the statute as a whole. Such an opinion would not have been beyond the pale. 
Nor would this approach entirely lack precedential support. In the Benzene Case,60 the Court interpreted the Occupational Safety and Health Act in a way that would ensure against regulatory absolutism – not by requiring costbenefit balancing, but by saying, without textual support, that the agency could not regulate a risk not shown to be “significant.” In dissent, Justice Marshall, sounding very much like Justice Scalia in ATA, contended that conventional legal tools could not justify imposing a “significant risk” requirement, which, in 60 Industrial Union Department, AFL-CIO, v. API, 448 US 607 (1980). Justice Marshall’s view, was a judicial fabrication.61 In defending its conclusion, and its somewhat irreverent approach to the statutory text, the Benzene plurality emphasized that the government’s approach “would give OSHA power to impose enormous costs that might produce, little, if any, discernible benefit.”62 It would not have been a gigantic stretch from this line of reasoning to a holding, in ATA, that the EPA should be permitted to consider whether an enormous outlay of expenditures can be justified by the benefits received. In any case a number of lower court decisions have not stayed close to the statutory text where common sense seemed to require a measure of regulatory flexibility.63 
Nonetheless, the path that the Court chose was unquestionably the easier one, because it fit so much more naturally with the statutory language. To this we might add three further points. First, the pressure on the Court was greatly relieved by the fact that in the implementation process, costs could indeed be considered, and at multiple stages. This point sharply distinguished the CAA from OSHA, which lacks any comparable implementation stage.64 If Justice Scalia’s majority opinion in ATA is close to Justice Marshall’s dissenting one in the Benzene Case, it might be partly because the consequences of a cost-blind approach to standard-setting are not quite what they seem – as Justice Scalia was undoubtedly aware. Second, the “requisite to protect the public health” language might well be regarded as imposing a significant risk requirement. The Court said little on this issue, a gap to which I will turn in due course; but even on the Court’s view, the CAA does not require the EPA to remove all risks, regardless of their magnitude. For this reason the EPA’s position in ATA was far more cautious, and modest, than OSHA’s more draconian position in the Benzene Case. Third, textualism might be regarded as a kind of “penalty default,” imposing a burden on Congress, and on relevant interest groups, to provide a corrective for any problems introduced by a cost-blind approach. There is room for reasonable debate about whether Congress is sufficiently responsive to penalty defaults of this kind. But some comfort can come from the fact that a legislative corrective is certainly possible, and has in fact been provided in a somewhat analogous context.65 61 Id at 640 (Marshall, J., dissenting). 62 Id. at 620. 63 See, eg., State of Michigan v. EPA, supra. 64 This is because the CAA requires states to produce implementation plans to bring about compliance with federal standards, whereas OSHA regulations are directly binding on the private sector. 65 See 21 USC 376(b)((5)(B) (the Delaney Clause), as discussed in Public Citizen v. Young, 831 F2d 1108 (DC Cir 1987). In 1996, Congress amended the Delaney Clause in various ways, softening its satisfying. The Court appears to have delivered a death blow to those who have sought to revive the nondelegation doctrine. Why, exactly, did the Court do this? The Court’s opinion is quite wooden on the point, attempting only to show that the delegation in the CAA was no different from those delegations found acceptable in many other cases. This was an example of dullest and the most unambitious form of analogical reasoning – the sort of thing recently associated with Chief Justice Rehnquist, or even former Chief Justice Burger, and very different from what usually emerges from the Justice Scalia’s pen. 
Most strikingly, the Court made no effort to explain exactly why it was so willing to allow Congress to grant considerable discretionary authority to the EPA. Nor did the Court explain why it was uninterested in reviving the nondelegation doctrine in general. With respect to analogies, the Court basically skimmed the surface, arguing, plausibly if not entirely convincingly, that the delegation here was not evidently greater than the delegation in other cases. The Court offered exceedingly little in the way of detail. It would not have been terribly hard to distinguish those cases. The EPA, for example, is authorized to cover a much larger portion of the American economy than was involved in the precedents, and it would be at least plausible to say that its discretion was broader as well. To know whether this latter argument is convincing, it would be necessary to say much more about the meaning of the relevant provision of the Clean Air Act. But here the Court’s efforts were thin to the point of being comical. The repetition of statutory terms – “requisite to protect the public health” – was not much aided by the Court’s (repeated) explanation that these terms meant “no less, but also no more” than necessary to protect the health of the public. 
We might say that with respect to the nondelegation doctrine, the Court’s opinion is remarkably shallow, in the sense that it lacks any theoretical ambition, but also remarkably wide, in the sense that it appears to dispose of a wide range of imaginable nondelegation challenges.66 The Court has made it clear that the nondelegation doctrine will be used, if at all, only in the most extreme cases. But absolutist character. See 21 USC 321 (d), by replacing the flat ban with a requirement of a “reasonable certainty of no harm,” defined in the legislative history to reflect a policy of reducing cancer risks in the exposed population to no more than 1-in-1-million additional lifetime risk. For discussion, see Robert Percival et al., Environmental Regulation 496 (3d ed. 2001). 66 On shallowness/depth and narrowness/breadth, see Cass R. Sunstein, One Case At A Time (199). resources so that they save more lives or produce a cleaner environment— regulators must often take account of all of a proposed regulation’s adverse effects, at least where those adverse effects clearly threaten serious and disproportionate public harm. Hence, I believe that, other things being equal, we should read silences or ambiguities in the language of regulatory statutes as permitting, not forbidding, this type of rational regulation.”81 The reason that EPA could not consider costs here was that other things were not “equal”; the statutory structure and history showed “a congressional decision not to delegate to the agency the legal authority to consider economic costs of compliance.”82 This is a centrally important cautionary note, one to which I will return. 
Justice Breyer’s third goal was to establish the rationality of the legislative scheme as he understood it. He wanted to show that the “natural” construction made sense, not nonsense. His overall suggestion was that as construed, the statute would, at pertinent phases, allow consideration of costs after all, and that the EPA would be permitted to construe the statute reasonably, by making it less draconian than it appeared. To this end he offered four points, each of considerable interest. 
pollution control technology as given but to force technological innovation. This was hardly an unrealistic hope.83 In fact the catalytic converter was developed as a result of a seemingly draconian statutory mandate. And because the statute was expressly designed to force technological innovation, regulatory efforts to calculate the costs of compliance were “both less important and more difficult”84 -- because the relevant calculations would be based on speculation about the cost of unknown future technologies. These calculations “can breed timeconsuming and potentially unresolvable agreements about the accuracy and significance of cost estimates.”85 In these circumstances, cost-benefit analysis might itself fail cost-benefit analysis. considered.86 These factors are relevant, for example, to states deciding on the mix of control devices used to achieve compliance; and those facing 81 Id. at 921. 82 Id. at 921-22. 83 Id. at 922. 84 Id.at 923. 85 Id. 86 Id. economic hardship can seek an exemption from state requirements. The EPA is also permitted to consider costs, not in setting standards, but in setting deadlines for attainment. Congress is also available to extend deadlines if necessary.87 The relevant provision of the CAA might be costblind; but this is not at all true for the statute as a whole. at any economic cost, however great.” Standards “requisite to protect the public health” need not produce “a world that is free of all risk—an impossible and undesirable objective.”88 In fact the terms “requisite” and “public health” should be understood in context. It is relevant for the EPA to consider “the public’s ordinary tolerance of the particular health risk in the particular context at issue.” Hence EPA is allowed to produce a kind of common law of “acceptable” risks, rather than eliminating all risks as such. In deciding what is “requisite to protect the public health,” the EPA is allowed “to consider the severity of a pollutant’s adverse health effects, the number of those likely to be affected, the distribution of the adverse effects, and the uncertainties surrounding each estimate.”89 On this count, Justice Breyer is saying more cautious than, but something not terribly far from, what was said by the court of appeals. In his own way, Justice Breyer was urging the EPA to develop standards for separating acceptable from unacceptable risks. safety overall.”90 If a rule causes “more harm to health than it prevents,” it is unlawful.91 In this wage Justice Breyer endorsed the lower court’s conclusion that the EPA is required to consider the health benefits of ground-level ozone, not just the health risks. 
What is most important about Justice Breyer’s opinion is the effort not merely to read the statutory terms, but also to make sense out of them – to show that the statutory framework is not as silly, or absurd, as it might seem to be in the abstract. There is a noteworthy contrast here between Justice Scalia’s approach, for the majority, and the approach favored by Justice Breyer.92 And 87 Id. 88 Id. at 924. 89 Id. 90 Id. 91 Id. 92 I will not discuss here the more general jurisprudential issues raised by formalism and pragmatism in statutory interpretation. A large issue is empirical – whether one or another approach can be defended on pragmatic grounds. If judges are undistinguished, formalism starts because it pays such attention to the pragmatic issues, Justice Breyer’s opinion might well prove to be the more important for the future. Each of the four points just mentioned has significant implications for EPA decisions and indeed for regulation more generally, as we shall shortly see. 
III. 
ATA leaves many issues open. For the future, the three crucial questions involve (a) the place of the “cost-benefit default principles,” which have played a large role in the last two decades of federal administrative law; (b) the status of the nondelegation doctrine; and (c) the legal standards governing ambient air quality standards, including the very standards involved in ATA. I discuss these issues in sequence. 
The Court was well-aware that a number of lower court decisions have established a new interpretive principle: Where statutes are ambiguous, agencies are allowed to consider costs.93 What is the status of this principle, after ATA? While things are not entirely clear, the best answer is that the principle is unaffected. In fact Justice Breyer went out of his way to endorse the basic idea.94 Justice Breyer was careful to say that courts “should read silences or ambiguities in the language of regulatory statutes” to permit consideration of “all of a proposed regulation’s adverse effects,” at least where those effects would clearly be serious and disproportionate.”95 For its part, the majority specifically referred to the cases establishing the principle and worked to distinguish them from the case at hand: “None of the sections of the CAA in which the District of Columbia Circuit has found authority for the EPA to consider costs shares [this provision’s] prominence in the overall statutory scheme.”96 Thus the Court was at pains to cite, with apparent approval, the key cases creating the basic principle, and appeared to be saying that the EPA might well be permitted to consider costs if the statute did not expressly forbid it from doing so. to look a lot better; hence the best defenses of formalism are pragmatic in character. For the best discussion, see Adrian Vermeule, Interpretive Choice, NYU L Rev (2000); in the same spirit, see Cass R. Sunstein, Must Formalism Be Defended Empirically?, U Chi L Rev (1999). 93 See, e.g., Michigan v. EPA, 213 F.3d 663, 678-79 (DC Cir 2000). 94 121 S Ct at 921. 95 Id. 96 121 S Ct at 910 n. 1. 
On the other hand, Justice Breyer was clearly concerned that the Court’s approach would override the cost-benefit default principle.97 Justice Breyer urges that "In order better to achieve regulatory goals—for example, to allocate resources so that they save more lives or produce a cleaner environment— regulators must often take account of all of a proposed regulation's adverse effects, at least where those effects clearly threaten serious and disproportionate public harm. Hence, I believe that, other things being equal, we should read silences or ambiguities in the language of regulatory statutes as permitting, not forbidding, this type of rational regulation."98 This point was meant as a rejoinder to the majority, which Justice Breyer took to be saying that to allow costs to be considered, Congress was required to be “clear.” But at first glance, Justice Breyer’s concern seems baseless. The Court was saying only that in view of the clarity of the main provision of the Clean Air Act, judges would be reluctant to find permission to consider costs elsewhere, since Congress “does not alter the fundamental details of a regulatory scheme in vague terms or ancillary provisions—it does not, one might say, hide elephants in mouseholes.”99 This is a standard approach to statutory interpretation. It does not suggest that where a statute’s “fundamental details” are vague, they will be interpreted to forbid consideration of cost. 
But it would not be impossible to read the Court's opinion a bit more broadly. Recall that in concluding that EPA need not consider costs in issuing national standards, the Court emphasized that some provisions of the CAA explicitly refer to costs, and explicitly require them to be taken into account. Here the Court was using the canon of construction, “expressio unius est exclusio alterius”: the expression of one thing is the exclusion of another. In the particular context of environmental statutes, the “expressio unius” canon could have explosive implications. When Congress does not explicitly refer to costs, agencies may not consider them, and for one simple reason: Congress often does explicitly refer to costs. If the canon is to govern the future, the cost-benefit default principles are in some trouble. 
There is a further point. The Court seems to suggest that a statute should not be taken to confer broad discretionary authority on agencies: “We find it implausible that Congress would give to the EPA through these modest words the power to determine whether implementation costs should moderate national 97 121 S. Ct. at 921. 98 Id. at 910 99 Id. air quality standards.”100 To support the view that ATA is best taken to disallow agencies to interpret ambiguous statutes to allow consideration of costs, it would be necessary to make a simple, two-step argument. First: Statutes should be construed so as to give agencies less rather than more in the way of discretion. Second: A construction of a statute that would allow agencies to decide whether to consider costs significantly increases agency discretion. Now the claim here is not that a statute requiring cost-benefit analysis is itself disfavored on delegation grounds. The claim is instead that an interpretation should be disfavored if the consequence of the interpretation would be to authorize the agency to decide whether to engage in cost-benefit balancing. If this claim is accepted, then the default rule in favor of allowing agencies to consider costs stands as repudiated. 
But it is most unlikely that the Court would accept these lines of argument. The “expressio unius” canon can be a useful guide to statutory construction, and the more natural, cost-blind reading of “public health” is certainly supported by the fact that some provisions of the CAA make explicit reference to costs. But here as elsewhere, the “expressio unius” idea should be taken with many grains of salt. If Congress has not, under some ambiguous statutory term, referred to costs, it will often be because Congress, as an institution, has not resolved the question whether costs should be considered. And if this is so, the agency is entitled to consider costs if it chooses.101 The fact that Congress explicitly refers to costs under other provisions is not a good indication that, under an ambiguous text, costs are statutorily irrelevant. This would be an extravagant and therefore implausible inference. The use of the “expressio unius” approach in ATA is best taken as a sensible way of fortifying the most natural interpretation, and not at all as a way of urging that explicit references to cost, in some provisions, means that costs may not be considered under ambiguous provisions. 
What about concerns about agency discretion? Agencies are typically allowed to interpret statutory ambiguities,102 and in countless cases in which that principle is invoked, the agency exercises a great deal of discretion over basic issues of policy and principle.103 To allow an agency to decide to consider costs is not to allow it to exercise more discretion than it does in numerous cases. Where the statute is unclear, agencies should be authorized to seek “rational regulation”; and nothing in ATA suggests otherwise. This is especially so in light 100 Id. 101 See Chevron v. NRDC, 47 US 837 (1984); see also note supra. 102 See Chevron v. NRDC, 47 US 837 (1984).. 103 See, e.g., id.; Babbitt v. Sweet Home Chapter, 515 US 687 (1995); Young v. Community Nutrition Institute, 476 US 974 (1986) of the fact, emphasized by both the Court104 and Justice Breyer,105 that the Clean Air Act allows EPA to consider costs at numerous stages in the implementation process. I conclude that ATA is best taken not to question the cost-benefit default principle, and indeed that the most reasonable reading of the opinion is that the Court has explicitly embraced that principle. 
The Court’s nondelegation ruling seems to be a kind of return to normalcy —as an effort to place the doctrine where it has been since 1935: In the constitutional coffin. This is a reasonable reading of the opinion, with the proviso that the doctrine remains available, now as before, for the extreme cases. What makes a case extreme? Apparently an extreme case would be one in which the agency has far more discretion than does the EPA under the “requisite to protect the public health” language. Cases of that sort are, by the logic of the Court’s opinion, very few and far between. Schechter Poultry106 remains good law, and the Court was careful to say that when the area of agency authority is very broad, the statutory principle will certainly have to be intelligible. But the Court’s basic message was that its own precedents suggest that almost all nondelegation challenges will be unavailing—and indeed that a mere repetition of any statutory standard will be a sufficient response. 
nondelegation ruling was no mere a return to normalcy, and for a simple reason: It places some longstanding doctrine in disarray. In a series of cases, of which ATA was the culmination, the court of appeals for the District of Columbia circuit has held that a narrowing construction by an agency will be a sufficient and necessary condition for saving an otherwise objectionable delegation of authority.107 The doctrine originated in a challenge to the statute granting the President the authority to fix prices and wages.108 The relevant provisions appeared not to limit the President’s discretion – to allow the President to set prices and wages however he chooses. In upholding the statute against constitutional attack, the court said not only that Congress had set out an intelligible principle, but also that the executive was 104 121 S Ct at 910. 105 121 S. Ct. at 921. 106 ALA Schechter Poultry Corp. v. US, 295 US 495 (1935). 107 Amalgamated Meat Cutters v. Connally, 337 F Supp 737 (DDC 1971); International Union, UAW v. OSHA, 938 F.2d 1310 (DC Cir 1991); International Union, UAW v. OSHA, 37 F3d 605 (DC Cir 1994). 108 Amalgamated Meat Cutters v. Connally, 337 F. Supp. 737 (DDC 1971). obliged to come up with “subsidiary” principles to cabin its own discretion. This part of the Court’s opinion owed its origins to some imaginative writing from Kenneth Culp Davis.109 The requirement of “subsidiary” principles seemed important to the decision, but it need not be seen as indispensable to it. It would be possible to read the relevant statute, in its context, as sharply limiting executive discretion, and thus to uphold it without relying at all on the need for a narrowing construction by the agency.110 
But the basic idea was significantly extended in an important case involving the Occupational Safety and Health Act.111 The relevant provision authorized the agency to set standards “reasonably necessary or appropriate” to provide safe and healthful places of employment.112 But what does this mean? Is this an open-ended delegation of authority? Would it be sufficient to say that the statute told the agency to do no more, and no less, than was “reasonably necessary or appropriate”? The court of appeals did not think that that would be sufficient. It was concerned that the statutory terms could mean any number of things. It could mean, for example, that OSHA should engage in cost-benefit analysis; perhaps a standard is not “reasonably necessary or appropriate” unless the benefits justified the costs. Or it could mean that OSHA was supposed to regulate all “significant risks” to a maximally protective point, subject perhaps to a constraint that the regulation be “feasible” for industry. Or it could mean something else. Because of the statutory term’s apparent plasticity, the court was obviously tempted to strike down the statute on nondelegation grounds. But instead of doing so, the court remanded the case to OSHA, concluding that the agency could save the statute with a narrowing construction. On remand, the agency generated what the court found—barely—to be a sufficient response.113 According to the agency, the statute required it to regulate only “significant risks,” and only to the point of “feasibility,” and within those constraints the agency was required to select the standard that would be most protective of workers. The court said that this was enough to satisfy the constitutional concern. 109 See Kenneth Culp Davis, A New Approach to Delegation, 36 U Chi L. Rev 713 (1969). 110 In brief: The wage and price freeze was a response to the perception that the nation was in the midst of “cost-;push” inflation. That was the statutory background. If that is what the statute was about, then the President could not play favorites, or reward his friends and punish his enemies. I say a bit more on this issue below. 111 International Union, UAW v. OSHA, 928 F2d 1310 (DC Cir 1991). 112 29 USC 652 (8) 113 International Union, UAW v. OSHA, 37 F3d 605 (DC Cir 1994). 
Notwithstanding the Supreme Court’s ruling in ATA, the approach of the court of appeals is not impossible to understand. If we are concerned about an absence of accountability, and also about values associated with the rule of law, a narrowing construction at the agency level can do a great deal of importance. Such a construction can expose the agency’s standard to public oversight and review; it increases transparency and to that extent accountability. And by ensuring that agency action will be undertaken pursuant to a clear standard, a narrow construction can go a long way toward alleviating the concern of arbitrary, unpredictable agency action, treating the similarly situated differently. But we should be careful not to say that the purposes of the nondelegation are the doctrine itself, or to dissociate the doctrine from Article I, its legal source. At most, a narrowing construction can be helpful when the Court is otherwise in equipoise. The nondelegation concern is not eliminated by such a construction. 
This is basically the Court’s response in ATA.114 And it is evident that after ATA, the court of appeals’ approach is entirely unacceptable. The Supreme Court has made clear that a narrowing interpretation by the agency cannot save an otherwise objectionable delegation. And in the end the Court’s reasoning makes perfect sense. If the problem is that Congress has failed to lay down standards for agencies to follow, how can the agency’s own standards resolve that problem? The question seems all the harder to answer in light of the fact that the source of the nondelegation doctrine is Article I, section 1, which vests legislature power in a “Congress of the United States.” The purpose of the nondelegation doctrine, it would seem, is to require Congress to legislate. Agency narrowing is neither here nor there. 
court of appeals decisions be understood? Might the Court have (inadvertently?) given a new boost to the nondelegation doctrine, by suggesting that a narrowing construction cannot be helpful? These questions are not simple to answer. The case of freezing wages and prices is the easier to handle. To be sure, Congress did not give clear standards in the text of the statute. But statutory terms receive meaning from context, and the context behind the Act suggested a desire to protect the nation from a certain kind of inflationary pressure, captured in the notion of “cost-push inflation,” in accordance of which unions and employers create a kind of psychological spiral, one that needs to be broken through law.115 114 See 121 S. Ct. at 111. 115 See Stephen Breyer and Richard B. Stewart, Administrative Law and Regulatory Policy 80-83 (2d ed. 1985). The statute itself did not refer to this theory, but it is clear, from context, that Congress did not seek to give the President the authority to freeze prices and wages in a way that involved political favoritism. If the statute should be interpreted so as to avoid the constitutional difficulty, it would not be an intolerable stretch to say that any executive action should be reviewed with the particular context in mind—and hence that statutory purpose sharply limited presidential discretion. Perhaps it will be responded that Congress should be required to say all this clearly in the statutory text itself. But would much be gained by requiring this step? This is far from obvious. 
But the contested provision of OSHA is much harder. The phrase “reasonably necessary or appropriate” seems, on its face, to leave everything open. To be sure, the phrase might at first appear similar to the standard that the Court found sufficient in ATA. But there is a big difference. In ATA, the Court held, immediately before upholding the statute against nondelegation attack, that the statute required a “health only” determination, and that it did not allow consideration of costs.116 The ATA Court held the statute constitutional in part because Congress was clear on this point; Congress itself decided that costs would not count, and the agency was not permitted to create the legal standard out of thin air. It seems to follow that if a statute itself requires consideration of costs, and hence a form of cost-benefit balancing, it would also be constitutional, notwithstanding the high levels of residual discretion that would remain. What makes the relevant provision of OSHA much harder is that it seems to give the agency discretion to decide whether the statute does or does not allow consideration of costs, and thus to decide what the statutory standard is. This was Chief Justice Rehnquist’s objection to another provision of OSHA117—an objection that was rejected on the ground that that provision could be authoritatively construed sharply to discipline the agency’s discretion.118 
Nothing in ATA, in short, resolves the question whether a court should uphold a statute that leaves the agency the authority to constructs its most fundamental meaning. Now it is important to be careful with this point. As we have seen, lower courts have created an interpretive principle authorizing agencies to consider costs if they see fit, and ATA seems to approve of this 116 121 S. Ct. at 109-110. 117 See Industrial Union Department, AFL-CIO v. API, 448 US 607, 624 (1980) (Rehnquist, J., concurring in the judgment) (objecting on nondelegation grounds to 29 USC 655 (b)(5)). 118 See id at 611 (interpreting provision in a way that implicitly rejects the nondelegation challenge); see also Whitman v. American Trucking Association, 121 S. Ct. at 911 (interpreting the Benzene Case as rejecting nondelegation challenge). principle.119 It would be implausible to suggest that a statute is unconstitutional if it allows an agency to decide whether or not to consider costs; this kind of decision has been found acceptable in many contexts, and ATA cannot be taken to say that the underlying statutes are now unconstitutional. After ATA, the appropriate answer to the constitutional question is this: Statutes that allow agencies to decide whether to consider costs admittedly confer considerable discretion. But after ATA, this is the further thing from fatal. The rest of the relevant statutes typically contains real limits on agency discretion. Even if agencies are allowed to decide whether or not to consider costs, they do not, under those statutes, have anything like a blank check. 
But OSHA’s “reasonably necessary or appropriate” language seems quite different on this count. In the abstract, that language allows the agency to choose the statutory standard. The significance of ATA is that if this conclusion is to be avoided, it cannot be for the reason invoked by the court of appeals. It must be because the statute is best construed to cabin agency discretion to some degree. We know, for example, that the statute bars measures that are unreasonable, or inappropriate, as means of achieving safe work places. In this way the statute can be taken to require cost-effectiveness, and also to require the agency to pursue the end of worker safety. But is this enough? The end of worker safety can be pursued in multiple ways. To be sure, the constitutional doubt would be removed, under ATA, if the statute were construed to require cost-benefit balancing.120 We can easily imagine a judicial opinion that would so construe the statute; perhaps a step is “reasonably necessary or appropriate” if and only if it survives balancing, all things considered. But the court of appeals was probably right to say that this interpretation, while possible, is not ordained by the statutory text. And it is because the interpretation is not ordained that the court remanded the case to the EPA for an authoritative construction. The problem, after ATA, is that such a construction is, with respect to the nondelegation issue, neither here nor there. 
In these circumstances, future courts have only three options. The first would be a version of the route taken in ATA: to point to other statutes giving agencies broad discretion (such as the “public interest, convenience, and necessity” standard of the Federal Communications Commission) and to urge 119 See below. 120 I have suggested above that a cost-benefit standard by itself leaves agencies a great deal of discretion, especially in the valuation of benefits. But under ATA, this degree of discretion, which is in fact quite standard, cannot possibly be taken to raise a serious constitutional problem. that the disputed provision is not much different. The problem with this approach is that it would be somewhat irresponsible without some effort to show how the agency is not permitted to do whatever it chooses. The second would be to generate an interpretation of the provision that adequately cabins agency discretion. This would certainly be possible, but the strategy in ATA—merely repeating the statutory language, with the added words “no less and no more”— would not be helpful. The third would be to strike down the statute on nondelegation grounds. It is ironic but true that this route may have been made more rather than less likely as a result of the Supreme Court’s rejection of the approach of the court of appeals. 
Should the court take this step? In general, I think that the ATA Court was entirely correct to suggest that the nondelegation doctrine deserve little place in modern constitutional law. For this reason, some combination of the first and second routes might be best: An effort to construe the statute to impose some limitations, with a recognition that a great deal of discretion is constitutionally legitimate. But if the nondelegation doctrine deserves any place at all, the “reasonably necessary or appropriate” language, in context, would not be the worst place for judicial invalidation. 
After ATA, a major question is how a plaintiff might be able to challenge a national standard, if costs cannot be considered and if the constitutional route is unavailable. ATA offers little guidance here. If the regulation is less stringent than is “requisite to protect the public health,” with an “adequate margin of safety,” it will be unlawful. If the regulation is more stringent than is requisite, it will also be unlawful. In the easy cases, the lessons are clear. A regulation will be subject to challenge if it allows significant adverse health effects, at least if the agency cannot reasonably explain that the adverse effects are insignificant. If we accept the evidence described above, it would therefore be reasonable to argue that the EPA was required, and not merely permitted, to produce a new regulation for particulates. A regulation will also be subject to challenge if significant adverse effects cannot be expected at levels that the EPA forbids. On a reasonable reading of the evidence governing ozone, the new EPA regulation is unlawful for that reason. 
I will return to these issues below. There are three major possibilities, and each of them would be entirely reasonable. In the end I will urge that the particulates rule should probably be upheld, but that the ozone standard should probably be remanded, so that the EPA can give a better, more quantitative explanation of why it chose the particular regulatory "points" that it selected. This judgment is tentative. What is more important than the conclusion is an appreciation of the grounds on which the three possibilities might be criticized and defended. 
evidence to support the view that both pollutants produce significant adverse health effects at currently permitted levels -- and hence the new controls are, in the administrator’s reasonable judgment, “requisite to protect the public health.” The statutory requirement of an “adequate margin of safety” might well be taken to support this view. As I have emphasized, the evidence supporting regulation of ozone seems a good deal weaker than the evidence supporting regulation of ozone, especially if we take into account the fact that ground-level ozone seems to have nontrivial health benefits. But perhaps a court should say that there is much scientific uncertainty here, and that the EPA should be allowed to resolve the doubts as it sees fit. If the court took this route, it would be following the direction established in the Lead Industries case, in which the EPA was given a great deal of room to maneuver.121 
There are several advantages to this approach, especially if we consider the institutional role of the courts. A serious problem with intense judicial review of agency action is that it creates delay—and hence ensures a bias in favor of the status quo.122 It is exceptionally easy for a skillful adequate to challenge a national standard as either too high or too low. On the basis of EPA’s own data, an environmental group would have had a quite plausible argument that the regulation of particulates was insufficiently stringent under the statute. In order to allow agencies room to maneuver in the face of scientific uncertainty, it would be reasonable to say that on the basis of minimally plausible evidence, courts should simply uphold the relevant decisions. It is a special virtue of this approach that the Bush Administration would be permitted to come to a different conclusion from the Clinton Administration, and vice-versa, because different judgments of value could lead to different conclusions about how to proceed in the face of ambiguous science. 121 See Lead Industries Association v. EPA, 647 F.2d 1130 (DC Cir 1980). 122 See Jerry Mashaw and David Harsft, The Struggle for Auto Safety (1993). invalidate the ozone regulation while upholding the regulation of particulates. The simple claim here would be that on the evidence given, the new particulates standard was requisite to protect the public health – but the new ozone standard was not, especially if we take into account the health benefits of ground-level ozone.123 We have seen the possibility that all things considered, the regulation would increase rather than reduce, health problems. 
Even if this is overstated, Justice Breyer’s opinion rightly suggests that the EPA is not supposed to remove all risks from the air, or to make the air “riskfree,” and that the EPA should take account of context to compare the risk at hand to risks that people face in ordinary life. It does appear that the statistical risks from low levels of ozone are smaller than the statistical risks that people find acceptable in multiple domains. Particulates are very different on this count. Here the existing hazards do seem high, on a plausible reading of the evidence. By upholding the particulates standard, and asking the EPA to explain itself more thoroughly with respect to ozone, the court would be contributing to the development of a kind of common law of acceptable risks, of the sort that Justice Breyer seemed to be encouraging. 
The approach I am suggesting—upholding the particulates standard as requisite, while invalidating the ozone standard as not requisite—would certainly be reasonable. The principal objection would be institutional; it would involve the special limitations of judicial review. The evidence shows the possibility of nontrivial health gains from the ozone regulation, and in the face of scientific uncertainty, the agency should be permitted to make whatever (reasonable) policy choices it likes. Especially in view of the risk of status quo bias, perhaps the court should refuse to invalidate a judgment like that in the ozone case, even if the judgment seems wrong. 
regulations as arbitrary or as inadequately justified, not because the risks are too low, but because EPA did not explain on what grounds it chose these particular regulations, rather than regulations that would be somewhat more strict or somewhat more lenient. This would be the administrative law analogue of what 123 I am not attempting here to reach definitive conclusions about the scientific data. I am simply suggesting how a reviewing court might reasonably respond to the data that the EPA has compiled. Judge Williams took to be a constitutional requirement in ATA.124 The Supreme Court’s holding that the nondelegation doctrine does not require this form of specificity says nothing about whether such specificity might be required as a matter of administrative law. If the EPA cannot explain, in concrete terms, why it chose the particular levels it chose, how can courts know that the agency’s decision was not arbitrary? 
In doctrinal terms, judicial invalidation on these grounds might take one of two routes. First, the court might say that it cannot know whether the particular level chosen is “requisite to protect the public health” unless it has a clear sense of why the EPA reached that conclusion. Without numbers and criteria, it is impossible to obtain any such sense. Second, the court might say that it cannot tell whether the agency’s action is arbitrary or capricious, within the meaning of the Administrative Procedure Act,125 unless the EPA has given a more detailed explanation of its choice. Either of these conclusions would be relatively conventional, and neither would mark a huge departure from current law. 
But there would be a genuine innovation here. Thus far, courts have not required anything like a quantitative basis for health and safety regulation. Sometimes they have required agencies to show that the costs are not grossly disproportionate to the benefits126; sometimes they have prohibited agencies from acting when it seems as if there are not benefits at all.127 But the relevant decisions have been more qualitative than quantitative, and when they have been quantitative, the overall judgment has seemed overwhelmingly clear.128 It would be a significant step from these decisions to a holding that agencies must quantify the effects of pollutants at various levels, so as to explain, in specific terms, why one level was chosen rather than another. 
Would the step be worthwhile? While I cannot discuss the issue in detail, I believe that it would be, at least for the ozone rule, where the evidence of harm is relatively thin. 129 Such a ruling would not impose an unacceptable informational burden on EPA. In fact EPA routinely gathers enough information to provide the necessary explanation. At the same time, such a requirement would provide a 124 See supra. 125 5 USC 706. 126 Corrosion Proof Fittings v. EPA, 947 F2d 1201 (5th Cir 1991). 127 See Chemical Manufacturers Assn. v. EPA, 217 F.3d 861 (DC Cir 2000). 128 See Corrosion Proof Fittings, at 1205-1211. 129 For a detailed discussion, see Sunstein, supra note. useful spur to theagency, one that would also produce a higher degree of rationality andcoherence. The result would be to show when, and why, environmental groupsor industry would be able to mount a successful challenge to an ambient airquality standard. 
In sum: I believe that on remand, the court should uphold the particulates standard, on the ground that on a reasonable view of the evidence, the agency had sufficient basis to conclude that that standard was “requisite to protect the public health,” even without an attempt to quantify. At the same time, the court should remand the ozone rule, on the ground that the EPA has not given an adequate explanation of why that rule is “requisite.” The court should encourage the EPA to be as quantitative as possible. And on remand, the EPA should take up the invitation, attempting in the process to give a clear sense, for the first time, of why it has chosen one regulatory “point” rather than another. But my principal goal has not been to urge any particular result, which will obviously turn on close engagement with the record. I have attempted instead to give a sense of the arguments that are available, post-ATA, and a general sense of the grounds on which one might choose among them. 
The Court’s principal rulings in ATA represented a return to normalcy – a rejection of some imaginative suggestions about how to read both the Clean Air Act and the Constitution. The Court established, quite correctly under existing law, that national standards should be set without regard to cost, and that the nondelegation doctrine has a small place in constitutional doctrine – or perhaps no place at all. 
At the same time, the Court’s unambitious, lawyerly opinion leaves a number of questions unresolved. There is some ambiguity in the case, but the opinion is best taken not to question, but on the contrary to endorse, the costbenefit default principles developed by lower courts. And while the Court showed no interest in the nondelegation doctrine, its rejection of the approach of the lower court now makes it impossible to invoke an agency’s narrowing construction to support an otherwise objectionable delegation. For that reason, the nondelegation doctrine is not quite dead. I have attempted to show that the little life that remains in the nondelegation doctrine might well support a constitutional attack on the Occupational Safety and Health Act. 
ATA also leaves open a number of challenges to national ambient air quality standards, including the very standards at issue in the case. The principal challenges would involve the language of the relevant provision of the CAA (“requisite”) and the arbitrary or capricious standard of the APA. On the basis of the evidence before the agency, it would make sense for a court to uphold the particulates standard as having been reasonably judged “requisite” while also invalidating the ozone standard as not shown to qualify as such. For the EPA itself, it would certainly make sense to move in the direction of greater quantification, in which national standards are issued only after an effort to specify the expected benefits, to compare them with the expected benefits of alternatives, and in that way to produce clear standards for choosing appropriate levels of ambient air quality. This is essentially an administrative task, not one for the courts. But it would be entirely appropriate for courts to spur regulatory agencies in this direction. Justice Breyer wrote only for himself, and his pragmatic, consequence-centered concurring opinion attracted no additional justices; but I believe that it will exert an enduring influence on the law of risk regulation. If so, the ATA decision will stand not only as a responsible resolution of the principal questions in the case, but also as a modest step toward a more sensible system of environmental protection. 
William M. Landes, Copyright Protection of Letters, Diaries and Other Unpublished Works: An Economic Approach (July 1991). 
Richard A. Epstein, The Path to The T. J. Hooper: The Theory and History of Custom in the Law of Tort (August 1991). 
Cass R. Sunstein, On Property and Constitutionalism (September 1991). Richard A. Posner, Blackmail, Privacy, and Freedom of Contract (February 1992). Randal C. Picker, Security Interests, Misbehavior, and Common Pools (February 1992). 
Tomas J. Philipson & Richard A. Posner, Optimal Regulation of AIDS (April 1992). Douglas G. Baird, Revisiting Auctions in Chapter 11 (April 1992). 
William M. Landes, Sequential versus Unitary Trials: An Economic Analysis (July 1992). 
William M. Landes & Richard A. Posner, The Influence of Economics on Law: A Quantitative Study (August 1992). 
Alan O. Sykes, The Welfare Economics of Immigration Law: A Theoretical Survey With An Analysis of U.S. Policy (September 1992). 
Douglas G. Baird, 1992 Katz Lecture: Reconstructing Contracts (November 1992). Gary S. Becker, The Economic Way of Looking at Life (January 1993). 
J. Mark Ramseyer, Credibly Committing to Efficiency Wages: Cotton Spinning Cartels in Imperial Japan (March 1993). 
Cass R. Sunstein, Endogenous Preferences, Environmental Law (April 1993). Richard A. Posner, What Do Judges and Justices Maximize? (The Same Thing Everyone Else Does) (April 1993). 
Lucian Arye Bebchuk and Randal C. Picker, Bankruptcy Rules, Managerial Entrenchment, and Firm-Specific Human Capital (August 1993). 
J. Mark Ramseyer, Explicit Reasons for Implicit Contracts: The Legal Logic to the Japanese Main Bank System (August 1993). 
William M. Landes and Richard A. Posner, The Economics of Anticipatory Adjudication (September 1993). 
Kenneth W. Dam, The Economic Underpinnings of Patent Law (September 1993). Alan O. Sykes, An Introduction to Regression Analysis (October 1993). 
Richard A. Epstein, The Ubiquity of the Benefit Principle (March 1994). Randal C. Picker, An Introduction to Game Theory and the Law (June 1994). William M. Landes, Counterclaims: An Economic Analysis (June 1994). J. Mark Ramseyer, The Market for Children: Evidence from Early Modern Japan (August 1994). 
Robert H. Gertner and Geoffrey P. Miller, Settlement Escrows (August 1994). Kenneth W. Dam, Some Economic Considerations in the Intellectual Property Protection of Software (August 1994). 
Cass R. Sunstein, Rules and Rulelessness, (October 1994). 
David Friedman, More Justice for Less Money: A Step Beyond Cimino (December 1994). 
Daniel Shaviro, Budget Deficits and the Intergenerational Distribution of Lifetime Consumption (January 1995). 
Douglas G. Baird, The Law and Economics of Contract Damages (February 1995). Daniel Kessler, Thomas Meites, and Geoffrey P. Miller, Explaining Deviations from the Fifty Percent Rule: A Multimodal Approach to the Selection of Cases for Litigation (March 1995). 
Geoffrey P. Miller, Das Kapital: Solvency Regulation of the American Business Enterprise (April 1995). 
Richard Craswell, Freedom of Contract (August 1995). 
J. Mark Ramseyer, Public Choice (November 1995). 
Kenneth W. Dam, Intellectual Property in an Age of Software and Biotechnology (November 1995). 
Cass R. Sunstein, Social Norms and Social Roles (January 1996). 
J. Mark Ramseyer and Eric B. Rasmusen, Judicial Independence in Civil Law Regimes: Econometrics from Japan (January 1996). 
Richard A. Epstein, Transaction Costs and Property Rights: Or Do Good Fences Make Good Neighbors? (March 1996). 
Cass R. Sunstein, The Cost-Benefit State (May 1996). 
William M. Landes and Richard A. Posner, The Economics of Legal Disputes Over the Ownership of Works of Art and Other Collectibles (July 1996). John R. Lott, Jr. and David B. Mustard, Crime, Deterrence, and Right-to-Carry Concealed Handguns (August 1996). 
Cass R. Sunstein, Health-Health Tradeoffs (September 1996). 
G. Baird, The Hidden Virtues of Chapter 11: An Overview of the Law and Economics of Financially Distressed Firms (March 1997). 
Richard A. Posner, Community, Wealth, and Equality (March 1997). 
William M. Landes, The Art of Law and Economics: An Autobiographical Essay (March 1997). 
Cass R. Sunstein, Behavioral Analysis of Law (April 1997). 
John R. Lott, Jr. and Kermit Daniel, Term Limits and Electoral Competitiveness: Evidence from California’s State Legislative Races (May 1997). 
Randal C. Picker, Simple Games in a Complex World: A Generative Approach to the Adoption of Norms (June 1997). 
Richard A. Epstein, Contracts Small and Contracts Large: Contract Law through the Lens of Laissez-Faire (August 1997). 
Cass R. Sunstein, Daniel Kahneman, and David Schkade, Assessing Punitive Damages (with Notes on Cognition and Valuation in Law) (December 1997). William M. Landes, Lawrence Lessig, and Michael E. Solimine, Judicial Influence: A Citation Analysis of Federal Courts of Appeals Judges (January 1998). John R. Lott, Jr., A Simple Explanation for Why Campaign Expenditures are Increasing: The Government is Getting Bigger (February 1998). 
Lisa Bernstein, The Questionable Empirical Basis of Article 2’s Incorporation Strategy: A Preliminary Study (May 1999) Richard A. Epstein, Deconstructing Privacy: and Putting It Back Together Again (May 1999) William M. Landes, Winning the Art Lottery: The Economic Returns to the Ganz Collection (May 1999) Cass R. Sunstein, David Schkade, and Daniel Kahneman, Do People Want Optimal Deterrence? (June 1999) Tomas J. Philipson and Richard A. Posner, The Long-Run Growth in Obesity as a Function of Technological Change (June 1999) David A. Weisbach, Ironing Out the Flat Tax (August 1999) Eric A. Posner, A Theory of Contract Law under Conditions of Radical Judicial Error (August 1999) David Schkade, Cass R. Sunstein, and Daniel Kahneman, Are Juries Less Erratic than Individuals? Deliberation, Polarization, and Punitive Damages (September 1999) Cass R. Sunstein, Nondelegation Canons (September 1999) Richard A. Posner, The Theory and Practice of Citations Analysis, with Special Reference to Law and Economics (September 1999) Randal C. Picker, Regulating Network Industries: A Look at Intel (October 1999) Cass R. Sunstein, Cognition and Cost-Benefit Analysis (October 1999) Douglas G. Baird and Edward R. Morrison, Optimal Timing and Legal Decisionmaking: The Case of the Liquidation Decision in Bankruptcy (October 1999) Gertrud M. Fremling and Richard A. Posner, Market Signaling of Personal Characteristics (November 1999) Matthew D. Adler and Eric A. Posner, Implementing Cost-Benefit Analysis When Preferences Are Distorted (November 1999) Richard A. Posner, Orwell versus Huxley: Economics, Technology, Privacy, and Satire (November 1999) David A. Weisbach, Should the Tax Law Require Current Accrual of Interest on Derivative Financial Instruments? (December 1999) Cass R. Sunstein, The Law of Group Polarization (December 1999) Eric A. Posner, Agency Models in Law and Economics (January 2000) Karen Eggleston, Eric A. Posner, and Richard Zeckhauser, Simplicity and Complexity in Contracts (January 2000) Douglas G. Baird and Robert K. Rasmussen, Boyd’s Legacy and Blackstone’s Ghost (February 2000) David Schkade, Cass R. Sunstein, Daniel Kahneman, Deliberating about Dollars: The Severity Shift (February 2000) Richard A. Posner and Eric B. Rasmusen, Creating and Enforcing Norms, with Special Reference to Sanctions (March 2000) 2000) 2000) 2000) 
Position (August 2000) 
Internet (November 2000) 
System (November 2000) 
Relations: A Rational Choice Perspective (November 2000) 2000) 
Liability, Class Actions and the Patient’s Bill of Rights (December 2000) 
Economic Approach (December 2000) (January 2001) 
Finance (February 2001) (March 2001) 
Political Theory Perspective (April 2001) 
Age (April 2001) the Conceptual Foundations of Corporate Reorganization (April 2001) 
William M. Landes, What Has the Visual Arts Rights Act of 1990 Accomplished? (May 2001) Cass R. Sunstein, Social and Economic Rights? Lessons from South Africa (May 2001) Christopher Avery, Christine Jolls, Richard A. Posner, and Alvin E. Roth, The Market for Federal Judicial Law Clerks (June 2001) Douglas G. Baird and Edward R. Morrison, Bankruptcy Decision Making (June 2001) Cass R. Sunstein, Regulating Risks after ATA (June 2001) 
Follow this and additional works at: https://chicagounbound.uchicago.edu/ public_law_and_legal_theory Part of the Law Commons Chicago Unbound includes both works in progress and final versions of articles. Please be aware that a more recent version of this article may be available on Chicago Unbound, SSRN or elsewhere. 
Cass R. Sunstein 
This paper can be downloaded without charge at: The Social Science Research Network Electronic Paper Collection: http://papers.ssrn.com/paper.taf?abstract_id=296657 Preliminary draft 3/7/01 All rights reserved 
Cass R. Sunstein* 
Here is one of the central differences between late eighteenth century constitutions and late twentieth century constitutions: The former make no mention of rights to food, shelter, and health care, whereas the latter tend to protect those rights in the most explicit terms. A remarkable feature of international opinion – firmly rejected in the United States – is that socio-economic rights deserve constitutional protection. 
But should a democratic constitution really protect the right to food, shelter, and medical care? Do “socio-economic” rights of this sort belong in a Constitution? What do they have to do with citizenship? Do they promote or undermine democratic deliberation? If such rights are created, what is the role of the courts? 
My aim in this Essay is to shed light on these questions, largely by discussing an extraordinary decision by the Constitutional Court of South Africa, one that carries some significant lessons for the future.1 In the Grootboom decision, the Court set out a novel and promising approach to judicial protection of socio-economic rights. This approach requires close attention to the human interests at stake, and sensible priority-setting, but without mandating protection for each person whose socio-economic needs are at risk. The distinctive virtue of the Court’s approach is that it is respectful of democratic prerogatives and of the limited nature of public resources, while also requiring special deliberative attention to those whose minimal needs are not being met. The approach of the Constitutional Court stands as a powerful rejoinder to those who have contended that socio-economic rights do not belong in a constitution. It suggests that such rights can serve, not to preempt democratic deliberation, but to ensure democratic attention to important interests that might otherwise be neglected in ordinary debate. It also illuminates the idea, emphasized by the Court itself, that all rights, including the most conventional and uncontroversial, impose costs that must be borne by taxpayers. 
To be sure, it is far too early to say whether the Court’s approach can fully accommodate the concerns of those who object to judicial protection of socio-economic rights. But for the first time in the history of the world, a constitutional court has initiated a process that might well succeed in the endeavor of ensuring that protection without placing courts in an unacceptable managerial role. This point has large implications for how we think about citizenship, democracy, and minimal social and economic needs. * Karl N. Llewellyn Distinguished Service Professor of Jurisprudence, University of Chicago, Law School and Department of Political Science. A different version of this essay will appear as a chapter in Cass R. Sunstein, Designing Democracy: What Constitutions Do (forthcoming 2001). 1 Government of the Republic of South Africa v. Grootboom, 11 BCLR 1169 (CC) (2000). 
A. A Debate and A Resolution economic rights, sometimes known as socio-economic rights, belong in a constitution.2 The debate has occurred with special intensity in both Eastern Europe and South Africa. Of course the American Constitution, and most constitutions before the twentieth-century, protected such rights as free speech, religious liberty, and sanctity of the home, without creating rights to minimally decent conditions of life. But in the late twentieth century, the trend is otherwise, with international documents, and most constitutions, creating rights to food, shelter, and more. 
Some skeptics have doubted whether such rights make sense from the standpoint of constitutional design. On one view, a constitution should protect “negative” rights, not “positive” rights. Constitutional rights should be seen as individual protections against the aggressive state, not as private entitlements to protection by the state. For people who share this view, a constitution is best understood as a bulwark of liberty, properly conceived; and a constitution that protects “positive” rights can be no such bulwark, because it requires government action, rather than creating a wall of immunity around individual citizens. 
But there are many problems with this view. Even conventional individual rights, like the right to free speech and private property, require governmental action. Private property cannot exist without a governmental apparatus, ready and able to secure people’s holdings as such. So-called negative rights are emphatically positive rights. In fact all rights, even the most conventional, have costs.3 Rights of property and contract, as well as rights of free speech and religious liberty, need significant taxpayer support. In any case we might well think that the abusive or oppressive exercise of government power consists, not only in locking people up against their will, or in stopping them from speaking, but also in producing a situation in which people’s minimal needs are not met. Indeed, protection of such needs might be seen as part of the necessary wall of immunity, and hardly as inconsistent with it. 
If the central concerns are citizenship and democracy, the line between negative rights and positive rights is hard to maintain. The right to constitutional protection of private property has a strong democratic justification: If people’s holdings are subject to ongoing governmental adjustment, people cannot have the security, and independence, that the status of citizenship requires. The right to private property should not be seen as an effort to protect wealthy people; it helps ensure deliberative democracy itself. But the same things can be said for minimal protections against starvation, homelessless, and other extreme deprivation. For people to be able to act as citizens, and to be able to count themselves as such, they must have the kind of independence that such minimal protections ensure. 
On the other hand, a democratic constitution does not protect every right and interest that should be protected in a decent or just society. Perhaps ordinary politics can be trusted; if so, there is no need for constitutional protection. The basic reason for constitutional guarantees is to respond to problems faced in ordinary political life. If minimal socio-economic rights will be protected democratically, why involve the Constitution? The best answer is that to doubt the assumption and to insist such rights are indeed at systematic risk in political life, especially because those who would benefit from them lack political power. It is not clear if this is true in every nation. But certainly it is true in many places. 
Perhaps more interestingly, critics of socio-economic rights have made a point about democratic institutions. In particular, they have argued that socio-economic rights are beyond judicial capacities.4 On this view, courts lack the tools to enforce such guarantees. If they attempt to do so, they will find themselves in an impossible managerial position, one that might discredit the constitutional enterprise as a whole. How can courts possibly oversee budget-setting priorities? If a state provides too little help to those who seek housing, maybe it is because the state is concentrating on the provision of employment, or on public health programs, or on educating children. Is a court supposed to oversee the full range of government programs, to ensure that the state is placing emphasis on the right areas? How can a court possibly acquire the knowledge, or make the value judgments, that would enable it to do that? There is a separate point. A judicial effort to protect socio-economic rights might seem to compromise, or to preempt, democratic deliberation on crucial issues, because it will undermine the capacity of citizens to choose, in accordance with their own judgments, the kinds of welfare and employment programs that they favor. Of course some of these points hold for conventional rights as well. But perhaps social and economic rights are especially troublesome on this count, because they put courts in the position of overseeing largescale bureaucratic institutions. 
It would be possible to respond to these institutional concerns in various ways. Perhaps constitutions should not include socio-economic rights at all. Perhaps such rights should be included, but on the explicit understanding that the legislature, and not the courts, will be entrusted with enforcement. Section IV of the Indian Constitution expressly follows this route, contained judicially unenforceable “directive principles” and attempting to encourage legislative attention to these rights without involving the judiciary.. [Could you mention the section(s) in the Indian Constitution?] The advantage of this approach is that it ensures that courts will not be entangled with administration of social programs. The disadvantage is that without judicial enforcement, there is a risk that the constitutional guarantees will be mere “parchment barriers,” meaningless or empty in the real world. 4 See Davis, The Case Against Inclusion of Socio-economic Rights in a Bill of Rights Except as Directive Principles, 8 SAJHR 475 (1992). was intensely debated before ratification of the South African Constitution.5 The idea of including socio-economic rights was greatly spurred by international law, above all by the International Covenant on Economic, Social and Cultural Rights, to which I will return. Much of the debate involved the appropriate role of the judiciary. In part this was a relatively abstract debate, posing a concrete real-world issue but founded on a set of theoretical considerations just sketched, involving judicial capacities and the proper place, if any, of socio-economic rights in a democratic constitution. But aside from these points, the debate was greatly influenced by the particular legacy of apartheid and by claims about what to do about that legacy at the constitutional level. In the view of many of those involved in constitutional design, the apartheid system could not plausibly be separated from the problem of persistent social and economic deprivation. In the end the argument for socio-economic rights was irresistible, in large part because such guarantees seemed an indispensable way of expressing a commitment to overcome the legacy of apartheid – the overriding goal of the new Constitution. 
We should emphasize a general point here about constitutionalism.6 Some constitutions are preservative; they seek to maintain existing practices, to ensure that things do not get worse. This is of course Edmund Burke’s conception of the English constitution. By contrast, some constitutions are transformative; they set out certain aspirations that are emphatically understood as a challenge to longstanding practices. They are defined in opposition to those practices. The American constitution is a mixture of preservative and transformative features, with some provisions looking backward, and others very much looking forward. The South African constitution is the world’s leading example of a transformative constitution. A great deal of the document is an effort to eliminate apartheid “root and branch.” Constitutions are often described as precommitment strategies, designed to ensure against myopic or mistaken decisions in ordinary politics.7 If is it apt to describe the South African Constitution in these terms, this is because the document is designed to ensure that future governments do not fall prey to anything like the evils of the apartheid era. The creation of socio-economic rights is best understood in this light. 
But what, in particular, is the relationship among socio-economic rights, courts, and legislatures? The South African Constitution hardly speaks unambiguously on this topic. The rights in question typically take the following form, in an evident acknowledgement of limited resources: available resources, to achieve the progressive realisation of this right. 5 See note 1 supra; see also Chaskalson et al., Constitutional Law of South Africa 41-3-41-4 (2000). 6 See Lawrence Lessig, Code and Other Laws of Cyberspace (1999). 7 See Stephen Holmes, Passions and Constraint (1995). 
This is the basic form of constitutional rights to “an environment that is not harmful to their health or well-being” (section 24); housing (section 26); and health, food, water, and social security (section 27). 
A provision of this kind does not clearly create or disable judicial enforcement. On the basis of the text alone, it would be easy to imagine a judicial ruling to the effect that enforcement is reserved to nonjudicial actors within “the state.” On this view, the South African Constitution is, with respect to judicial enforcement, closely akin to the Indian Constitution. But it would also be easy to imagine a ruling to the effect that courts are required to police the relevant rights, by ensuring that the state has, in fact, taken “reasonable legislative and other measures, within its available resources, to achieve progressive realisation of this right.” If, for example, the state has done little to provide people with decent food and health care, and if the state is financially able to do much more, it would seem that the state has violated the constitutional guarantee. 
In certifying the Constitution, the South African Constitutional Court resolved this question in just this way, concluding that socio-economic rights are indeed subject to judicial enforcement.8 The Court said that such rights “are, at least to some extent, justiciable.” The fact that resources would have to be expended on them was hardly decisive, for this was true of “many of the civil and political rights entrenched” in the Constitution. The Court correctly said that many rights, including so-called negative rights, “will give rise to similar budgetary implications without compromising their justiciability.” But in a final sentence, the Court added new ambiguity, by suggesting that at “the very minimum, socio-economic rights can be protected negatively from improper invasion.” This last sentence added considerable ambiguity, because it did not say whether and when courts could go beyond the “minimum” to protect rights “positively”; nor did it make entirely clear what it would mean to invade socio-economic rights “negatively.” Perhaps the Court’s suggestion was that when the state, or someone else, actually deprived someone of (for example) shelter, say by evicting them form the only available source of housing, judicial enforcement would be appropriate. But if this is what the Court meant, the socio-economic rights would be hardly justiciable at all; this would be an exceedingly narrow use of judicial authority in overseeing the relevant rights. 
rights carries both particular and general interest. It is of particular interest in South Africa, where a substantial percentage of the population lives in desperate poverty. Does the Constitution do anything to help them? For example, might the judiciary play a role in ensuring that governmental priorities are set in the way that the Constitution apparently envisages? Or might judicial involvement in protecting socio-economic rights actually impair reasonable legislature efforts to set sensible priorities? The outcome has general interest because it should tell us a great deal about the social and democratic consequences, both good and bad, of constitutional provisions creating socio-economic rights. Thus far discussion of this issue has both highly speculative and uninformed by 8 Ex Parte Chairperson of the Constittuonal Assembly, 1996 ( para 78. actual practice.9 The South African experience will inevitably provide a great deal of information. 
The Constitutional Court has now rendered its first major decision involving these rights, in a case involving the right to shelter. It is to that case that I now turn. 
It is impossible to understand the South African dispute over the right to shelter, or the proceedings in the Constitutional Court, without reference to the effects of apartheid. The central point is that in the view of most observers, the system of apartheid is directly responsible for the acute housing shortage in many areas of the nation. 
One of the central components of apartheid was a system of “influx control” that sharply limited African occupation of urban areas.10 In the Western Cape, the government attempted to exclude all African people and to give preference to the colored community. The result was to freeze the provision of housing for African people on the Cape Peninsula in 1962. Nonetheless, African people continued to move into the area in search of jobs. Lacking formal housing, large numbers of them moved into “informal settlements,” consisting of shacks and the like, throughout the Peninsula. The inevitable result of the combination of large African movements into urban areas and inadequate provision of housing was to produce shortages, amounting to over 100,000 units by the mid-1990s. Since that time, governments at national and local levels have enacted a great deal of legislation to try to handle the problem. Nonetheless, many thousands of people lack decent housing. At the same time, the South African government has limited sources and a large variety of needs, stemming from the AIDS crisis, pervasive unemployment (about 40%), and persistent, pervasive poverty. 
The Grootboom case was brought by 900 plaintiffs, of whom 510 were children. For a long period, the plaintiffs lived in an informal squatter settlement named Wallacedene. Most of the people there were desperately poor. All of them lived in shacks, without water, sewage, or refuse removal services. Only 5% of the shacks had electricity. The named plaintiff, Irene Grootboom, lived with her family and that of her sister in a shack of about twenty square meters. 
Many of those at the Wallacedene settlement had applied for low-cost housing from the municipality. They were placed on the waiting list, where they remained for a number of years. In late 1998, they became frustrated by the intolerable conditions at Wallacedene. They moved out and put up shacks and shelters on vacant land that was 9 An exception is Hungary. See Andras Sajo, How the Rule of Law Killed Hungarian Welfare Reform, 5 Eastern European Constitutional Review 31 (1996). Here we’ll obviously need the full reference 10 I draw here from the Grootboom opinion. privately owned and earmarked for formal low-cost housing. A few months later, the owner obtained an ejectment order against them. But Grootboom and others refused to leave, contending that their former sites were now occupied and that there was nowhere else to go. Eventually they were forcibly evicted, with their homes burnt and bulldozed. Their possessions were destroyed. At this point they found shelter on a sports field in Wallacedene, under temporary structures consisting of plastic sheets. It was at this stage that they contended that their constitutional rights had been violated. It is worthwhile to pause over the nature of human existence for those at Wallacedene. For them, insecurity was a fact of daily life. It should not be controversial to say that the status of citizenship is badly compromised for people in such conditions. 
26, which provides: “26( 
available resources, to achieve the progressive realisation of this right 
without an order of court made after considering all the relevant circumstances . No legislation may permit arbitrary evictions.” The second was section 28( “28 Every child has the right – . . . (b) to family care or parental care, or to appropriate alternative care when removed from the family environment: (c) to basic nutrition, shelter, basic health care services and social services.” At the outset several points should be made about these sections. First, section 26( imposes a duty on the private sector, not only on government. Under this section, it is unconstitutional for a private person to evict another private person, or to demolish a home, without judicial permission. From the constitutional point of view, this is a striking innovation, for constitutions do not typically impose obligations on private landlords. From the standpoint of economic policy, it also raises several interesting questions. Obviously the goal of section 26( housing; but the creation of a kind of property right in continued occupancy is likely to have some unintended bad consequences. If it is difficult to evict people, landlords will have a decreased incentive to provide housing in the first instance. The result might be a diminished stock of private housing. Another result might be extensive private screening of prospective tenants, since landlords will be entirely aware that once a tenancy is allowed, it will be very difficult to terminate it. The extent of these effects is of course an empirical question. 
Do the following pages confuse sections 26 and 28? I indicated all the specific spots where I think it should be changed. THANKS! 
For purposes of constitutional interpretation, the largest puzzle has to do with the relationship between sections 26 and 28. It would be possible to read section 28 as giving children unqualified rights to various goods – ensuring that children have those goods even if resources are scarce. On this view, the government has an absolute obligation to ensure that children eat, are housed, and have health care and social services. Under this interpretation, section 26 creates as qualified right for everyone (“progressive realisation”) whereas section 28 requires an unqualified right for children in particular. Whether or not it is correct, this is a textually plausible reading. 
The lower court proceeded in exactly this way, holding that section 28 creates a freestanding, absolute right, on the part of children, to the protections thus mentioned. On this interpretation, the rights are not qualified by “available resources” or by the “progressive realisation” clause. Perhaps children are given, by that clause, two sets of rights: first to the care of adults, preferably parents; second to state support of basic needs. 
In Grootboom, the Constitutional Court rejected this interpretation of section 28 . At the same time, it held that section 26 imposes a judicially enforceable duty on government; that what is required in “reasonableness”; and that the plaintiffs’ constitutional rights had been violated, because of the absence of a program to ensure provide “temporary relief” for those without shelter. In short, the Court held that the Constitution required not only a long-term plan to provide low-income shelter, but also a system to ensure short-term help for people who had no place to live. I believe that this is the first time that the high Court of any nation has issued a ruling of this general sort. What is most striking about that ruling is the distinctive and novel approach to socioeconomic rights, requiring not shelter for everyone, but sensible priority-setting, with particular attention to the plight of those who are neediest. I will say more by way of evaluation below; let us begin by tracing the Court’s explanation of its decision. 
understood without reference to international law, which firmly recognizes such rights, and which seems to put the weight of international opinion behind them. Hence the Court began by emphasizing the significant background provided by the International Covenant on Economic, Social, and Cultural Rights (a covenant signed but not yet ratified by South Africa). 
Section 11.1 of the Covenant provides that the parties “recognize the right of everyone to an adequate standard of living for himself and his family, including adequate food, clothing and housing, and to the continuous improvement of living conditions.” Hence the “parties will take appropriate steps to ensure the realization of this right . . .” A more general provision of the Covenant, applicable to all relevant rights, makes a promise “to take steps . . . to the maximum of its available resources, with a view to achieving progressively the full realisation of the rights recognized in the Covenant by all appropriate means, including particularly the adoption of legislative measures.”11 
But what does this mean? The United Nations Committee on Economic, Social and Cultural Rights is entrusted with monitoring the performance of states under the Covenant. In its interpretive comments, the Committee urges that states face a “minimum core obligation,” consisting of a duty to “ensure the satisfaction of, at the very least, minimum essential levels of each of the rights.” The Constitutional Court referred to this idea with some interest, suggesting the possibility of “minimum core obligations” imposed by section 26. But in the Court’s view, that idea had many problems, because judicial enforcement would require a great deal of information to be placed before the court, in order to “determine the minimum core in any given context.” In this case, sufficient information was lacking, and in any event the Court thought that it would not be necessary to define the minimum core in order to assess Grootboom’s complaint. 
emphasis on the fact that all people have a right, not to shelter regardless of financial constraints, but to legislative and other measures designed to achieve “the progressive realization of this right.” At the same time, the state, and “all other entities and persons,” are constitutionally required “to desist from preventing or impairing the right of access to adequate housing.” By itself this idea is quite ambiguous; what counts as prevention or impairment? 
The Court explained that to implement the right, the state faced two kinds of duties. With respect to “those who can afford to pay for adequate housing,” the state’s duty is to “unlock[] the system, providing access to housing stock and a legislative framework to facilitate self-built houses through planning laws and access to finance.” What is most striking here is the Court’s emphasis on the “unlocking” role of the Constitution. On one interpretation, at least, the state is under a duty to ban a system of monopoly in housing – to create market s sufficiently flexible to provide housing to those who can pay for it. But it is not clear that this is all, or even most, of what the Court had in mind. The idea of “planning laws” and “access to finance” might be taken to mean something other than, or in addition to, a competitive housing market. But it is certainly worth noticing that the analysis of a duty for “those who can afford to pay”operates along its own separate track, requiring a kind of open housing market for those with the resources to participate. 
For poor people, of course, the state’s obligation is different. Here the constitutional duty might be discharged through “programmes to provide adequate social assistance to those who are otherwise unable to support themselves and their dependents.” In this case, the central issue was whether the government had created “reasonable” measures to ensure progressive realization of the right. The Court concluded that it had not, notwithstanding the extensive public apparatus to facilitate access to housing. The reason for this conclusion was simple: “there is no express provision to facilitate access to temporary relief for people who have no access to land, no roof over their heads, for people who are living in intolerable conditions and for people who are in crisis because of natural disasters such as floods or fires, or because their homes are under threat of demolition.” 
The Court acknowledged that it would be acceptable not to have a provision for those in desperate need “if the nationwide housing programme would result in affordable houses for most people within a reasonably short time.” Note that “most people” does not mean all people; hence the clear implication is that a deprivation of housing, for some, would not necessarily be unreasonable or inconsistent with the constitutional plan. In this respect, the constitutional right involved the creation of a system of a certain kind rather than the creation of fully individual protections. But under the existing governmental program at the national level, it could not be said that “most people” would have “affordable houses” within a reasonably short time. Hence the nation’s housing program is constitutionally unacceptable insofar as “it fails to recognize that the state must provide for relief for those in desperate need. . . . It is essential that a reasonable part of the national housing budget be devoted to this, but the precise allocation is for the national government to decide in the first instance.” 
The Court also acknowledged that the constitutional obligation might be adequately carried out at the local level and that the local government, Cape Metro, had put in place its own land programme specifically to deal with desperate needs. But that programme had not been implemented, in large part because of an absence of adequate budgetary support from the national government. “Recognition of such needs in the nationwide housing programme requires” the national government “to plan, budget and monitor the fulfilment of immediate needs and the management of crises. This shall ensure that a significant number of desperate people in need are afforded relief, though not all of them need receive it immediately.” 
In the Court’s view, the Constitution did not create a right to “shelter or housing immediately upon demand.” But it did create a right to a “coherent, co-ordinated programme designed to meet” constitutional obligations. The obligation of the state was therefore to create such a program, including reasonable measures specifically designed “to provide relief for people who have no access to land, no roof over their heads, and who are living in intolerable conditions or crisis situations.” It is here that we can find a novel, distinctive, and promising approach to a democratic constitution’s socio-economic rights, an issue that I take up in more detail below. 
So much for Section 26 . What of Section 28 , which, it might be recalled, was understood by the lower court to create an absolute right to shelter for children? In brief, the Court refused to interpret section 28 in this way. Instead it understood section 28 to add little to the basic requirements of section 26?. In the Court's view, section 28 creates no independent socio-economic rights. This was an exceedingly narrow reading of section 28 , evidently a product of pragmatic considerations. The Court’s responsiveness to those pragmatic considerations is itself noteworthy, especially insofar as it suggests judicial reluctance to intrude excessively into priority-setting at the democratic level. 
The Court's central holding was that with respect to children, the obligation to provide shelter and the like "is imposed primarily on parents and family, and only alternatively on the state." What this means is that when children are removed from their parents, the state must protect the specified rights by, for example, ensuring that children are housed and fed. But section 28 "does not create any primary state obligation to provide shelter on demand to parents and their children if children are being cared for by their parents or families." 
their parents and families. The state "must provide the legal and administrative infrastructure necessary to ensure" compliance with section 28, through, for example, "passing laws and creating enforcement mechanisms for the maintenance of children, their protection from maltreatment, abuse, neglect or degradation." The state is also obliged to comply with the various independent protections of socio-economic rights. But section 26 created no freestanding obligation for the state to shelter children within the care of their parents. Since the children in Grootboom were being cared for by their parents, the state was not obliged to shelter them "in terms of section 28( 
the text of the provision. Apparently the Court was led to that reading by what it saw as the "anomalous result" of giving those with children "a direct and enforceable right to housing" under that section, while depriving those "who have none or whose children are adult." This would be anomalous because it would allow parents to have special access to shelter if and because they had children. In any case a holding to this effect would make children into "stepping stones to housing for their parents." But would this really be so anomalous? It might seem to make sense to say that children should have a particular priority here – that their right should be more absolute – and hence that adults with children would have a preferred position. Why would that view be especially peculiar? 
The Court also expressed a stronger concern. If children were taken to have an absolute right to shelter, the document's limitations on socio-economic rights would be quite undone. The "carefully constructed constitutional scheme for progressive realisation of socio-economic rights would make little sense if it could be trumped in every case by the rights of children." Here, I think, is the heart of the Court’s skepticism about the idea that section 28 should be taken to create absolute rights. If section 28 were so understood, it would trump even reasonable priority-setting, thus disallowing the state from deciding that in view of sharply limited resources, certain needs were even more pressing. 
What I will urge here is that the approach of the South African Constitutional Court answers a number of questions about the proper relationship among socioeconomic rights, constitutional law, and democratic deliberation. There should be little question that people who live in desperate conditions cannot live good lives. People who live in such conditions are also unable to enjoy the status of citizenship. 
On the other hand, legislatures in poor nations, and perhaps in less poor ones, cannot easily ensure that everyone lives in decent conditions. An especially plausible concern with socio-economic rights is the difficulty, for courts, of steering a middle course between two straightforward positions: (a) the socio-economic rights are nonjusticiable and (b) the socio-economic rights create an absolute duty, on government's part, to ensure protection for everyone who needs them. The second position is of course the standard approach to most constitutional rights. If the government has violated someone’s right to free speech, or to freedom of religion, it does not matter that the rights of most people, or almost everyone else, have been respected. This sentence needs completion. 
As I have emphasized, however, all rights have costs.12 The right to free speech, or for that matter to freedom from police abuse, will not be protected unless taxpayers are willing to fund a judicial system willing and able to protect that right and that freedom. In fact a system committed to free speech is also likely to require taxpayer resources to be devoted to keeping open certain arenas where speech can occur, such as streets and parks. In protecting the most conventional rights, the government must engage in some form of priority-setting. But when cases go to court, conventional rights are and can be fully protected at the individual level, and not merely through the creation of some kind of “reasonable” overall system for protection. The existence of a reasonable overall system for protecting free speech rights is not defense to a claim that, in a particular case, a right to free speech has been violated. 
By their very nature, socio-economic rights are different on this count, certainly in the light of the “progressive realisation” clause. No one thinks that every individual [Should it be “everyone” or “every individual”?] has an enforceable right to full protection of the interests at stake. In these circumstances it is difficult indeed to find an approach that avoids two unappealing courses: creation of fully enforceable individual rights or a conclusion of complete nonjusticiability. The only alternative to these extremes is an approach to public law that is generally unfamiliar in constitutional law but that is the ordinary material of administrative law, governing judicial control of administrative agencies: a requirement of reasoned judgment, including reasonable priority-setting. 
In a typical administrative law case, an agency is faced with a burden of explanation. It must show why it has adopted the program that it has chosen; it must account for its failure to adopt a program of a different sort. For courts, a special attraction of this position is that it protects against administrative arbitrariness while also recognizes the democratic pedigree of the agency and the simple fact of limited resources. If an agency has allocated resources in a rational way, it has acted lawfully. 12 See Stephen Holmes and Cass R. Sunstein, The Cost of Rights (1999). 
What the South African Constitution Court has basically done is to adopt an administrative law model of socio-economic rights. Courts using that model are hardly unwilling to invalidate an agency's choices as arbitrary. That, in effect, is what the Constitutional Court did in Grootboom. The Court required government to develop, and fund, a program by which a large number of poor people are given access to emergency housing. What the Court call for is some sort of reasonable plan, designed to ensure that relief will be forthcoming to a significant percentage of poor people. On this view, the Constitution constrains government, not by ensuring that everyone receives shelter, but by requiring government to devote more resources than it otherwise would to the problem of insufficient housing for the poor. More particularly, the Court requires government to maintain a plan for emergency relief for those who need it. This is the particular gap found unacceptable in Grootboom. 
But there is a twist here. For those whose socio-economic rights are violated, the real problem is one of government inaction – a failure to implement a program of the sort that, in the view of some, the Constitution requires. The plaintiffs in Grootboom were seeking government action that had not, to that point, been forthcoming, in the particular form of a right to emergency relief. Hence the Grootboom Court’s approach is most closely connected to a subset of administrative law principles, involving judicial review of inaction by government agencies. In cases of this kind, everyone knows that the agency faces a resource constraint and that in the face of a limited budget, any reasonable priority-setting will be valid and perhaps even free from judicial review. At the same time, there should be a duty of reasonableness in priority-setting, and an agency decision that rejects a statutory judgment, or that does not take statutory goals sufficiently seriously, should be held invalid. In the constitutional context, this is what the South African Court ruled in Grootboom. 
The broader point here is that a constitutional right to shelter, or to food, can strengthen the hand of those who might be unable to make much progress in the political arena, perhaps because they are unsympathetic figures, perhaps because they are disorganized and lack political power. A socio-economic guarantee can have an enduring function. It can do so in part by promoting a certain kind of deliberation, not by preempting it, as a result of directing political attention to interests that would otherwise be disregarded in ordinary political life. 
Should constitutions protect social and economic rights? It is certainly relevant that if basic needs are not met, people cannot really enjoy the status of citizens. A right to minimal social and economic guarantees can be justified, not only on the ground that people in desperate conditions will not have good lives, but also on the ground that democracy requires a certain independence and security for all citizens. But there are many complexities here. A government might attempt to meet people’s needs in multiple ways, perhaps by creating incentives to ensure that people will help themselves, rather than by looking to government. Perhaps there is no special need for constitutional safeguards here; perhaps this is an issue that can be settled democratically. In any case social and economic guarantees threaten to put courts in a role for which they are quite ill-suited. While modern constitutions tend to protect those guarantees, we can understand the judgment that, in some nations, they would create more trouble than they are worth. 
In Grootboom, the Constitutional Court of South Africa was confronted, for the first time, with the question of how, exactly, courts should protect socio-economic rights. The Court’s approach suggests, also for the first time, the possibility of providing that protection in a way that is respectful of democratic prerogatives and the simple fact of limited budgets. 
In making clear that the socio-economic rights are not given to individuals as such, the Court was at pains to say that the right to housing is not absolute. [Shouldn’t it be “even children’s right to housing”? with regards to adults, the constitution is explicit that these rights are not absolute. -- stet] This suggestion underlies the narrow interpretation of the provision involving children and also the Court’s unambiguous suggestion that the state need not provide housing for everyone who needs it. What the constitutional right requires is not housing on demand, but a reasonable program for ensuring access to housing for poor people, including some kind of program for ensuring emergency relief. This approach ensures respect for sensible priority-setting, and close attention to particular needs, without displacing democratic judgments about how to set priorities. This is now the prevailing approach to the constitutional law of socio-economic rights in South Africa. 
Of course the approach leaves many issues unresolved. Suppose that the government ensured a certain level of funding for a program of emergency relief; suppose too that the specified level is challenged as insufficient. The Court's decision suggests that whatever amount allocated must be shown to be "reasonable"; but what are the standards are resolving a dispute about that issue? The deeper problem is that any allocations of resources for providing shelter will prevent resources from going elsewhere – for example, for AIDS treatment and prevention, for unemployment compensation, for food, for basic income support. Undoubtedly the Constitutional Court will listen carefully to government claims that resources not devoted to housing are being used elsewhere. Undoubtedly those claims will be stronger if they suggest that some or all of the resources are being used to protect socio-economic rights of a different sort. 
What is most important, however, is the Constitutional Court’s adoption of a novel and highly promising approach to judicial protection of socio-economic rights. The ultimate effects of the approach remain to be seen. But by requiring reasonable programs, with careful attention to limited budgets, the Court has suggested the possibility of assessing claims of constitutional violations without at the same time requiring more than existing resources will allow. And in so doing, the Court has provided the most convincing rebuttal yet to those who have claimed, in the abstract quite plausibly, that judicial protection of socio-economic rights could not possibly be a good idea. We now have reason to believe that a democratic constitution, even in a poor nation, is able to protect those rights, and to do so without placing an undue strain on judicial capacities. Readers with comments should address them to: 
Legal proceedings, both judicial and administrative, frequently raise questions about what people, including experts, ordinarily do.' If 
% Associate Professor, Pediatrics; Associate Director, Neonatology, Assistant Director, MacLean Center for Clinical Medical Ethics, University of Chicago. 
tt Karl N. Llewellyn Distinguished Service Professor of Jurisprudence, Law School and Department of Political Science, University of Chicago. 
We are grateful to Gertrud Fremling, James Heaton, Anup Malani, John Monahan. and Richard A. Posner for valuable comments on an earlier draft. Laura Warren provided excellent research assistance and valuable comments. 
negligence actions, custom is considered informative but not definitive. RICHARD A. EPSTEIN, CASES AND MATERIALS ON TORTS 201-42 (7th ed. 2000); PROSSER AND KEETON ON THE LAW OFTORTS 193,195 (W. Page Keeton ed., 5th ed. 1984) [hereinafter PROSSER AND KEETON]: [E]vidence of the usual and customary conduct of others under similar circumstances is normally relevant and admissible, as an indication of what the community regards as proper.... [A]s a general rule, the fact that a thing is done in an unusual manner is merely evidence to be considered in determining negligence, and is not in itself conclusive. 
For the classic case, see The TJ.Hooper, 60 F.2d 737,740 (2d Cir. 1932) (holding that conformity to custom does not preclude a finding of negligence). See also Trimarco v. Klein, 436 N.E.2d 502, 506 (N.Y. 1982) (refusing to give custom in shower door installation conclusive weight). 
In many jurisdictions, however, medical malpractice is an exception to the rule, and custom defines reasonable conduct EPSTEIN, supra,at 225 (identifying the role of custom in medical malpractice cases as "conclusive on the standard of medical care"); PROSSER AND KEETON, supra, at 189 ('T]he standard of conduct becomes one of 'good medical practice,' which is to say, what is customary and usual in the profession."); see eg., Burgess v. Superior Court, 831 P2d 1197, 1206 (Cal.1992) (holding that the legal standard of care required for doctors is the standard of practice required by their own profession); Cross v. Huttenlocher, 440 A.2d 952, 954 (Conn. 1981) ("A physician is under a duty to his patient to exercise that degree of care, skill and diligence which physicians in the same general line of practice ordinarily posa doctor is accused of negligence, for example, it is necessary to know about the customary practice of doctors.2 Of course, negligence judgments depend, at least in part, on an assessment of ordinary practice.' But how is ordinary practice by professionals or others to be assessed? 
In trials and on appeal, the basic answer is that the assessment comes via statements from expert witnesses who describe the ordinary practice.4 There can be no doubt that experts know a great deal about topics on which ordinary people lack information. But experts, no less than other people, are subject to predictable biases.' Their judgments about risk are affected by the same heuristics and biases to sess and exercise in similar cases."); Spensieri v. Lasky, 723 N.E.2d 544, 548 (N.Y. 1999) (stating that a physician will usually be insulated from tort liability where there is evidence that he or she conformed to accepted community standards of practice); Toth v. Cmty. Hosp., 239 N.E.2d 368, 372 (N.Y. 1968) ("The law generally permits the medical profession to establish its own standard of care."); MacPherson v. Ellis, 287 S.E.2d 892,896 (N.C. 1982) (holding that failure to adhere to customary practices of informed consent results in civil liability); Boone v. Fisher, No. 13-96-001-CV, 2000 Tex. App. LEXIS 4244, at *4 (Tex. App. June 22, 2000) (stating that a trier of fact, when determining whether a doctor's actions were reasonable, must be advised of standard medical practice). But cf McNeill v. United States, 519 F. Supp. 283, 288 (D.S.C. 1981) (cautioning that customary medical treatment is not conclusive on the exercise of care); Vassos v. Roussalis, 625 P.2d 768, 772 (Wyo. 1981) ("[Tlhe skill, diligence, knowledge, means and methods are not those 'ordinarily' or 'generally' or 'customarily' exercised or applied, but are those that are 'reasonably' exercised or applied."). 
2- See supra note 1. 
to which law should simply incorporate, or improve upon, the existing standard of care. See supra note 1; see also, e.g., Advincula v. United Blood Servs., 678 N.E.2d 1009, 1027 (IlI. 1996) (suggesting that existing custom is not determinative); Roach v. Springfield Clinic, No. 73,394, 1992 Il.LEXIS 204, at *33-*36 (Ill. Dec. 4, 1992) (same); Darling v. Charleston Cmty. Mem'l Hosp., 211 N.E.2d 253, 257 (Il1.965) (same). On any view, the customary practice is relevant, and that is sufficient for our purposes here. 

JUDGMENT AND DECISION MAKING 324, 325 (Terry Connolly et al. eds., 2d ed. 2000) (discussing the decisional factors and biases of expert judgment); David Faust & Jay Ziskin, The Expert Witness in Psychology and Psychiatry,in JUDGMENT AND DECISION MAKING, supra, at 336, 341-44 (discussing factors that limit clinical judgments); Gary Gaeth & James Shanteau, Reducing the Influence of IrrelevantInformation on Experienced Decision Makers, in JUDGMENT AND DECISION MAKING, supra,at 305, 306-16 (outlining empirical evidence of experts' use of irrelevant information in decisionmaking); Amos Tversky & Daniel Kahneman, Judgment Under Uncertainty: Heuristicsand Biases,in JUDGMENT UNDER UNCERTAINTY: HEURISTICS AND BIASES 3, 18 (Daniel Kahneman et al. eds., 1982) (presenting the intuitive bias of statistical experts). which most people are subject, even if (and this is a disputed question) expertise tends to reduce the most serious errors.6 
To correct the resulting problems, we offer a simple proposal: The legal system should rely, whenever it can and far more than it now does, on statisticaldata about doctors' performance rather than on the opinions of experts about doctors' performance. For the first time, it is becoming possible for law to rely on this evidence, for the simple reason that it is becoming increasingly available. If our argument is convincing in the medical context, it should apply in many other settings in which experts are asked to testify about negligence or deviations from ordinary practices. In many settings, the fallible opinions of isolated experts should be supplemented or replaced by statistical data. Those opinions should be seen as a kind of crude second-best, far inferior to the data that it approximates. A step toward reliance on such data would dramatically increase the sense and rationality of tort law, and perhaps other areas of law as well. 
We are hopeful that the use of statistical data will simplify some of the central issues in litigation and reduce the role of strategic behavior in the litigation process. Battles between isolated experts will be easier to mediate when there is a large pool of evidence on which to draw. But .our larger claim is that by using statistical data, the legal system will reach far more accurate results. If the law is seeking to determine the standard of care, it should not depend on fallible memories and recollections of local experiences. It should draw instead on more global evidence of the kind that is increasingly available for use in court and elsewhere. 
(2001) (discussing errors in risk-related judgments by judges). 
work in what is sometimes called behavioral law and economics. See, e.g., Christine Jols et al, A BehavioralApproachto Law and Economics,50 STAN. L REV. 1471 (1998) (proposing that law and economics analysis can be improved by increased attention to patterns of actual human behavior). Our proposal also is in line with recent work on the general value of using social science in law. For a valuable discussion of the increased use of social science evidence in law, see generally Laurens Walker & John Monahan, SocialFrameworks:A Ness, Use ofSocialScience in Law, 73 VA. L. REV. 559 (1987). 
It is now well known that most normal people tend to be "risk optimists," in the particular sense that they believe themselves to be relatively insulated from risks that are faced by others who are similarly situated.! This is one of the most robust findings in cognitive psychology. For example, ninety percent of drivers believe that they are safer than most drivers and less likely to be involved in a serious accident.9 Most people believe that they are distinctly unlikely to be subject to various risks, such as cancer, heart disease, and divorce.'0 In some important domains, people appear not only to believe themselves less likely than others to fall prey to certain risks, but also to underestimate the actual risk to which they are subject." With respect to automobile accidents, people believe that the danger they face is less than it is as a matter of statistical fact. Smokers appear to know BEHAVIORAL LAW AND ECONOMICS 288, 290-92 (Cass R. Sunstein ed., 2000) (discussing excessive optimism); Neil D. Weinstein, Unrealistic Optimism About Future Life Events, 39 J. PERSONALITY & SOC. PSYCHOL. 806,807-12 (1980) (discussing individuals' tendency to believe themselves to be less vulnerable than other people to adverse events). 
W. Kip Viscusi, in his article Using Warnings to Extend the Boundaries of Consumer Sovereignty, 23 HARV. J.L. & PUB. POL'Y 211 (2000), offers an intriguing qualification, suggesting that although people might believe that they are less subject to certain risks than most other people, they also might have a statistically accurate understanding of the risks to which they are subject. Id. at 227. In other words, they might believe that they are less likely to have a car accident than most people, but also have an accurate sense of the lifetime risk that they will die in a car crash (say, 1 in 1000). Id. Viscusi offers evidence to support this hypothesis. Id. at 216-17 (describing evidence of accurate risk assessments of workers). But see John Ayanian & Paul Cleary, Perceived Risks ofHeart Disease and Cancer Among Cigarette Smokers, 281 JAMA 1019, 1020-21 (1999) (suggesting an underestimate of the actual risk for smokers); Jolls, supra, at 291 (same in the context of car accidents). In any event, resolving this debate is not necessary for our purposes. The very possibility of expert error and overoptimism, confirmed by the studies described in this Essay, is enough to justify reliance on statistics rather than on isolated experts. 
the statistical risks of smoking, 3 but they believe that they are less likely than most smokers to fall victim to the various risks." In one study, less than half of smokers surveyed believed that they had a higher-than-average risk of cancer or cardiovascular disease; indeed, most heavy smokers (at least forty cigarettes per day) believed that they were not at any increased risky Only one group of people does not show a tendency to excessive optimism: the clinically depressed.6 
It would be natural to infer that unrealistic optimism, in many cases, is adaptive. Though such optimism may cause false predictions of outcomes, an optimistic attitude may increase the probability of a good outcome and otherwise create hedonic benefits." This may be particularly true in the medical context; perhaps an excessively optimistic doctor is more likely to get good results. If so, optimism is well adapted to the social role of a doctor. It also might be thought that some social settings work against optimistic bias. In markets, for example, entrepreneurs might have an incentive toward realism, especially because market pressures-it might be expected-would punish and drive out those who are unrealistically optimistic. But even if markets move people toward realism, entrepreneurs, no less than anyone else, have been shown to suffer from unrealistic optimism." It is noteworthy that expert witnesses are not disciplined by ordinary market pressures.' If experts err, they might even help their clients, individuals of the statistical risks of smoking). 

depressed people); Peter Lewinsohn et al., Social Competence and Depression, 89 J. ABNORMAL PSYCHOL. 203,207-08,210-11 (1980) (same). 
selffulfilling prophecies, see NICHOLAS CHRISTAKIS, DEATH FORETOLD: PROPHECY AND PROGNOSIS INMEDICAL CARE 135-62 (1999) (evaluating the role of self-fulfilling prophecies in medicine). There are analogies in the domain of athletics. Consider the comments of basketball coach Lenny W"ikins: "The shooter should think the shot's going in. Everyone else should be thinking he's going to miss." Phil Jasner, Sixers Find the Road Better Titan Home, PHILA. DAILY NEws, Dec. 30,2000, LEXIS, Phila. Daily News File. 
1& See KENNETH R_ MACCRIMMON & DONALD A. WEHRUNG, TAKING RIsKs 7, 260 (1986) (discussing the optimism of entrepreneurs); Daniel Kahneman & Dan Lovallo, Timid Choices and Bold Forecasts:A CognitivePerspectiveon Risk Taking, 39 MGMT. Sc. 17,27-28 (1993) (same). 
course there is something of a market for expert witnesses, so that people will lack credibility, and opportunities to be witnesses, if they are systematically (shown to be) wrong. But it would be heroic to suggest that these pressures ensure accuracy from experts. and hence excessive optimism might well be favored, rather than disfavored, by whatever market pressures exist in this domain. B. 
It seems reasonable to speculate that those with specialized knowledge are far less prone to optimistic bias and to other motivational and cognitive errors. Unfortunately, this appears not to be the case. It has been found almost universally that physicians, no less than other people, are prone to make cognitive mistakes and indeed have a substantial tendency to err in predicting outcomes. 0 "Virtually all" of the existing studies of physicians "have documented frequent and large errors in predictions., 2' No study finds a high level of accuracy. The errors tend in a particular direction: "physicians are prone to an optimistic bias.'' 2 
As an example, consider a study published in 1972 that showed that in making predictions about the length of survival for cancer patients, only 47% of the physicians provided prognoses that were even roughly accurate and that 80-90% of the mistakes were excessively optimistic2. A subsequent study published in 1987 found overestimates of survival time in 88% of the cases, by roughly a factor of three.24 A recent, large-scale study found inaccurate predictions in 80% of cases, with 63% of these showing overestimates." Interestingly, patient characteristics (age, race, sex, illness duration) were not correlated with inaccurate, optimistic estimates, but one factor was: "the better the doctor knew the patient-as measured, for example, by the length and intensity of their contact-the more likely the doctor was to err in the prognosis, most frequently by overestimating survival.' ' 6 The overall picture is that in cases involving cancer patients, physicians accurately predict survival only 10-30% of the time, and the rest of the time they overestimate survival by a factor of two to five.2 
All of this is highly suggestive but not decisive with respect to the particular point that we mean to urge here: that experts make erroneous judgments about the ordinary standard of medical care, and that they tend to err in a particular direction. Because expert witnesses in medical malpractice cases typically are doctors, it would be plausible to think that their general tendency toward optimism with their patients would affect their testimony as experts, causing them to overstate systematically the efficiency and effectiveness of ordinary practice. At the same time, it also would be possible to imagine that (a) doctors' predictions about their patients' prospects are systematically overoptimistic, but (b) doctors have an accurate sense of what is ordinarily done by themselves and other doctors. If there is no systematic error with respect to (b), there would be little need to substitute statistical data for the expert testimony of doctors. We now offer the first real evidence in the legal literature on the central question, showing that medical experts systematically err.2 
offering excessively optimistic or pessimistic prognoses in their clinical practices. Both of these behaviors are understandable in context. Excessively pessimistic prognoses, commonly referred to as "hanging crepe," usually accompany discussions of procedures with particularly problematic outcomes (e.g., surgery for aortic aneurisms, carotid cndarterectomies, cardiopulmonary resuscitation of twenty-three-week-gestation infants). Mark Siegler, Pascal's Wager and the Hanging of Crepe, 293 NEW ENG. 3. MED. 853, 853-57 (1975). In these cases, the underlying idea is that the patients should be fully prepared for the statistically expected result (a poor outcome or even death), and that in the unlikely, but possible, event of a good outcome, both physician and patient alike will be pleasantly surprised. Id. at 853-54. If patients are prone to optimistic bias, excessively pessimistic prognoses might even be justified as a possible corrective. There also are well-recognized examples of what appears to be excessive clinical optimism. Endof-life prognostications are notoriously "optimistic" in that the time to death is commonly overestimated. Nicholas A. Christakis & Elizabeth B. Lamont, ErtentandDeterminants ofErrorin Doctors'Prognosesin Terminally III Patients: Prospective Cohort Study, 320 BRIT. MED. . 469, 472 (2000). This error also is understandable in context-to the extent that there is any element of "self-fulfilling prophesy" in these situations, physicians may wish their patients to benefit from this effect. 
II. STANDARD OF CARE: EXPERTS VERSUS DATA 
Consider the problem of bacterial meningitis in children.29 For the purposes of the current discussion, it is necessary to know only three things about this disease and its treatment. Bacterial meningitis is an infection of the brain; it (usually) can be treated with antibiotics; and, as a general rule, the sooner the antibiotics are administered, the better the outcome will be.' 
In medical malpractice cases, the question often arises whether treatment of a particular child was unduly "delayed,"' because delay can cause serious harm.' Of course a key issue is how a legal decisionmaker would know what kind of delay counts as "undue." For 
Both of these conditions are simply tangential to the claims made here. We are suggesting that expert descriptions of actual practice, not the discussions that accompany these practices, are often inaccurate, and predictably so. Practitioners may or may not be well motivated in their attempts to exaggerate outcomes in order to convince their patients of the desirability of a particular course of action ex ante. This may or may not be a good thing to do. But it is irrelevant to expert testimony. Our purpose here is to minimize the possibility that, when charged with describing the actual spectrum of clinical care, experts exaggerate their descriptions ex post. 
angle in William Meadow et al., Ought "StandardCare"Be the "Standardof Care"?A Study of the Time to Administration of Antibiotics in Children with Meningitis, 147 AM. J. DISEASES CHILD. 40, 40-44 (1993) (concluding that "standard of care" is highly subjective and should be defined by data analysis rather than expert opinion). 
constitutes a medical emergency. 2 EMERGENCY MEDICINE: CONCEPTS AND CLINICAL PRACTICE 1213 (Peter Rosen ed., 4th ed. 1998). Death may occur within minutes to hours unless rapid diagnoses are made and appropriate antibiotics are administered immediately. ROSCOE N. GRAY & LOUISE J. GORDY, ATTORNEYS' TExTBOOK OF MEDICINE § 33.62(la) (Lois L. Caswell et al. eds., 3d ed. 1992). "Some of the immediate complications of bacterial meningitis include coma with loss of protective airway reflexes, seizures, cerebral edema, vasomotor collapse, disseminated intravascular coagulation, respiratory arrest, dehydration, pericardial effusion, and death." 3 EMERGENCY MEDICINE, supra,at 2202. 
(holding that "evidence which shows ... that negligent delay in diagnosis or treatment... lessened the effectiveness of treatment is sufficient to establish proximate cause" in medical malpractice cases). 
administration of antibiotics and the likelihood of subsequent permanent brain damage. The appropriate time scale (not minutes, perhaps hours, certainly days) to evaluate these time intervals is best addressed in Michael Radetsky, Durationof Symptoms and Outcome in BacterialMeningitis,11 PEDIATRIC INFECTIOUS DISEASE J. 694, 694-98 (1992). See also Meadow et al., supra note 29, at 43 (discussing how this presumed correlation breaks down when it is considered along the time scale of minutes). obvious reasons, juries are assumed to be ignorant of the general medical facts in such cases, and consequently are instructed to rely on the testimony of expert witnesses. The instruction that jurors should rely on expert opinions for their knowledge about the standard of medical care answers one question, but it raises another-on what should the experts themselves rely? 
In the traditional formulation, experts are instructed to rely on their personal "knowledge and training." For the reasons stated, however, such admonitions raise many problems. In the context of bacterial meningitis, for example, it would be reasonable to predict that experts would tend to be excessively optimistic-that their recollections of the amount of time that elapsed before antibiotics were initiated in children with meningitis would be tilted toward shorter durations than actually occurred. And if this is the case, these experts, relying on their "knowledge and training," will provide inaccurate accounts of behavior. 
We attempted to test this hypothesis by surveying doctors about their experiences in cases of childhood meningitis and comparing these survey responses to detailed descriptions of physicians' behaviors in actual cases of meningitis.3 We began by identifying doctors with expertise in the two most relevant subspecialties of pediatrics, pediatric emergency room medicine (ER) and pediatric infectious diseases (ID). These are the fields from which potential experts most likely would be drawn to testify in lawsuits about deviations from the standard of care in meningitis cases. We presented these doctors with a questionnaire about a child who was brought to the emergency room with symptoms suggestive of bacterial meningitis. We asked how long, in their opinion, it would take to administer antibiotics in the national meeting of the Society for Pediatric Research in 1990. The respondents were identified only by specialty: general pediatrics, pediatric infectious diseases, pediatric emergency room medicine, or other. Respondents also indicated their year of graduation from medical school, the city in which they worked, and the type of hospital in which they worked (children's hospital, university hospital, "community" hospital, or other). Twenty-three respondents identified themselves as pediatric infectious disease specialists and fifty-four identified themselves as pediatric emergency room specialists. The questionnaire presented the scenario of a child presenting to their emergency room with symptoms consistent with bacterial meningitis. Two related questions were asked: 1) "What do you think is the median time which elapses between the presentation of such a patient to the emergency room and the administration of intravenous antibiotics?"; and 2) "What length of time - from initial presentation to the ER. until I.V. antibiotic administration - would encompass 95% of all such cases; i.e. of all such patients presenting to the E.R., what time would demarcate the point where 95% of children have received IV. antibiotics?" William Meadow, Questionnaire, 1990 (on file with the Duke Law Journal). such cases in their own hospitals. We then compared the responses of these doctors to the actual time to antibiotic administration (ABTIME) observed in ninety-three children treated at two large university-associated pediatric centers in Chicago. 
We found, as we had hypothesized, that the recollections of the potential expert witnesses were wrong. Moreover, they were wrong in the anticipated direction-that is, biased in favor of the response that, in retrospect, would be desired (in this case, toward a shorter ABTIME). The median estimate of the fifty-four ER specialists was 46 minutes; the median estimate of the twenty-three ID specialists was 80 minutes. By contrast, the median ABTIME of ninety-three actual cases at the two university centers was 120 minutes, significantly longer than the estimates of either of the sub-specialty groups." When we reviewed the medical literature in an attempt to extend these observations beyond Chicago, we found two hundred reported cases of children with meningitis from hospitals in South Carolina" and California. 6 These two hundred cases had a median ABTIME (114-126 minutes) comparable to the ninety-three Chicago cases, and longer than the estimates of either of the two sub-specialties. 7 
Whatever "expert" opinion means in this context, it does not mean an accurate opinion. Both groups of potential pediatric experts were wrong, and substantially so, in their opinions about the amount of time that elapses before doctors administer antibiotics to children with meningitis. The statistical probability that the potential experts were correct was less than one part in 100 for the ID doctors, and less than one part in 1000 for the ER doctors. As we have emphasized, the inaccuracy found in the estimates was consistent with our hypothesis: responses were biased toward the outcome perceived to be desired, that is, toward shorter waiting periods. 
Note that this bias operates independently of the pressures imposed by the adversary system, which would naturally lead litigants to hire experts whose accounts will favor their side. The ID and ER respondents were not asked to consider their responses in the context of a review for either a plaintiff or defendant in this survey. Moreover, the optimistic bias presented by the potential experts here is operating outside of the bounds of the usual doctor-patient relationship, where it has been suggested that optimism, even if inaccurate, may serve a therapeutic purpose.' We think it entirely possible that other circumstances (e.g., smaller hospitals, clinics, or doctors' offices) may not be comparable to academic medical centers, and that ABTIME in these settings consequently might be longer than at university centers. We would then argue that claims about care in those smaller centers ought to be met with ABTIME data gathered from these clinics or offices. Our main point here is that if the standard of care is normative, appropriate data-based comparisons are best for determining what is normative. 
malpractice setting and other contexts involving negligence. For example, the question, "How difficult would ithave been for the worker to have avoided this hazard?" might lead to the (optimistic) suggestion that it would not have been difficult at all, whereas the question, "How ex 
Others, in varying contexts, have demonstrated a similar disconnect between physicians' descriptions of their own practices and objective determinations of the same behavior. As examples, consider patient education by physicians in general practice. Physicians are committed to the value of patient education,39 and they devote con4 siderable time and resources to health promotion." Clearly, the desired practice is to spend this time explicitly informing patients about health-related behaviors, such as the uncontested virtues of smoking cessation and preventive oncology. Nevertheless, when physicians' reports of their office practices are compared with taped interviews of these same encounters, there is only a weak, insignificant correlation between the self-reported patient education activities and the actual performance." Not surprisingly, physicians' recollections err in the predicted direction; that is, more explicit counseling is recalled than actually occurred. 
Here too the cognitive expectation is confirmed-anecdotal recall proves to be inaccurate, biased, and, inevitably, systematically imperfect. If our findings here are the rule and not the exception, as the psychological evidence suggests, it is hardly clear that the law should continue to base a system of medical-legal jurisprudence on such a shaky foundation. pensive would it have been to have installed a safety device?" might lead to the (optimistic) suggestion that this would not have been expensive at all. Hence the framing of the question might lead optimistically biased agents in different directions. In either case, statistics are better than expert testimony, simply because they are more accurate. 
Medical Practice,259 JAMA 2883, 2887 (1988) (finding face-to-face advice from physician and nouphysician counselors to be the best predictor of smoking cessation); Henry Wechsler ct al., The Physician'sRole in Health Promotion-A Survey of Primary-CarePractitioners,308 NEW ENG. J. MED. 97, 98-99 (1983) (finding that nearly three of four physicians feel it is a physician's responsibility to educate patients on risk factors including smoking, alcohol, drugs, stress, exercise, and diet, and that eighty-one percent of physicians interviewed personally provide such education in place of a nurse or other health professional). 
Practitioners,14 PREVENTIVE MED. 636,643 (1985) (surveying 610 family practice physicians on client demographics and medical treatment); see also Ulrich J. Grueninger et al., PatientEducation in the Medical Encounter: How to Facilitate Learning,Behavior Change, and Coping, in THE MEDICAL INTERVIEW 122, 122-33 (Mack Lipkin, Jr. ed., 1995) (encouraging physicians not to simply provide information, but to incorporate educational and behavioral techniques). 
and Screening Activities in Primary CarePractice,16 PREVENTIVE MED. 277, 277 (1987) (conparing estimates by Wisconsin physicians of preventive performance with their actual preventive practices). 
We acknowledge that the evidence we have offered is suggestive rather than demonstrative. Perhaps there are contexts in which doctors, or other experts, have an accurate sense of the ordinary standard of care. But the general evidence of mistakes, together with the empirical evidence introduced here, is sufficient to show that both error and optimism are highly likely to infect a wide range of expert testimony.4 2 This point operates independently of the ordinary incentive to assist one's side in an adversary system." 
It therefore makes sense, in the context of malpractice suits and probably more generally as well, to move toward greater reliance on actual data and less reliance on the recollections of isolated experts. Until recently, the legal system was unable to rely on statistical data for the simple reason that it did not exist. But it is now increasingly possible to develop data sets about physician choices and behavior, and the legal system will have an increasing amount of information on which to draw. Perhaps government should generate accounts of ordinary practice, based on the information that is available; or perhaps people in the private sector should do so. In either case, the goal would be to ensure the compilation of actual data about practice, substituting accurate empirical evidence for.the fallible judgments of individuals. 
Our proposal would have two notable advantages. First, it could simplify the task of discerning the truth. It is now exceedingly difficult for juries to decide which expert has best described the prevailing standard of medical care in the community. Under current practice, this issue may well turn on unreliable judgments about credibility or on sympathy that is unrelated to the standard of care. Second, and more important, use of statistical evidence would increase the accuracy of assessments of the standard of care, reflecting the comparative advantage of pooled data over individual recollections. There is no good reason for courts to continue to rely on the latter when they have access to the former. 
crroneous and overly optimistic decision-framing of probabilities). 
ofSelf-Serving Biases,in BEHAVIORAL LAW AND ECONOMICS, supranote 8, at 355,355-65 (describing self-serving bias in judgments about fairness). 
Our proposal might be criticized on the ground that statistical analysis is itself subject to error. Of course there can be no assurance that any particular compilation or interpretation of data is correct. Perhaps there will be several accounts of data that will compete with one another. If this is so, it might be objected, the use of statistical evidence will merely usher in a new phase in the battle between imperfect experts. How much would be gained by that? 
This objection has a degree of plausibility, but rather than impeaching our proposal, it suggests the need to develop good methods to evaluate particular claims about the meaning of statistical data. In the event of disagreement, there is no escaping the need to test the data in the ordinary way through the presentation of conflicting views, including those of experts. Data might be met with contrary data; it also might be met with a professional critique. And of course it is possible that an individual expert will be able to show persuasively that some purported evidence does not establish what it claims to establish. These sorts of disputes can be handled in the standard fashion. We suggest that because individual experts are prone to systematic error, it would be far better to begin the process with reliable evidence rather than with individual recollection. If individual experts can show that the statistical data are wrong, the legal system will be better off for the demonstration. In the long run, however, we predict that these demonstrations will be the exception rather than the rule. In any case, an inquiry into the evidence will anchor the legal process in something that is likely to be more reliable. 
Statistical data are not typically used in negligence cases. The data might be excluded as inadmissible on several grounds. Most important, questions might be raised about whether the data are relevant and about whether they are hearsay. 
It is possible to object that statistical data are not relevant, because they cannot determine the proper standard of care. In some states, there is a continuing dispute about the proper role of common practice in negligence cases.' Where common practice is not determinative of the standard of care' s statistical data cannot resolve the legal issue. It would be possible to argue that in such circumstances, the data should be inadmissible on grounds of irrelevance. But this argument should be rejected. Even when common practice is not determinative, it is pertinent, and its relevance is reflected by the very fact that experts frequently are allowed to testify about what most doctors do. Statistical data should be found admissible for the same reason that expert testimony is admissible. 
It also might be thought that statistical data should be seen as hearsay. Perhaps the witness seeking to introduce the data is attempting to repeat what others have said in their reports of the data. If so, his testimony might be thought to violate the prohibition on hearsay." But this analysis would misread the hearsay rules. Under federal law, for example, experts are allowed to rely on "statements contained in published treatises, periodicals, or pamphlets on a subject of history, medicine, or other science or art, established as a reliable authority by the testimony or admission of the witness or by other expert testimony or by judicial notice."' Federal rules also allow experts to base opinions on facts and data of the kind reasonably used by experts in the field when they form opinions or inferences." Statistical data, if published, should be permitted under the first rule just quoted; if unpublished, they should be admissible to the extent that they are of the sort reasonably relied on by experts. Under either scenario, however, data should not be treated as inadmissible hearsay. 
(finding that evidence of compliance by the defendant blood bank with the professional standard of care imposed under a blood shield statute was not conclusive proof of "due care"); Roach v. Springfield Clinic, No. 73,394, 1992 III. LEXIS 204, at *33-*36 (IlL Dec. 4, 1992) (finding that the exclusion of evidence describing medical standards of care did not prejudice a malpractice trial outcome); Darling v. Charleston Cmty. Mem'l Hosp., 211 N.E-2d 253,258 (IlL 1965) (finding that the hospital's standard practice and conformity to regulations did not constitute conclusive evidence ofreasonable care). 
expert to disclose the facts and opinions from others' evaluations on which he relied in diagnosis); Schrag v. Chi. City Ry. Co., 106 N.E. 828, 829-30 (IlL 1914) (prohibiting the admission of scientific books into evidence, as well as their use to contradict an expert witness not relying upon them). 

references to treatises on the ground that their authors are not available for cross-examination). The Roach decision seems to us an anachronistic and odd reading of the prohibition on hearsay. 
Perhaps there are other solutions, besides reliance on statistical data, to the problem of idiosyncratic recollections of individual experts. It might be urged that the opinions of experts, as reported in textbooks or journal publications, should define the standard of care. By this view, when actual practices differ from the theoretical or recommended standards, the practitioners are at fault. They may not know the standard, may disagree with it, or may have misinterpreted it, but the burden of proof falls on them. 
We believe that this approach would be oversimplified and misguided. Under the law, ordinary practice is important for its own sake, regardless of whether it is decisive of the legally acceptable standard of care. If courts are going to consider the ordinary practice, they should have an accurate understanding of what it is; here, textbook and treatise authors, like all other experts, are prone to error. Unless a textbook or treatise actually draws its conclusions from ordinary practice, it will not report the ordinary practice reliably. 
Perhaps textbooks and journals merely state recommendations that are worth consideration even if they do not capture ordinary practice. This is not implausible. But interpreting these recommended standards is not always straightforward. In the particular context of bacterial meningitis, textbooks recommend that antibiotics should be administered "early,"49 "promptly,"' or "immediately""1 to children with suspected meningitis. But what, in the minds of a lay jury, is a reasonable interpretation of this recommendation? In such a context, is thirty minutes too long? Is one hour, or six hours? The recommendation in one text that antibiotics be administered "within 30 minutes after the diagnosis of meningitis is established"52 helps a good deal, but it does not resolve the question, as it merely presents another troubling question-when exactly is the "diagnosis of meningitis established?" 
But there is a more general point. At most, the author of a textbook can offer an opinion about the standard practice or about the 
DISEASES 302 (1981). 
(Waldo E. Nelson ed., 13th ed. 1987). 
738 (1979). practice that is desirable. If the opinion involves what is desirable, it should be evaluated as such. But if the opinion involves what is standard, the question should be whether the opinion is correct. To the extent that the question inquires into ordinary practice-into what doctors do-it would be far better to rely on actual data about physician performance. 
Our plea here has been for attention to data, rather than to opinions. Ultimately we might hope for authoritative texts on what is established by the data, and such texts might come from the government or from the private sector. To the extent that there is competition among sources of data, all the better. And to the extent that data about actual practice are available, the data should inform, and improve, the debate about the extent to which the legal system should be bound by the ordinarystandard of care when determining the appropriatestandard of care.' 
It is well known that ordinary people make systematic errors in assessing probabilities and risks.' It is less well known that experts are subject to the same biases and likely to make the same mistakes." We have suggested here that there is general reason to believe that expert judgments about the standard of medical care will be erroneous, and that the errors will run in a predictable direction because of optimistic bias. People tend to believe that things can be done, and are in fact done, more easily, more rapidly, and more successfully than the evidence suggests. To establish this claim, we have drawn on existing evidence, highly suggestive on this point, and on more particular evidence, presented here, that experts do not accurately report the ordinary standard of care. 
Our principal innovation has been to suggest that in light of the evident divergence between expert beliefs and empirical reality, the legal system should rely, whenever possible, not on the former but on statistical evidence of the latter. The best reason for the legal system's longstanding reliance on individual recollections has been historicalthe simple absence of statistical evidence. But the current practice is an anachronism. The reason is that this gap in statistical evidence is rapidly being filled with a great deal of reliable data, and it may be completely filled in the next generation. Our emphasis has been on the question of medical malpractice, but the general implication is far broader. In any case in which a disputed question calls for expert testimony about ordinary practice, it is hazardous to rely on what particular experts recall. If the goal is accuracy in adjudication or regulation, it is far more sensible to make the outcome turn on statistical evidence. 
Follow this and additional works at: https://chicagounbound.uchicago.edu/law_and_economics Part of the Law Commons Recommended Citation Cass R. Sunstein, "Switching the Default Rule" ( John M. Olin Program in Law and Economics Working Paper No. 114, 2001). 
SWITCHING THE DEFAULT RULE 
Cass R. Sunstein This paper can be downloaded without charge at: 
The Chicago Working Paper Series Index: http://www.law.uchicago.edu/Publications/Working/index.html The Social Science Research Network Electronic Paper Collection: 
http://papers.ssrn.com/paper.taf?abstract_id=255993 Preliminary draft 1/9/01 Forthcoming, NYU L Rev symposium on behavioral economics and labor law All rights reserved 
Switching the Default Rule 
Cass R. Sunstein* 
There is a standard analysis of default rules in contract law, including those forms of contract law that fall under the label of employment law. But behavioral economics raises many complications. The default rule can create an endowment effect, making employees value certain rights more simply because they have been granted such rights in the first instance. Similarly, the default rule for savings plans, set by employers or law, seems to have a large effect on employee behavior. When the default rule affects preferences and behavior, conventional economic analysis seems indeterminate; either default rule can be efficient. In employment law, analysis of distributive consequences also suggests the difficulty of deciding which default rule to favor, because any switch in the rule is unlikely to have significant redistributive effects. Nonetheless, switching the default rule can, in certain circumstances, have desirable effects on workers’ welfare. A central question is whether the stickiness of the default rule reflects a genuine change in values, or instead employee confusion or bargaining strategy. 
I. 
Begin with three puzzles: —Legislators in a Midwestern state want to give employees greater protection against arbitrary discharges. But critics contend that a new law, flatly banning discharges without cause, would be too rigid and would ultimately hurt employees themselves. Some people have urged a compromise, one that would give employees a right to be discharged only for cause unless they waive that right through contract. Would the compromise have good effects? Would it have any effects at all? * Karl N. Llewellyn Distinguished Service Professor, University of Chicago, Law School and Department of Political Science. I am grateful to Christine Jolls, Eric Posner, and Richard Posner for helpful comments and Richard Thaler for helpful discussions. —Employers in a large Midwestern state typically allow their employees to participate in certain savings plans. Under the existing plans, employees can elect to devote a specified level of their salary to savings, in return for tax relief and some contribution from employers. But only about 20% of employees participate in these plans. State legislators are considering a law that would require an “automatic enrollment plan,” by which employees would be enrolled in the plan when hired, but would be expressly authorized to opt out at the same time. Would the new law have any effect on savings rates? —There is no ban on age discrimination in a European country. Instead of proposing a ban, legislators have urged that employees should have a presumptive right to be free from age discrimination, but that employees should be permitted to relinquish that right through voluntary agreements. Would legislation to this effect be desirable? What would it accomplish? 
In many areas of labor and employment law, it is possible to imagine two different sorts of default rules. The employer might be presumed to have the relevant entitlement, but the employee might be entitled to bargain for it. This is a system of waivable employers’ rights. Alternatively, the employee might be presumed to have the entitlement, but the employer might be entitled to bargain for it. This is a system of waivable employees’ rights. The choice between the two cuts across many substantive issues: job security, vacation time, parental leave, health care, savings plans, pensions, occupational safety, even unionization itself. In these cases, and more, the legal system might give the initial entitlement to one or another side. We could imagine a legal system in which employers enjoy all or most initial entitlements; we could imagine default rules granting all or most initial entitlements to employees. 
At common law, employers are typically given almost all of the initial entitlements. In that sense, the default rules set by the common law create a system of waivable employers’ rights. Of course employees have a presumptive right to their own time and labor; employers may assert a right to the time and labor of workers if and only if the workers have bound themselves to work. But employees must specifically bargain for everything else. When the contract is silent, an employer may discharge an employee for any reason or for no reason at all; may refuse to provide vacation time, parental leave, or health care; need not offer a safe workplace; need not provide a pension plan; need not allow unionized workers on the premises. 
Contrary to appearances, there is nothing natural or inevitable about this state of affairs. When an employer is authorized to deny an employee a safe workplace, or vacation time, and when there is no contractual provision on the point, the law has made a choice about the (right) starting place for bargaining. It would be easy to imagine a legal system, fully committed to freedom of contract, that began with default rules giving certain entitlements to employees. My principal questions in this essay are simple: Would a switch of the entitlement matter? If so, exactly how? 
Of course many statutes create nonwaivable rights. They bypass the question of default rules entirely, by banning bargaining altogether. There are many reasons why legislatures and courts might take this approach. Perhaps third party effects argue against waiver. Perhaps waivers would be inadequately informed; behavioral economics offers a number of reasons why this might be so.1 Perhaps nonwaivable rights can be justified, in the context of accommodation mandates, on redistributive grounds.2 But I seek here to cast light on a different question: When should employment and labor law proceed, not by preventing bargaining, but by switching the relevant entitlement from employers to employees? 
Much attention has been paid to so switching entitlements; for example, freedom from age discrimination now takes the form of a waivable workers’ right.3 Anecdotal evidence suggests that people are often asked to waive race and sex discrimination claims, though such waivers are most unlikely to be enforceable. Another example is the Fair Labor Standards Act, which allows employees to waive their right not to work more than forty hours a week, but also at a governmentally determined premium (“time and a half”).4 Or consider the Model Employment Termination Act, which allows employers and employees to waive the right to for cause discharge, but only on the basis of an agreement by the employer to provide a severance agreement in the event of a discharge not based on poor job performance.5 
For purposes of the present discussion, let us put to one side those cases where transactions costs impede bargaining. Everyone agrees that these are situations in which the default rule will matter, and prove “sticky,” simply because it is costly to contract around it.6 I will focus instead on cases in which there are no such transactions costs. I urge, most generally, that in such cases, the default rule will matter, but that we can find little guidance from the traditional criteria of efficiency and distribution. The 1 See Cass R. Sunstein, Human Behavior and the Law of Work, Va. L. Rev. (2001). 2 See Christine Jolls, Accommodation Mandates, Stan. L. Rev. (forthcoming 2001). 3 20 USC 626(f)(1).. Note that the right is waivable for past violations, not for future violations. 429 USC 207(f) 5 Model Employment Termination Act, Section 4©, reprinted in Mark Rothstein and Lance Liebman, Employment Law 208-19 (Statutory Supplement) (1997), 6 For a much-cited discussion, see Ian Ayres and Robert Gertner, Filling Gaps in Incomplete Contracts: An Economic Theory of Default Rules, 99 Yale LJ 87 (1989). key questions are whether the switch of the entitlement genuinely changes values, as opposed to bargaining strategies, and if so whether the switch improves workers’ welfare. In some cases, the switch in entitlement will indeed be defensible as a way of making workers’ lives better. More particularly, I attempt to support the following sets of points. 
the endowment effect – the effect of the initial allocation of the right on people’s valuations, possibly employers and almost certainly workers.7 When the endowment effect is at work, preferences and valuations are affected by the initial allocation of the entitlement; contrary to the Coase theorem, there is no prelegal “preference” from which the legal system can work. The default rule might matter because it has a legitimating effect, carrying important information about what most people are expected to do.8 If workers value a right more simply because it has been initially allocated to them, and less because it has not been so allocated, a switch in the initial allocation will matter by definition. The principal qualification here is that people might be unaware of the legal rule. They might order their affairs on the basis of norms, rather than law.9 If this is so, the switch in the entitlement is unlikely to matter. 
of “penalty defaults,” alongside a behaviorally informed understanding that employee likely err about the law. Employees often lack information about their legal rights, showing excessive optimism,10 and the switch of the entitlement from employers to employees might increase the flow of information between the parties and to the legal system.11 Suppose, for example, that if employees are given certain rights by the default rule, employers will want to buy those rights. If this is the case, we will see a system in which certain information is disclosed to employees, simply as part of the process by 7 See Richard Thaler, Quasi-Rational Economics 169, 184-86 (1993). The endowment effect might be small or nonexistent for employers if legal entitlements are, for employers, akin to money tokens. See id. at 176. We would not expect hardware stores to show an endowment effect for mousetraps and hammers; for hardware stores, these goods are a form of cash. The question, not yet resolved, is whether legal entitlements have the same characteristic for employers in the context of labor and employment law. Some evidence is provided in Jennifer Arlen, Matthew Spitzer, and Eric Talley, forthcoming, with the finding that endowment effects are greatly reduced when agents are acting for others. Id. Perhaps those acting for employers – such as supervisors and personnel officers -- are generally “agents,” not subject to the endowment effect. 8 I will discuss this possibility in several places below; I believe that in some settings, it accounts for what is described as an endowment effect, and perhaps should be described as a different effect from the endowment effect. Social scientists have yet to sort out the relationship between this legitimating effect and the endowment effect, as found in Daniel Kahneman, Jack Knetch, and Richard Thaler, Experimental Tests of the Endowment Effect and the Coase Theorem, in Behavioral Law and Economics 211 (Cass R. Sunstein ed. 1999). 9 See Robert Ellickson, Order Without Law (1993). 10 See Richard Freeman and Joel Rogers, What Workers Want 118-22 (1999). 11 See Samuel Issacharoff, Contracting for Employment, 74 Tex L Rev 1783 (1994). which employers bargain. A switch in the legal rule, moving the initial allocation of the entitlement from employer to employee, might give important information to workers when they would otherwise overstate their legal rights.12 
or motivational failure will lead the employee to demand an excessive amount in order to trade. Suppose, for example, that employees would gain little from a contractual right to be fired only for “just cause.” (Employees might have little to gain if discharges rarely occur without cause; in that case, the contractual provision would give employees a right that they enjoy in any event.) Suppose too that if employees are initially given that right, they will not trade it, even for a high price. If this is so, a switch in the entitlement, from employers to employees, would be hard to defend as a way of improving the welfare of workers. Because employees would refuse to trade the right, even though it does them little good, the result would be a situation in which their overall compensation package is actually inferior. 
to switch, will not be simple to justify on grounds of efficiency or redistribution. When transactions costs are zero, either allocation will be efficient. From the standpoint of redistribution, the effects are likely to be modest, at least as a general rule. The reason is that the contractual setting will usually allow adjustments by employers, the apparent “losers.” 
crude proxy for the former. A switch in the default rule might improve social welfare in general and workers’ welfare in particular. Consider the second case given above, where the switch might well increase savings without having any significant adverse effect on workers. There is a related point: If workers care about relative economic position, but not absolute economic position, a switch of the default rule to workers might be justifiable on welfare grounds, because the switch might give the employees real benefits while also not imposing real costs on them. 
It is also possible that switching the default rule will have desirable effects on norms and preferences, because it will inculcate a more appropriate sense of how, and how much, to value the interests at stake. To those who object that this approach is unacceptably paternalistic, the best response is simple: When the default rule has an inevitable effect on valuation, there is no escaping issues of this kind, and it is hopeless 12 On workers’ ignorance of their legal rights, see Richard Freeman and Joel Rogers, What Workers Want 118-122 (1999). to attempt to “defer” to workers’ preferences – which are, by hypothesis, a function of the legal rule. It is possible, however, that the endowment effect reflects no change in welfare with different initial entitlements, but merely a difference in the bargaining situation of employers and workers, and perhaps confusion about the existence of opportunity costs. 
The rest of the discussion will be devoted to an elaboration of these ideas. 
According to the Coase theorem, a change in the default rule does not matter, at least if there are no transactions costs.13 No matter the default rule, the parties will bargain their way to a result that is both efficient and the same. It is obvious that this claim has large implications for labor and employment law in particular. If the Coase theorem is correct, the default rules set by the law, for workers and employers alike, does not matter, at least if transactions costs are low. It is irrelevant whether employees or employers are given initial rights with respect to leave time, vacation, health care, job security, age discrimination, and more. 
Of all existing claims in behavioral law and economics, perhaps the most wellknown is that on this point, the Coase theorem is entirely wrong.14 No one claims that the Coase theorem is wrong insofar as it says that under the stated conditions, either allocation of the entitlement will produce efficiency. Where the Coase theorem blunders is in suggesting that no matter the initial allocation of the entitlement, people will bargain to the same result. The reason that this is a blunder is that the initial allocation seems to create an endowment effect.15 When the endowment effect is at work, those who initially receive a legal right value it more than they would if the initial allocation had given the right to someone else. 13 See Ronald Coase, The Problem of Social Cost, J. Law & Econ. (1961). It has long been acknowledged, however, that wealth effects may mean that the initial entitlement will affect ultimate outcomes. See Richard A. Posner, Economic Analysis of Law 51 (5th ed. 1999). But wealth effects are generally thought to be small. See infra. The traditional analysis ignores the endowment effect. See id. 14 See Daniel Kahneman, Jack Knetch, and Richard Thaler, Experimental Tests of the Endowment Effect and the Coase Theorem, in Behavioral Law and Economics 211 (Cass R. Sunstein ed. 1999). 15 See Thaler, supra note. 
There is a great deal of evidence to this effect.16 One of the initial studies involved the effect of the initial allocation of mugs and chocolate bars.17 Here the endowment effect was found to be instantaneous: People initially allocated the relevant good demanded a great deal more to sell it than people not initially allocated the good were willing to pay to obtain it. Countless studies have found a disparity between willingness to pay (for a good owned by someone) else and willingness to accept (payment for a good already owned).18 Some of these studies have shown an endowment effect in a contractual setting, akin to that involved in labor and employment law.19 As I have suggested, the default rule might matter, not because of a “pure” endowment effect, but because it carries information about what most people do, or about what it is most reasonable to do. In this event too, the default rule can have significant consequences. 
In the context of insurance, an unplanned, natural experiment showed that the default rule can be very “sticky.”20 New Jersey created a system in which the default insurance program for motorists included a relatively low premium and no right to sue; purchasers were allowed to deviate from the default program and to purchase the right to sue by choosing a program with that right and also a higher premium. By contrast, Philadelphia offered a default program containing a full right to sue and a relatively high premium; purchasers could elect to switch to a new plan by “selling” the more ample right to sue and paying a lower premium. In both cases, the default rule tended to stick. A strong majority accepted the default rule in both states, with only about 20% of New Jersey drivers acquiring the full right to sue, and 75% of Pennsylvanians retaining that right. Experiments confirm the basic effect, showing that the value of the right to sue is much higher when it is presented as part of the default package.21 
It is not clear that the difference reflects an endowment effect, involving a change in valuation as a result of the initial entitlement. But it does demonstrate a large consequence from a change in the default rule. It seems reasonable to speculate that in many cases, the default rule carries information about what ordinary or sensible practice.22 This too is a central reason that the default rule can matter. 16 See, e.g, Russell Korobkin, Behavioral Economics, Contract Formation, and Contract Law, in Behavioral Law and Economcis, supra, at 311. 17See Kahneman, Knetch, and Thaler, supra. 18 For citations, see Thaler, supra note, at 168. 19 See Korobkin, supra, in Behavioral Law and Economics. 20 See Colin Camerer, Prospect Theory in the Wild, in Choices, Values, and Frames 294-95 (Daniel Kahneman and Amos Tversky eds. 2000); Eric Johnson et al., Framing, Probability Distortions, and Insurance Decisions, in id. at 224, 238. 21 Id. at 235-38. 22 See below. 
From all this we might reach a simple conclusion: When labor and employment sets a default rule, it is likely to prove “sticky,” because of its effect on employees’ judgments and valuations. If there is a default rule against age discrimination, the ultimate outcome will be quite different from that produced by a default rule permitting age discrimination. If a default rule creates a right to a generous pension program, the outcome will differ from that produced by a default rule creating no such right. In fact the default rule selected by employers will matter no less than that set by law. I now turn to an example. 
Some people think that workers do not save enough for retirement. Without arguing the point, let us simply suppose, for purposes of argument, that they are right. Might employees’ failure to save be a function of the default rule? Might a change in the default rule alter savings rates? The evidence seems clear. A mere change in the default rule will dramatically alter employee behavior 23 – probably because in some contexts, the default rule carries information about the ordinary and sensible course of action. Whatever the mechanism, the default rule has significant consequences for employee behavior. Note that the issue here involves an employer’s decision, not about whether the employer or employee should enjoy an entitlement, but about the default rule governing the allocation of workers’ benefits between salary and retirement savings. 
An important study has demonstrated the basic effect.24 For some years, employees would not be enrolled in a 401(k) plan unless they affirmatively chose to do so. The employer instituted a change by which employees would be automatically enrolled, and would be removed from the program only if they so chose. The change in the default option had dramatic effects. When employees had 3-15 months of tenure, the participation rate was 37% under the old plan; under the new plan, the participation rate, for those with the same amount of tenure, was no less than 86% for employees with that amount of tenure. Automatic enrollment had especially pronounced effects on the participation rates of women and African-Americans. 
In a separate phenomenon, the default rule also had a significant effect on the chosen contribution rate. The default contribution rate (3%) tended to stick; a majority of employees maintained that rate even though this particular rate was chosen by less than 1% of employees hired before the automatic enrollment. The same result was found for the default allocation of the investment: While less than 1% of employees 23 B.C. Madrian and D. Shear, The Power of Suggestion: An Analysis of 401(k) Participation and Savings Behavior, available on SSRN.com. 24 Id. chose a 100% investment allocation to the money market fund, a substantial majority of employees chose that allocation when it was the default rule. 
As I have suggested, the significant effect from the default rule is probably a product of its informational signal. With respect to savings, the initially proposed plan carries a certain legitimacy, perhaps because it seems to have resulted from some conscious thought about what makes most sense for most people. This understanding is supported by the finding that the largest effects, from the new default rule, are shown by women and African-Americans. We might speculate that members of such groups tend to be less confident in their judgments in this domain and to have less experience in assessing different savings plans.25 
Richard Thaler and Schlomo Benartzi have exploited the endowment effect and loss aversion to propose a new plan, Save More Tomorrow, designed to increase savings by workers.26 The authors’ suggestion is that employers should offer employees the option to choose a retirement plan that favors savings. Under the Save More Tomorrow plan, employees are invited to join a plan in which they can to precommit to give a certain amount of their future salary increases to be used in their retirement savings. The rationale behind the proposal is that while people would be reluctant to give some of their current salary to savings (because they would then suffer a loss), they would be willing to give some of their future salary increases to savings (because they would then remain “gainers” after the increase). Would this plan have any effect? Thaler and Benartzi found a dramatic impact. A company that adopted the plan found that the plan was chosen by over three-quarters of those offered it; that almost all of those who adopted it remained in it; and that the consequence of the plan was to increase average savings rates of those enrolled in the plan from 3.5% to 9.4% over sixteen months. Significantly, only a tiny portion of those who joined the plan (less than 3%) dropped out after two pay increases. 
The studies that I have just described show significant effects from changes in the default option – but these changes are produced by employers, not by law. Understood in this narrow form, the point itself carries considerable importance. It suggests that voluntary behavior by employers, perhaps in concert with unions, can greatly affect employees, not by mandating any particular course, but by suggesting a default option.27 Employers, acting together with employees, might design any number of 25 I am grateful to Christine Jolls for pressing this point. 26 See Richard Thaler and Schlomo Benartzi, Save More Tomorrow: An Easy Way to Increase Employee Savings (unpublished manuscrupt July 2000) 27 The point might help to explain the finding in Freeman and Rogers, supra note, that workers in unionized firms are almost unanimously satisfied with their union and would not like a change. For most of these workers, default compensation packages, giving employees some mixture of take-home pay, vacation time, pension plan, job security, and protection from discrimination. What we now know is that the default plan will have significant behavioral consequences. 
Perhaps it will be thought that any suggested default plan would represent an objectionable interference with workers’ freedom of choice. But because any default option is likely to stick, this objection is implausible.28 A sensible approach for the future would involve consultations between employers and groups of workers, not to mandate any particular outcome, but to identify default rules that are likely to be in the interest of most workers. Since any such default rule will not work well for everyone, there should be an opportunity, in ordinary circumstances, for workers to opt out if they wish. 
The point has implications for law as well. With respect to savings, it is easy to imagine a statutory intervention designed not to mandate anything in particular, but to ensure a particular default rule. Employers might be encouraged (through information or economic incentives) or required to propose a certain savings package for employees, while at the same time allowing employees to divert some of the money to salary if they wish. Perhaps a statutory program to this effect could build on the “Save More Tomorrow” plan. It is even possible to imagine a system of social security reform that would make creative use of default rules to offer workers starting points that would be most likely to improve their welfare. 
The domain of savings might reflect an unusually loud informational signal, but we know enough to know that the initial entitlement is likely to have large consequences.29 To the extent that large effects can be expended from a change in the default rule, there is a general lesson for labor law. If employees are given a waivable right to be fired only for cause, to take vacation leave, to have a certain level of occupational safety, or to be free from age discrimination, they are likely to value that right more than they would if the right were allocated to employers in the first instance. The conclusion is that if a default rule is switched, so that entitlements initially enjoyed unionization has become the default option. Of course we cannot know the extent to which workers’ satisfaction is the product of this effect, or whether workers who have voted for a union are likely to like unions. 28 See Thaler and Benartzi, supra. 29 See Russell Korobkin, Behavioral Economics, Contract Formation, and Contract Law, in Behavioral Law and Economics 116 (Cass R. Sunstein ed. 2000); Christine Jolls et al., A Behavioral Approach to Law and Economics, in id. at 13, 30 (discussing study by Jonathan Gruber). by employers are now owned by employees, the outcome would not be the same. Very generally, the entitlement will have a tendency to stick. 
But there is an important qualification. The default rule might not matter for a reason unrelated to the Coase theorem: People might not know about the default rule, and they might not order their affairs by reference to it. Suppose, for example, that workers and employers decide on job security, or vacation time, or pension plans, without reference to the legal rule. Robert Ellickson has shown, in some domains, that it is a mistake to be “lawcentric”; people might well produce outcomes with little reference to legal rules.30 Norms, not traceable to law, may do the work of law. To the extent that this is so, switching the default rule will have no impact, because people do not enter into agreements with anything like close reference to it. 
Is this likely to be the case in most domains of labor and employment law? No general answer would make sense. But in the area of labor-management relations, employers, at least, are likely to know about the nature of the governing default rules, and they are likely to act on the basis of that understanding, If law confers initial entitlements on employees, employers are likely to know about the fact, and to respond. Perhaps this step will not be necessary if employers know that employees are unaware of what the law has done; in that case, employers will have little to worry about in the event that they do not “buy back” the relevant right. But a sensible employer is unlikely to be willing to take his chances with employee ignorance. I now turn to the resulting questions. 
The simplest effect of switching the default rule will therefore be to increase the likelihood that it will end up where it was initially placed. If workers are initially given certain rights or options, those rights or options will tend to stick. But what will be the consequence for bargaining in the labor market? A potential result – for labor and employment law, a fortunate one -- will be to ensure that more information is disclosed to workers who might otherwise have overestimated their legal rights, and also to the legal system.31 In particular, a switch of entitlement from employer to employee will increase the likelihood that workers will know what the law has and has not given them, and bargain accordingly. The optimistic view would that a switch of that sort might even overcome a market failure, in the form of inadequate information on the part of employees. When the employer is given the initial entitlement, bargains might 30 See Robert Ellickson, Order Without Law (1991). 31 This is a large theme in the law of contract, see Ayres and Gertner, supra note. not represent anything like a meeting of the minds. A switch of the default rule can solve the problem. 
In one sense, this point is entirely old hat. It is well known that default rules can operate as “penalties” that impose the burden of disclosure on those who have it, for the benefit of those who need it.32 But the point has not been greatly discussed in the context of labor and employment law.33 What I add here is a behavioral suggestion: For as yet ill-understood reasons,, workers frequently, and generally, have a false and exaggerated understanding of their legal rights. It is for this reason that switching the entitlement for employers to employees might be especially desirable, as a means of correcting employee ignorance. 
To see why this is so, imagine a situation, by itself apparently unproblematic, in which the right is initially vested in employers, and it rarely happens that employees bargain for that right. It might be, for example, that employers have a right to fire employees at will, or to replace strikers permanently, or to deny employees parental leave, occupational safety, or freedom from age discrimination. If the entitlement tends to stay where it was initially allocated, the reason might be (a) significant transactions costs, (b) little employee enthusiasm for purchasing the right at the market price, (c) an endowment effect, or (d) an employees’ failure to know that they do not have the right in any case. Reason (a) and (b) can be analyzed conventionally. If transactions costs are high, a central efficiency question is whether the default rule “mimics the market,” in the sense it reflects what the parties would have done if they had bargained.34 If employees do not attempt to purchase the right at market price, there seems to be no problem from the standpoint of efficiency, though perhaps there is some other reason for concern.35 I have just discussed the endowment effect. For present purposes, the interesting case is (d) – employee ignorance about the content of the default rule. 
There is growing evidence that workers overestimate their legal rights – a phenomenon that we might label the “fairness heuristic,” by which employees bleive that the law is what (they think) fair law would be. For example, Pauline Kim has shown that employees generally believe that the legal rule is one of “for cause rather than “at will” – that, in other words, employees can be fired only for cause.36 Of course 32 See id. 33 The leading exception is Samuel Issacharoff, Contracting for Employment, 74 Tex L Rev 1783 (1994), from which I have learned a great deal. The point is also treated in Pauline Kim, Norms, Learning, and Law, 1999 Univ of Ill L Rev 447. 34 This oversimplifies some complex issues. See Ayres and Gertner, supra. 35 There is a possibility of uninformed or otherwise objectionable preferences, discussed below. 36 Pauline Kim, Norms, Learning, and Law, 1999 Univ of Ill L Rev 447; Pauline Kim, Bargaining With Imperfect Information: A Study of Worker Perceptions of Legal Protection in an At-Will World, 83 Cornell L Rev 105 (1997); employees are wrong on this point; without a contractual provision giving them job security, they do not, in fact, have a right not to be discharged “at will.” On this point employees systematically err; they think that they have a right that they lack. Richard Freeman and Joel Rogers have greatly generalized this finding, showing that workers believe that they have a number of rights that they in fact lack.37 Workers believe, for example, that employers cannot hire permanent replacements for strikers; that employers cannot require employees to do dangerous work; that workers have ample rights against arbitrary discharge.38 
These findings do not establish that employees should be given a right to job security or to anything else. But they do appear to establish an apparently serious problem with the current situation, in the form of pervasive worker overestimation of their legal rights.39 To be sure, it is possible that this is not a reason for special concern, at least if workers are speaking for employer’s ordinary practices (as seems plausible in the context of job security). If employees have the law wrong but the practices right, the problem is not so troublesome. But something does seem to be amiss if workers believe that they have legal rights that they lack as a matter of law. An obvious remedy would be to switch the default rule, not to ensure that employees have job security and so forth, but to ensure that whatever they have, it is a product of informed bargai ning. Here the likely consequence of a switch in the default rule, giving employees a right (for example) to be fired only for cause, would be to ensure that employers would buy the right via contract if that is where, on conventional grounds, the right belongs.40 And if the idea is sound, it might be applied in many other areas in which employees erroneously believe that they have certain legal rights. 
Nor is this idea entirely foreign to labor and employment law. State courts have made significant inroads on the at will rule, by taking ambiguous “promises” from employers as a basis for creating a right to job security.41 A possible understanding of these cases is close to what I have suggested here: Employees should not be unaware of their rights, and doctrines should be developed to ensure that when they are entering into employment contracts, they are aware of what they do and do not have. With a doctrine that takes ambiguous statements, apparently promising job security, as a basis for rights of job security, it is possible to give employers a good incentive to tell employees exactly where they stand. In fact employers in Michigan have done exactly 37 See Freeman and Rogers, What Workers Want (1999). 38 Id. at 118-121. 39 See id. 40 See Issacharoff, supra note. 41 See, eg, Touissant v. Blue Cross, 408 Mich 579 (1980); McDonald v., Mobil Coal, 820 P. 2d 986, 987 (1991). For an overview, see Mark Rothstein et al., Employment Law 530-32 (1994). this – responding to the cases by giving employees a clear signal that they lack job security. The advantage of this approach is that it promotes a better information flow between the parties, so that workers are more likely to know what they are (not) getting.42 
Of course an understanding of behavioral economics raises many questions about allowing waiver at all.43 Perhaps employees will be unaware of what it is that they are waiving. Perhaps they will think that they have the relevant rights even after wiaver. Perhaps they will engage in wishful thinking. But at the very least, a switch of the default rule will increase the information that is provided to employees – and that probably counts as a good thing. 
For those concerned about improving the welfare of employees, it seems to make sense to create default rules that favor workers. If what matters is the welfare of workers, shouldn’t workers be given certain rights, with the proviso that employers might be able to buy them if workers find the bargain to be worthwhile? 
In general, the implication of the question is not entirely wrong (see section V). But there is an important qualification. Suppose that a significant endowment effect accompanies the switch of the default rule. Suppose too that employees are extremely reluctant to trade the relevant right, so much so that employers are infrequently willing to offer enough to produce a trade. At first glance, nothing seems amiss; the entitlement should stay where it was initially allocated, because employees value it more than employers do (given that initial allocation). 
But here we can find another twist from behavioral economics: Some employees might refuse to waive because of cognitive and motivational problems. Suppose, for example, that most workers have little to gain from a right to be fired only for cause, because in a market economy, such workers are not really at risk.44 Why would an employer fire someone without cause? In that event, it should be expected that workers would be willing to waive for a relatively low price. But at least it is possible that workers will refuse to do so, simply because they “overvalue” the right, and hence will 42 There is, however, a possible disadvantage: Employees might not really be getting anything they don’t really have, because arbitrary discharges are (on one view) unlikely in a competitive market. If this is so, the switch of the rule produces wasted paperwork. Note also that it is possible that even with a change in default rule, employees will not understand, or believe, that they are waiving their rights. Kim’s data suggest that many employee will not even believe in the legal effectiveness of a waiver. See Kim, supra. 43 See Sunstein, supra note. 44 See Richard Epstein, In Defense of Contract At Will, 51 U Chi L Rev. 947 (1984). not sell except for a price that is too high, from the standpoint of their own welfare. Workers might, for example, overestimate the probability that the small risk will come to fruition, perhaps because they can think of a salient example.45 We do not have the empirical work that would justify an unequivocal view on this issue. But we know enough to know that the risk is real. 
The cautionary lesson here is that concern about workers’ waivers is a doubleedged sword. In some circumstances, that concern justifies the creation of nonwaivable workers’ rights.46 But in some circumstances, the same concern justifies a system of waivable employers’ rights, on the theory that a default rule in favor of workers might jeopardize workers’ own welfare. 
The largest question remains. In the presence of an endowment effect, or some other stickiness in the initial entitlement, which default rule is best? To answer this question, it is necessary to identify the criteria by which to answer that question. 
On a familiar view, the default rule should be chosen by the efficiency criterion. The question is which default rule will promote efficiency – the standard claim in the economic analysis of law.47 
To be sure, no sensible person would contend that efficiency is the only thing that labor and employment law should be concerned with. Few people, however, would deny that an efficiency loss is at least relevant to the inquiry, because it is relevant to the welfare of workers and everyone else.48 But there is a serious problem in using the efficiency criterion to choose among default rules (when transactions costs are zero). If the endowment effect is at work, the two results are different, but they will both be efficient. If an employee refuses to trade a right to be free from age discrimination except at a very high price, and if employers will not offer that price, the outcome – no trade – is efficient. If an employee is not willing to buy a right to be free from age discrimination, and if employers will not sell that right except at a very high price, the outcome – no trade – is also efficient. In the presence of an endowment effect, the efficiency criterion is indeterminate and therefore unhelpful. 45 Cf. W. Kip Viscusi, Fatal Tradeoffs (1993) (suggesting that people overestimate low probability risks). 46 See Sunstein, supra note. 47 See Ayres and Gertner, supra. 48 See below. 
To be sure, familiar economic analysis can suggest a default rule in the face of transactions costs, by seeing whether one or another rule “mimics the market” by replicating the likely outcome of bargaining, or this inquiry proves difficult, by choosing a penalty that produces clarity from the parties.49 But if there are no transactions costs, and if different default rules produce different but efficient outcomes, economic analysis has little to say about how to choose among the competing possible rules. 
The point can be clarified with a simple example. Suppose that most employees would be willing to purchase a right to job security for $200. Suppose that most employers would be willing to sell it for $250. In these circumstances, most contracts will be at will; this is the efficient outcome. But suppose that if employees are initially given the right, they will trade it only for $300 – and that in the same circumstances, most employers (showing o endowment effect) would be willing to buy it for $250. In these circumstances, most contracts will be for cause. Two different outcomes are efficient, and they are different only because of the difference in the initial allocation of the entitlement. 
At this stage the efficiency criterion seems indeterminate. For the moment, we should note that willingness to pay (WTP) and willingness to accept (WTA) are important only insofar as they provide administrable indications of the welfare gain from a trade. When the endowment effect means that WTP and WTA are different, there are two interesting possibilities. First, the difference might measure actual welfare differences, depending on the default rule. Second, the difference between WTP and WTA might have nothing to do with welfare at all; it might be produced by confusion, by bargaining behavior, or something else not directly related to workers’ welfare under different default rules. I will return to the problem shortly. 
Might some progress be made by asking about distribution? Suppose that there is a general view that in cases of doubt, the law should distribute resources to employees rather than employers. Certainly the impetus for much of labor law has been to transfer resources in this way. 
To be sure, it is not clear that this view makes sense, even for those who favor more egalitarian distributions. Many employees are not poor; many poor people are not 49 See Ayres and Gerner, supra note. employees; and efforts to redistribute resources from employers to employees are not the same as efforts to redistribute directly from rich to poor. In any case the effect of those efforts may be to increase prices, which is hardly good for poor people. Increased prices are a kind of regressive tax; they are especially hard on the poor. A progressive income tax is far more likely to be an effective method of redistributing income than employment and labor law.50 But at least it is possible to favor a transfer of resources from employers to employees -- perhaps on the ground that workers as such receive too little of the proceeds from work, perhaps with the thought that anything that distributes resources from employers to employees will tend to increase equality in the distribution of income. If these thoughts turn out to be wrong, we might be able to venture more targeted regulations that have the desired effect. 
At first glance, there does seem to be a plausible argument, on grounds of redistribution, for switching entitlements from employers to employees, regardless of the extent of the endowment effect. Employee wealth would appear to be increased by a decision allocating entitlements to employees rather than employers. Suppose, for example, that employees are suddenly given a right to participate in all company decisions – a right that employers can buy for a fee.51 At least for current employees, the new entitlement should have significant redistributive effects, because employers are likely to be required to, and to be willing to pay. a good deal to reclaim the right. If, then, the entitlement is quite valuable, the shift from employer to employee should, at first glance, have substantial redistributive consequences. The extent of the consequence depends on the value of the entitlement. 
But in the contractual setting that typifies labor and employment law, things are far more complicated than that,52 and far less promising for those who seek to promote redistribution through switching the entitlement. If employees are given an entitlement, whether alienable or inalienable, the rest of the contractual package might well be adjusted accordingly. A right to participate in company decisions, if not traded, will be met with some kind of market response – through, for example, higher prices, less employment, or lower wages. As a result of the enactment of workers’ compensation and parental leave programs, for example, many workers lost almost as much in wages as they gained in the relevant rights.53 In the context of protection against discrimination 50 See, eg, A. Mitchell Polinsky, Introduction to Law and Economics (2d ed 199X). 51 I owe the example to Eric Posner. 52 An excellent, detailed treatment in the context of requirements of accommodation, see Christine Jolls, Accommodation Mandates, Stan L Rev (forthcoming). 53 See Jonathan Gruber, The Incidence of Mandated Maternity Benefits, 84 Am. Econ. Rev. 622 (1994), and Price Fishback and Shawn Kantor, Did Workers Pay for the Passage of Workers’ Compensation Laws, 110 Q J Econ. 713 (1995) (finding substantial wage offsets): “Analysis of the effect of the introduction of workers’ compensation on wages shows that in the coal and lumber industries, workers experienced substantial wage offsets. In the coal on the basis of disability, employers appeared to respond not by cutting wages but by decreasing hiring of disabled people.54 
From the standpoint of redistribution, the question for labor and employment law is the incidence of the cost, faced only initially by employers, of the switched entitlement. There is a growing body of work, both theoretical and empirical, on the distributional consequence of mandatory terms,55 and the issues are not altogether different for switched default rules. A grossly simplified overview: The central questions involve (a) the responsiveness of consumers to price increases, (b) the responsiveness of prospective and current workers to wage decreases, and (c) the cost, to employers, of reducing the number of workers. If the cheapest response to higher costs, for employers, is to raise prices, that is what they will do; if the cheapest response is to cut wages or decrease the number of employees, they will choose that route. In the employment context, no general prediction makes sense.56 But there is a significant risk that any redistributive gain will be nullified, at least in part, through readjustment of the wage package. On plausible assumptions, workers will lose, in wages, some or much of what they gain as a result of the switched entitlement.57 
This does not mean that a switch cannot be justified on distributive grounds. Workers’ compensation programs, for example, seem to have produced distributive gains in the unionized sector, by giving workers a benefit that was not offset by wage cuts.58 But there is no reason for great confidence, in the abstract, that there will be a significant distributive gain from the switch. If this is true for nonwaivable rights, it is true too for waivable rights, many of which will be waived, with no significant distributional shifts. 
In the standard analysis of mandates directed to workers as a whole,59 distributive gains and efficiency gains march hand-in-hand; without efficiency gains, there can be no efficiency gains. Because of the empirical considerations just discussed, I think that this claim is too simple: It is imaginable that workers will benefit, on balance, from an inefficient mandate. But even if this is right, the claim that a switch in industry the offsets were large enough to cover not only the expected monetary value the benefits, but also the employers’ costs of purchasing the insurance to provide those benefits.” Id. at 736. 54 See Christine Jolls, Accommodation Mandates, Stan L Rev (forthcoming). 55 See Lawrence Summers, Some Simple Economics of Mandated Benefits, 79 Am Econ Rev (Papers & Proceedings) 177 (1989); Jolls, supra note; note supra. 56 See id. 57 See the discussion of wage cuts in the nonunionized sector in Price Fishback and Shawn Kantor, A Prelude to the Welfare State 64-69 (1999). 58 See id. (showing no wage offsets in unionized firms). 59 See Summers, supra note. entitlements will make for distributional improvements is quite fragile, with the exception of accommodation mandates, on which I am not focussing here.60 
Sensible conclusions, from what has been said thus far, are that (a) a switch of the entitlement will be neither good nor bad on efficiency grounds, and (b) the switch might possibly be justified on redistributive grounds, but this depends on a complex inquiry into the incidence of the burden represented by these greater costs. Is there any other way to resolve the problem? 
In theory, at least, it is worthwhile to ask whether the aggregate welfare of employers and employees is improved with one or another default rule. I have noted that “efficiency,” as understood through the criterion of willingness to pay (or willingness to accept) is at best a proxy for welfare, suitable for guiding policy only because it provides an administrable, though quite crude, way of inquiring into welfare consequences.61 If the efficiency calculus proves unhelpful, perhaps we should avoid the middleman and go to welfare directly. If the difference between WTP and WTA has nothing to do with a difference in welfare, we have reason to think that a switch in the entitlement will matter neither to efficiency nor to welfare. If a new default rule would give the winners more than the losers lose, it will be justified on welfare grounds, endowment effect or no endowment effect. The most serious difficulty here is that the legal system lacks direct access to welfare consequences, and in this setting, the normal proxies are unreliable.62 
savings. If a default rule increases savings, and if the increase improves workers’ welfare, the case for a pro-savings default rule seems extremely plausible. Suppose, for example, that workers lose little or nothing from the reduction in take-home pay but that they gain significantly from the increase in savings. If so, and if employers lose nothing, the switch seems easily defensible on welfare grounds. 
What I am suggesting here is that with the suggested default rule, workers will be better off, because they will accumulate more in the way of savings, and because they will not much suffer, in terms of welfare, from the somewhat lower weekly takehome pay. Of course some workers will be somewhat worse off with a reduction in take-home pay, and for some, that welfare loss will not be overcome by the increase in 60 See Jolls, supra note. 61 See Mathew Adler and Eric Posner, Rethinking Cost-Benefit Analysis, 109 Yale L.J (1999). 62 See id. savings. And at some point, a transfer of money from salary to savings would certainly hurt most or all employees. But we are speaking here only of a default rule, allowing workers to opt out; and for the small salary reductions involved here, automatic enrollment plans, suggested by employers or mandated by law, seem substantial improvements. 
discussed are far more complicated. If we had direct access to the welfare of employers and employers, we might be able to make choices on grounds of welfare. Of course the idea of “welfare” can be specified in many different ways.63 To simplify matters, let us assume, without insisting the point, that “utility” is what is most important. Might one or another default rule increase utility? This is certainly possible. After a switch, aggregate utility might be increased, perhaps because employees gain more than employers lose. Suppose, for example, that employees’ utility is far higher if they have a right to job security, or parental leave, or paid vacations; suppose too that the utility loss (because of costs faced initially by employers, with the accompanying incidence of those costs) is not very high. If this is so, the argument for the switch seems quite plausible. 
In pointing to the importance of relative (as opposed to absolute) economic position, some people have made an argument of just this sort on behalf of mandatory terms in employment contracts.64 The argument is that mandatory terms can improve workers’ welfare if they give workers an important good (such as improved safety or increased leisure time) while diminishing absolute, but not relative, income. A shift of this kind would improve workers’ income if the reduction in absolute income does not really reduce workers’ welfare and if the new good leads, for all or most workers, to a welfare improvement. 
If it is correct, this argument is based on reasonable claims about what really promotes workers’ welfare. Some such claims might also be reasonable in the context of a switch in the default rule, especially, but not only, if relative economic position is what matters to workers. If relative position is what matters, the switch in the default rule might turn out to be a special case of the argument just given: Any income loss will not matter much, because relative position will be held constant, and relative position is what matters. If relative position is not what matters, it is nonetheless possible that 63 See Martha Nussbaum, Women and Human Development: The Capabilities Approach (2000): Amartya Sen, Development As Freedom (1999). 64 See Robert H. Frank, Choosing the Right Pond; see also Robert H. Frank and Cass R. Sunstein, Cost-Benefit Analysis and Relative Position, U Chi L Rev (forthcoming); Cass R. Sunstein, Human Behavior and the Law of Work, Va. L Rev (forthcoming). employee welfare will be improved after the switch. The problem is that the legal system is likely to lack the tools to know whether this is so. One possible avenue for inquiry would involve objective indicators of welfare – suicide attempts, health, longevity65 – but of course it will be most difficult, in this context, to control for confounding variables. 
the basis for the endowment effect in the particular case. If we know why the endowment effect exists, we might be able to make some progress in deciding on the default rule. In asking about whether to use “willingness to pay” or instead “willingness to accept,” some progress has been made on this question.66 Suppose, for example, that willingness to pay is lower than willingness to accept because of wealth effects: Willingness to pay is constrained by existing holdings, as willingness to accept is not. If this is so, we might choose willingness to accept, on the ground that it is a more accurate measure of the value of the good in question.67 But there is a serious problem with this argument: The difference between WTP and WTA might have nothing to do with a general wealth effect, as demonstrated by the fact that it has been observed in many contexts lacking significant wealth effects.68 When workers demand more for an entitlement than they would be willing to pay for it, the reason need not be the wealth effects of the default rule. 
What, then, accounts for the endowment effect? The question has no obvious answer.69 It might be that the effect is “hard-wired”; it might be a fact of human psychology that more is demanded to relinquish ownership of X than to obtain X in the first instance.70 Or perhaps the endowment effect has to do, not with the higher actual value of things owned, but with the some asymmetry in anticipated after-the-fact regret. People might fear that they would regret a change from the existing rule, and this bias in anticipated regret might play a role in creating the endowment effect. Perhaps people, and workers in particular, believe that the initial allocation of the entitlement carries a certain moral weight, or presumptive validity, so much so as to drive a wedge between WTP and WTA. In some circumstances, selling a good might be appear illegitimate, an insult to dignity.71 Or perhaps some people simply ignore, much of the time, the existence of opportunity costs.72 For goods that are not simply money 65 See Robert H. Frank, Luxury Fever (1998). 66 As suggested in Comment, Policymaking and the Offer/Asking Price Gap, 46 Stan L Rev 663 (1994). 67 See Herbert Hovenkamp, Legal Policy and the Endowment Effect, 20 J Legal Stud 225 (1991). 68 See note, supra, at 680. 69 See id. at 689-97. 70 Cf. Richard A. Posner, Rational Choice, Behavioral Economics, and the Law, 50 Stan L Rev 1551 (1999). 71 Id. 72 See Thaler, supra note. tokens, people appear to think that continued ownership is costless, or that the cost of not selling is far less than it is in effect. This might well be simple confusion. If there is the source of the endowment effect, we should not use WTA, and there is no good reason to switch the default rule. 
Perhaps a better understanding of the source of the endowment effect could help us to know whether the default rule should be switched on welfare grounds. One thing that we would like to know is whether the difference between WTP and WTA, in the context of labor law, reflects real welfare or utility differences between different states of affairs, or is instead an artifact of the bargaining situation and confusion about opportunity costs. I suspect that in many contexts, bargaining considerations, alongside that confusion, are responsible; the mugs and chocolates experiments seem to support that conclusion. If this is so, WTP is better than WTA, because the higher amount represented by WTA does not really mean that people’s utility will be higher as a result of having the initial allocation. And if this is so, the existence of an effect, from the initial entitlement, does not argue for shifting the entitlement to workers. 
More controversially, we might think that people’s lives will simply be better if the endowment effect pushes valuation in one direction rather than another. To make the point vividly, suppose that a legal system is considering whether to give employees a presumptive right to be free from sexual harassment, or instead to say that employers can engage in sexual harassment unless employees can buy a right to be free from it. It is easy to imagine that if employees are given that presumptive right, they will be most reluctant to give it up. They would demand a high premium for the right not to be subject to (some probability of) sexual harassment; they might even refuse the trade that right even for a large fee. The clear implication is that workers will value the right to be free from sexual harassment more highly if they have an initial right to that freedom. If it is believed that the higher valuation of that right is better, then a right, in employees, might well be best. It is even possible to think, on this ground, that the right should be inalienable.73 But short of accepting this conclusion, a shift of the right from employer to employee might well be best. 
It is possible to object that an inquiry of this kind is unacceptably partisan – that it takes a stand on appropriate preferences and values. But this is a weak objection.74 If 73 See Sunstein, supra note. 74 It should be recalled here that as a historical matter, enthusiasm for markets was itself a product, not of neutrality about preferences, but instead of a desire to produce preferences of a certain sort, but encouraging independence, the endowment effect is at work, there is no avoiding a legal effect on workers’ preferences. Whatever the content of the legal rule, preferences will be affected (if there is an endowment effect). A preference-shaping effect, from the default rule, is inevitable. If this is so, a sensible question is how labor and employment law might create a preferable system of values. To be sure, it is not easy to answer that question, not least because it raises normative issues on which reasonable people will differ. But let me begin with some controversial suggestions: It would be highly desirable if workers placed a very high premium on ensuring that workplaces are safe; in spending time with young children, with relatives who are sick, and with their families; and in making decent provision for retirement. To the extent that default rules will increase workers’ interests in these goods, a switch in the default rule, by employers or by law, would be a good idea. 
In this Essay I have explored the possibility of producing labor law reform through a simple step: Switch the default rule. More particularly, labor law reform might promote a situation in which workers, rather than employers, have more presumptive rights, to be tradeable only through voluntary bargaining. 
Echoing the emerging orthodoxy in behavioral law and economics, I have argued that the default rule might well matter. If the legal rule has an endowment effect, it is potentially important to ultimate outcomes, even in the absence of transactions costs. The principal qualification here is that in some domains, workers and employers might order their affairs with little or no reference to legal rules. To the extent that this is so, a switch in the default rule should not matter; but usually this will not be so. I have also urged that a switch in the default rule, to an initial allocation in favor of employees, might have the fortunate result of ensuring that important information is disclosed to employees – a corrective to what seems to be a “fairness heuristic” by which people identify likely legal rules. By itself this is an argument in favor of the switch. 
I have also urged that considerations of efficiency and distribution are unlikely to argue strongly in favor of maintaining or switching the default rule. If transactions costs are zero, the outcome will be efficient, no matter the initial allocation. If the endowment effect is at work, the outcome will not be the same; but it will be efficient. At first glance, the efficiency criterion therefore seems indeterminate. As a distributional matter, a grant of entitlements to employees might make employees somewhat wealthier. But entrepreneurship, and indifference to certain ascriptive characteristics. See Albert Hirschmann, The Passions and the Interests (19d). market readjusments will ultimately force someone – perhaps workers, perhaps consumers -- to bear the resulting cost, and it is quite possible that the adjustment will swallow the redistributive effect, perhaps through changes in the rest of the wage package. Significant distributive changes should not be expected from switching default rules in labor and employment law. 
We have also seen the possibility that the endowment effect reflects no real difference in terms of welfare under different default rules, but something about the different bargaining situations in which owners and buyers find themselves. Nonetheless, I have suggested several grounds on which a switch in the default rule might be justified. Sometimes such a switch will produce relatively clear improvements in terms of workers’ welfare. If, for example, the consequence of the switch is to increase savings without produce any real harm, the new default rule seems to produce an unambiguous improvement. More controversially, I have suggested that a switch might be justified because of its desirable effects on individual and social valuations of the rights at stake. This is a more controversial basis for a choice of default rule. But in some cases, at least, it seems the only basis on which the choice might be made. Readers with comments should address them to: Cass R. Sunstein Karl N. Llewellyn Distinguished Service Professor of Jurisprudence Law School and Department of Political Science University of Chicago 1111 East 60th Street Chicago, IL 60637 csunstei@midway.uchicago.edu 773-702-9498 William M. Landes, Copyright Protection of Letters, Diaries and Other Unpublished Works: An Economic Approach (July 1991). 
Richard A. Epstein, The Path to The T. J. Hooper: The Theory and History of Custom in the Law of Tort (August 1991). 
Cass R. Sunstein, On Property and Constitutionalism (September 1991). 
Richard A. Posner, Blackmail, Privacy, and Freedom of Contract (February 1992). Randal C. Picker, Security Interests, Misbehavior, and Common Pools (February 1992). Tomas J. Philipson & Richard A. Posner, Optimal Regulation of AIDS (April 1992). Douglas G. Baird, Revisiting Auctions in Chapter 11 (April 1992). 
William M. Landes, Sequential versus Unitary Trials: An Economic Analysis (July 1992). William M. Landes & Richard A. Posner, The Influence of Economics on Law: A Quantitative Study (August 1992). 
Alan O. Sykes, The Welfare Economics of Immigration Law: A Theoretical Survey With An Analysis of U.S. Policy (September 1992). 
Douglas G. Baird, 1992 Katz Lecture: Reconstructing Contracts (November 1992). Gary S. Becker, The Economic Way of Looking at Life (January 1993). 
J. Mark Ramseyer, Credibly Committing to Efficiency Wages: Cotton Spinning Cartels in Imperial Japan (March 1993). 
Cass R. Sunstein, Endogenous Preferences, Environmental Law (April 1993). Richard A. Posner, What Do Judges and Justices Maximize? (The Same Thing Everyone Else Does) (April 1993). 
Lucian Arye Bebchuk and Randal C. Picker, Bankruptcy Rules, Managerial Entrenchment, and Firm-Specific Human Capital (August 1993). 
J. Mark Ramseyer, Explicit Reasons for Implicit Contracts: The Legal Logic to the Japanese Main Bank System (August 1993). 
William M. Landes and Richard A. Posner, The Economics of Anticipatory Adjudication (September 1993). 
Kenneth W. Dam, The Economic Underpinnings of Patent Law (September 1993). Alan O. Sykes, An Introduction to Regression Analysis (October 1993). 
Richard A. Epstein, The Ubiquity of the Benefit Principle (March 1994). 
Randal C. Picker, An Introduction to Game Theory and the Law (June 1994). 
William M. Landes, Counterclaims: An Economic Analysis (June 1994). 
J. Mark Ramseyer, The Market for Children: Evidence from Early Modern Japan (August 1994). 
Robert H. Gertner and Geoffrey P. Miller, Settlement Escrows (August 1994). Kenneth W. Dam, Some Economic Considerations in the Intellectual Property Protection of Software (August 1994). 
Cass R. Sunstein, Rules and Rulelessness, (October 1994). 
David Friedman, More Justice for Less Money: A Step Beyond Cimino (December 1994). (November 2000) 
System (November 2000) 
Relations: A Rational Choice Perspective (November 2000) 
Liability, Class Actions and the Patient’s Bill of Rights (December 2000) 
Approach (December 2000) 
LEGAL REASONING AND ARTIFICIAL INTELLIGENCE: How 
KEVIN ASHLEY, KARL BRANTING, HOWARD MARGOLIS, CASS R. 
SUNSTEIN 
HOWARD MARGOLIS: I look forward to what I'm going to learn this afternoon [November 3, 2000]. Artificial intelligence and the law has its roots about twenty years ago. It has been going rather strong for the last decade, in particular in the hands of two of our guests. 
Karl Branting, who is a professor at the University of Wyoming, is going to speak on some of the philosophical and broader issues. Kevin Ashley has been much more concerned'with the practical problems of creating modules of encapsulated legal judgments that actually work. Cass Sunstein, who as you all know knows everything, will comment. [laughter] 
So we will proceed in a logical fashion. Kevin is going to take twenty minutes because we want to get some concrete examples on the table so we really know what we are talking about. He will take twenty minutes to talk about three concrete examples. Then, Karl will talk in a more philosophical way about how research such as Kevin's links to other things going on in the area of artificial intelligence and scientific discovery and how well it's doing within law. Cass, then, will comment on whatever they say. I am allowed to say as few declarative sentences as possible and to ask questions. [laughte] And so we can proceed, beginning with Kevin Ashley of the University of Pittsburgh School of Law. [applause] 
KEVIN ASHLEY: Thank you very much, it's a pleasure to be here. I'm pretty sure I could not get a roomful of people on a Friday afternoon at the University of Pittsburgh Law School to discuss Af1 and law, but the free booze would help. 
I want to provide three examples of computational models of legal reason1. Ardficial intelligence ing and show what you can do with them. Computational models of analytical legal reasoning are comprised of a knowledge representation and an inference mechanism. The knowledge representation captures some important aspects of legal knowledge. And the inference mechanisms are algorithms that enable a program to use those elements of legal knowledge that are represented in order to solve problems. 
II I! i 1! H ii ii 
"~ Legd inml 
Iun 
Algoidns to. 
om~aireces -, 
M&P,ar w I C~ (a-~) 
C 
D~~3UIIAI 
This is an idealized illustration of a computational model of legal reasoning. It comprises a knowledge representation and an inference mechanism. The knowledge representation is a conceptual hierarchy of legal information dealing with some particular type of legal claim. At the bottom level it relates cases and their facts to the elements of some legal claim and, ideally, ultimately to the legal policies and principles that underlie that legal claim. The algorithms of the inference mechanism use that information. For instance, the inference mechanism may take a problem situation and compare it to other cases in light of the other cases' analyses, draw inferences about how that problem should be decided, and generate arguments using the information in the computational model. 
Now there are three general issues in designing these computational models. 
One is: How does one connect the facts of cases to the statutory elements of a legal claim? My approach in two programs that I have worked on, HYPO2 and CATO, 3 was to introduce an intermediate level of factors. These are stereotypical patterns of facts that tend to strengthen or weaken a plaintiff's argument in support of its claim. In my work, when the program compares a problem to cases, it is comparing them in terms of these factors. In Karl Branting's GREBE program, 4 a different approach was used. 
Secondly, notice that the case texts are not in the model, not in the knowledge representation. This is because AI programs can't read yet. They can't understand natural language text in general. So someone has to represent the facts of the case manually in such a way that the program can know what the facts are and can determine how to analyze the facts. 
The third general problem is how does one implement, how does one represent, the underlying legal principles and policies of a legal claim? How does one implement their roles in analyzing cases? And what about the dialectical role of the cases in filling out the meanings of these abstract legal principles and policies? This is a problem that I have not solved in any program, but I have some ideas and I hope that I can show you a couple of them. 
misappropriation law. See Kevin Ashley, Modeh'ngLgalA gumenk Reasoningwith CasesandHpotheicals(MIT 1990). 
Througb a Model anedExarples(1997) (PhD dissertation, University of Pittsburgh Graduate Program in Intelligent Systems), available at http://www.cs.emu.edu/-aleven/dissertation.htm. See also Kevin D. Ashley, DedgnigEledroni CasebooksThat Talk Back"The CATO Prgram,40JuimetricsJ 275 (2000). 
integrates legal precedents with statutory and common sense rules for legal analysis. See L Karl Branting, Reasoning with Rules andPrecedentrA ComputationalModelofLegalAna#ysis (Kluwer 1999); L. Karl Branting, BuiJngExplanation, with Rules andStrectured Cases, 34 Intl J Man-Machine Studies 1 (1991); L Karl Branting and Bruce W. Porter, Rules andPrecedents as Compkmentay Warrant, Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI-91), Anaheim, California, July 14-19, 1991; L Karl Branting, Reasoning with Poions of Precedents, Proceedings of the Third International Conference on Artificial Intelligence and Law, Oxford, EnglandJune 25-28, 1991. 
Slide 2 5 CATO FaCtOr IE ar 
Y (.AeuHt 97 i~&~w 97 + ++ ++ 
[ -vi( +~ Mhaiht~d + + _ __ _._._. 
Now that's an idealized model. This is about as close as I've come to realizing that type of computational model. This is a Factor Hierarchy that my student, Vincent Aleven, designed for the claim of trade secret misappropriation. At the top level are the elements of the claim. For example, is the information a trade secret? Is there a confidential relationship? Were improper means used? And for each of those there would be a Factor Hierarchy. I'm just showing you the Factor Hierarchy for one: Is the information a trade secret? 
At the bottom level are the various factors, the stereotypical patterns of facts that strengthen or weaken a claim. For instance, here's Fl 5, factor 15: "unique product." It stands for a stereotypical fact pattern that one often sees in trade secret cases. The plaintiff's claim is stronger to the extent that its product is unique in the industry. It's relevant to the issue of whether that information is a trade secret in two different ways. It shows that the information is valuable. It also suggests that the information is not known in the industry. The Factor Hierarchy is a graph, that is to say any given factor on it can have more than one (cited in note 3). 
Symposium: LegalReasoningandAr ifidalIntelligence parent and we'll come back to the significance of that in just a second. 
Factor Hierarchy =>Multiple Intepretations 
nmsures tol ep its infrmaticu secret [F6], ard def. entered into an agement not to coopete vith pltf. [F13], plff. should in a claimof trade secrets misapptiation, as inthe ElcorCase. !Ecoris distinguishable. It is strngr for ptff. than is the current problem In E/cor,pltf.'s prouct was uniqu or the ramdkt [15] and thef= e substantial sinrilarities beteenpltf.'s and def.'s products [F18]. Not so in MBL ... [FI5] is not an inpotantdistinction. InMBL def.'s access to pl.'s info enabled it to develop its product in less firm or atloer cos[t8]. It follosthat inboth cases, pl.'s info was valuable for pl.'sbusiness [P104]. [P15] is an-aceddistincticn Shos that inEco, the info appaently was not knowv outside pl's business [F106], x~Ndeas inMBL, pL's info was known outside pl.'s business [F106]: P. disclosed its prauct info to outsiders [Flo] and pl.'s info wasgenerally known in the industry [MO0]. 
LC-0&RM 
QfM :DAie 4 
One can use a computational model like this to generate legal arguments and even to generate alternative interpretations of cases. This is a sample argument that the CATO program generates for a problem called the MBL case.6 I don't want to get into the details here, but I just want you to see the structure of it. It starts with an argument by analogy (first row of Slide 3). CATO argues that the MBL case should have the same result as the Elcor case,7 that is, the plaintiff should win. And it elaborates an analogy in terms of the factors that the two cases share. Then the program switches hats and argues that Elcor is distinguishable from the MBL case (second row of Slide 3). It's now arguing on behalf of the defendant. It points out, among other things, that the product was unique in Elcor, that is to say that factor F15 applies. But that was a strength that one doesn't find in the MBL problem. 
I would also like you to focus on the last two parts. Here, CATO downplays the significance of that distinction on behalf of the plaintiff (third row of Slide 3). It argues that the fact that the product was unique in Elcor is not an important distinction. It argues that the reason that that factor matters is that it shows that the product is valuable, that it has value. That was abstract factor 104 in the Factor Hierarchy. CATO points out that in the MBL case, there was other evidence that the product was valuable. In other words, CATO is arguing, given the reason why the distinction matters, that these two cases are basically the same. In this last argument, CATO responds for the defendant, it switches hats again, and now it's emphasizing the significance of this distinction (fourth row of Slide 3). It's saying here that the reason that that factor matters is that it shows that the information was not known outside of the plaintiffs business. And it goes on to say that in MBL there's evidence that the information was generally known, and it points to some other factors in the MBL case. In other words, CATO is arguing that, given the real reason why the distinction matters, the two cases are really quite different. In other words, it's doing analogical reasoning here. 
In downplaying and emphasizing this distinction, from where does the knowledge come about why these differences matter from a legal point of view? Well, it comes from the Factor Hierarchy (Slide 2). Let me just show you that again. In making this argument, downplaying and then emphasizing the distinction, CATO is working up from factor F15 on two alternative paths through that Factor Hierarchy. In arguing for the plaintiff that the cases are similar, it's drawing an analogy at the level of that abstract factor 104 that this information is valuable. In arguing that the cases are actually quite different, it's following a different path from F15 through 106 and making a connection to other facts in the MBL case, factor F10 and F20, and using that information to argue at an abstract level that these cases are really quite different. Now, CATO has algorithms that enable it to decide which paths to follow in this Factor Hierarchy and how high up to go in selecting an abstract way of characterizing the significance of the differences. Those are the algorithms. That's the inference mechanism. 
We use this feature in a program that teaches law students basic argumentation skills. And my student Vincent Aleven, in his dissertation, 8 evaluated the CATO program in a controlled experiment involving first-year legal writing students with some good results. Another practical application of it might be as a kind of brief writer's assistant. One could imagine having a specialized Factor Hierarchy and case database that's updated periodically with good coverage of a particular kind of claim and that would help associates in a law firm analyze claims and make arguments. So that's my first example of a computational model. 
My student, Steffi Brdininghaus, is using a similar kind of computational 
Symposium: Legal ReasoningandArhlfidalIntelligence model in a different way. This time it's an attempt to harness the model to help another computer program called SMILE 9 to learn to classify new texts automatically. This SMILE program learns how to assign factors to the raw text of new trade secret cases based on a corpus of manually marked-up texts that we've prepared for the CATO program. So this time, we're trying to bring those case texts into the computational model. 
! Automated Case Indexing 
SMILE (ah*&-AqI5)9 
Algorithm to: *examples • imc text classifiers Cbse am 
SMILE learns from a set of training instances, sentences that are positive or negative instances of a factor in a given case. The four sentences [on the left side of Slide 5] are positive instances of a factor that you've seen before, factor F15: "unique product." It's on the basis of sentences like these that a human reader might conclude that a particular factor applies to the case from which the sentences come. All the other sentences in the case are treated as negative instances of F15. 
Knowkge IoLearningAgorthmsfor lndeng Legal Cares,in Proceedings of the Seventh International Conference on Artificial Intelligence and Law (ACM 1999), available athttp://www.pittedu/-steffi/papers/icai99.ps. 
SMILE uses a learning algorithm called ID310 to learn a decision tree for distinguishing the positive instances of sentences from the negative instances of a particular factor. On the right [of Slide 5] is a part of the decision tree that SMILE actually learned from the positive instances. It corresponds to a rule for classifying sentences. If the text of the sentence includes the term "unique," then conclude that factor F15 applies. Otherwise, if it contains "known" and "general," conclude factor F15 applies, and so forth. It may seem like a naive rule, but in an evaluation we showed that it does a pretty good job of distinguishing the sentences that are positive instances of a factor from those that are not. 
[Plaintiff] intoduc evidence that [plaintiff's product] wasa unkiie product in the induryr. lnmtne s:Bowen "It appeaim that onexmvld notorda [plairtitrs prouct inany I l other tha that of the plaintiff."tan YJa&LmielU Xtiay 'qheinfonmtion in the diagnrn is not generally knm to the public nor to any of [the plaintiffs] conipetitor" Th-Tron PVetto 'Sewralfentures of the proo.s ,mere entirdy unique in [product-type] trnufctnurinSI' y RandYv.R&htn (L0 , 31JYntdt.2 
C(li3h. Xs,.D MAy 
To Assign Facton 
Viv F15 al:e 
pat 9-:A 
R info rut gffle n 
priie F15 ceoies: chis rit c to mT2e Puxid epatle 
F15 lies: aa \ 7 
To refine the decision trees we need to get more mileage out of the examples. Steffi has focused on trying to make the examples more general. For instance, take a look at that first sentence [in Slide 5]. It actually said in the original "Innovative." That's the name of the plaintiff. "Innovative introduced evidence that Parl Brick," that's the name of the plaintiff's product, "was a unique prod10. See Ross Quinlan, C4.5: ProgramsforMaehineLeaming(Morgan Kaufmann 1993). 
Symposium: Legal ReasoningandArfidal ntel'gence uct in the industry." Obviously, the terms "Innovative" and "Panl Brick" don't appear very often in trade secret cases. I made the substitutions of "plaintiff' and "plaintiff's product" manually to make the examples more general. If we can get SMILE, as a program, to make these kinds of substitutions automatically, then it can generalize the instances itself and learn more powerful rules from them. For instance, a rule might be: "If plaintiff's product is unique, then conlude factor F15." We're using information extraction to create better examples for the program to generate more powerful rules. 
Where would this be useful? Well, for one thing, if you've got a program like CATO, SMILE would automatically add new cases to the database. For another, if you are using Westlaw, you know that Westlaw retrieves texts and orders them by statistical criteria. A program like SMILE could process those cases that are retrieved and highlight the important stereotypical facts and weaknesses in the text of the case. 
Okay. Now I'm down to my last example. And the final example is a computational model for practical ethical, rather than legal, reasoning. My former student Bruce McLaren studied a set of more than four hundred decisions of the Board of Ethical Review of the National Society of Professional Engineers. He created a program called SIROCCO" which, given a problem situation, retrieves past ethics cases and ethics code provisions that are relevant to the analysis of the problem. We used that program to investigate empirically an interesting feature of ethical reasoning that also applies to law. 
Normative principles in professional ethics are very abstract rules. For instance, here's an example: "Engineers shall recognize that their primary obligation is to protect the safety, health, and welfare of the public." Now, how does one know how to apply that abstract principle? In ethics, as in law, principles cannot be defined intentionally and applied deductively. There are no readily available sources of authoritative rules that bridge the gap between the high-level principles of the top and the low-level factual scenarios at the bottom. Nevertheless, we observed that, like judges, the Board, in deciding cases, cited relevant ethics code provisions and also cited relevant past cases. And we hypothesized that the decided cases and their explanations of how the principles apply, in effect, flesh out a meaning for those very abstract principles at the top level. We say that the cases "operationalize" the principles. We believed that these casebased extensional definitions of the principles could be represented and also used for improving the program's retrieval ability. Now, notice that this time I have not used factors as a way of bridging the gap between the case facts and the higher level principles. Instead, I've used something called instantiations, Mcar1e1n.,ASsIsReOsiCngCtOhe R(Seylesvteamce ofofCraIsnestealnlidgPendtndRpeletrsieUvsailngoOfpeOrpaieornaatihoznaa6liiozneTdecCniaqsueess(a1n9d99)C O(PdheDs). diSseseertBatriuocne, University of Pittsburgh Graduate Program in Intelligent Systems), available at http://www.pitt.edu/-bmclaren/publications.html. which are the result of the Board's exercise of these operationalization techniques, in effect, to bridge the gap between case facts and abstract principles. 
()peratonaliziig Ethical IP~inciples 
SIROQ3J N&L-&A~iuyvJ 
Retrieve relevant cases / ethics codes ~Th 
fl MSe~ 
Let me show you a little about how operationalizations link facts and abstract principles. [Slide 7] contains a list of the operationalizations that Bruce cataloged during his analysis. These are the Board's techniques for bridging the gap between the abstract codes and the specific fact situations. For instance, code instantiation is a kind of more concrete interpretation of a code in the context of a case. In essence, the Board links combinations of selected facts in the case to a code provision when they instantiate it. For instance, in one case, the Board instantiates that public safety code provision that I cited to you before. ["Engineers shall recognize that their primary obligation is to protect the safety, health, and welfare of the public.'] It was a case involving a building inspection. Basically, the Board said that once an engineer discovers that an apartment building, which he has been hired to inspect, presents a danger and he knows that the building authority should be informed, then he has an obligation to warn of the danger even though his client instructs him to withhold the infor 
Syjbposium: Legal ReasoningandArtfidallntelligence marion. Well, that's a little concrete set of facts with which we annotate the broad principle and then we can use that information in improving the program's retrieval ability. I'll just close by showing you how that works. 
Slide 7 
Operatonalizaflon Techniques Code O ramizaiom 
in NSPE BER Case Set: (1)CadeI1n9sa 
t (e.g ofILl.a. fom90-5-1: "...It appars that gIn eerA, having becorm aware of the irint danger to the stuure, had an obligation to rmke absolutely cera that the teats and public authorities ae ne inmdivately aware ofthe dangers thatexisted ) (2)Apply Codeto HLothdic Scenaio (3)Rewitea Code (4)Gro 
Codes (5) esignaleSipiorCodein Contrt (e.g., .l.a. over ILI.c.finm90-5-1: "...in cases where the public health and safety is endangered, engimers not nly have the righit lht also the ethical responsibility to reveal such fhcts to the ppr persons.') 
We compared SIROCCO to five other methods, including an oblated version of SIROCCO, that is to say, a version of SIROCCO that doesn't use the operationalization knowledge. We called that "Non-Op SIROCCO. 1' (his is a nice feature of a computational model. One can turn a knowledge source off or on at will for purposes of experimentation.) We performed the tests using a case base of 184 foundational cases and fifty-eight trial or test cases. Historically, the fifty-eight test cases came after the foundational cases. For each trial case, we compared the cases and code provisions that SIROCCO, the program, recommended as being relevant with the cases and code provisions that the Board actually said were relevant. And we compared the overlap using a number called the F-measure. It's a combination of precision and recall. The graph [in Slide 8] in note 11). 4Ii I 
II shows the mean F-measure per method over all fifty-eight trial cases. We used two different kinds of experiments here. 
The point I want to make to you now is that in both instances SIROCCO outperformed Non-Op SIROCCO and the differences were statistically significant. This difference is the contribution to retrieval effectiveness that comes from the Board's operationalizations. We were able to sort of bottle it, if you will, and take advantage of it in improving the program's ability to retrieve cases. 
Slide 813 I IICoxtrbution of prfe mict .5 S 0.4-/ II 0.30.2 0 0.1-/ 
So, in conclusion, those are my three examples of computational models and what you can do with them. You can generate arguments with them and even generate alternative interpretations of the significance of important similarities or differences. You can use them to help a program learn to index cases automatically. And you can use them to conduct interesting empirical investigations of such phenomena as operationalizing principles. 
Thank you. [applause] Figure 4-5 (cited in note 11). 
Syposium: LegalReasoningandArlifidalIntelligence 
KARL BRANTING: Thank you, Howard. 
So Kevin has presented several computational models of legal reasoning and how they can be used for analysis and for tutoring. It's my opinion that these models can be useful in jurisprudence for helping to evaluate alternative jurisprudential theories by actually implementing them and testing on examples. But rather than pursuing that question, what I'd like to talk to you about, in the few minutes that I have for my presentation, is to try to make the case that this field is relevant to all of you. I think that, in the long term, it is going to change the character of the American legal system. I am going to make my case slightly more emphatically than I really believe it, just to engender a little bit of discussion about this. 
But anyway, my claim is that the development of computational models of legal reasoning that can actually be used for problem solving-systems that I'll call legal expert systems-are really going to change the practice of law and the American legal system, and that this is going to happen during your careers. To substantiate this claim, I am going to appeal to five separate factors, which I'll enumerate, and then I'll say a little bit about each one of them. 
Here's factor number one, the one I'll talk about the most, the claim that legal expert systems, adequate for many routine legal problems, already exist and new computational models will continue to be developed. That's claim number one. Claim number two is that there is a vast, unmet demand for legal services by the public. Richard Susskind, in his book The Future ofLawp,14 terms this the latent market-all those people out there who can't afford lawyers. Three, the World Wide Web constitutes a kind of electronic infrastructure for the distribution of legal services. Four, legal expert systems constitute a new vehicle for marketing and distributing the expertise of lawyers. And five, funding limitations on governmental bodies, which we know are always short of money, are inevitably going to drive them to automate a larger and larger proportion of the services that they deliver. 
So, in summary, I am suggesting that legal services can be viewed as a kind of commodity, for which the public is the consumer, attorneys are the producers, the World Wide Web is the highway, and legal expert systems are the vehides. And by decreasing distribution costs and increasing economies of scale, legal expert systems will inevitably lead to increased consumption of this commodity. Less efficient producers will inevitably be priced out of the market. So that's my claim. 
Let me consider each of these factors in turn. The one that I'll talk about the most is number one, the claim that legal expert systems adequate for many routine legal tasks already exist. To substantiate this claim actually would take quite a bit of work, maybe a whole semester class would get it started, so I am only going to be able to make a few comments in support of it. 
Let me start by making the observation that there are quite a few different participants in the legal system and, in my view, for each one of these participants there may be various different legal tasks each requiring a separate computational model. By the participants, I mean we have the members of the public, clients, attorneys, judges, clerks, legislators, all people who are participants in the overall process. Just for simplicity, let's hone in maybe on the most typical garden variety sort of legal problem solving episode that we might imagine, which is when an individual comes to consult an attorney about some legal problem. 
What kinds of separate tasks does an attorney perform? Well, I claim that there is a whole series of them. The first one is what I call problem formulation. When a client explains a problem to an attorney, the attorney has to elicit the legally relevant facts, steer the client away from the legally irrelevant stuff (indignation and so forth), and the attorney needs to formulate the problem that is being posed by the client in terms of legally relevant concepts. The second step is retrieval. The attorney, the legal problem solver, needs to think of some legal authorities that are relevant to the problem that has been formulated in the first step. Next is what we might call problem analysis. This is determining what sort of legal consequences might follow from the application of the legal authorities to the facts as elicited by the attorney. Next is the task of prediction. That is, for each of the possible outcomes of some legal action, litigation for example, estimating the probability of those outcomes. What is the expected return on them? How much would it cost to go to trial on a certain issue? How much might you win? How likely are you to win? Other tasks are planning, deciding what sort of actions should be taken on behalf of the client's goals, document drafting, and others. I won't enumerate them all. 
The character of each of these tasks is rather different from the others. Not all of these tasks are amenable to modeling, only a subset. But the subset that, in my view, is most amenable includes the task of retrieving authorities, the analysis task, the prediction task, and the document drafting task. I'll just hone in on those. So far, I am still trying to support my contention that there are models of legal reasoning that are adequate for routine tasks. Number one, retrieval, I'm not going to talk about. There is a long history of AI models doing it. But right now the dominant models, e.g., the LEXIS/Westlaw kind of model, don't use much in the way of Al. I won't talk about that one. Instead I'll start with analysis. 
Analysis. That's the idea that if I have some well-defined facts and I have some well-defined authorities, can I derive some arguments from them? Well, I think that if the problem is sufficiently well posed, then there are a variety of different models for finding arguments for and against a different legal conclusion. The simplest and historically the oldest one simply maps legal rules onto computational rules or logical rules. Such systems are useful if the case facts are 
Symposium: Legal ReasoningandArtfiallntelligence relatively stereotyped and clear-cut and it's the legal rules themselves that give rise to the complexity of case analysis, rather than the vagueness, ambiguity, or context dependency of the legal concepts in those rules. So, in other words, something like the UCC15 is much more appropriate than a legal problem involving reasonable care, let's say. 
Computational models based on these rules were first developed in the 1970s. They are nothing new. They, in turn, were based on logical models developed notably by Layman Allen in the 1950s. 16 So they have been studied for quite a long time. People have also been familiar with the weaknesses of these models for quite a long time, because we know that lawyers don't treat legal rules as a static body of legal formulations but rather legal rules are the tools that lawyers use for achieving their goals. Now the kinds of models that Kevin has been showing you are more sophisticated. They involve reasoning by analogy. As it happens, the earliest analogical models were developed in the 1970s and the early 1980s. These analogical models tend to involve a much larger knowledge acquisition effort. In other words, constructing these systems tends to be a more involved process. 
The second task that has been intensively studied is prediction. As I mentioned, after the analysis, if we are thinking of our interaction between a client and an attorney, there is the legal analysis, but then prediction is an important part as well. Now the largest consumers of predictive systems are insurance companies, which formalize the expertise of claims adjusters and attorneys through a lengthy process of interview and observation to produce systems that predict the settlement value of insurance claims. So there are a large number of such systems, but they are almost always proprietary. Oddly enough, there is not a large amount of literature on predictive systems. 
But in my view, there is reason to believe that all of the participants in the legal system could profit from predictive systems. Psychological studies, notably by Elizabeth E. Loftus and W. Wagenaar,17 have shown that attorneys systematically overestimate their likelihood of success at trial. [kaughtu Why is that? Well, there is a reason for it. Optimism is rewarded. In fact, the most successful trial lawyers are those whose estimates are least realistic, that is, are most overly optimistic. So what does this mean? This means that as an institution, courts are rewarding behavior that isn't optimally beneficial to the system as a whole. In other words, the best strategy for an attorney is not necessarily the best for the client. And it is almost certainly not best for a society as a whole that has to pay for lawsuits that would never take place if people had a realistic estimate of their 
J 833 (1957). 
1988). probability of success or, more precisely, the expected return on the lawsuit. 
A third task is document drafting. As we all know, there is already a large commercial market for the very simplest document drafting systems. There are lots of sophisticated models of document drafting, including some based on state of the art linguistics and speech-act theory. 
So in summary, I think that there is, at least for these three tasks-for document drafting, prediction, and analysis-a history of computational models. They vary widely in their flexibility and explanatory power and development costs. And moreover, the relative merits of these models are, of course, a matter of dispute among computer scientists and scholars of jurisprudence. But the fact is that, at least at the low end, executable models-legal expert systems-already exist. 
So that was all on claim number one. My other claims will be briefer. Factor one, the claim that models exist. Two, the unmet need for legal services. I think I can just appeal to the familiar experience that we all have that attorney fees are quite expensive. As a result, individuals are frequently unable to afford answers to basic legal questions. The cost of getting an answer to a legal question is often greater than the value of the claim that the question applies to. Of course, that is not true of large institutions, but for ordinary citizens this tends to be the case. And these costs are exacerbated by uncertainty in the legal system, which is the result of the fact that the law is in a state of evolution and therefore unsettled, the delays of litigation, which are worse in some places than in others, and the overall lack of predictability in the process. 
So far I've said there are good models. There is also an unmet demand for legal services. A third factor is the World Wide Web. The World Wide Web is a wonderful development for people in my area of study because it provides a uniform computer interface familiar to a very high proportion of litigants and attorneys. It largely eliminates the problem that we used to have in software distribution, that is, hardware inconsistencies and interface inconsistency and unfamiliarity. So even the most technology averse lawyer is likely to be familiar with web browsers, if only because he or she has seen his or her children using them. [Iaughter] 
Claim number four, legal expert systems as a distribution mechanism for legal expertise. It is my observation that the economic motivation for the law firms that are most active in development of web-based legal expert systemssuch as London-based Linklaters, Sidney's Blake, Dawson & Waldren, Ernst & Young, and others 18 -is that, well, first of all, that legal expert systems perform work that wouldn't otherwise be done by the firm. So the idea is that clearly you don't want to make an expert system that you market for less than the amount it would cost one of your own attorneys to perform the same work. But the motivation is that the legal expertise can be marketed to a larger number of consum18. See Alan Cohen, LegalAdice IWithott the Layers, New York LJ (Nov 15, 1999). 
Symposium: LegalReasoningandArifidalIntelligence ers if it is formalized as a computer program that is then delivered over the web. And the second factor is that users of the expert systems may become customers for more complicated and more lucrative personal services. People who pay less to get some advice may then have an incentive to say, "Hey, I want to know some more than this program can deliver to me." The bottom line is that these law firms are betting that legal expert systems can improve the profits that they obtain from the marketing of their legal expertise. 
And finally, the last comment about government services. There is an immense demand for routine legal information from state, local, and federal governments. This has already given rise to quite a few web-based legal expert systems produced by government agencies. Right now there are legal expert systems for the Advisors on Employment Standards Administration (ESA), the Mine Safety and Health Administration (MSHA), the Occupational Safety and Health Administration (OSHA), the Pension and Welfare Benefits Administration (PWBA), and the Veterans' Employment and Training Service (VETS).19 
Web-based delivery of legal services is also a promising strategy for addressing the needs of pro se litigants. Of course, there are actually two arguments about that There is one view that "Geez, if you can't afford a lawyer, how can you afford a computer or how likely are you to be able to understand how to use a computer?" But it is my surmise that familiarity with computers is becoming quite ubiquitous in our society. 
So I've made this claim about the growing economic importance of legal expert systems. Let me end by adding a couple of provisos. There are some countervailing factors. One of them is technical. You may have been struck during Kevin's presentation by the fact that this system that he was showing to you seemed quite elaborate. And it is true that the expertise of these systemswe can imagine them as embodying expertise-is hard to come by. It is a very laborious process to take someone's expertise and formalize it in a manner that is executable on a computer. So the development costs of legal expert systems are very high and, in my view, they are only going to come down when there are significant improvements in knowledge acquisition, that is, the process of automating the formalization of expert knowledge. And then in particular, that improved natural language processing is going to be key. But once again, Kevin's student is a typical researcher engaged in improving those techniques. 
There are also institutional and cultural barriers. A really major one is time billing. Lawyers are apt to be reluctant to make use of legal expert systems to perform some of their work if they bill by the hour and such systems reduce the amount of time it takes them to solve a problem. So time-based billing is antithetical to the acceptance of these techniques. But task-based billing, on the other hand, creates an economic incentive to the automation of the more routine things that you can automate. Partnership promotion practices discourage activities that are not billable. In the firms that I mentioned earlier that have invested a great deal of effort in creating legal expert systems, for internal institutional reasons there is not a lot of internal pressure for the people involved to be billing hours constantly because there is this huge upfront cost in the time of legal experts that is only amortized over the lifetime of the use of the program. 
And the last one is the rather difficult institutional barrier, the ill-defined standards for the unauthorized practice of law. I think that there is going to be litigation on this subject in increasing amounts, because there are many attorneys at the low end of the food chain that are going to be directly threatened, already are threatened, by these systems, who are going to find daims of unauthorized practice of law as a way of attempting to stanch this flood. 
Let me conclude with some predictions concerning the effects on the legal community. I think that legal expert systems aren't going to reduce demand for high-end legal services. In fact, I think, to the contrary, it is going to improve the delivery of high-end legal services by automating some of the more routine aspects. As an example, there is a new product that uses natural language processing to do proofreading pretty effectively. And it is marketed by saying that this is a mechanism to retain associates who would otherwise get so discouraged at being kept up late proofreading documents over and over again that they would move to some other firm. 20 Plausible or not, I don't know. 
On the other hand, the providers of routine legal services are going to face increasing competition from legal expert systems. And I think, as I said, that solo practitioners are already under such pressure. It may be that a new field of legal information engineers is going to develop consisting of attorneys whose job is to organize information for electronic mass distribution. And finally, I think that interactions with low-level government functionaries will increasingly be replaced by simple web-based legal expert systems. Am I out of time? MARGOLIS: Yes. [laughte] BRANTING: Thank you. [applause] 
CASS SUNSTEIN: This is extremely interesting material. A major question is: What can we learn about artificial intelligence and what can we learn about legal reasoning from bringing them into contact? That's what I'm going to try to say something about. 
There's a weak version of the enthusiasm for artificial intelligence in lawweak meaning less ambitious-and that is that this is like really upscale LEXIS and Westlaw. It bears the same relationship to LEXIS and Westlaw as LEXIS and Westlaw bear to Shepard's. In this view, it's extremely helpful for lawyers, who can find a lot of cases quickly. Plug in a problem and they'll see lots of cases like it and potential similarities and differences. That seems to me a convincing claim. Kevin Ashley has demonstrated it. That's the weak version and I'm all for that. 

The strong version, which both speakers actually endorsed, is that artificial intelligence as we now have it can engage in analogical reasoning or does engage in analogical reasoning. To phrase it a little more polemically than is probably fair, I'll say that's just a mistake because at the present state of the art artificial intelligence cannot engage in analogical reasoning or legal reasoning. They can't do it. And the view that they can do it, or are doing it, is based on a misunderstanding of what analogical reasoning is, one that disregards the inescapably evaluative or normative dimension to my claim that one case is "like" another case. To engage in analogical reasoning, to do it, there has to be an evaluative argument showing that this case is like that case. There has to be a principle, and at the current state of the art, artificial intelligence can't generate good principles, or principles at all. I'm hoping this will be helpful. 
Suppose you have someone who's been fired by an employer, a copilot, say, for refusing to fly an airplane on the ground that it's not safe. The employer has fired the copilot, and the copilot wants his job back or wants some money. Let's suppose, to make it very simple, that we're in a jurisdiction in which one court has held that you can't fire someone for refusing to commit a crime and another court has held that you can fire someone for reporting that the bank for which he works hasn't engaged in advertising activity in low-income communities. This is a world with just three cases: the case at hand, one case the employee wins, another case the employee loses. What's to be done? This is a problem in analogical reasoning. 
A going account of analogical reasoning is by Edward Levi,21 and the title of this subsection of my talk is "Levi's Mistake." What Levi suggested was, in engaging in analogical reasoning, judges ask which case is more similar to the case at hand or which case has more similarities to the case at hand. Is it dear that that's not a very helpful way of doing analogical reasoning in our pilot case? Is the pilot case more similar to the bank case or is it more similar to the crime case? To figure that out you can count similarities. But is that what you're going to do? It's not an exercise in counting. You have to do something else, and let's make a little amendment to Levi and say you can search for relevant similarities. Now that's helpful, or at least more helpful than counting for "more." You need to find relevant similarities and HYPO, the computer program, can do that, but that's not helpful enough. To know whether a similarity is relevant, you need to figure out the principle for which the first case stands, and the first case doesn't tell you that. Is the idea in the crime case that you can't fire someone for refusing to inflict harm on third parties? If so, then our pilot maybe is going to be okay. Or is the principle instead you can't fire someone for refusing to commit a crime? If so, then our pilot's in trouble. 
The ideas of "relevant" similarities and "more" similarities are pretty much non-starters. You need to figure out what the principle is that links or separates the various cases. Ronald Dworkin,22 maybe the subsequent generation's Levi, gave some help on this, a kind of clue. He says what you do when you're engaging in legal reasoning is you put the previous decision in the best constructive light. You try to make the best sense out of it. So Dworkin says analogy without theory is blind. An analogy is a way of stating a conclusion, not reaching one, and theory must do the real work, where theory is the principle that links cases or that separates them. The upshot of this is that in any case that's a real case, to figure out whether something's analogous to something else, you have to generate a principle by which the two cases get linked or separated. Lists of factors will be a start, better than Westlaw and LEXIS, but they won't be analogical reasoning. That's not what analogical reasoning is. 
To make progress here, we shouldn't give up on artificial intelligence and its potential. A lot more can be done. Good reasoners are going to deal with our copilot case. Can our copilot be fired? Probably anyone in this room, given ten minutes, could figure out ways of thinking the copilot should win or ways of thinking the copilot should lose by reference to the previous cases, reporting on the bank's violations on one hand and the person refusing to commit a crime on the other hand. But how can we make better progress? One thing we might consider is empirical: What are the consequences if you give copilots a right not to fly planes that they see as dangerous? If they can't lose their jobs for that, you might ask, is that going to make people safer? If so, that's a point for the copilot. If copilots do get this right, if the right is given to them, is this going to make it very much harder to run airplanes? Is this going to decrease convenience and order for airplanes and passengers? Those are empirical questions, which Judge Posner, in the relevant case,23 thought relevant. He's surely right on that. To do the analogical job well, one thing to pursue is these empirical questionsnot empirical in the sense that Professor Ashley suggested, not about collecting cases and factors, but an empirical inquiry into the real world effects of one or another legal rule. There's no reason in principle that a computer can't be helpful with that. 
If we're not going to get empirical, then what we'd want to do is square our judgment of principle about whether the copilot should win with the rest of the things we think in imaginable cases. Then we'd have to be very creative and go beyond the cases at hand, the precedents, and hypothesize lots of analogies and think what makes best sense of our system of labor law insofar as it bears on this. A lot of really good judges go that route. So far as I can tell from Professor Ashley's really quite outstanding book,24 HYPO isn't able to do that. What HYPO can do is come up with cases, and it can be pretty exhaustive in that, telling how they might be similar and how they might be different, but in a kind 
of blind fashion, one that is not alert to the need for a guiding principle that might justify a claim of similarity. HYPO can't do what needs to be done. 
The upshot of all of this is that artificial intelligence in the current state of the art can be a wonderful advance over LEXIS and Westlaw. What Professor Branting suggested seems to me quite convincing-that this can be a real aid. It's not so much different in the analogical domain from a computer program that can just tell you what the rule or law is. That's very good. What can't be done yet is to do analogical reasoning-to do what lawyers, at least decent lawyers or judges, actually do. 
Two qualifications with which I'll end. It may be that in some domains, and I bet that trade secrets is one, you can generate cases that are so sharply hemmed in by precedents that if you look at the precedents in even a kind of crude way, without any principles, you're going to know all you need to know. In some trade secrets cases the fact pattern in question will be one which is not plausibly distinguishable from the precedents. In a case like that, HYPO, or your computer program, is going to do all of the work. In a case like that, by the way, Westlaw and LEXIS are going to do all the work. It's just going to take a little more time with Westlaw and LEXIS than it would with HYPO. 
The second qualification seems to me more interesting for the future. There's no reason, so far as I know, in principle to think that in the long run computers won't be able to make the empirical and principled judgments that a good analogizer has to make. The co-panelists would know a lot more about that. To ask whether the social consequences of one or another rule would be A or B, why can't a computer do that? No reason not. So too for generating good normative principles. If a computer can win chess games against pretty good chess players, why couldn't they do that too? If they're doing that, then they're engaging in legal reasoning. Not yet. [applause] 
MARGOLIS: I'm allowed to ask a question and it happens to have a certain kinship to what Cass was talking about, so I'll ask it and then we'll ask the two other speakers to comment. I have occasion to caution my students against what I call logical democracy. And logical democracy is you just list the arguments on one side and you list the arguments on the other and you count them up and majority wins. And the reason why that's so pernicious is sometimes some arguments are really good and there's a large number'of bad arguments on the other side. And so I share Cass's uneasiness at when the AI systems will be ready for that kind of judgment. Why don't you comment? 
ASHLEY: Well, I'd like to first respond to something that Cass said and that is that although HYPO's arguments might look like lists of factors, in my work and in the work of other people in AI law, we have been moving in a direction that Cass, I think, would approve. For instance, in my example with the argument that CATO generated, it wasn't just factors but it was reasons why the factors mattered to the legal claim. So I am connecting factors to reasons, and 
HeinOnline -- 8 U. Chi. L. Sch. Roundtable 21 2001 the arguments are working with those reasons. In work that's being done by colleagues in Europe, Henry Prakken and Giovanni Sartor, they are representing values, principles that are at stake in cases, and those are being worked into the arguments as well. So at least these concepts, these normative concepts, are being worked into the arguments. 
Now whether judgment is being applied is another question. But I will opt in favor of the weak AI approach. My game is, I think, to try to find ways in which representing knowledge drawn from our models of how we reason in law, how can that knowledge be applied to do a better job of doing those weak tasks, like retrieval of the right cases at the right time or, for the cases that are retrieved, highlighting what's interesting about them, what's useful about them in the context of an argument. My game is not to try to reproduce the hard tasks of legal reasoning so much as to try to use the knowledge of how we do the hard tasks of legal reasoning to try to build better tools to support those tasks. 
We, I think, have been careful not to compare strengths of arguments in terms of numbers. In HYPO there was no comparison of numbers. It was comparisons of sets in terms of set overlap, which is quite a different thing. And we're also, many of us, very sensitive to any attempts to assign numerical weights to anything like principles or values or factors or whatever and to collapse pluses and minuses in that way. We tend to eschew that kind of thing. So I think that we are sensitive to these concerns. We're just gradually working our way upward into the more complicated kinds of arguments that Cass and Howard are talking about. 
BRANTING: I guess I'd have several responses. First of all, the legal expert systems that I was describing would clearly all fall into the weak category. They are useful tools. They may be performing some functions other than just weighing arguments. For example, document drafting and prediction are somewhat different tasks. But I guess I would want to emphasize also that people who work in the field of artificial intelligence and law by and large are sensitive to the fact that analogical legal reasoning is not a mechanical process and that the current computational models don't do an adequate job including the evaluative factors that Professor Sunstein pointed to. 
On the other hand, in my view, what artificial intelligence is about is really two things. One is making useful artifacts. But a second thing is self-knowledge. That is to say, for example, not that this is what you said, but to say that certain kinds of problem solving or certain kinds of analysis cannot be modeled is kind of a way of saying we can't know ourselves, we can't understand how we solve certain problems well enough to define it with a specificity that's required of a computer. So from my point of view, one of the benefits of this field is that the exercise of trying to formalize legal knowledge in a computer-executable fashion is it forces you to make explicit every piece of knowledge that goes into that decisionmaking and can sometimes make obvious the gaps. For example, some 
of the more naive views of legal problem solving are that it is very rule-driven. One of the ways to demonstrate the inadequacy of this naive view is to code up some rules and observe that the resulting system doesn't reason anything like a lawyer. I guess my last comment is that I think that the AI and law field progresses through criticisms of the sort that Professor Sunstein just made. The process we'd like to see is: "Here's the argument that's generated by my system and what's wrong with it?" 
MARGOLIS: You wanted to add to that? 
ASHLEY: I had one thing to add to what Karl just said, and that is that once you have taken the trouble to build a computational model of some interesting phenomenon of legal reasoning, you have a program that works on a range of examples and you can use that as a framework for investigating what that program can't do. So if Cass comes forward with an example of a kind of reasoning that it cannot do, I'm in a position to start playing with the model, to tweak it, change it, see how it has to be revamped or modified. Thus, AI is a kind of empirical methodology for making progress on modeling the phenomena that we all think are so interesting and important. 
MARGOLIS: Let me offer Cass just a moment for a comment on a comment on the comments if he wishes, and then open it to the floor. 
SUNSTEIN: I want to hear what the audience has to say. There's a computer in the back also that I know has a question. [laugbt] 
MARGOLIS: Do we have any questions? Yes, please. 
AUDIENCE: I have a question or a suggestion, and I wanted to get your response, primarily from Mr. Branting and Mr. Ashley. What would you say to the charge that what you are proposing would be the worst thing for legal reasoning, with LEXIS and Westlaw being the second worst, because whatever you produce will ratify the weaknesses of the person doing the inputs. Here is what I mean. With LEXIS and Westlaw, one of the problems, I think, that's happened is that while you can retrieve large numbers of cases based on the contexts or words that you enter, what happens is that you only produce those cases that happen to match the concepts or terms that you were smart enough to pick. And so you get enough case law to produce the set of precedents or a legal reasoning argument that will end up being pretty good. The only dilemma being that you get nothing else that might have made some kind of analogical reasoning that might have given you additional terms you should have looked for. 
It seems like the same kind of thing could happen here, because whether you are using factors or another means of deconstructing a case before inputting into the computer, someone has to make a judgment about what factors are worth mentioning. And somebody conceivably could input ten personal injury cases and put in all the facts, and they could all be the same slip and fall case and never think that it was worth mentioning whether the plaintiff was black or white. What would happen if, in the end, the case was that all of the black plain 
HeinOnline -- 8 U. Chi. L. Sch. Roundtable 23 2001 Products Liabiliy,39 UCLA L Rev 731 (1992). 
far as the quality of advice or analysis that a system is able to produce, the question about describing the facts to a computer system, I think that that is a separate issue, one that I did not have time to get to. 
I listed a number of the tasks that go on in even the most garden variety interaction between an attorney and a client One of them, the very first one, was problem formulation, and when I was listing the things for which we have good computer models, I didn't include that one. I think we don't have a good computer model of this process of taking a sort of undirected narrative by someone who isn't familiar with legal concepts and reformulating it into a fashion that can be manipulated by one of these legal models. What that means is that the consumers of legal expert systems are initially going to fall into two categories, it seems to me. One category is comprised of lawyers who are able to perform this problem formulation themselves. And the second includes people whose problems are extremely stereotyped and for whom the problem formulation is extremely simple. For example, people who want to know advice about, let's say, social security benefits or, maybe, domestic relations. How do I get a divorce? Can I get a protection order? 
AUDIENCE: Computers are progressing very quickly of course, and I was just wondering if you could say very briefly, not what the goal is in the next five years, but what the goal is in the next thirty years, the next fifty years, and the goals for computers in the future? 
BRANTING: The long range goal is for computers to become more like the ones in the movies. That's the goal. 
AUDIENCE: I'd like to take the question that arose before and point it in the other direction. I agree that there's always a problem of choosing input, and therefore we will have limited output. Now, if we go back to the strong AI argument, which is let's let these computers take over at least part of what lawyers do, then what Professor Sunstein said, if I understood correctly, is that there are some things which simply cannot be done with computers. But, are there things which most lawyers, and I'm not talking about the best lawyers, I'm not talking about professors who have the best knowledge and some quantum leaps of thinlking, but most lawyers ... Most lawyers have a very regular way of approaching things. Most lawyers deal with a limited practice of law. They deal with limited case law of which they are aware of or are willing to search for. Therefore these programs, maybe not today, but in the near future, will probably be able to outperform at least the low end of what lawyers do today. If you take that with the economic arguments of if I'm going to buy from the low end of the food chain, then I'm going to get some of these results, wouldn't it be better to pay less, get a computer program which will probably not do the best but will do at least as well as the lawyer I would probably go to anyway and save money? 
SUNSTEIN: Clearly. The issue, I think, is even more interesting than we've gotten a hold of so far. Some of what you say, and some of what's been said, 
HeinOnline -- 8 U. Chi. L. Sch. Roundtable 25 2001 raises interesting issues about what artificial intelligence can ultimately do and also about what legal reasoning really is. You know, there's a joke among the faculty that you could imagine a computer program-I bet someone could do it-that could write a law and economics article about any topic. Choose a topic and promptly it's the case that you take your favorite or least favorite methodology, and you could imagine a computer program that could do that. I think you're absolutely right that these programs can perform as well as or better than really busy people who don't have time to think a whole lot about what's the best principle to construct for an area of law. 
One of the great parts of Professor Ashley's book talks about the relationship between HYPO's performance-HYPO is the computer program-and judicial performance. They're pretty close. That tells us a lot, actually, about the legal system. It shows us that in daily legal reasoning often what does happen is seizing on one or two relevant differences that have been established, not terribly reflectively, as being super salient. That tells us a lot about what our judges are doing. Posner's own opinion in the copilot case26 was pretty brief. My hunch is that a computer program could improve on it a lot, along one dimension certainly, and maybe in a couple of others. The dimension it could certainly improve on is that it would have access to and use a much larger universe of precedents. Judge Posner just used a couple. The computer could give him a whole lot more. And maybe it could refine the principle by forcing him to grapple with the analogies. 
There are intuitive leaps that are involved in chess and in driving. My research assistant, who may be in the room here, found what some of you may not know, that a computer program can drive extremely long distances across the United States at sixty-three miles an hour on average while being able to navigate all but a very small percentage of miles. Now that small percentage is really important if you want to be safe, but it's a small percentage, and what ordinary people would call intuitive leaps for driving, computers can do that. 
A lot of creativity in the law, even by people who are very busy, consists of giving a meaning to a case or series of cases that nobody has seen before. This is much more mundane than it sounds, but really thrilling moments in a lawyer's life are when you can create a new pattern out of preexisting materials. That's where creativity lies and in the last few years alone long established cases have been given exceedingly new meanings. Judge Posner, not so recently, but not ages ago, understood the common law as about promoting economic efficiency2. 7 No one thought that way before, and it gave a whole new meaning to a tremendous pattern of cases. For artificial intelligence really to take off in law, and this probably isn't short term, it would have to become capable of being a little like a literary critic reading a poem, not in the sense of making nonsense 
out of it but in creating a new pattern to it that you didn't see before. That's what even daily lawyers, not the people that are just trying to find out what the law is, but people who actually litigate, that's what they do. 
MARGOLIS: We have time for one last question. 
AUDIENCE: You were talking about models of analogical reasoning. I was wondering if anything has been done with, say, models of statutory interpretation or constitutional interpretation? 
ASHLEY: There has been a lot of work and a lot of progress in representing bodies of statutory rules in a computable way. There has not been very much progress in what we would call statutory interpretation. So, for instance, one sees in civil law jurisdictions people drawing inferences from the structure of the code, for instance, about the meaning of a statutory predicate. We haven't come close to that, I don't think, in AI and law. Also, we have not succeeded in representing the alternative policies that the legislature must have had in mind for a particular statutory provision and trying to look at a problem situation through the statute in light of these alternative policies, actually modeling that. We'd like to, but I don't believe we have yet. 
HeinOnline -- 8 U. Chi. L. Sch. Roundtable 27 2001 
The Arithmetic of Arsenic 
What does cost-benefit analysis mean, or do, in actual practice? When agencies engage in cost-benefit balancing, what are the interactions among law, science, and economics? This Article attempts to answer that question by exploring, in some detail, the controversy over the EPA's proposed regulation of arsenic in drinking water The largest finding is that often science can produce only "benefit ranges, " and wide ones at that. With reasonable assumptions based on the scientific data before the EPA at the time it made its initial decision, the proposed arsenic regulation can be projected to save as few as 0 lives and as many as 112. With reasonable assumptions, the monetized benefits of the regulation can range from $0 to $560 million. In these circumstances, there is no obviously right decision for government agencies to make. These points have numerous implications for lawyers and courts, suggesting both the ease of bringing legal challenges on grounds specified here and the importance of judicial deference in the face of scientific uncertainty. There are also policy implications. Agencies should be given the authority to issue more targeted, cost-effective regulations. They should also be required to accompany the cost-benefit analysis with an effort to identify the winners and losers, to see if poor people are mostly hurt or mostly helped. 
Americans may disagree on a lot of things, but drinking arsenic isn't one of them .... When you turn on the kitchen sink, you ought to be able to drink what comes out, without worrying about being poisoned.' "What we know is a drop, what we do not know-an ocean." In spite of significant gains in knowledge, we are still moving mainly in the dark when dealing with the quantitative importance of risk factors in chemical carcinogenesis, the mechanisms of action of chemical carcinogens, and hence, their detection and the assessment of their risks to human health. The basic understanding... is still missing.2 * Karl N. Llewellyn Distinguished Service Professor of Jurisprudence, Law School and Department of Political Science, University of Chicago. A revised and abridged version of this Article will appear in CASS R. SUNSTEIN, RISK AND REASON (forthcoming 2002). 1am grateful to Laura Warren for outstanding research assistance and to Jonathan Baron, Cary Coglienese, Robert Hahn, Lisa Heinzerling, Christine Jolls, Eric Posner, Richard Posner, and participants in a work-in-progress lunch at the University of Chicago Law School for valuable suggestions on a previous draft. 
CHI. TRIB., July 2 
omitted). 
2255 Because the shape of the dose-response curve in the low-dose region cannot be verified by measurement, there is no means to determine which shape is correct.... [W]hen modeling the risks associated with lower doses, the dose/risk range in which regulatory agencies and risk assessors are most frequently interested, there is a wide divergence in the risk projected by [different models, all of which fit existing.evidence.] ... In fact ... the risks predicted by these ... models produce a 70,000-fold variation in the predicted response.3 Additional epidemiological evaluations are needed to characterize the doseresponse relationship for arsenic-associated cancer and noncancer end points, especially at low doses. Such studies are of critical importance for improving the scientific validity of risk assessment.4 Anyone5 who's read an Agatha Christie mystery knows that arsenic is a poison. 
INTRODUCTION 
Within the past two decades, cost-benefit analysis (CBA) has become one of the most widely discussed topics in all of regulatory law. 6 Much of the discussion is occurring within the three branches of government. The Office of Management and Budget (OMB) has overseen a series of executive orders calling for cost-benefit balancing,7 and OMB has attempted to give concrete guidance for agencies to follow. 8 Courts have adopted a series of cost-benefit default principles, authorizing agencies to engage in cost-benefit balancing unless Congress requires otherwise. 9 Congress itself has shown considerable interest in requiring agencies to compile information on the costs and benefits of regulation.' ° At the same time, there has been renewed academic interest in CBA, resulting in explorations of the technique from a remarkable variety of perspectives. " 
In all of these contexts, the discussion has tended to be quite abstract. Within the legal culture, there has been little discussion of what CBA specifically entails or of how the technique might be used or improved by agencies. 12 To date, there appears to be no sustained investigation within the legal culture of any regulation in which CBA proved pivotal to the outcome. In this Article, I hope to begin to fill the gap. I do so by exploring one of the most contested early decisions of the Environmental Protection Agency (EPA) under President George W. Bush: the suspension of the EPA regulation of arsenic in drinking water.' 3 Much of the contest over that decision has involved a debate about the relevant costs and benefits of the regulation. As we will see, it is possible to draw a range of general lessons from the arsenic controversy. 
My principal finding is simple: Sometimes the best that can be done is to specify an exceedingly wide "benefits range," one that does not do a great deal to discipline judgment. Much of this Article will be devoted to establishing this insufficiently appreciated point, with some effort to specify the judgments that must be made both to identify the health benefits and to monetize them. As a result of this finding, it would be wrong to have confidence that the EPA's proposed rule in the Clinton Administration was either right or wrong, based on the evidence before the agency at the time. I also offer three more positive suggestions. First, CBA, even with wide ranges, provides an important improvement over the "intuitive toxicology" of ordinary people, in which general affect helps to determine judgment.' 4 This intuitive toxicology can lead people to large blunders in thinking about risk, not excluding the public's excessive reaction to the Bush Administration's decision to suspend the arsenic rule issued by the Clinton Administration.- 5 Second, considerable progress could be made by authorizing the EPA both to use market incentives and to target drinking water controls to areas where they would do the most good. Third, the EPA should be required to provide, if feasible, a distributional analysis showing exactly who would be helped and hurt by regulation. In its voluminous materials on the effects of the new arsenic rule, for example, the EPA does not say a word about whether poor people would bear the sometimes significant costs of the regulation. It would be easier to assess the new rule with a clearer sense of the benefited and burdened classes. 
More particularly, I suggest that an understanding of the arsenic controversy offers seven general lessons: (1) The illusion of certainty. CBA can sometimes produce an illusion of certainty.' 6 Even when, as in the arsenic case, science has a great deal to offer, the most that the agency can be expected to do may be to specify a range, sometimes a wide range, without assigning probabilities to various points along the spectrum. This suggestion should be taken as an attack not on CBA, but on what might be described as the false promise of CBA: the thought that science and economics, taken together, can produce bottom lines to be mechanically applied by regulatory agencies. "[T]here is wide recognition among experts-but not necessarily in the public opinion-that current approaches to the regulation of most agents remain judgmental." 17 (2) The wide benefits range. With respect to health benefits, plausible assumptions can lead in dramatically different directions. In the case of arsenic, it would be possible to conclude that the annual number of lives saved from the EPA's proposed regulation would be as low as 5 or as high as 112-and that the annual monetized benefits of the proposed standard would be as high as $1.2 billion or as low as $10 million! It is worthwhile to pay special attention to the dose-response curve, on which direct information is typically absent. I will make a particular effort to connect the legal and economic issues involved in cost-benefit balancing to the underlying scientific questions. (3) The potentially extraordinary power of creative lawyers. If literate in some basic science and economics, an adroit lawyer, on either side, might mount apparently reasonable challenges to any EPA decision about whether and how to regulate arsenic in drinking water. An industry lawyer should be able to urge, not without some force, that any new regulation of arsenic is too severe, because the costs exceed the benefits. An environmental lawyer should be able to urge, not without some force, that nearly any imaginable regulation of arsenic is too lenient, because the benefits of further regulation would exceed the costs. Both challenges would be plausible for a simple reason: It is easy to identify assumptions that would drive the numbers up or down. Hence one of my principal goals is to provide a kind of primer on how informed lawyers can integrate science, economics, and law to challenge regulatory outcomes. 
L.J. 1981 (1998). 
(4) The needfor judicialdeference. In part because of point three, and in light of the scientific and economic complexities, courts should play an exceedingly deferential role in overseeing CBA at the agency level. To say the least, judges are not specialists in the relevant topics, some of which are highly technical, and because good lawyers will be able to raise so many plausible doubts, the best judicial posture is one of deference. In the arsenic case and in many other contexts, agencies must decide in the midst of considerable scientific uncertainty and on the basis of judgments of value on which reasonable people can differ. If agencies have been both open and reasonable, the judicial role is at an end. It follows, for example, that the Clinton Administration's arsenic rule, if it had been finally issued and challenged, should have survived judicial review. It also follows that a less stringent regulation, if it had been chosen by the Bush Administration, should survive judicial review too. The claim for judicial deference, in both cases, is rooted in institutional considerations and above all a sense of the likely problems of intensive judicial review-not in approval of any particular agency decision. Of course courts should invalidate arbitrary or indefensible judgments, but the EPA's approach here was neither arbitrary nor indefensible. (5) CBA as indispensable information. The false precision of CBA is a significant cautionary note, but it should not be taken as a fundamental attack on the method itself, at least if CBA is understood as a way of compiling relevant information. In the arsenic case, an assessment of costs and benefits cannot determine the appropriate regulatory outcome. But even so, the assessment is indispensable to informing the inquiry and to ensuring that discretion is exercised in a way that is transparent rather than opaque. Without some effort to ascertain the effects of regulation, agencies are making a mere stab in the dark. At the very least, an understanding of the data helps show exactly why the decision about how to regulate arsenic is genuinely difficult-and why, and where, reasonable people might differ. This is itself a significant gain. (6) Targeting. The Safe Drinking Water Act (SDWA), 18 designed to control pollution in drinking water, has been amended to require cost-benefit balancing, partly to permit the EPA to relax regulatory requirements when the benefits are low and the costs are high.' 9 At the same time, however, the SDWA continues to have a high degree of rigidity. The EPA is not authorized to impose regulation selectively or in those areas in which regulation would do the most good; it is required to proceed with a uniform, national regulation. 20 The EPA is also forbidden to create trading programs, which might well make best sense for some pollutants. 
Statutory amendments would be sensible here, especially under a statute dedicated to cost-benefit balancing. Regulatory statutes generally should authorize agencies to target regulations to areas where the benefits exceed the costs and should also allow agencies to use market incentives when appropriate. 2 ' (7) The importance of distributional information. It would be extremely valuable to assemble information about the distributionalconsequences of regulation. The benefits of some regulations are enjoyed disproportionately by people who are poor and members of minority groups. 22 The burdens of some regulations are imposed disproportionately on exactly the same groups. To assess the arsenic rule, it would be highly desirable to know whether poor people are mostly helped or mostly hurt. Would they bear high costs? Would the regulation operate as a regressive tax? Unfortunately, the EPA has not answered that question, though it would almost certainly be easy for it to do so. Existing executive orders calling for CBA should be amended to require a careful distributional analysis as well.23 
This Article comes in several parts. Part I offers a general overview of the movement toward cost-benefit balancing, a movement in which the SDWA stands as the most dramatic legislative endorsement. It also gives a brief description of the public outcry over President Bush's decision to suspend the regulation, to fortify the case for CBA. Part II provides a brief outline of the SDWA and of the EPA's rationale in its regulation of arsenic. Part III explores the very different analysis coming from the American Enterprise InstituteBrookings (AEI-Brookings) Joint Center for Regulatory Studies. Part IV-in many ways the heart of the Article-shows how apparently reasonable assumptions lead to a dramatically diverse set of benefit numbers, both monetized and nonmonetized. Part V explores how lawyers and courts might respond to the data. Part VI discusses the role of policymakers and explains that agencies should be permitted to issue targeted regulations and to use economic incentives and that, in keeping with its informational functions, CBA should include a description of the expected winners and losers from regulation. 
A. ARSENIC AND THE PUBLIC 
My principal topic will be the contest over the appropriate analysis of existing data relating to arsenic, but it will be useful to begin with a puzzle. In April 2001, the Bush Administration suspended the Clinton Administration's arsenic regulation after calling for further study.2 4 There seems to be little question that of all the controversial environmental actions of the Bush Administration's first year, the suspension of the arsenic rule produced the most intense reaction. 
fifty-six percent of Americans rejected the Bush decision, whereas only thirtyfour percent approved of it-and that majorities of Americans opposed the decision in every region of the nation. At various points, the public outcry combined concern, certainty, and cynicism. "Arsenic Everywhere, and Bush is Not Helping," according to one newspaper.2 6 "You may have voted for him, but you didn't vote for this in your water," wrote the Wall Street Journal2. 7 In an editorial, the New York Times demanded that "Americans should expect their drinking water to be at least as safe as that of Japan, Jordan, Namibia and Laos," all of which impose a 10 parts per billion (ppb) standard.2 8 A respected journalist asked, "How callous can you get, Mr. Compassionate Conservative? '29 The public reaction came to a head during the legislative debates on the issue, particularly within the House of Representatives, which voted to reinstate the Clinton rule on the theory that arsenic "is a poison.' '3° 
Here is the puzzle: With respect to arsenic, the underlying issues are highly technical, and very few people are expert on the risks posed by exposure to low levels of arsenic. What accounts for the public outcry? I believe that the reason is simple: Arsenic was involved, and so was drinking water. 
These two facts made the controversy seem highly accessible, and it was easy to be outraged. Why was the Bush Administration allowing dangerously high levels of arsenic to remain in drinking water? This appeared to be a rhetorical question. By contrast, many environmental problems are both obscure and technical, and people do not have an easy or intuitive handle on them. Is carbon dioxide a serious problem? Most people have no idea. But arsenic is wellknown, and it is well-known to be a poison, not least because of the exceedingly popular movie, Arsenic and Old Lace.3 1 An influential environmental group, the Natural Resources Defense Council, exploited exactly this reference with its work on the arsenic problem, under the title, Arsenic and Old Laws. 32 The public reaction would have been different if the controversy involved a water contaminant with an obscure name, such as cadmium. 
Ordinary people seem to be "intuitive toxicologists," with a set of simple rules for thinking about environmental risks.33 Among those simple rules is a belief that substances that cause cancer are unsafe and should be banned.34 That intuitive toxicology does not easily make room for issues of degree. It does not accommodate the judgment that low levels of admittedly carcinogenic substances should sometimes be tolerated because the risks are low and the costs of eliminating them are high. It does not show an understanding of the different imaginable dose-response curves and the possibility of safe thresholds or even benefits from low exposure levels. 35 
As part of intuitive toxicology, people rely on the "affect heuristic," through which rapid, even automatic responses greatly affect judgments about risks.36 Consider, for example, the remarkable fact that stock prices increase significantly on sunny days, a fact that is hard to explain in terms that do not rely on affect.37 With respect to risks, affect often operates as a kind of mental shortcut, substituting itself for a more careful inquiry into consequences.3 8 Something very much of this sort has happened with the Bush Administration's suspension of the arsenic standard, partly because of skepticism about President Bush, but mostly because of the associations of arsenic. "If there is one thing we all seem to agree on is that we do not want arsenic in our drinking water. It is an extremely potent human carcinogen ....It is this simple: arsenic is a killer."39 We could easily imagine public outrage over any decision to allow arsenic in drinking water, even if the permissible level were exceedingly low. The outrage would likely be promoted by cascade effects in which people's concern would 
HEALTH ANALYSIS OF ARSENIC OCCURRENCE IN DRINKING WATER, ITS HEALTH EFFECTS, AND EPA's OUTrDATED ARSENIC TAP WATER STANDARD (2000), available at http://www.nrdc.org/water/drinking/arsenic/ exesum.asp. 

(2001), http://papers.ssrn.comlsol3/papers.cfm?abstract-id=265674. 
be heightened by the fact that other people were concerned. Indeed, the Bush Administration's suspension of the arsenic rule seems to have created a cascade effect in which many people objected to the suspension because other (reasonable) people seemed to have objected.4 ° In fact one of the most compelling arguments, within both the House of Representatives and the public at large, was that other countries regulated arsenic at the level of stringency proposed in the Clinton Administration. 4 1 The practices of other countries seemed to operate as a kind of mental shortcut, showing what it is right to do-notwithstanding the reasonable questions that might be asked about the scientific bases for those practices. 
There is a deeper point here. The problems in intuitive toxicology and the crudeness of the affect heuristic seem strongly to support the use of CBA,4 2 understood not as a way to stop regulation, but to ensure that when government acts, it does so with some understanding of the likely consequences. CBA might well be understood as a way of moving beyond intuitive toxicology toward a form of toxicology that is actually supported by data. This point raises some much larger issues, involving significant trends in the nature of government regulation, to which I now turn. 
B. THE EMERGING COST-BENEFIT STATE 
More than any other federal statute, the SDWA, as a result of the 1996 amendments, reflects a strong commitment to cost-benefit balancing. The rise of interest in cost-benefit balancing signals a dramatic shift from the initial stages of national risk regulation. Those stages were undergirded by what might be called "1970s environmentalism," which placed a high premium on immediate responses to long-neglected problems, emphasized the existence of problems rather than their magnitude, and was often rooted in moral indignation directed at the behavior of those who created pollution and other risks to safety and health.4 3 Important aspects of 1970s environmentalism can be found in the apparently cost-blind national ambient air quality provisions of the Clean Air Act44 and in statutory provisions requiring that standards be set by reference to "the best available technology" without a requirement of cost-benefit balancing or even an effort to quantify benefits. 45 
It is clear that 1970s environmentalism has done a great deal of good. It has helped to produce dramatic improvements in many domains, above all in the context of air pollution, where ambient air quality has improved for all major pollutants.4 6 Indeed, 1970s environmentalism appears, by most accounts, to survive cost-benefit balancing, producing aggregate benefits in the trillions of dollars, well in excess of the aggregate Costs. 4 7 But even though the overall picture is no cause for alarm, a closer look at federal regulatory policy shows a wide range of problems. 
Perhaps foremost is the problem of exceptionally poor priority-setting, with substantial resources sometimes going to small problems and with little attention paid to some serious problems.4 8 The point has been dramatized by repeated demonstrations that some regulations create significant substitute risks49-and that with cheaper, more effective tools, regulation could achieve its basic goals while saving billions of dollars. 50 According to one study, each embodying admittedly rough calculations, better allocations of health expenditures could save 60,000 additional lives each year at no additional cost-and better allocations could maintain the current level of lives saved with $31 billion in annual savings.5 1 
have not been rooted in especially controversial judgments about what government ought to be doing. They have been rooted instead in a more mundane search for pragmatic instruments designed to reduce the problems of poor priority-setting, excessively costly tools, and inattention to the unfortunate side-effects of regulation. By drawing attention to costs and benefits, it should be possible to spur the most obviously desirable regulations, to deter the most obviously undesirable ones, to encourage a broader view of consequences, and to promote a search for least-cost methods of achieving regulatory goals.5 z Notice here that so defended, CBA is not only an obstacle to unjustified regulation; it should be a spur to government as well, showing that regulation should attend to neglected problems. If cost-benefit balancing is supported on these highly pragmatic grounds, the central question is whether that form of Implications, in ECONOMIC ANALYSES AT EPA: ASSESSING REGULATORY IMPACT 455, 455-56 (Richard D. Morgenstern ed., 1997); Paul R. Portnoy, Air Pollution Policy, in PUBLIC POLICIES FOR ENVIRONMENTAL PROTECTION 77, 101-05 

18- 
& Jonathan Baert Wiener eds., 1995). 
253-56 (2000); Robert N. Stavins, Market-Based Environmental Policies, in PUBLIC POLICIES FOR ENVIRONMENTAL PROTECTION, supra note 46, at 31, 35-55. 
CostEffectiveness, 15 RISK ANALYSIS 369 (1995). 
balancing is actually producing what can be taken as policy improvements by people with diverse views about appropriate policy. 
On these counts, the record of CBA-at least within the EPA-is generally encouraging. 53 Assessments of costs and benefits has, for example, helped produce more stringent and rapid regulation of lead in gasoline, promoted more stringent regulation of lead in drinking water, led to stronger controls on air pollution at the Grand Canyon and the Navajo Generating Station, and produced a reformulated gasoline rule that promotes stronger controls on air pollutants.5 4 In these areas, CBA, far from being only a check on regulation, has indeed spurred government attention to serious problems. 
CBA has also led to regulations that accomplish statutory goals at lower cost, or that do not devote limited private and public resources to areas where they are unlikely to do much good. For regulation of sludge, protection of farm workers, water pollution regulation for the Great Lakes, and controls on organic chemicals, CBA helped regulators produce modifications that significantly reduced costs.55 With respect to asbestos, an analysis of benefits and costs led the EPA to tie the schedule for phasing down (and eventually largely eliminating) asbestos to the costs of substitutes and also to exempt certain products from a flat ban.5 6 With respect to lead in gasoline and control of chlorofluorocarbons (destructive of the ozone layer), CBA helped promote the use of economic incentives rather than command-and-control regulation.5 7 In this case, economic incentives are much cheaper and make more stringent regulation possible in the first place. For modem government, one of the amnocset wseirtihoeuxsepcruotibvleembsraanpcphearersqutiorebme ennotts atgheant caygeunsceieosfeCnBgaAg,ebiunt sfurecqhuaennatlynsoisn.c5o8mpli 
Of course, CBA is hardly uncontroversial.5 9 Insofar as both costs and benefits are measured by the economic criterion of "private willingness to pay," there are many problems. Poor people often have little ability and hence little willingness to pay. Some people will be inadequately informed and hence show unwillingness to pay for benefits that would make their lives go better.6 ° Perhaps regulatory agencies should seek not private willingness to pay, but public judgments as expressed in public arenas.6 ' Society is not best taken as 
note 46, at 131; Albert L. Nichols, Lead in Gas, in ECONOMIC ANALYSES AT EPA, supranote 46, at 49. 
Regulation?Deeperand Wider Cost-Benefit Analysis, 150 U. PA. L. REV. 1489, 1489-90 (2002). 
Are Distorted,in COST-BENEFIT ANALYSIS, supra note 6, at 269, 292-94. 
204-10 (1993). some maximizing machine in which aggregate output is all that matters. Sometimes a regulation producing $5 million in benefits but $6 million in costs will be worthwhile, if those who bear the costs (perhaps representing dollar losses alone?) can do so easily, and if those who receive the benefits (perhaps representing lives and illnesses averted?) are especially needy. 
In view of these problems, the strongest arguments for cost-benefit balancing are based not only-or even mostly--on neoclassical economics, but also on an understanding of human cognition, on democratic considerations, and on an assessment of the real-world record of such balancing.6 2 All of these points are directly relevant to the arsenic controversy. Begin with cognition: Ordinary people have difficulty calculating probabilities, and they tend to rely on rules of thumb, or heuristics, that can lead them to make systematic errors.6 3 CBA is a natural corrective here. Because of intense emotional reactions to particular incidents, people often make mistakes in thinking about the seriousness of certain risks. 64 Cost-benefit balancing should help government resist demands for regulation that are rooted in misperceptions of facts. Unless people are asked to seek a full accounting, they are likely to focus on small parts of problems, producing inadequate or even counterproductive solutions.65 CBA is a way of producing that full accounting. 
With respect to democracy, the case for CBA is strengthened by the fact that interest-groups are often able to use these cognitive problems strategically, thus fending off regulation that is desirable or pressing for regulation when the argument on its behalf is fragile.6 6 Here CBA, taken as an input into decisions, can protect democratic processes by exposing an account of consequences to public view. With respect to pragmatic considerations, a review of the record suggests that cost-benefit balancing leads to improvements, not on any controversial view of how to value the goods at stake, but simply because such balancing can lead to more stringent regulation of serious problems, less costly ways of achieving regulatory goals, and a reduction in expenditures for problems that are, by any account, relatively minor.67 All of these points help explain the content of the SDWA, as we shall now see. 
LEGAL STUD. 1059, 110 RISK 94-112 (Douglas MacLean ed., 1986). 
JUDGMENT UNDER UNCERTAINTY: HEURISTICS AND BIASES 3, 11 (Daniel Kahneman et al. eds., 1982); see generally Roger G. Noll & James E. Krier, Some Implications of Cognitive Psychology for Risk Regulation, 19 J. LEGAL STUD. 747 (1990). 
SITUATIONS 186 (1996). 
REV. 683, 724-29 (1999). 

A. STATUTORY BACKGROUND 
Regulatory statutes typically instruct agencies to require as much as "feasible ' 68 or to "protect the public health. ' 69 Only a few such statutes expressly require agency decisions to turn on cost-benefit balancing. 70 The SDWA is an intriguing hybrid, combining an analysis of public health and feasibility with reference to CBA as well. Indeed, the cost-benefit provisions of the SDWA go as far as any other federal statute in requiring close attention to costs and benefits, and because Congress has been quite interested in imposing more general costbenefit requirements,7 ' the SDWA might well be a harbinger of the future. For this reason alone, the implementation of the statute is worth careful attention. 
EPA is asked to set "maximum contaminant level goals" (MCLG) for water pollutants.72 The goals must be set "at the level at which no known or anticipated adverse effects on the health of persons occur and which allows an adequate margin of safety. ' 73 In practice, this statutory standard will frequently call for a MCLG of zero because many contaminants cannot be shown to have safe thresholds and because the "adequate margin of safety" language will, in these specific circumstances, seem to support a zero MCLG. 4 Second, the EPA is told to specify "a maximum contaminant level [MCL] for such contaminant which is as close to the maximum contaminant level goal as is feasible."75 The statute defines feasible (not terribly helpfully) to mean "feasible with the use of the best technology, treatment techniques and other means which the [EPA] finds .. . are available.",76 Third, the EPA is required to undertake a risk assessment for pollutants, discussing the level of the danger and the costs of achieving the requisite reduction. 7 The risk assessment is supposed to give an account, for the MCL being considered and for all alternative levels being be available"), 7412(d)(2) (2000) ("achievable"), 7411(a)(1) (2000) ("has been adequately demonstrated"). 

Fungicide, and Rodenticide Act, 7 U.S.C. §§ 136-1 36 y (2000). 

141, 142) (establishing a goal of zero for lead); National Primary Drinking Water Regulations, 66 Fed. Reg. 6976 (Jan. arsenic). But see Chlorine Chemistry Council v. EPA, 206 F.3d 1286 chloroform regulation on ground that setting a MCLG of zero was arbitrary and capricious). considered, of the "[q]uantifiable and nonquantifiable health risk reduction benefits for which there is a factual basis in the rulemaking record"; 78 the "[q]uantifiable and nonquantifiable CoStS"; 7 9 the "incremental costs and benefits associated with each alternative";80 and "[a]ny increased health risk that may occur as the result of compliance, including risks associated with co-occurring 8 contaminants." ' 
The risk assessment is no mere disclosure provision. The SDWA expressly permits-but does not require-the EPA to set a MCL at a level other than the feasible level if it determines that the benefits of that level "would not justify the costs of complying with the level. ' 8 2 On the basis of that determination, the EPA is permitted to set a maximum level "that maximizes health risk reduction benefits at a cost that is justified by the benefits. 8 3 Courts are authorized to review the EPA's judgment about whether the benefits of a certain level justify the costs, but only by asking whether that judgment is "arbitrary and capriCiOHS.,,84 
What does all this mean? The SDWA is quite different from the Toxic Substances Control Act (TSCA), which expressly requires the EPA to base decisions on a simple comparison of costs and benefits.85 The SDWA is more indirect, even circuitous, in its endorsement of cost-benefit requirements. But the difference between the SDWA and the TSCA is more apparent than real. In regulating contaminants in drinking water, the EPA is required to show that its judgment about cost-benefit balancing is not "arbitrary," and this standard is essentially the same as applied under the TSCA.8 6 Perhaps the SDWA gives the EPA somewhat more room for the exercise of discretion. But at most, the difference is one of degree. It is clear that courts are authorized to invalidate an arbitrary or unreasoned assessment on the cost or benefit side or on the question of whether the benefits justify the costs. As we shall see, this point raises many questions for the future. 
B. ARSENIC AND THE FEDERAL GOVERNMENT 
Arsenic is commonly found in nature as a part of the mineral compound arsenopyrite.87 As a result of soil and rock erosion, arsenic is released into the 
1991). 

(Nov. 1998), availableat http://ehpnetl.niehs.nih.gov/docs/1998/106-1 1/innovations-abs.html. water supply, where it can be found in many regions, including New England, eastern Michigan, and the southwestern United States.8 8 It has long been known that arsenic can be toxic, 89 even carcinogenic, 90 and since 1942, the EPA has had in place an arsenic regulation calling for an MCL of 50 ppb.91 But in the past decades, some evidence has suggested that arsenic may have significant adverse effects at levels well below the 50 ppb standard. The principal evidence comes from epidemiological studies in Chile, Argentina, and above all Taiwan. The evidence from these studies suggests that exposure levels of 300 to 600 ppb cause significant increases of various cancers and other adverse effects.9 2 These levels are of course far higher than 50 ppb; but if we extrapolate from the risk at high levels to what might well happen at lower levels, there could be serious reason for concern. I will return to this point shortly. 
In 1996, Congress directed the EPA to propose a new standard for arsenic by January 1, 2000. 9 At the same time, Congress told the National Academy of Sciences and the EPA to study the health effects of arsenic in order to assist the rulemaking effort.94 In 1996, the EPA requested that the National Research Council (NRC) of the National Academy of Sciences conduct an independent review of arsenic toxicity data and recommend changes to the EPA's arsenic criteria. 95 In its 1999 report, the NRC located few studies that examined arsenic effects at low-level concentrations and even fewer studies in agreement.96 A 1995 Japanese study found cancer mortality near or below expectation among persons exposed to arsenic in drinking water at less than 50 ppb.97 Domestic research in the same year revealed no association between bladder cancer risk and arsenic exposure, where eighty-one of eighty-eight Utah towns (ninety-two percent) had concentrations below 10 ppb, and only one town exceeded the 50 ppb standard.9 8 A 1999 assessment of Utah mortality rates, which the EPA described as "the best U.S. study currently available," 99 found no increased 
Drinking Water, 108 ENVTL. HEALTH PERSP. 655 (July 2000), at http://ehpnetl.niehs.nih.gov/docs/2000/ 108p655-661morales/abstract.html. 
(codified at 40 C.F.R. pts. 9, 141, 142). 

Followed for 33 Years, 141 AM. J. EPIDEMIOLOGY 198, 206 (1995); see also SUBCOMM. ON ARSENIC IN DRINKING WATER, NAT'L RESEARCH COUNCIL, supra note 4, at 99 (finding results statistically unstable). 
141 AM. J. EPIDEMIOL. 523, 525-26 (1995). 
at 40 C.F.R. pts. 9, 141, 142). bladder or lung cancer risks after exposure to arsenic levels of 14 to 166 ppb per liter. °° More recent studies in Finland and Taiwan, however, linked increased risks of bladder cancer and cerebrovascular disease to groundwater arsenic consumption as low as 0.1 to 50 ppb.' 0 ' The Taiwan study, with its significant population base, seemed especially impressive. 0 2 
These results could have led the NRC in several different directions. It would not have been entirely astonishing for the NRC to find that the evidence was too inconclusive to support a new rule. Nonetheless, the NRC concluded that the Taiwan studies provided the best available evidence on human health effects of arsenic. 103 The NRC used linear extrapolations from these data to obtain cancer risks at exposure levels below 50 ppb per liter and subsequently recommended that the EPA should significantly lower its current standard.' °4 Indeed, the NRC concluded that "considering the data on bladder and lung cancer noted in the studies ... a similar approach for all cancers could easily result in a combined cancer risk on the order of 1 in 100" from exposure at 50 ppb.'0 5 The 1 in 100 risk figure is a special source of concern because the EPA is usually attentive to environmental risks at or below 1in 1 million,' °6 and a risk of t in 100 seems plainly intolerable. 
Critics attacked the NRC's recommendation on the grounds that Taiwanese cooking and health practices put Taiwanese citizens at greater risk for arsenic toxicity than Americans, as demonstrated by the absence of a single report of U.S. arsenic-induced cancer.' 0 7 The Taiwanese population is much poorer than the American population and suffers from a number of dietary and nutritional deficiencies, including a higher intake of arsenic from food and a deficiency in selenium, zinc, and vitamin B 12, all of which can reduce the toxicity of arsenic. Animal studies even suggest the possibility that arsenic may be a nutritional requirement, though there is insufficient data to indicate any nutritional role in human health.' 0 8 Despite these criticisms, the EPA relied heavily upon the NRC's scientific conclusions when redeveloping its current MCL. 
the ground that no safe level could be identified; an MCL of 3 ppb, on the ground that this was the lowest feasible level; and a regulatory ceiling of 5 ppb, on the ground that the CBA justified this approach but not any more stringent mandate.' 0 9 The EPA also requested comments on regulatory ceilings of 3, 10, and 20 ppb, for which it provided accounts of both benefits and costs. On January analysis as the proposal, but with a crucial change to a regulatory ceiling of 10 ppb rather than 5 ppb." 0 The EPA urged that its assessment of costs and benefits for the four different levels of stringency justified the 10 ppb level."' The rule was to become effective on March 23, 2001, with a compliance date of January 23, 2006. 12 
The new regulation would have required several thousand water systemsserving about 10 million people-to install new equipment.1 3 The overall cost of the 10 ppb standard would have been about $206 million.' 14 This aggregate figure is not entirely informative because the additional payments would vary considerably across the nation. For most households, the annual increase in water bills would be in the range of $30." 5 Water systems with 500 or fewer customers, however, would face significantly higher costs, ranging up to $325 per household." 16 These water systems represent a small fraction of the total number of people affected by arsenic, and they tend to involve rural communities. 
As it was required to do, the EPA also calculated the costs of alternative levels of regulation. A 20 ppb standard would cost about $70 million; a 5 ppb standard, $470 million; and a 3 ppb standard, $790 million." 17 Here, too, the disaggregated figures are important. The most stringent standard of 3 ppb would cost an average of $41 per affected household, while the 20 ppb standard would cost about an average of $24." 8At the extremes, the 20 ppb standard is actually more expensive ($351) than the 3 ppb standard ($317), because of the particular control technologies that would be involved. 1 9 Consider the following summary: 
The EPA did not offer a population-wide breakdown to show the numbers of people served by the various system sizes and to examine whether the people who would bear the costs could do so easily or with difficulty. One analysis, admittedly from a group with a particular point of view, suggests that eightyseven percent of people who consume arsenic at a significant level in their tap water (over 1 ppb) are served by systems serving more than 10,000 customers. 12' This data suggests that almost nine out of ten of the people who will have to pay for water technology would face annual increases of less than $30-not entirely trivial perhaps, but certainly not a huge expenditure. 
Within the EPA, the much harder issues involved the benefits of the 10 ppb requirement.' 2 2 The most easily quantified benefits involve prevented cases of bladder and lung cancer; here the epidemiological data, mostly from Taiwan, allowed quantitative estimates to be made.' 2 3 For two reasons, however, even these estimates should be taken with some grains of salt. The first reason is that there are differences between the population of Taiwan and that of the United States.' 24 The second reason is that a great deal turns on the nature of the 
subsequent evidence appears to have strengthened the case for stringent regulation. See SUBCOMM. ON ARSENIC IN DRINKING WATER, NAT'L RESEARCH COUNCIL, supra note 13. 
dose-response curve. If the curve is "linear," meaning that cancer cases do not drop sharply at low exposure levels, many more cancers will be predicted than if the curve is "sublinear," meaning that after exposure declines to a certain level, the number of cancer cases drops off. 125 Lacking any data on the question, the EPA decided to assume that the dose-response curve is linear, noting that "[t]he use of a linear procedure to extrapolate from a higher, observed data range to a lower range beyond observation is a science policy approach that has been in use by Federal agencies for four decades."' 26 The EPA added that the policy objectives were "to avoid underestimating risk in order to protect public health and be consistent and clear across risk assessments."' 127 From these remarks, it seems clear that the default assumption of linearity is not based on science-which cannot produce a standard default assumption-but on a policy judgment, designed to err on the side of protecting ihmeaplothrtabnyt iesnsusuerbineglowag.1a2i8nst underestimation of the risks. I will return to this 
Armed with the assumption of linearity, the EPA thought that estimates were feasible for bladder and lung cancer and calculated bladder and lung cancer risks using the analysis of the NRC. 129 The NRC used the Taiwan data to calculate a I to 1.5 per 1000 lifetime risk of male fatal bladder cancer at the current 50 ppb standard; it also examined the Chile and Argentina studies and concluded the rates of cancer were comparable to the Taiwan data.' 30 The EPA assessed lung cancer risks, which are known to be two to five times greater than bladder cancer risks, but for many of the health effects from arsenic, the EPA concluded that quantification was impossible.' 3' a. Lives and Health: Quantities. The EPA estimated that the 10 ppb requirement would prevent twenty-one to thirty cancer deaths and sixteen to twenty-six cases of curable cancer.' 32 By comparison, a 20 ppb requirement would prevent ten to eleven deaths and nine curable cancers; a 5 ppb requirement, twenty-nine to fifty-four deaths and twenty-two to forty-seven curable cancers; and a 3 ppb 
presented ten potential dose-response models based upon interpretations of the original Taiwan data. Morales et al., supra note 89. The EPA rejected those models with a comparison population because they resulted in supralinear dose-response relationships (higher than a linear response). 66 Fed. Reg. at had concluded that the dose-response relationship for arsenic at low levels should be either linear or sublinear, with a preference for the latter. Id. The EPA chose the linear model based upon the above-mentioned policies. These various points are treated in detail below. 
mate" consisting of the most probable one. The goal of this suggestion is correct. When the underlying science and economics allow analysts to come up with a "best estimate" and to assign probabilities to the alternative outcomes, this indeed should be done. In terms of monetizing the relevant values, it seems correct to say that the cancer risk deserves a premium as compared to workplace risks, but also to insist on discounting the monetary value of the risk to take account of the latency period and the fewer life-years saved. Hence, rough estimates of $4.5 million per life saved and $1.5 million per nonfatal cancer prevented seem as reasonable as anything else, even if somewhat arbitrary. At the very least, the EPA's $6.1 million figure appears too high in light of the long latency period, and the AEI-Brookings $1.1 million figure appears too low in light of the high discount rate that it reflects and the various factors suggesting that the workplace studies understate the monetary value of the risk involved here. 
But with respect to health benefits, science does not allow best estimates to be provided here. It would be reasonable to suggest that the high estimate of 112 lives saved is unrealistically high and a bit of a scare tactic in light of the problems in the Taiwan data and the probability that the dose-response curve is sublinear. The estimate of zero lives saved is highly improbable. But it does seem to me sensible to move toward concern with life-years saved rather than lives saved, and because of the long latency period, the quantified benefits are most unlikely to be enormously higher than the $210 million price tag.2 ' On the other hand, they might well be higher whether or not they are much higher, and for reasons to be elaborated shortly, the "bottom line" numbers need not be dispositive. 
C. LESSONS 
Does all this suggest that CBA is, in cases of this sort, unhelpful? It would not be hard to imagine an affirmative answer to that question. A skeptic might conclude that because the range of uncertainty is so large, any number at all could be justified and the ultimate decision is essentially political or based on "values." This view is not exactly wrong, but it should not be taken as a convincing challenge to CBA. 
An analysis of benefits and costs cannot resolve the ultimate judgment, but it can certainly inform it. Once we understand the potential effects of different arsenic regulations and see where the uncertainties come from, we are in a much better position to know what to do. Of course a decision on that count will be a product of "values"-how could it be otherwise? The point is that the values should be identified as such, so that when the government acts, its new regulation. New data suggest that the monetized benefits may indeed be much higher than the monetized costs. See SUBCOMM. ON ARSENIC IN DRINKING WATER, supra note 13, at 4. reasons are transparent and explicable. If what I have said thus far is correct, the choice of a new arsenic rule was a genuinely hard question on the data before the EPA in 2000. Under the best case scenario, the benefits will exceed the costs, though perhaps not by a great deal. Under the worst case scenario, the costs will exceed the benefits. It is a tribute to CBA that we know exactly why the ultimate judgment is hard. 
We are now in a position to see the multiple possible challenges to any agency decision, that involves cost-benefit balancing. Because such balancing has become a staple of regulatory practice, it is important for lawyers to have some understanding of the underlying ideas and of how agencies might be said to have gone wrong. There are lessons for courts as well, mostly involving the need for deference to agencies. 
A. LAWYERS: HOW TO MAKE BENEFITS GO WAY UP OR WAY DOWN 
With respect to the regulation of social risks, the legal culture is increasingly required to pay close attention to both science and economics, 21 2 and here legal understandings remain in a primitive state. If we keep in mind the arithmetic of arsenic, we can see how creative lawyers representing water systems or environmentalists might be able to mount plausible challenges to the EPA's decisions regardless (almost) of the content of those decisions. I do not mean to endorse these challenges, but simply to give a sense of what they might look like. The following list catalogues the potential challenges. 
A great deal depends on the dose-response curve, and at low levels the scientific evidence will often be inconclusive. With the assumption of a linear curve, the benefits of regulation will seem far higher than they might otherwise be. But from the scientific point of view, that assumption might be vulnerable. In the case of arsenic, the most striking point is that the independent entities on which the EPA relies actually split on the issue, with the Scientific Advisory Board supporting linearity and the National Research Council tentatively favoring sublinearity. In addition, linearity makes more sense for genotoxic carcinogens, and there is a dispute about whether arsenic is genotoxic. A decision to assume linearity, in the face of scientific uncertainty, is best seen as a (reasonable) policy judgment. Under a statute that calls for a "margin of safety," such a judgment is plainly supportable. It may be that an agency can indulge such a and economic issues involved in regulating particulates and ozone), rev'd, Whitman v. Am. Trucking Ass'ns, 531 U.S. 457 (2001); Corrosion Proof Fittings v. EPA, 947 F.2d 1201 (5th Cir. 1991) (discussing the economic costs and scientific evidence relating to asbestos regulation). judgment under a statute that calls for cost-benefit balancing, but certainly not if science clearly points in the other direction. 2 13 For a lawyer objecting to regulation that seems too stringent, the best claim is that sublinearity is more likely. 2 14 For a lawyer objecting to regulation that seems inadequate, the best claim is that in the absence of specific data, linearity is the standard default assumption on policy grounds. As a legal matter, the EPA is probably on solid ground in relying on the linearity assumptions if the scientific evidence is unclear. 
When regulating a pollutant, the EPA will often have to rely on evidence from other times and areas, and it will be easy to suggest that there are relevant differences between the population at issue and the population from which that evidence derives. In the case of arsenic, the Taiwan data could certainly be challenged as inadequate in light of the absence of data from the United States confirming the basic results. 
If cancer risks are involved, the agency's decision to use its ordinary VSL can be challenged on the ground that good evidence shows a higher VSL for risks that are dreaded and uncontrollable (as cancer risks are likely to be). Lawyers objecting to insufficiently aggressive regulation could use this evidence to suggest that the VSL numbers from workplace studies are simply too low. Lawyers objecting to overaggressive regulation could insist that the only reliable data come from the workplace studies and that any efforts to produce higher numbers are too speculative.21 5 
crude substitutes. These are easily subject to challenge. In the arsenic case, an environmental lawyer could urge that the chronic bronchitis numbers are far too low because a case of cancer is very likely to produce higher willingness to pay than a case of chronic bronchitis.2 16 For their part, industry lawyers could urge that chronic bronchitis is comparable or perhaps even worse, simply because it is chronic. Perhaps a case of cured cancer, even if it is entirely cured, is not much more serious than a case of any other curable disease, and perhaps a Cir. 2000) chloroform is a threshold pollutant under the SDWA). 

REDUCTION (1999). 
serious chronic condition such as chronic bronchitis is more serious than a cured cancer. In either case, it would be easy to challenge the actual numbers used for chronic bronchitis as unreliable, because they were generated through responses by shoppers in North Carolina to hypothetical questions. Even if well-designed, that study is not likely to produce reliable numbers. 
If an agency uses lives rather than life-years, there may be a serious problem, at least if the regulation would protect a large number of children or elderly people. For protection of children, the $6.1 million figure is arguably far too low; for protection of elderly people, that same figure is arguably far too high.2 17 
For a lawyer on either side, it is not hard to argue that unquantified benefits should be quantified, if this is at all possible.2 18 Without quantification, how can agency decisions be evaluated? And once a decision is made to quantify benefits that had formerly been unquantified, agency judgments are subject to challenge because the judgment about how to quantify will be highly speculative. If the agency has not specified a range but has relied on a fairly specific projection, it will be extremely vulnerable. 
The level of monetized benefits will differ dramatically with the chosen discount rate. It would be easy to challenge any agency's decision not to discount a risk that will come to fruition in the future. A monetary loss or a loss to health is worse today than years hence. Once the agency has chosen to discount, any particular discount rate might well be challenged. Economists disagree about the proper approach. If the agency chooses a discount rate for health in the vicinity of the discount rate for money-that is, seven to ten percent-its choice might well be challenged on the ground that no good evidence supports the view that health problems averted should be discounted at the same rate as financial losses averted. But if the agency chooses a discount rate below seven percent, it would not be hard to challenge that choice as essentially arbitrary and unsupported by evidence. 
B. AGAINST SCIENCE COURTS 
Notwithstanding the availability of countless legal challenges, the basic lesson for courts is simple: Hands off. When courts are reviewing an agency's to be wealthier, and they might well be willing to pay large amounts to protect relatively few life years. judgments about health benefits and how to monetize them, they should give agencies the benefit of every reasonable doubt. 
The reasons are threefold. First, the issues are exceedingly complex, and judges are not specialists in the area at hand. Like everyone else, they are prone to error. There is no systematic reason to think that a firm judicial hand will make things better rather than worse. Second, any judicial judgment will perpetuate the status quo and make rulemaking more difficult. 2 '9 Because it is extremely time consuming to make rules and because a clever advocate likely will be able to produce a plausible challenge to whatever an agency does, an aggressive judicial posture will essentially freeze whatever rule is currently in place. In many domains, people have expressed concern with the "ossification" of rulemaking. 220 When a statute calls for cost-benefit balancing, any nondeferential judicial posture will magnify the risk of ossification. Third, many of the underlying decisions involve values, not facts. We have seen that often the choice between a linear and sublinear dose-response curve cannot be based on direct evidence. Any choice has a large policymaking dimension. "[T]here is wide recognition among experts-but not necessarily in the public opinionthat current approaches to the regulation of most agents remain judgmental."2 2' In this light, courts should be reluctant to displace the judgments of administrators, who have advantages both as technocrats and public representatives. 
This does not mean that agencies should be permitted to do whatever they want. We can easily imagine genuinely arbitrary decisions.2 22 But so long as the agency has not done something truly unreasonable, its efforts to quantify and monetize health benefits should be held acceptable. 
A. NO OBVIOUSLY BEST CHOICE 
On the analysis thus far, it should be clear that there was no obviously best choice for the EPA. Of the options considered, the most dramatic would be the two poles: to retain the existing 50 ppb standard or to select the 3 ppb standard-which the EPA deemed feasible. Neither of these choices would have been ludicrous, and neither should be seen as violative of the SDWA. Notwithstanding the NRC report, it would not be entirely irrational to conclude that the existing data-most of it from Taiwan-simply does not justify further restric219. See JERRY L. MASHAW & DAVID L. HARFST, THE STRUGGLE FOR AUTO SAFETY 224-26, 245-46 (1990). 
DuKE L.J. 1385 (1992). 

chloroform rule arbitrary and capricious). tions, especially in light of studies suggesting no adverse effects from low levels of arsenic in drinking water. And notwithstanding the AEI-Brookings study, it would not be impossible to produce numbers suggesting that the 3 ppb standard might well be justified, at least if the nonquantified benefits are taken into account and pegged at the higher points in the range. 
Could it be possible to resolve the controversy through some general, background considerations? When individuals and governments are not sure what to do, they often invoke "second-order" principles designed to simplify the inquiry in the event of difficulty.223 There are several possibilities here. 
One solution would be to invoke the "precautionary principle," which says that reasonable doubts should be resolved in favor of protecting safety, health, and the environment. 224 The precautionary principle has had a significant influence on both national and international environmental policy.22 5 It also seems to track private behavior. People purchase smoke alarms and insurance; perhaps regulation of arsenic can be seen as analogous. In the face of scientific uncertainty, why not make an expenditure that might well turn out to avert serious harm?2 26 In a catchphrase: Better safe than sorry.227 
prevent speculative harms, it is certainly correct. But everything depends on the size of the investment and the speculative nature of the harm. Taken seriously, one problem with the precautionary principle is that it would lead to huge expenditures, exhausting the relevant budget before the menu of options could be thoroughly consulted. Indeed, the precautionary principle would lead to paralysis because there are risks on all sides of the equation.22 8 Recall that many households would be required to spend more than $300 per year for water; EPA Administrator Whitman has expressed a concern that the increased expenditure will lead many people to use small, local wells, which have heavily AND ECONOMICS 187, 192-20 
the UnitedStates, in PROTECTING PUBLIC HEALTH & THE ENVIRONMENT: IMPLEMENTING THE PRECAUTIONARY PRINCIPLE XXi, xxiii (Carolyn Raffensperger & Joel Tickner eds., 1999). 
PRECAUTIONARY PRINCIPLE 1, 3-7 (Julian Morris ed., 2000) 
high-risk outcomes, and would be willing to pay relatively high amounts so as not to run the relevant risks. See Daniel Kahneman & Amos Tversky, ProspectTheory: An Analysis of Decision UnderRisk, in CHOICES, VALUES, AND FRAMES 17, 22-25 (Daniel Kahneman ed., 2000). 
EXPERTS DISAGREE ON ENVIRONMENTAL ISSUES, 75-92 (1996). 
RETHINKING RISK AND THE PRECAUTIONARY PRINCIPLE, supra note 225, at 190-220. polluted water.22 9 In these circumstances, the precautionary principle suggests that new arsenic regulation is undesirable because it might sacrifice lives. Recall too that expensive regulations can have adverse effects on life and health and hence that the $210 million expenditure for arsenic regulation has, as a worst case scenario, significant adverse health effects, with perhaps as many as thirty to forty lives lost. 230 If this is so, the precautionary principle seems to argue against new regulation. It seems clear that precaution, by itself, can be taken to argue for no regulation, much regulation, and every point in between. On reflection, the idea is insufficiently helpful.2 3' 
Perhaps a more refined argument is better. For most of the country, the incremental cost of the arsenic regulation is low-less than $30 per year. If the vast majority of people would receive additional protection at a cost that is high in the aggregate ($210 million, for example) but extremely low for each affected family, shouldn't government proceed with regulation, perhaps with exemptions or subsidies for those who would have to pay more? The argument is not implausible, but it proves too much. In many cases, it would be possible to do some good by asking everyone to pay, say, $2 per year. Should the EPA ask every American to pay $2 year, so as to create a $500 million fund to be used to pay for additional reductions in sulfur dioxide emissions? Carbon monoxide emissions? Benzene emissions? Clean-up of lead paint? Anti-tobacco advertising? Childhood immunizations? Relief of poverty? Because the list of possibilities is endless, it is unhelpful to treat small per-family costs as if they were zero or its moral equivalent; we do better to ensure that those funds are used for purposes that would do more good than harm or for the most possible good. This does not mean that a regulation imposing small per-family costs (say, $30 per family for 200 million people) should be treated as identical to a regulation imposing the same aggregate but higher per-family costs (say, $300 per family for 20 million people). High per-family costs do raise particular concerns. But a regulation that badly fails cost-benefit balancing should not be accepted on the ground that each family or person will pay little. (Imagine a program that would require every American to pay $1 per year for little or no return.) 
occurring at up to 700 and more parts per billion, where the cost of remediation has forced water companies to close, leaving people with no way to get their water, save dig wells. And then they are getting water that's even worse than what they were getting through the water company." Evans, Novak, Hunt & Shields (CNN television broadcast, Apr. with Christine Todd Whitman, EPA Administrator). 
Sunstein, Health-HealthTradeoffs, in SUNSTEIN, supra note 71, at 298-317. 
23 1. For an effort to understand the precautionary principle in a way that takes account of multiple risks, see INDUR M. GOKLANY, THE PRECAUTIONARY PRINCIPLE: A CRITICAL APPRAISAL OF ENVIRONMENTAL RISK ASSESSMENT (2001). 
Yet another tiebreaker is possible. Perhaps the EPA should refrain from further regulation on the theory that government should not act unless there is a clear demonstration that it is desirable, all things considered. Perhaps we should adopt a presumption against regulatory controls unless CBA shows that they are justified or unless there are special reasons-perhaps distributional in characterthat support them. Perhaps the government should not require costly expenditures here in view of the fact that the same expenditures might be used for other goals, such as crime reduction and automobile safety, where they could do more good.232 
The problem is that the same kind of argument could have been used against a wide range of environmental regulations even though those regulations have, on balance, been vindicated by history.2 33 In the context of air quality regulation, a contemporaneous assessment of costs and benefits would in many cases have given rise to the same kind of uncertainty found here. Note that this is not to suggest that in such cases the costs would have been found to outweigh the benefits. The problem is instead that the most that could have been done was to identify a benefits range leaving a great deal of uncertainty about what to do. If the past is any guide, it suggests that inaction in such circumstances would be a foolish course. 
While no particular approach would be obviously best or obviously unreasonable, the more reasonable approaches would appear to be between the poles. On the existing numbers, the 3 ppb standard is hard to justify. No data support the view that there would be significant health gains from moving from a 10 ppb ceiling to one of 3 ppb.23 4 In view of the significant expense of the restriction, 10 ppb seems better. At the same time, the data do suggest that the 50 ppb standard is insufficiently protective. A new regulation might be seen as a kind of insurance policy, one without an enormous price tag. A choice that falls between the 50 and 3 ppb ceiling would seem to be best--especially if it would be possible to relieve the high burdens imposed on some households. Certainly the 10 ppb approach is reasonable. 
This last point raises a more general one, overlooked thus far: The EPA's menu of alternatives has been relatively narrow and has lacked much creativity. The EPA discussed four different permissible exposure levels without thinking more imaginatively about how to minimize the costs of arsenic regulation. The blame for this narrow focus lies not with the EPA, but with Congress. I now discuss several other approaches, designed to show more flexibility toward those burdened by drinking water regulation. 

ppb rule. 
B. ARSENIC TARGETING AND ARSENIC WAIVING 
A possible approach would involve "targeting," that is imposing regulation on water systems when the cost-benefit ratio is especially good. Recall that for much of the country, the cost of compliance with the 10 ppb standard is quite low. On plausible-which is not to say certainly correct-assumptions, the cost-benefit ratio for those systems is adequate to justify the regulation-perhaps even adequate to support a 5 ppb or 3 ppb standard. These points suggest a simple alternative: Impose a targeted rule, with a sliding scale of regulations ensuring that the cost-benefit ratio supports the outcome in each area. When, for example, the annual cost of regulation is less than $50 per household, government might impose a 5 ppb standard; when the annual cost is less than $150, it might impose a 10 ppb standard; when it is less than $350, it might impose a 20 ppb standard. 
An approach of this kind would undoubtedly be controversial. Critics would ask: Why should people in some parts of the country be subject to more arsenic in their drinking water than people in other areas? Why should some people have very low levels of arsenic, and other people less low levels? These questions might seem especially difficult to answer if, as seems likely, many of those subject to the more lenient standard would be relatively poor. Why should poor people, and especially poor children, face levels of arsenic found unacceptably dangerous in other parts of the country? But these questions have much more rhetorical force than they deserve. If acceptable levels of risk are a function of both cost and benefit, it makes perfect sense to say that such levels will vary depending on the costs and benefits of controls in different localities. In some areas of the country, it will be worthwhile to "purchase" an additional increment of safety; in other areas, it will not be. 
This point seems sufficient to suggest that the EPA should have the authority to impose national standards that are not uniform.2 35 But the SDWA forbids any such nonuniform standards. In keeping with its cost-benefit focus, the statute should be amended to allow the EPA greater flexibility. 
If the EPA cannot adopt a targeted regulation, could it allow waivers for areas in which the benefits do not justify the costs? Once the data are disaggregated, it seems reasonable to consider the following option: Adopt the 10 ppb regulation for most water systems, when the per-family cost of compliance is low, but offer a variance for water systems when the per-family cost is high. This approach would be quite close to one involving targeted regulation. The SDWA does allow waivers, but only for short periods of time, and hence waivers produce less satisfactory outcomes than does targeting. 
In contemporary environmental law, some of the most dramatic developments have involved the rise of market instruments for pollution control. These instruments take many forms, but among the most popular are "cap and trade" systems, in which the total level of emissions is capped at a certain level and polluters are allowed to trade licenses as long as the cap is respected.236 A chief advantage of cap-and-trade systems is that they ensure the lowest-cost means of achieving regulatory goals. Those who can eliminate pollution cheaply will do exactly that. Those for whom reductions are expensive will purchase additional permits. 
Why not create a system of tradeable emissions rights, involving the right to subject people to arsenic? The idea may well seem macabre. But if so, the reason is likely to be a belief that arsenic is a poison seriously dangerous at any level. This is a form of intuitive toxicology. If we suppose that within the range under discussion (say 3 ppb to 20 ppb) dangerously high exposure levels will not occur, and we suppose that the issue is one of appropriate degrees of safety, we could easily imagine a cap-and-trade system. For example, government could create an overall cap on arsenic and give licenses to subject people to 15 ppb, but also allow trading. As a result, companies that can reduce at low cost will do so, whereas those that can do so at only high cost will stay at 15 ppb or perhaps buy licenses to subject people to higher levels. Because of the familiar "hot spots" problem, government would, under this regime, take steps to ensure that no one is subjected to unacceptably high levels-say, 25 ppb or higher. 
As compared with a system of national command-and-control, it is likely that a system of this kind would produce much lower costs. Indeed, a system of tradeable rights would likely spur considerable innovation in arsenic control technology, which would be a significant gain. To evaluate it, we would want to know the aggregate cost of the system and also compare the likely benefits to those that would be enjoyed under the alternatives. It is not unimaginable that a properly designed cap-and-trade system would produce both lower costs and higher benefits than the command-and-control alternative. Note in this regard that the Clinton Administration proposed a 10 ppb ceiling to be applied nationally, but that a cap-and-trade system could ensure that people would have levels well below 10 ppb in much of the country. 
As compared with a system of arsenic targeting, the chief advantage of a cap-and-trade system is that it imposes less of an informational demand on the government, allowing the market rather than the EPA to ascertain the costs of arsenic reduction. Under arsenic targeting, the EPA would have to decide, in every area of the country, the real costs of reduction to various points-a difficult determination for which error is inevitable. Under cap and trade, those with low costs will trade their licenses, whereas those with high costs will attempt to acquire more in the way of arsenic rights. Of course the same objections that might be made to arsenic targeting might be made to a system of cap-and-trade. Perhaps poor people will be subject to unusually high arsenic levels. But if these objections are not convincing there, they are also unconvincing in this context. 
Under the SDWA, however, the EPA lacks the authority to implement a trading system for arsenic. This is a serious gap. The statute should be amended to allow the EPA to permit trading if the evidence justifies that step. I emphasize that trading should not be allowed to create what are, under existing science, unacceptable hot spots. 
D. ARSENIC REDUCTION SUBSIDIES 
It might be suggested that the EPA should impose a stringent regulation of arsenic but that the federal government should subsidize communities for which the annual cost is high. Of course the EPA cannot offer subsidies on its own. But perhaps Congress should do so. In fact, Congress has made federal financial assistance available for water systems, and while the relevant programs contain a degree of discretion, it is certainly possible for financially strapped water systems to receive federal help.2 37 
Recall that the total cost of the 10 ppb regulation would be about $210 million each year. To say the least, this would not be a large sum in the federal budget. If the federal government restricted itself to paying the cost of compliance in areas in which the annual per-household cost exceeds $100, its total taxpayer bill would be about $10 million-hardly a large sum to pay. 
This would not be a foolish approach to the arsenic problem. In fact, it seems quite reasonable. The major difficulty is that the numbers do not tell us whether this is the best way to spend limited taxpayer dollars. Suppose that the risks that the regulation is reducing are quite small, so that the regulation will save somewhere between 0 and 0.5 lives. Is it really worth spending $10 million to save between 0 and 0.5 lives? Many government programs are designed to decrease risks to life and health; some of those programs attempt to reduce violent crime. Perhaps the $10 million would be better spent on those programs. It would be possible to say that, as a practical matter, any $10 million subsidy is more likely to come from some other, less valuable use and that by using it to protect people against the health hazards of arsenic, we would not really be diverting resources from a more valuable use. Among the universe of imaginable government expenditures, a $10 million subsidy is hardly the worst. But in light of existing data, we cannot be sure that it is the best. The same considerations that justify cost-benefit balancing in the first place suggest that the hard issues cannot be avoided by arguing for an across-the-board 10 ppb standard accompanied by a federal subsidy for those who face a difficult financial burden. 
(codified at 40 C.F.R. pts. 9, 141, 142). 
E. ARSENIC DISCLOSURE 
An alternative possibility would be to rely less on regulation and more on information. In many domains of regulatory policy, government has moved to replace command-and-control regulation with efforts to require companies to disclose their activities and the accompanying risks to the public.23 8 In the context at hand, the suggestion would be simple: Require companies to meet some statutory requirement-perhaps 30 ppb-so that people are not exposed to clear harm, but beyond that point, require companies to disclose the level of arsenic in their drinking water, perhaps with information that would put the numbers into some context. Perhaps the disclosure requirement would not apply if companies reached some low level (5 ppb?). We could thus imagine a kind of three-tiered rule with flat mandates, a disclosure requirement for a certain range, and a "floor" below which companies would have no disclosure duties. 
For arsenic, this strategy would have both advantages and disadvantages. Because people are not likely to be happy to learn that their drinking water has relatively high levels of arsenic, one advantage of disclosure is that it could spur companies to reduce arsenic levels on their own, without governmental requirements.2 39 For companies who chose that route, it is likely that the reductions would not be terribly expensive. Public pressure may produce low-cost reductions in some areas while also allowing companies to maintain certain levels of arsenic if the public in those areas was not so concerned in light of the mix of health benefits and water costs. In this way, disclosure may even produce a kind of "drinking water federalism." Another advantage of disclosure is that it may perform an important educative role by ensuring that people will learn that some carcinogenic substances are not especially dangerous at low levels and also alerting people to the need for tradeoffs (in the form of a higher water bill). 
But of course there are pitfalls as well. We have seen that the very idea of arsenic in drinking water seems to cause serious public alarm, in part because of the operation of intuitive toxicology. For a certain percentage of the population, disclosure of arsenic would itself signal reason for concern and perhaps produce excessive fear, even panic. Many people may ask why, exactly, companies are disclosing this fact and whether disclosure means that they are, in some sense, being poisoned. The point suggests that sometimes disclosure will not really inform people because their background beliefs will lead them to read the information badly. The question remains whether it is possible to give some contextual information so that people have an accurate sense of what the disclosure actually means. In this context, we should probably be unsure whether the contextual information would really help. 
Performance Benchmarking, Precursor to a New Paradigm?,89 GEO. L.J. 257 (2001); Cass R. Sunstein, InformationalRegulation andInformationalStanding:Akins andBeyond, 147 U. PA. L. Rav. 613 (1999). 
As a legal matter, the issue is simple because the EPA has no authority to use information disclosure as a substitute for regulation. In the particular context of the SDWA, Congress's choice for regulatory mandates may even make sense. But in the future, it would be useful to allow agencies to experiment in this vein to see if disclosure will, in some cases, do more good than alternative approaches. 
F. THE MISSING QUESTION: DISTRIBUTIONAL ISSUES 
There is one significant gap in the discussion thus far: A full account of the distributionaleffects of different arsenic regulations. To have an adequate sense of whether and how to proceed, it would be most valuable to match the assessment of the range of costs of the rule with an account of the income and wealth of those who will be subject to those costs. If, for example, those who would bear $300 or more in increased annual costs are also disproportionately poor, there is good reason for government to hesitate before imposing the regulation. It is easy to imagine a situation in which water quality regulation is "regressive" in the sense that its costs come down especially hard on poor people. That is not a decisive objection to the regulation, but it is certainly an important point to consider. 
It would be easy to imagine the following sort of rejoinder: Shouldn't poor people have water that is as safe as that of wealthy people? Why should poor people, including poor children, have water quality inferior to that enjoyed by wealthy people? The simplest answer is that safety is a matter of degree, and if safer water quality is very expensive, then poor people are better off without it than with it. Cars should certainly be safe, but wealthy people are more likely than poor people to buy Volvos. It would not be a good idea for the government to force poor people to buy Volvos, and the reason is that if you are poor, you might reasonably use what money you have on something other than adding an additional margin of safety to your car. Perhaps you will use that money on food, or medical care, or shelter. The same is true for water quality. If the consequence of decreasing (small) risks is to decrease significantly family income for poor people, then it is perfectly legitimate for the government to refuse to act. Of course it is possible that the benefits of environmental regulation will be enjoyed disproportionately by poor people and that they will bear disproportionately few of the costs. 
The more general suggestion is that whenever an agency is producing a regulatory impact analysis, it should consider a distributional analysis as well. It is important to know who will bear both the benefits and the burdens of regulation. A recent study shows, for example, that the benefits of pollution control in California have gone disproportionately to poor people and minority group members. 240 It would be extremely desirable to assemble similar information for drinking water regulation. 

CONCLUSION 
My aim in this Article has been to cast light on the actual practice of CBA by considering the EPA's most highly publicized decision under the federal statute that most explicitly calls for that form of analysis. The basic message is that even when there is a considerable amount of scientific data, it is possible that CBA will identify only a range of reasonable choices, and often a wide range at that. With plausible assumptions, the nonmonetized health benefits of new controls on arsenic in drinking water can be made to seem very small or very large. Once the health benefits are monetized, the range becomes larger still, making it extremely difficult to compare costs against benefits. 
It would be possible to take this demonstration as an attack on CBA on the ground that a specification of benefits and costs tells us little that we did not already know. 24' If CBA is justified as a way of actually producing decisions in hard cases, CBA has indeed been criticized by the analysis here. But this would be the wrong lesson. As a substitute for intuitive toxicology and for the crudeness of the affect heuristic, an effort to trace both costs and benefits can inform inquiry making decisions less of a stab in the dark. This is indeed a substantial gain. Once the range is specified, a judgment of value, and not of fact, will be involved in the ultimate decision whether or not to proceed. But the judgment of value will be easier to identify once we understand what we understand and what we do not know. A real virtue of CBA is that it helps to explain exactly why the choice of regulation in the case of arsenic is genuinely difficult. In this way CBA is a large improvement over the "intuitive toxicology" seen in the public reaction to the decision of the Bush Administration. 
I have also attempted to provide a kind of lawyer's primer on the law of CBA, showing how future cases might be litigated. There is no question that courts will eventually be asked to assess the kinds of questions raised in this Article.2 42 Lawyers can drive predicted benefits up or down by manipulating the dose-response curve, by raising epidemiological questions, by challenging the discount rate, by asking about the voluntariness and controllability of the risk, and by quantifying difficult-to-quantify risks. We could easily imagine a dozen kinds of opinions invalidating a 10 ppb standard as too stringent; we could easily imagine the same number of opinions invalidating that standard as too lenient. Indeed, we could easily imagine an emerging set of doctrines in which courts produce a kind of common law of CBA. In view of the complexity of the underlying questions, many diverse views would undoubtedly be expressed by federal courts. I have urged that things would be simplified, and generally better, if courts maintained a posture of deference, rejecting agency views only 
evaluate a number of issues explored here, including the appropriate discount rate and both arbitrary and nonarbitrary ways to balance costs and benefits). in cases in which those views are patently unreasonable. There is indeed an emerging common law of cost-benefit analysis, 2 43 but thus far it is being generated by agencies rather than courts. This is entirely proper. 
I have also discussed the underlying policy issues. The EPA could make many reasonable decisions here, and in the range below 50 ppb and above 5 ppb, there is no obviously correct choice. But my principal claims have involved broadening the agencies' view screen. First, agencies should have the flexibility to produce variable standards targeting regulation to areas where it would survive cost-benefit balancing and also adopting economic incentives to ensure low-cost solutions. Second, agencies should be required to identify the winners and losers produced by regulation-to show when poor people or wealthy people are disproportionate losers or gainers. A distributional analysis should not be taken as conclusive, but it will help to inform the analysis. An effort to increase agency flexibility and also to identify both winners and losers would be natural steps not toward placing regulatory judgments in an arithmetic straightjacket, but toward ensuring that when government acts, it does so in a way that is informed by a full account of the consequences. 
Dose-Response Curves 
To evaluate risks associated with toxic substances and to undertake costbenefit analysis, it is often important to have a sense of the dose-response curve. For arsenic, clear evidence is absent. This appendix offers a sense of the possibilities. 
The dose-response curve can have a variety of shapes, including linear, where response increases proportionally with dose. Figure (A)(1) displays the linear relationship between dietary dose of organophosphate insecticide dioxathion and inhibition of the enzyme cholinesterase in rats. 
Figure (A)(2) demonstrates another linear relationship, this one between subcutaneous administration of carcinogenic hydrocarbon dibenzanthracene and tumor incidence in mice. 
Chemicals such as benzene, radon, and formaldehyde exhibit sublinear doseresponse relationships, where elicited responses are less than proportional. Figure (B)(1) displays a sublinear relationship for primary pulmonary (lung) tumors in rats following exposure to plutonium dioxide. 
Figure (B)(2) exhibits a sublinear dose-response relationship between the number of female rat liver foci (a precursor to cancer) and the log dose of phenobarbital expressed as picomole per kilogram. 
Some chemicals produce no adverse effects below a certain level, resulting in a threshold curve. Threshold-model agents include dioxins and chrysotile asbes243. See COST-BENEFIT ANALYSIS, supra note 6, at 310-11. tos; in addition, nongenotoxic carcinogens are generally assumed to have threshold doses. Figure (C)(1) demonstrates a threshold for the carcinogenic hydrocarbon benzpyrene causing sarcomas in mice. 
Dose-response relationships exceeding proportionality, such as vinyl chloride, are supralinear. Figure (D)(1) demonstrates a supralinear curve for the inhibition of carboxylesterase enzyme activities in rats as a function of insecticide dioxathion dose. 
The slight concave-upward pattern in Figure (D)(2) demonstrates a weaker supralinear relationship between exposure to radiation via an atomic bomb and cancer deaths per 10,000 people. 
Hermatic chemicals such as essential nutrients and vitamins exhibit beneficial effects at low doses, coupled with toxic effects at high doses, resulting in a u-shaped curve. The dose-response relationship of fluoride, which exerts positive effects at lower doses but is toxic at high doses, is outlined in Figure (E)(1). (A) LINEAR RELATIONSHIPS 
(1) Cholinesterase 60" 50 40 2o 10 0 
0 100 5 10 15 20 25 S.D. Murphy & K.L.Cheever, Effects of FeedingInsecticides:Inhibitionof Carboxylesterase and CholinesteraseActivities in Rats, 17 ARCHIVES ENVTL. HEALTH, 749 (1968), reprinted in CASARETT AND DOULL's TOXICOLOGY: THE BASIC SCIENCE OF POISONS 19, fig.2-2 (Mary 0. Amdur et. al. eds., Pergamon Press 5th ed. 1996). 
(2) Dibenzanthracene Casarett and Doull's Toxicology: The Basic Science of Poisons 23, fig.2-6 (Mary 0. Amdur et. al. eds., Pergamon Press 5th ed. 1996), modified from W.R. Bryan & M.B. Shimkin, QuantitativeAnalysis of Dose-Response Data Obtained with Three Carcinogenic Hydrocarbonsin Strain C3HMale Mice, 3 J. NAT'L CANCER INST. 503 (1943). 
(1) Plutonium Dioxide S70+ ° l iiiilii l 100 50 0 200.05 0.1 5 10 
40 
C.L. Sanders & D.L. Lundgren, Pulmonary Carcinogenesisin the F344 and Wistar Rat After Inhalationof Plutonium Dioxide, 144 RADIATION RES. 206, 212 (1995). (2) Phenobarbital 
,. ~I Kirk T. Kitchin et al., Dose-Response Relationship in Multistage Carcinogenesis: Promoters,102 ENVTL. HEALTH PERSP. 255, 257 (1994). (C) THRESHOLD RELATIONSHIPS 
(1) Benzpyrene 100 -20 , 
CASARETT AND DOULL'S TOXICOLOGY: THE BASIC SCIENCE OF POISONS 23, fig.2-6 (Mary 0. Amdur et. al. eds., Pergamon Press 5th ed. 1996), modified from W.R. Bryan & M.B. Shimkin, Quantitative Analysis of Dose-Response Data Obtained with Three Carcinogenic Hydrocarbonsin Strain C3HMale Mice, 3 J. NAT'L CANCER INST. 503 (1943). 70 60 a 50 
0 : 40 .2 S.D. Murphy & K.L. Cheever, Effects of FeedingInsecticides:Inhibitionof Carboxylesterase and CholinesteraseActivities in Rats, 17 ARCHIVES ENVTL. HEALTH, 749 (1968), reprinted in CASARETT AND DOULL's TOXICOLOGY: THE BASIC SCIENCE OF POISONS 19, fig.2-2 (Mary 0. Amdur et. al. eds., Pergamon Press 5th ed. 1996). 
(2) Atomic Bomb Radiation 
(1) Carboxylesterase 
Dose in Centi-Sieverts (rems) JOHN W. GOFMAN, RADIATION-INDUCED CANCER FROM Low-DoSE EXPOSURE: AN INDEPENDENT ANALYSIS, fig. 14A, 14F (1990), availableat http://www.ratical.org/radiation/CNR/ RIC/chp 14F.html#fig 14e. (E) U-SHAPED RELATIONSH4IPS 
(1) Fluoride 0 
30 " 20 0 10 -10 
Gordon A. Fox, EVR 2001: Risk and Toxicity 9-10, fig.2 (2000), at http://chuma.cas.usf. edu!-gfox/EVR2001/risk and-toxicity.pdf (last visited Sept. 4, 2002). 
Follow this and additional works at: https://chicagounbound.uchicago.edu/law_and_economics Part of the Law Commons Recommended Citation Cass R. Sunstein, "The Laws of Fear" ( John M. Olin Program in Law and Economics Working Paper No. 128, 2001). 
THE LAWS OF FEAR 
Cass R. Sunstein 
This paper can be downloaded without charge at: 
The Chicago Working Paper Series Index: http://www.law.uchicago.edu/Lawecon/index.html The Social Science Research Network Electronic Paper Collection: http://papers.ssrn.com/paper.taf?abstract_id=274190 
The Laws of Fear 
Cognitive and social psychologists have uncovered a number of features of ordinary thinking about risk. Giving particular attention to the work of Paul Slovic, this review-essay explores how an understanding of human cognition bears on law and public policy. The basic conclusion is that people make many mistakes in thinking about risk and that sensible policies, and sensible law, will follow statistical evidence, not ordinary people. The discussion explores the use of heuristics, the effects of cascades, the role of emotions, demographic differences, the role of trust, and the possibility that ordinary people have a special “rationality” distinct from that of experts. Because people are prone to error, what matters, most of the time, is actual risk, not perceived risk. 
In the late 1980s, the Environmental Protection Agency embarked on an ambitious project, designed to compare the views of “the public” and “EPA experts” on the seriousness of environmental problems.1 The project revealed some striking anomalies, for the two groups sharply diverged on some crucial issues. 
With respect to health risks, the public’s top five concerns included radioactive waste, radiation from nuclear accidents, industrial pollution of waterways, and hazardous waste sites.2 But in the view of EPA experts, not one of these problems deserved a “high” level of concern. Two of the public’s top concerns (nuclear accident radiation and radioactive waste) were not even * Karl N. Llewellyn Distinguished Service Professor, Law School and Department of Political Science, University of Chicago. I am grateful to Daniel Kahneman, Martha Nussbaum, and Richard A. Posner for very helpful comments on a previous draft. 1 Counting on Science At EPA, 249 Science 616 (1990). 2 Id. ranked by EPA experts.3 Of health risks considered by the public, the very lowest ranked were indoor air pollution and indoor radon—both ranked “high” by experts. EPA concluded that there was a remarkable disparity between the views of the public and the views of its own experts. It also noted, with evident concern, that EPA policies and appropriations seemed to reflect the public’s preoccupations, not its own. If law and policy reflect a combination of “hysteria and neglect,” 4 the public’s own concerns may be largely responsible.5 
With respect to risks, the persistent split between experts and ordinary people raises some of the most interesting problems in all of social science. For purposes of understanding these disputes, we might distinguish between two approaches: the technocratic and the populist. Good technocrats tend to think that ordinary people are frequently ill-informed and that the task of regulators is to follow science, not popular opinion.6 On the technocratic view, the central question is what the facts really show, and when people are mistaken on that point, they should be educated so that they do not persist in their errors. Of course technocrats acknowledge that science will often leave gaps and that the proper course of action cannot be determined by science alone. But they urge that facts are often the key issue, and that when they are, government should follow the evidence, rather than public beliefs. 
For their part, populists tend to distrust experts and to think that in a democracy, government should follow the will of the citizenry rather than a selfappointed technocratic elite.7 On this view, what matters, for law and policy, is 3 Id. 4 See John Graham, Making Sense of Risk, in Risks, Costs, and Lives Saved: Getting Better Results from Regulation 183, 183 (Robert Hahn ed. 1996). 5 Of course interest groups play a large role, both independently and in dampening and heightening public concern. 6 This view is represented, in various ways, by Stephen Breyer, Breaking the Vicious Circle(1993); Howard Margolis, Dealing With Risk (1997). 7 This view is represented in Elizabeth Anderson, Value in Ethics and Economics (1993); Lisa Heinzerling Political Science, U Chi L Rev (1995) (reviewing Stephen Breyer, Breaking the Vicious Circle (1995)). See also the remarks of Senator Joseph Biden in Justice Breyer’s confirmation hearings: “The American people have no doubt that more people die from coal dust than from nuclear reactions, but they fear the prospect of a nuclear reactor more than they do the empirical data that would suggest that more people die from coal dust, having coal-fired burners. They also know that more lives would be saved if we took that 25 percent we spend in the intensive care units in the last few months of the elderly’s lives, more children would be saved. But part of our culture is that we have concluded as a culture that we are going to rightly, or wrongly, we are going to spend the money, costing more lives, on the elderly. . . . I think it’s incredibly presumptuous and elitist for political scientists to conclude that the American people’s cultural what people actually fear, not what scientists, with their own, inevitably fallible judgments, happen to think. For populists, ordinary intuitions have normative force, and deserve to count in the democratic arena. 
To make progress on the disagreement, it would be very valuable to have a clearer sense of what, exactly, accounts for the split between experts and ordinary people. Is one or another group biased? Are ordinary people systematically ill-informed, and if so exactly why? Are intuitions likely to reflect mistaken judgments of fact, or worthy judgments of value? Once we answer these questions, there will remain normative problems, raising questions about what should be done in the face of the relevant divisions. Perhaps what matters is not whether people are right on the facts, but whether they are frightened. Perhaps ordinary people have a kind of “thick” rationality, as worthy in its own way as that of experts. Certainly experts can have their own biases and agendas.8 Perhaps the real issue is how to increase the public’s role in risk regulation, so that government will respond to their concerns. 
Of all those who have contributed to an understanding of the division between experts and ordinary people, Paul Slovic has been the most systematic and wide-ranging. Slovic has engaged in a series of empirical studies designed to elicit people’s perception of risk—to see when they are frightened and when they are not, and exactly why. Slovic’s own views defy easy categorization, but he has strong populist leanings. In some of his most striking papers, Slovic urges not that ordinary people are irrational or confused, but that as compared with experts, they display a kind of “rival rationality” that is worthy of consideration and respect.9 Insisting that “risk” is not simply a matter of numbers, Slovic urges that a good system of risk regulation should be democratic as well as technocratic –and that it should pay a great deal of attention to what he sees as the structured and sometimes subtle thinking of ordinary people.10 values in fact are not ones that lend themselves to a cost-benefit analysis and presume that they would change their cultural values if in fact they were aware of the cost-benefit analysis.”7 Confirmation Hearings for Stephen G. Breyer, to be an Associate Justice of the United States Supreme Court, Senate Committee on the Judiciary, 103d Cong., 2d Sess. 42 (July 14, 1994) (Miller Reporting transcript). I take up the relationship between Breyer’s views of risk and Slovic’s findings in Part V below. 8 For a colorful popular treatment, see Sheldon Rampton and John Stauber, Trust Us, We’re Experts (2001). 9 See, e.g., Paul Slovic et al., Regulation of Risk: A Psychological Perspective, in Regulatory Policy and the Social Sciences (Roger Noll ed. 1985). 10 See id. 
The essays in this illuminating and important book, written over a number of years with many coauthors,11 cover a great deal of ground. Among other things, Slovic deals with trust and distrust12; the “social amplification” of risk13; risk-taking by adolescents14; smokers’ (lack of) awareness of the risks of smoking15; the role of emotions in assessing, taking, and avoiding risks16; differences across lines of race and gender17; the nature of “intuitive toxicology”18; and much more. In this space it would be foolhardy to try to examine all of these issues in detail. Instead I will focus on Slovic’s own unifying theme: the different risk judgments of experts and ordinary people. I will also try to connect Slovic’s claims to issues in policy and law, issues with which he deals only briefly. 
In my view, though not necessarily Slovic’s, the overriding message of these essays is that because of predictable features of human cognition, ordinary people deal poorly with the topic of risk. This lesson has major implications for private and public law. . Slovic casts a new light on why the system of regulation has taken its current form, showing some of the cognitive mechanisms that produce “paranoia and neglect.” Some of his principal lessons involve how to make law and policy work—including how to structure information campaigns, which are unlikely to succeed unless their designers have a sense of how people perceive risks. At the same time, I believe that Slovic establishes, with new clarity, why sensible policymakers should generally follow science, not the public. This point bears on the design of government institutions, as well as the functioning of Congress, regulatory agencies, judges, and even juries 
I will make two basic objections to this admirable book. The first is that Slovic says too little about the social mechanisms by which individuals come to think that a risk is serious or instead trivial. These mechanisms have multiple connections with the cognitive points that Slovic emphasizes. Discussion with others, for example, can make a risk both vivid or salient, and when individuals see a risk as salient and vivid, they are likely to talk to others, thus increasing 11 I will refer to the author of the papers discussed here as “Slovic,” for ease of exposition, even though in many cases, the paper in question has more than one author. 12 Perceived Risk, Trust, and Democracy at p. 316. 13 The Social Amplification of Risk: A Conceptual Framework, at p. 220. 14 Do Adolescent Smokers Know the Risks?, at p. 364. 15 Id. 16 The Affect Heuristic in Judgments of Risks and Benefits, id. at 413. 17 Trust, Emotion, Sex, Politics, and Science, in id. at 390. 18 Intuitive Toxicology: Expert and Lay Judgments of Chemical Risks, in id. at 285. both salience and vividness. The second is that some of Slovic’s own findings seem to me to undermine his claim to have found a “rival rationality”—and that whatever Slovic’s intentions, much of the importance of his work is to the strong empirical support that it provides for a more technocratic view of regulation, one that draws ordinary intuitions into grave doubt. As we shall see, both of these objections have implications for law and for government institutions. 
The review comes in several parts, separating Slovic’s claims into various categories. In each part, I offer an outline of the relevant claims, evaluate them, and discuss some of their implications for law. Part I deals with the idea of heuristics, or mental shortcuts. My emphasis here in on the availability heuristic, by which people think a risk is more serious if an example can be readily brought to mind. I also deal with what Slovic calls “intuitive toxicology.” Part II discusses Slovic’s “psychometric paradigm,” the basis for his effort to claim that ordinary people have a kind of “rival rationality.” Part III explores the role of emotions and in particular Slovic’s claim that the “affect heuristic” helps to explain people’s reactions to risks. Part IV discusses some of Slovic’s fascinating findings about demographic differences, knowledge of risks, and trust. Part V offers a brief, general discussion of issues of policy and law. 
In several chapters, Slovic emphasizes that people use heuristics, or mental shortcuts, to assess the presence and magnitude of risks. As Slovic makes clear, he owes a large debt here to Daniel Kahneman and Amos Tversky, who, in a series of pathbreaking experiments, have uncovered several heuristics that people use to assess probabilities.19 Consider, for example, the “availability heuristic,” in accordance with which people assess the probability of an event by seeing whether relevant examples are cognitively “available.”20 Thus, for example, people are likely to think that more words, on a random page, end with the letters “ing” than have “n” as their next to last letter21—even though a moment’s reflection will show that this could not possibly be the case. 
In the relevant experiments, Kahneman and Tversky were focussing on cognition in general, and they did not deal with policy issues, or with people’s 19 See Judgment Under Uncertainty: Heuristics and Biases (Daniel Kahneman and Amos Tversky eds. 1983), for an overview. For an illuminating discussion, see Daniel Kahneman and Shane Frederick, Attribute Substitution (unpublished manuscript 2001, forthcoming). 20 See note 18 supra. 21 Id. evaluation of social risks. Slovic’s major contribution is to show the great importance of the availability heuristic in helping to generate ordinary judgments about risks to health, safety, and the environment. But Slovic also shows a more general way that ordinary people go wrong. They rely on “intuitive toxicology,” which contains a range of scientifically implausible judgments, many of them apparently working as mental shortcuts. Moving beyond individual cognition, Slovic also urges attention to the “social amplification” of risk. In this Part, I outline Slovic’s findings and offer one criticism, or perhaps friendly amendment: Slovic seems to me to have paid too little attention to the social forces by which people come to fear, and not to fear, certain hazards. I provide a brief discussion of how this gap might be filled. 
In Slovic’s view, “[t]he notion of availability is potentially one of the most important ideas for helping us understand the distortions likely to occur in our perceptions of natural hazards” (p. 14). These distortions have concrete consequences for behavior. For example, whether people will buy insurance for natural disasters is greatly affected by recent experiences (p. 40). If floods have not occurred in the immediate past, people who live on flood plains are far less likely to purchase insurance (id.). In the aftermath of an earthquake, insurance for earthquakes rises sharply—but it declines steadily from that point, as vivid memories recede (id.).22 
Note that the use of the availability heuristic, in these contexts, is hardly irrational. Insurance can be expensive, and what has happened before seems, much of the time, to be the best available guide to what will happen again. Imperfectly informed people might do well to rely on the availability heuristic. The problem is that the availability heuristic can lead to serious errors of fact. Here as elsewhere, the use of a reasonable heuristic can produce decisions that are wrong, from the standpoint of those who have an accurate of the actual probabilities. 
Do people know which risks led to many deaths, and which risks lead to few? They do not. In fact they make huge blunders. In some especially striking studies, Slovic demonstrates that the availability heuristic helps to explain people’s mistakes in assessing the frequency of various causes of death. In one study, people were told the annual number of deaths from motor vehicle 22 There is a link here with Slovic’s claims about the role of affect; see below. accidents in the United States (at the time about 50,000), and then asked to estimate the number of deaths from forty other causes of death (pp. 106-09).23 In another study, people were given two causes of death and asked to say which produced more fatalities (p. 38). People tended to make large mistakes, and when they did so, the availability heuristic was partly responsible. “In keeping with availability considerations, overestimated items were dramatic and sensational whereas underestimated items tended to be unspectacular events which claim one victim at a time and are common in non-fatal form.” (p. 107). Specifically, people significantly overestimated highly publicized causes of death, including tornadoes, cancer, botulism, and homicide. By contrast, they underestimated the number of deaths from stroke, asthma, emphysema, and diabetes (id.). At the same time, people tend to think that the number of deaths from accidents is higher than the number of deaths from disease, whereas the opposite is true. In the same vein, people mistakenly believe that more people die from homicides than from suicides. Availability can also “lull people into complacency,” as when certain risks, not easily accessible, seem invisible, and what is out of sight is “effectively out of mind” (p. 109). 
These points suggest that highly publicized events are likely to lead people to be exceedingly fearful of statistically small risks.24 Both law and policy are likely to be adversely affected by people’s use of mental shortcuts. Public officials, no less than ordinary people, are prone to use the availability heuristic.25 And in a democracy, officials, including lawmakers, will be reactive to public alarm. If people are extremely concerned about the risk of airplane accidents, we should expect aggressive regulation of airlines, perhaps to the point of diminishing returns. If people are worried about abandoned hazardous waste dumps, we might well expect a large amount of resources to be devoted to cleaning them up, even if the risks are relatively small.26 Similar problems will 23 There is an important point here, not discussed by Slovic: People have little idea what number is in the ballpark, and they need some kind of “anchor,” in the form of an annual death toll for some familiar activity, in order to reduce, to a manageable degree, the “noise” in the data. For a more recent study in the same vein, see W. Kip Viscusi. On the centrality of anchors when people have a hard time generating “ballpark figures,” see Cass R. Sunstein, Daniel Kahneman, and David Schkade, Assessing Punitive Damages, Yale LJ (1998). 24 For a vivid demonstration in the context of catastrophes, see Jacob Gersen, Strategy and Cognition: Regulatory Catastrophic Risk (unpublished manuscript 2001). 25 See Roger Noll and James Krier, Some Implications of Cognitive Psychology for Risk Regulation, 19 J Legal Stud 747, 749-60 (1990); Timur Kuran and Cass R. Sunstein, Availability Cascades and Risk Regulation, 51 Stan L Rev 683 (1999). 26 For evidence, see James Hamilton and W. Kip Viscusi, Calculating Risks (1999); Timur Kuran and Cass R. Sunstein, supra note. appear in courts, with juries and judges taking “phantom risks” quite seriously.27 There is also a lesson here about how to attract public attention to a risk: Make a vivid example of its occurrence highly salient to the public. This way of proceeding, far more than statistical analysis, is likely to activate public concern. 
Are ordinary people toxicologists? Slovic thinks so (p. 285). He uncovers the content of “intuitive toxicology” by comparing how experts (professional toxicologists) and ordinary people think about the risks associated with chemicals. The result is a fascinating picture. It is not clear that any identifiable heuristics are at work in intuitive toxicology. But it is clear that people are using mental shortcuts, and that these lead to errors. 
Slovic elicits the views of toxicologists and ordinary people on the following kinds of propositions (p 291): animals, then we can be reasonably sure that the chemical will cause cancer in humans. now than ever before. chemicals. malformed children had been born there during each of the past few years. The town is in a region where agricultural pesticides have been in use during the last decade. It is very likely that these pesticides were the cause of the malformations. 
Ordinary people agree with such statements, by pluralities or even majorities (p. 291). By contrast, toxicologists disagree with such statements, usually by overwhelming majorities (id.). What are ordinary people thinking? 27 See Phantom Risk, supra note, at 425–28 (discussing scientifically unsupportable outcomes involving “traumatic cancer” and harm to immune systems); id. at 137-46 (discussing lawsuits with unclear scientific basis). Can we discern some structure to their thinking? Three beliefs seem to be playing a large role. First, many people appear to believe that risk is an “all or nothing” matter. Something is either safe or dangerous, and there is no middle ground.28 Second, many people seem committed to a belief in the benevolence of nature. They think that the products of human beings, and human activities, are more likely to be dangerous than the products of natural processes.29 Third, many people tend to subscribe to the “zero risk” mentality, at least in some domains. Many people believe that it is both possible and appropriate to abolish risk entirely, a belief that appears closely connected with the notion that risk is a matter of “all or nothing.” 
Experts believe that all three beliefs are false. Moreover, it seems clear that with respect to them, experts are thinking far more clearly than are ordinary people. Why do people think this way? It might well be that at least some of these ideas work well in most contexts in which nonspecialists find themselves.30 People want, for example, to know whether an activity is “safe,” not to know about the statistical probability of harm; and the excessively simple category of “safe” can tell them essentially what they need to know.31 The problem is that ideas of this kind misfire in contexts in which regulatory choices, and some daily decisions, have to be made. Nonspecialists may do well to rely on such principles, but policymakers should do a good deal better. 
Slovic also finds that experts do not entirely agree among themselves (p. 311; pp. 292–93). Most interestingly, toxicologists employed by industry are far more optimistic about chemical risks than toxicologists employed by government or academic institutions; there is a large “affiliation bias,” so that people tend to believe what their institution would want them to believe (p. 311). I shall have 28 The point is connected with an aspect of prospect theory, described as the “certainty effect,” in accordance with which “people overweight outcomes that are considered certain, relative to outcomes that are merely probable.” See Daniel Kahneman and Amos Tversky, Prospect Theory: An Analysis of Decision Under Risk, in Choices, Values, and Frames 17, 20 (Daniel Kahneman ed. 2000). 29 For an interesting challenge from the perspective of ecology, see Botkin, Adjusting Law to Nature’s Discordant Harmonies, 7 Duke Env. Law & Policy Forum 25 (1996). 30 This is a standard defense of most heuristics. See Tversky and Kahneman, supra note; Gerd Gigerenzer, Simple Heuristics That Make Us Smart (1999). 31 It cannot, however, be said that a belief in the benevolence of nature is a sensible heuristic. In fact, this is a dangerous idea, because the unnatural is often safer than the natural. See Alan McHughen, Pandora’s Picnic Basket (2000). On ecology, see Botkin, supra note. The belief in the benevolence of nature might well be a holdover from certain theological views. See Janet Radcliffe Richards, Human Nature After Darwin (2001). more to say about that particular bias below. But the differences among toxicologists are dwarfed by the differences between toxicologists and ordinary people. We should conclude that one’s social role will move one’s judgment in predictable directions, and that experts are likely to be biased if they are working with someone having a stake in the outcome—but also that even acknowledging this point, experts are, on many fundamental issues, to be in basic accord with one another. 
As Slovic is aware, mental shortcuts do not operate in a social vacuum, and interpersonal influences can play a large role. With respect to risks, most of us, most of the time, lack independent knowledge, and we must therefore rely on the beliefs of others. 
Slovic’s principal treatment of this point comes in a discussion of what he calls the “social amplification of risk” (p. 232). Strikingly, the major point of this discussion is not to explain how social influences affect people’s perception of risks, but instead to show what might be missed by conventional efforts to tabulate the costs and benefits associated with risks. For example, the 1979 accident at Three Mile Island “demonstrated dramatically that factors other than death, injury, and property damage can impose serious costs and social repercussions” (p. 234). Even though no one was killed or even harmed by the accident, the result was to impose “enormous costs”—in the form of stricter regulations, reduced operation of reactors worldwide, greater public opposition to nuclear power, and a less viable role for one of the major long-term energy sources—on the entire nuclear industry and on society as a whole” (pp. 234–35). Slovic is concerned that a conventional risk analysis, focussed on the likelihood and probability of harm, will overlook these kinds of consequences. He adds that in some domains, society provides not amplification, but a kind of “social attenuation of risk” (p. 235), producing underestimation of and underresponse to potentially serious harms. As examples, he offers the cases of indoor radon, smoking, and driving without a seat belt (p. 235). 
In dealing with the social amplification of risks, then, Slovic is especially concerned about the “secondary impacts” of dangers usually viewed in narrower terms. When a problem emerges, there might be a sharp overall decline in social trust; liability and insurance costs might increase; people might impose pressure for new regulations; social disorder is quite possible (p. 239). Because dramatic events are an important determinant of individual risk judgments, a highly publicized incident might well move people in the direction of fear that is quite unwarranted by reality. The flow of information, especially via the media, can be extremely important here, not only in spreading facts but also in shaping perceptions. This is Slovic’s effort to cast light on the processes by which “seemingly minor risks or risk events often produce extraordinary public concern and social and economic impacts, with rippling effects across time, space and social institutions” (245). 
By emphasizing that social amplification of risks, Slovic seems to me to be pointing in a helpful direction, one that provides a valuable supplement to his general focus on individual cognition. But his discussion suffers from being too much of a collection of factors, one that does not give a clear sense of the mechanisms by which “social amplification” occurs. Here, then, is a criticism, or at least extension, of Slovic’s treatment of mental shortcuts, intuitive toxicology, and social forces. There is much to be said about the social origins of individual beliefs, emphasizing that people contribute to the creation and intensity of the same forces by which they are influenced. My aim here is to fill a gap in Slovic’s presentation by offering a sketch of the relevant influences. 
emerging work on the nature of social cascades, 32 and by sketching an understanding of “risk perception cascades” in particular. The starting point, and what makes cascades possible, is a simple recognition that in many domains, people lack much in the way of first-hand information. Certainly this is the case for many hazards. Because few people know, for a fact, about the dangers of arsenic in water, or global warming, or Lyme Disease, or asbestos in the workplace, they must rely on the signals given by other people. 
To see how risk perception cascades work, consider a stylized example. John is unsure whether global warming is a serious problem, but Sarah, whom John trusts, believes that it is. Giving consideration to Sarah’s views, John decides that global warming is indeed a serious problem. Frank would be inclined, on his own, to discount the risk; but confronted with the views of Sarah 32 See, e.g., Sushil Biikhchandani et al., Learning from the Behavior of Others, J. Econ. Persp., Summer 1998, at 151; Lisa Anderson and Charles Holt, Information Cascades in the Laboratory, 87 Am. Econ. Rev. 847 (1997); Abhijit Banerjee, A Simple Model of Herd Behavior, 107 Q. J. Econ. 797 (1992); Andrew Daughety and Jennifer Reinganum, Stampede to Judgment, 1 Am. L. & Ec. Rev. 158, 159-65 (1999). and John, Frank might well come to believe that global warming is indeed a serious problem. Pauline, a skeptic about global warming, would have to have a great deal of confidence to reject the shared beliefs of Frank, Sarah, John, and Frank. Before long, the members of this little community will come to share a belief that global warming is a matter of considerable concern.33 
Stylized though it is, this description seems to capture the dynamics of many social movements with respect to risk,34 captured in the notion of an informational cascade.35 The spreading belief in Lyme disease appears to be a case in point, with many people believing that they have the disease, and many doctors confirming the diagnosis, simply because they have heard from others of connection between Lyme disease and certain symptoms.36 Because of the availability heuristic, the cascade can be greatly fueled, or accelerated, if a salient event is easily brought to mind.37 
at work; reputation matters too. Because most people care about the views of others, it is easy to imagine reputational cascades with respect to actions or stated beliefs.38 Suppose, for example, that Allan and Betty would think ill of anyone who argues that the global warming is not a problem. Carl, who is not sure what to think about global warming, might be unmoved privately by the views of Allan and Betty, and might even consider them fanatical; but he might nonetheless not want to incur the wrath of Allan and Betty, or to seem ignorant, or to appear indifferent to the welfare of future generations. If so, Carl might show no opposition to dramatic steps to halt global warming , or might even agree publicly with Allan and Betty that such steps are necessary. If Deborah is otherwise in equipoise, she might be most reluctant to oppose Allan, Betty, and Carl publicly. Mounting reputational pressures might well lead Ellen, Frank, George, and Helen, and many more, to join the bandwagon. The eventual result 33 See Biikhchandani et al, supra note; David Hirschleifer, The Blind Leading the Blind, in The New Economics of Human Behavior (1998). 34 For many examples, see Penina Glazer and Myron Glazer, The Environmental Crusaders (1998). 35 See sources cited in note 33 supra. 36 See Stalking Dr. Steele, The New York Times Magazine, July 17, 2001, at 52. On informational cascades among experts, see Hirschleifer, supra note. 37 See Timur Kuran and Cass R. Sunstein, Availability Cascades and Risk Regulation, supra note. 38 See Kuran, supra note. would be to affect law and policy, as citizens seem support massive social efforts.39 
I believe that with respect to risk, informational influences are the most important, as individual fear grows with a sense that other (reasonable) people are frightened; but reputational influences are also pertinent. If the example just given seems artificial, consider the suggestion of a medical researcher who questions a number of Lyme disease diagnoses: “Doctors can’t say what they think anymore. . . . If you quote me as saying these things, I’m as good as dead.”40 Or consider the remarks of a sociologist who has publicly raised questions about the health threats posed by mad-cow disease, suggesting that if you raise those doubts publicly, “You get made to feel like a pedophile.”41 
In the context of regulation of hazardous waste dumps, it is clear that reputational factors actually helped to fuel a cascade effect, eventually leading to the Superfund statute.42 Lawmakers, even more than ordinary citizens, are vulnerable to reputational pressures; in fact that is part of their job. Hence they might support legislation to control risks that they know to be quite low. Nor is the only problem one of excessive legal controls. In a phenomenon similar to Slovic’s “social attenuation of risks,” we can imagine “unavailability cascades,” in which people’s relative indifference to statistically significant risks leads other people to be indifferent too.43 Undoubtedly informational and reputational forces of this kind help account for public indifference to many hazards that trouble experts. Thus perceptions of risk are often a product of those forces. 
Note that these points are complementary to Slovic’s emphasis on the availability heuristic and intuitive toxicology. What is “available” will be a function of informational and reputational forces: If people are talking about the risks associated with pesticides, or disposal diapers, those risks will be available in the public mind. And if people are spreading the idea that arsenic “causes cancer,” and should therefore be banned, the premises of intuitive toxicology will spread as if by contagion. 39 See id. 40 Stalking Dr. Steele, supra, at 56. 41 Wall Street Journal. 42 See Kuran and Sunstein, Availability Cascades and Risk Regulation, supra note. 43 See id. helps to strengthen these forces. It is well established that when a group of people engages in deliberation, group members will tend to move toward a more extreme position in line with their predeliberation inclinations. This is the typical pattern among deliberating bodies. Thus, for example, a group of people who fear the effects of second-hand smoke, or who believe that pesticides carry significant risks, is likely, after discussion, to believe that the health dangers here are extremely serious. So too, a group of people who tend to think that the risks of global warming have been exaggerated will tend to think, after discussion, that global warming is no cause for concern. In fact there is no reason to think that experts are immune to group polarization; they are certainly vulnerable to cascade effects.45 Massive evidence, from many different countries, supports the basic prediction.46 If like-minded people are talking, much of the time, with one another, it is especially likely that large groups of people will show heightened concern about certain risks, whatever the evidence suggests—and also that large groups of people will show no concern at all, even if the evidence gives cause for alarm. There can be no doubt that group polarization has contributed to social divisions with respect to risk regulation; indeed, Slovic’s demonstrated “affiliation bias” probably has a great deal to do with group polarization. And when society generally shows alarm, or indifference, group polarization is likely to be part of the reason. 44 See Roger Brown, Social Psychology (2d ed. 1986); Cass R. Sunstein, Deliberative Trouble: Why Groups Go To Extremes, 110 Yale LJ (2000). 45 See Hirschleifer, supra note. 46 Why does group polarization occur? Though no cascade need be involved, the two principal explanations are close to the explanations for informational and reputational cascades. See Brown, supra note, which I summarize here. The first involves informational influences. In a deliberating group with an initial tendency in favor of X and against Y, there will be a disproportionate number of arguments in favor of X, simply because most people will speak out on behalf of X. Group members will have thought of some, but not all, of the arguments in that direction. After deliberating, the arguments for X will seem stronger, to individual members, and the arguments for Y will seem even weaker. Hence it is to be expected that discussion will move people to a more extreme form of their original enthusiasm for X. The second explanation points to social influences. Most people care about their reputations and their self-conception. Suppose, for example, that you are inclined to think that nuclear power is not dangerous, but you are not entirely sure; suppose too that you find yourself in a group which also rejects the idea that nuclear power is dangerous. If you think of yourself as the sort of person who is, more than most, inclined to support nuclear power, you might move a bit, if only to maintain your reputation within the group and your self-conception on the issue at hand. The evidence strongly supports the proposition that this happens. 
In sum: Slovic has performed a valuable service in showing how the availability heuristic and intuitive toxicology help to produce inaccurate judgments about risk. As compared with ordinary people, experts are able to reach accurate judgments, if only because they have access to more information, making the easily-recalled incident a less important determinant of judgment and producing more accuracy than can come from the simple ideas on which ordinary people rely.47 Slovic is also right to emphasize that social forces can amplify the effect of the heuristic. The principal gap in his discussion lies in his treatment of those forces. There is much more to be done here, both at the level of theory and at the level of empirical detail. In addition, Slovic’s psychological claims offer some clues to the development of federal regulatory policy— showing, for example, how the vivid example can play such a significant role in producing new legislation or new rules at the agency level. Sometimes the use of such examples will be valuable, because it will activate social concern about previously neglected problems; but sometimes it will lead to perverse results, in the form of massive expenditures on small problems, or even nonproblems.48 Here too much remains to be done. 
Thus far the discussion has involved the vulnerability of ordinary people to mistaken beliefs about risks. But Slovic also insists that in many ways, ordinary people are not mistaken at all. Their judgments about risks involve evaluative judgments that are worthy of respect. For Slovic, experts seem, much of the time, to be obtuse. This is one of the most striking claims in Slovic’s book; it deserves careful attention, in part because the federal government is now taking Slovic’s arguments directly into account.49 
The idea that ordinary people are making subtle judgments of value is embodied in what Slovic calls the “psychometric paradigm” (p. 222) According to the psychometric paradigm, ordinary people certainly care, as experts do, 47 For a detailed treatment, see Howard Margolis, Dealing With Risk (1997). 48 See Noll and Krier, supra note (arguing for “striking when the iron is cold”); Phantom Risk, supra note. 49 See the “sensitivity analysis” for arsenic regulation, which includes a 7% increase in monetized risk for the involuntary nature of the exposure and also for its uncontrollability. See Fed. Reg. (2000). See also Environmental Protection Agency, Benefit-Cost Analysis (2000) (discussing these factors). about the number of lives at risk as a result of some product of activity. But there is also a significant difference between the two camps. Experts tend to focus on one variable: the number of lives at stake. But ordinary people have a less crude and more complex approach (p. 223). They care about a range of qualitative variables that can lead to materially different evaluations of statistically identical risks. 
Thus Slovic uncovers a long list of “factors” that can make risks more or less acceptable, holding expected fatalities constant. These include whether the risk is (1) dreaded; (2) potentially catastrophic; (3) inequitably distributed; (4) involuntary; (5) uncontrollable; (6) new; and (7) faced by future generations (p. 140). Because these factors are so crucial, “riskiness means more to people than ‘expected number of fatalities’” (p. 231). And because they focus on such factors, Slovic thinks that in an important respect, ordinary people think better, and more rationally, than experts do. According to Slovic, people’s “basic conceptualization of risk is much richer than that of experts and reflects legitimate concerns that are typically omitted from expert risk assessments” (p. 231). This is the basis for Slovic’s claim that experts and ordinary people display “rival rationalities” and that “each side must respect the insights and intelligence of the other” (id.). 
This is a striking and provocative claim. It has also been highly influential.50 And in some ways, it is clearly correct. The risks associated with voluntary activities (skiing, horseback riding) receive less public concern than statistically smaller risks from involuntary activities (food preservatives, pesticides, herbicides, certain forms of air pollution). People do seem to be willing to pay more to prevent a cancer death than to prevent a sudden unanticipated death,51 and one reason is that cancer is especially “dreaded.”52 In pointing to the importance of qualitative factors, Slovic has made a significant advance, one that deserves to be incorporated into regulatory policy, as indeed it appears to be doing.53 We can go further. If Slovic is right, the populist view has received strong empirical support, and the technocratic position has taken a serious blow. The reason is that technocrats miss important values to which ordinary people rightly call attention. Indeed, if Slovic is right, regulatory policy 50 See id. 51 See George Tolley et al., Valuing Health for Policy (1995). 52 See Richard Revesz, Environmental Regulation, Cost-Benefit Analysis, and the Discounting of Human Lives, 99 Col L Rev 941 (1999). 53 See the EPA’s use of involuntariness and uncontrollability as part of its sensitivity analysis in valuing arsenic reductions, Fed. Reg. (2001). should be rethought in quite fundamental ways, with government incorporating the relevant values far more thoroughly than it has yet done.54 
I do not believe, however, that Slovic’s evidence establishes as much he thinks he does, and some of the central ideas here—dread, voluntariness, control—seem to me underanalyzed. I also suspect that Slovic has missed an important part of the picture; and an understanding of what he has overlooked reinforces the view that when experts and ordinary people differ, experts are right and ordinary people are wrong. 
Evidence first. How do we know that ordinary people think that these qualitative factors are so important? The answer is not that people spontaneously point to such factors in explaining their assessments of risk. We do not have data to suggest that ordinary people have an accurate sense of the number of lives at stake, and that their judgments stem from qualitative judgments.55 Instead the answer is that Slovic and his fellow experimenters expressly identify these factors and set them before experimental subjects (p. 222, pp. 203–6). Slovic and his collaborators ask people to rate certain risks along these specific dimensions (id.). On a seven-point scale, people are asked to rate various risks in terms of their catastrophic potential, their dreadedness, their threat to future generations, their controllability, and so forth (id.). Is it at all surprising, or even informative, that the risks that people most fear tend to be rated most severely along these dimensions? 
Slovic finds that the most feared risks include DDT, pesticides, herbicides, and smoking (p. 143), whereas X-rays, microwave ovens, non-nuclear electric power, marijuana, and sunbathing (pp 144–45) are ranked far lower. He also finds that the most feared risks do worse, along several of the “qualitative” dimensions, than those risks that are least feared. But this finding does not establish that the qualitative dimensions are the grounds for people’s rankings of these risks. Nothing in the data is inconsistent with the possibility that people fear certain risks because they think that they are statistically large; that this is the judgment that best explains their rankings; but that people’s rankings of statistically large risks will, on the qualitative dimensions, suggest more 54 See id., which is simply a start toward a more thorough-going effort, some of whose implications are traced in Revesz, supra note. 55 In Slovic finds that people do not know the actual numbers. See supra. concern than their judgments, on those same dimensions, of statistically small risks. On this view, the rankings on the qualitative dimensions are explained by, and do not themselves explain, people’s concern about the large statistical risks associated with certain products and practices. Slovic is undoubtedly right to say that the qualitative factors matter. But his evidence seems to me too crude to disprove a competing hypothesis, to the effect that people’s rankings of risks reflect, in significant part, inaccurate statistical assessments, and are not mostly driven by a “richer rationality.”56 
If we look closely at Slovic’s actual list of hazards, it is not clear how to explain the results in terms of qualitative factors. Of ninety hazards, smoking is ranked ninth, while marijuana is ranked 85th (pp. 143–45); what “qualitative factors” on Slovic’s list could account for this dramatic difference? Are the risks of smoking less voluntary and more inequitably distributed? What is the richer rationality that produces these judgments? One hypothesis is that people know that the risks from smoking are quite large, but believe that the risks from marijuana are quite low, and that this is the reason for the otherwise inexplicably different rankings. Or consider the fact (id.) that pesticides and herbicides rank seventh and eighth on the list, while X-rays rank 30th, food preservatives 35th, and food irradiation 39th. Can this pattern of judgments really be explained in terms of voluntariness, equity, potentially catastrophic quality, and risk to future generations? This does not seem plausible. It is far from clear what accounts for this pattern of judgments. But the idea that people are showing a “richer rationality” has not been demonstrated.57 56 Consider the possibility, pressed by Margolis, supra note, that these factors operate as ex post justifications for decisions reached on other grounds. It is well established that people are not especially good at offering the actual grounds for their judgments, and that sometimes they will offer accounts that demonstrably diverge from reality. In an especially interesting experiment, people were asked to choose one item of clothing from a pair. Actually the two items were identical, and people generally picked the item on the left side, apparently for no other reason than that it was on the left side. But asked for the basis for their decisions, people offered elaborate accounts, citing superior texture and color. See id. In this light there is reason to be cautious about the suggestion that these qualitative factors are the actual basis for people’s judgments about risks. In fact things are still worse for Slovic’s causal hypothesis, because (as I have noted) the qualitative factors said to support ordinary risk perceptions are not generated by people on their own, but are suggested by experimenters. 57 One possibility, stressed by Slovic himself (p. 413) and taken up in detail below, is that the “affect heuristic” underlies people’s judgments. See below. ordinary people are less fearful than experts—as was the case, for many years, for cigarettes—it is because ordinary people are not looking at the risks, but instead only at the benefit side. In sum: Slovic’s findings might be explained not only or mostly by reference to rival rationality, but also and more fundamentally to some combination of the availability heuristic and a failure, on the part of ordinary people, to put all of the effects of risks “on screen.” 
There is a further problem. Many of the qualitative factors that are said to lead people to a “rich” conception of risk need a good deal of unpacking. Slovic seems to take the qualitative variables as marking dichotomies, or at least clear distinctions. But they raise many questions. 
risks are not. But what does this mean? In the abstract, to say that a risk is “dread” seems to be to say that people fear it—which suggests that the idea of dread is just a synonym for perception of risk, not an explanation for it. If so, it is no surprise that there is a correlation between risks perceived as serious and risks deemed to be dread. (Is it surprising that people are scared of things that they find frightening?) It is even possible that when people say that a risk is “dread,” what they mean, in part, is that the risk is large in magnitude. People do not “dread” being attacked by unicorns or Martians. But they do dread cancer, partly because the risk of getting cancer is not so low. 
Slovic himself uses the term “common” as an antonym to “dread” (p. 94), but that raises further problems. Cancer is a common risk. But it is also the prototype of a dread risk. In trying to explain how the qualitative factors of dread explains the divergence between experts and ordinary people, we seem to be reaching a dead end. 
If progress is to be made here, perhaps we can assume that a dread risk is one that is accompanied by significant pain and suffering before death.61 This is certainly an intelligible idea. Perhaps people are especially fearful of fatality risks that involve, not especially high probabilities of death, but deaths that are unusually difficult or gruesome. This speculation is supported by data suggesting that people are willing to pay significantly more to avert deaths from 61 I try to defend this view in Cass R. Sunstein, Bad Deaths, J. Risk and Uncertainty (1998). cancer than deaths from heart disease.62 But if dread is understood in this way, a great deal of work would be necessary to establish that this is what people are really thinking; and Slovic’s data do not specifically confirm the point. 
and controllability.63 At first glance, the risks associated with pesticides and herbicides might seem involuntary and uncontrollable, whereas the risks from smoking and driving might seem voluntary and controllable. People choose to smoke and to drive. At least this is what people seem to say, when asks to rank risks along the relevant dimensions on seven-point scales. 
On reflection, however, these issues are complicated. It is not especially difficult to avoid pesticides. Many people can and do select pesticide-free food. The decision to smoke seems voluntary, but smoking is addictive, and many people seek to quit but find, or say, that they cannot. The decision to drive might seem voluntary, but in many places, it is hard, or expensive, to get to work without a car. Perhaps the risks associated with driving are controllable, but many accidents are not the fault of one of the drivers involved. Are the risks from ambient air pollution involuntarily incurred? It might seem that they are, but people have a choice of where to live, and some areas have much cleaner air than others. Are the risks from flying uncontrollable? Many people seem to think so. But no one is under a legal obligation to fly. Why are the risks of flying so much less controllable than the risks from bicycles or microwave ovens (p. 142)? 
In this light, it is probably best not to see a dichotomy between voluntariness and involuntariness, but to start by asking about two issues: (a) whether those who are subject to a risk are informed of its existence and (b) whether it is costly, or burdensome, to avoid the risk in question. When a risk seems “involuntary,” it is usually because people who face the risk do not know about it, or because it is especially difficult, or costly, to avoid it. When a risk seems “voluntary,” it is usually because those who run the risk are fully aware of it, and because risk avoidance seems easy or cheap. And if we think of things in these terms, there is no sharp dichotomy between “controllable” and “uncontrollable” risks, or even between “voluntary” and “involuntary” risks. 62 See Revesz, supra note, at; Tolley et al., supra note, at. 63 These are the two factors emphasized by the EPA in its sensitivity analysis involving arsenic. See note supra. Indeed these terms seem far too crude to capture what is really important.64 To the extent that ordinary people rely on them, they might be gesturing toward a sensible way of thinking about risks. But it is too much to claim that they have a “rival rationality.” 
I have been claiming, not that Slovic is wrong to say that qualitative factors matter to ordinary perceptions of risk, but that he claims more than the evidence establishes, and that the same evidence said to support “rival rationality” might reflect simple errors of fact. An interesting way to test my claims would be to see whether people are able to generate statistically accurate judgments about certain risks. When specifically asked about the number of expected deaths from various sources, do people make roughly the same judgments that experts do? If so, then it might indeed be that when ordinary people diverge from experts, it is because of the qualitative factors to which Slovic points.65 But if ordinary people err in estimating the number of lives at risk, and if their perceptions of risk severity are correlated with their estimates, then their errors might well explain the divergences. 
Actually Slovic does provide some evidence on this point (pp. 105–07), and I believe that it undermines his claim. On the purely factual issues, he finds systematic mistakes by ordinary people (id.), and these mistakes must have some connection to their disagreements with experts. Other evidence supports Slovic’s findings here.66 Slovic has not sorted out the extent to which these errors, or instead qualitative judgments, underlie the relevant disagreements. There remains a large empirical agenda here. 
Where does this leave us? It suggests that many of the disagreements between experts and ordinary people stem from the fact that experts have more information, and are also prepared to look at the benefits as well as the risks associated with controversial products and activities. To the extent that experts focus only on the number of lives at stake, they are genuinely obtuse. It is 64 In this way they have much in common with intuitive toxicology, which also sees things in “all or nothing” terms. Is it unfair to say that the use of such dichotomies reflects a form of intuitive psychometrics? 65 The “might well” is necessary. It might be that when people are assessing risks, they are not much thinking about the number of lives at stake, and that when they are forced to think about the important matter, their assessments change. Cf. Kahneman on dates. 66 Viscusi. reasonable to devote special attention to dangers that are hard to avoid, or accompanied by special suffering, or faced principally by children. But there is no “rival rationality” in taking these factors into account. On the positive side, what is needed is more empirical work to determine the extent to which ordinary risk perceptions are based on errors or instead on values. On the normative side, what is needed to more thought about the nature of concepts like “dread,” “involuntary,” and “uncontrollable.” With respect to policy, what is needed is incorporation of people’s values, to the extent that they can survive a process of reflection. 
Thus far the discussion has focussed on individual and social cognition. As Slovic emphasizes, most psychological work on risk has been highly cognitive in orientation, asking whether mental heuristics produce errors, and how people generally depart from what is generally considered to be rational behavior. But there seems to be a gap in thinking about perceived risks only in these terms. With respect to risks, many of our ordinary ways of speaking suggest strong emotions: panic, hysteria, terror.67 Slovic also wants to explore some central question: How do people’s feelings affect their reactions to risks? 
Slovic’s interest in this topic appears to have been triggered by a remarkable finding, mentioned above: When asked to assess the risks and benefits associated with certain items, people tend to think that risky activities contain low benefits, and that beneficial activities contain high risks (pp. 415–16). In other words, people are likely to think that activities that seem dangerous do not carry benefits; it is rare that they will see an activity as both highly beneficial and quite dangerous, or as both benefit-free and danger-free. This is extremely odd. Why don’t people think, more of the time, that some activity is both highly beneficial and highly risky? Why do they seem to make a kind of general, gestalttype judgment, one that drives assessment of both risks and benefits? Aware that risk and benefit are “distinct concepts,” Slovic thinks that “affect” comes first, and helps to “direct” judgments of both risk and benefit. Hence it suggests an “affect heuristic,” by which people have an emotional, all-things-considered reaction to certain processes and products, and that heuristic operates as a mental shortcut for a more careful evaluation. 67 See George Loewenstein et al., Risk As Feelings (forthcoming 2001). 
To test this hypothesis, Slovic offers several studies. One of the most interesting is designed to provide new information about the risk associated with some item, and then to see if the information altered people’s judgments about the benefit associated with it—and also to provide new information about the benefit of some item, and to test whether that information would alter people’s judgments about the accompany risk (pp. 421–26). The motivation for this study is simple. If people’s judgments were purely cognitive, information about the great benefits of (say) food preservatives should not produce a judgment than the risks are low—just as information about the great risks of (say) natural gas should not make people think that the benefits are low. 
Strikingly, Slovic finds that information about benefits alters judgments about risks, and that information about risks alters judgments about benefits (pp. 423–25). When people learn about the low risks of an item, they are moved to think that the benefits are high—and when they learn about the high benefits of an item, they are moved to think that the risks are low. Slovic concludes that people assess products and activities through affect, and that information that improves people’s affective response will improve their judgments of all dimensions of those products and activities. Slovic acknowledges that work on emotion and risk remains in a preliminary state, but he proposes the affect heuristic as a useful place to start. 
Slovic concludes that people assess products and activities through “affect,” and that information that improves people’s affective response will improve their judgments of all dimensions of those products and activities. The central idea is to be that when presented with a risk, people have a general emotional attitude to it—hence an “affect”—and that this general attitude operates as a heuristic, much affecting people’s judgments about both benefits and dangers. 
Slovic’s analysis here is intriguing and important. The basic claim has considerable truth, for emotions play a large role in reactions to risks, and they help to explain otherwise anomalous behavior.68 Indeed, the “affect heuristic” operates in many social domains. To take some issues far afield from Slovic’s own concerns, consider the sharp split, along political lines, in people’s reactions to the impeachment of President Clinton and the Supreme Court’s decision in 68 See id. Bush v. Gore.69 In the abstract, there is no reason to think that Republicans would believe in a lower standard than Democrats for impeachment of the President; in the abstract, there is no reason to think that Republicans would be more sympathetic than Democrats to an equal protection challenge to a manual recount. In both cases, and for people on both sides, an intense emotional reaction, or affect, seems to have driven conclusions on technical issues in law. In fact there is empirical confirmation of this point.70 In the area of risk in particular, “all things considered” emotions do seem to play a role in explaining people’s perceptions. 
At the same time, Slovic’s analysis seems to me undertheorized, and also to raise a number of additional issues that require further research. The most basic problem is the distinction between cognition and emotion, about which Slovic says too little. The distinction is complex and contested,71 and the bare idea of “affect” cannot much help in unpacking that distinction. In the domain of risks (and most other places), any “affect” is based on thinking; it is hardly cognition-free. When a negative emotion is associated with a certain risk— nuclear power, for example—cognition is playing a central role. In fact there are large debates about whether an emotion is a form of thought, or whether thoughts are necessary and sufficient conditions for emotions, and whether emotions is a sense precede or outrun cognition.72 But it is clear that no simple line can be drawn between emotions and cognition in most social domains. Whatever they are, emotions can lead us astray; but the same is true for math, biology, and animal experiments. I am not entirely sure what Slovic understands an “emotion” to be, or how he thinks that emotions and cognition relate to one another. 
There are several ways to make progress here. Some scientific work suggests that the brain has special sectors for emotions, and that some types of emotions, including some fear-type reactions, can be triggered before the more cognitive sectors become involved at all.73 It is not clear, however, that fear in human beings is pre-cognitive or noncognitive, and even if it is in some cases, that kind of fear would be triggered by few of the risks involved in actual human lives. Perhaps more to the point, existing experiments, not mentioned by Slovic, 69 121 S. Ct. (2000). 70 See Drew Weston. 71 For varying views, see Ronald deSousa, The Rationality of Emotion (1993); Jon Elster, Alchemies of the Mind (1999); Martha Nussbaum, Upheavals of Thought (2001). 72 See id. 73 See Joseph LeDoux, The Emotional Brain (1996). suggest that when it comes to risk, a key question is whether people can imagine or visualize the “worst case” outcome—and that surprisingly little role is played by the stated probability that that outcome will occur.74 In other words, people’s reactions to risks are often based mostly on the badness of the outcome, and the vividness of that outcome, rather than the probability of its occurrence. Consider these points: 
—If people are asked how much they will pay for flight insurance for losses resulting from “terrorism,” they will pay more than if they are asked how much they will pay for flight insurance from all causes.75 
—When people discuss a low-probability risk, their concern rises even if the discussion consists mostly of apparently trustworthy assurances that the likelihood of harm really is infinitesimal.76 
—People show ”alarmist bias.” When presented with competing accounts of danger, they tend to move toward the more alarming account.77 
A possible conclusion is that with respect to risks, vivid images and concrete pictures of disaster can “crowd out” other kinds of thoughts, including the crucial thought that the probability of disaster is really small. With respect to hope, those who operate gambling casinos and state lotteries play on the emotions in the particular sense that they conjure up palpable pictures of victory and easy living. With respect to risks, insurance companies and environmental groups do exactly the same. With respect to products of all kinds, advertisers try to produce a good affect to steer consumers into a certain direction, often through the use of appealing celebrities, through cheerful scenes, or through the creation of an association between the product and the consumer’s preferred self-image. 
An important lesson follows: If government is seeking a method to ensure that people will take a more rational approach to risk, it might well attempt to appeal to their emotions. With respect to a cigarette smoking, abuse of alcohol, reckless driving, and abuse of drugs, this is exactly what government occasionally attempts to do. It should be no surprise that some of the most effective efforts to control cigarette smoking appeal to people’s emotions, by 74 See Loewenstein, supra note; Yuval Rottenstreich and Christopher Hsee, Money, Kisses, and Electric Shocks: On the Affective Psychology of Probability Weighting (Working paper, University of Chicago, 1999). 75 See Loewenstein, supra note. 76 See Paul Slovic, Perception of Risk, supra note. 77 W. Kip Viscusi, Alarmist Decisions With Divergent Risk Information, 107 Ec. Journal 1657, 1657-59 (1997) making them feel that if they smoke, they will be dupes of the tobacco companies or imposing harms on innocent third parties.78 
I now turn to three issues on which Slovic offers a number of intriguing findings. These involve demographic differences, paternalism, and the crucial issue of trust. 
Do members of different social groups disagree about risks? Slovic has many interesting findings. The most general is what he calls “the white male effect” (p. 399). Apparently white men are less concerned about risks than are members of any other group. With respect to nearly every risk, white women, black women, and black men are far more troubled than are white men (pp. 397– 99). But this is the punchline of the story, and it will be helpful to provide a few details. 
Slovic has asked large numbers of people to rank large numbers of risk; the ranking occurs by saying whether the hazard poses little or no risk; a slight risk; a moderate risk; or a high risk. Women rank nearly every risk as higher than men do (pp. 396–402). The same difference is observed for British toxicologists as well as for ordinary Americans; women toxicologists in Britain rank risks as more serious than do male toxicologists in Britain (p. 397). The differences are especially large with respect to nuclear waste, nuclear power, outdoor air pollution, alcoholic beverages, and suntanning (p. 397). There are other demographic differences too: As education and income increase, fear of almost all risks decreases (p. 399). 
Once the data are disaggregated, however, a more interesting and somewhat different picture emerges. It is too simple to draw a distinction between men and women. Race is important too (p. 400). Across a large number of hazards, white men perceive risks as consistently lower than do white females, non-white males, and non-white females (pp. 398–400). In fact there are no large-scale differences among the latter three groups. White men are the real outliers. Slovic shows that the results are still more interesting. What drives the “white male effect” is that not the general view of white males, but the view of 78 See JAMA (1997). about 30% of them, who believe almost all risks to be very low (pp. 398–99). The other 70% of white males are not greatly different from the other subgroups. 
What can be said about the key 30%? They tend to have more education and more household income, and also to be more conservative (pp. 399–400). They also tend, more than most, to disagree with the view that technological development is destroying nature, to reject the idea that they have very little control over risks to their health, and to think that future generations can take care of themselves when facing risks imposed as a result of today’s technologies. Slovic thinks that “affect” is part of what is responsible for this distribution of beliefs about risks (pp. 403–9). 
All this is extremely interesting. There is, however, a serious gap in Slovic’s studies. We know that white men are less concerned about certain risks than members of other demographic groups. But we do not know whether white men believe that the risks are lower, in terms of statistical risk of harm, than do members of other demographic groups. White men might believe that the risks of pesticides deserve a “2” on a seven-point scale, whereas others believe that those risks deserve a “5,” but all groups might have the same basic sense of the statistical risks. Does a lifetime risk of 1 in 1 million count as a “1” or a “5”? How can we tell? 
On the data that Slovic presents here,79 it is possible that white men have a more accurate understanding of the numbers; it is possible that demographic groups do not much differ in their judgments of the numbers, but that white men, or the key 30%, are simply less worried than are other people. If they are less worried, it might be because they have a better sense of the facts. Or it might be because people who are well-educated, and wealthy, have a sense of relative security, so that they believe that most of the risks of life are relatively low. Women and African-Americans, by contrast, have a sense of relative insecurity, and so think of most risks are higher. Two of Slovic’s other concerns are relevant here. If people are most concerned about risks that they consider involuntarily incurred, beyond their control, and unfairly distributed, it should be unsurprising that for most risks, white men are especially concern-free. And trust is relevant to people’s assessment of the severity of risks; it might well be that white men have comparatively greater trust in private and public institutions, and hence rank risks as relatively low. I now turn to this point. 79 Some of Slovic’s studies suggest that he has the relevant data. But 
Slovic also provides an illuminating discussion of the topic of trust, a greatly neglected issue in legal treatments of risk regulation (p. 316 et seq.). A basic puzzle here is the fact that in the last twenty years, our society has grown healthier and safer, in part because it has spent billions of dollars on risk reduction; but at the same time, the American public has become more, not less, concerned about risks (p. 317). Slovic emphasizes that a lack of trust has played an important role on controversies over managing hazardous technologies. He also shows that when people are concerned about a hazard, it is often because they do not trust those who manage it. Consider, for example, the fact that people tend to view medical technologies involving radiation and chemicals (such as X-rays) as high-benefit, low-risk, whereas they view industrial technologies involving radiation and chemicals (such as nuclear power and pesticides) as high-risk, low-benefit (id.). Far more important than technical risk assessments is the level of trust in those who try to manage risks and to give assurance. 
Slovic also stresses the fragility of trust. A distinctive feature of trust is that it is far easier to destroy it than to create it. Thus Slovic offers a study of forty-five hypothetical news events involving nuclear power; some of these events were designed to increase trust, whereas others were designed to decrease it (pp. 321–23). Negative events were judged to have a much more significant effect than positive events. For example, a nuclear power plant accident in another state was seen by many to have a powerful effect on trust, whereas careful selection and training of employees, and an absence of any safety problems in the past year, had little effect in increasing trust (id.). He also finds that sources of bad, trust-destroying news are seen as more credible than are sources of good, trust-creating news (pp. 322–23) This finding is in line with a related one, mentioned above: When people are unconcerned about a risk, discussion that is designed to provide still more assurance tends to increase anxiety, rather than to diminish it. 
With respect to trust, Slovic therefore establishes the existence of an “asymmetry principle” (p. 320): Events that might weaken trust have a significant effect, whereas events that strengthen trust do very little. Does this mean that people are confused or irrational? Slovic doesn’t think so. “Conflicts and controversies surrounding risk management are not due to public irrationality or ignorance but, instead, can be seen as expected side effects of these psychological tendencies, interacting with our remarkable form of participatory democratic government, and amplified by powerful technological and social changes in out society” (p. 323). Many of these changes involve the news media, which can publicize risk-related events from anywhere in the world instantaneously. And the news media, no less than ordinary people, give special emphasis to bad, trust-destroying events. “The young science of risk assessment is too fragile, too indirect, to prevail in such a hostile atmosphere” (p. 324). 
What can be done? Slovic favors an increase in public participation in decision-making, going well beyond public relations to include much more in the way of actual power-sharing (p. 325). His fear is that governmental efforts to reassure people are unlikely, without broad participation, to breed anything but further distrust.80 On this count Slovic might be right, but his own data raise questions about the value of public participation in increasing distrust. If bad news is more salient than good news, and if people are intuitive toxicologists, there is a risk than high levels of public participation in highly technical domains will simplify increase public fear, with unfortunate consequences for policy. 
I am not sure of the solution to this dilemma. Slovic is right to emphasize the importance of trust, and also to connect that issue to the split between experts and ordinary people. But efforts to increase public participation in the regulatory process raises many complexities, and if people come to that process with the intuitions that Slovic catalogues, then it is not clear that broad public involvement will be helpful for either sound regulation or trust itself. 
Do smokers know the risks associated with smoking? In an important study, W. Kip Viscusi has argued that they do.81 Viscusi asked both smokers and nonsmokers to say how many smokers are likely to die from various causes as a result of smoking. Viscusi’s basic finding is that people do indeed know the risks, and in fact they overestimate them. 
With respect to adolescents, at least, Slovic is quite critical of Viscusi’s findings. He urges that people tend to be overoptimistic, thinking that they themselves are peculiarly immune from risks that others face (p. 366). Hence Slovic believes that people’s ability to generate accurate statistical figures is consistent with the claim that smokers typically underestimate the risk that they 80 For some support, see Kuran and Sunstein, supra note. 81 See W. Kip Viscusi, Smoking (1992). themselves face. Slovic also contends that people’s quantitative judgments depend on the way that questions are asked, and hence that slight alterations in questions could generate far more inaccurate numbers (pp. 367–68). He objects as well that people who know the statistical risk do not have a sufficient sense of what it is actually like to experience the adverse effects of smoking. He claims, finally, that people are not likely to understand either the cumulative nature of the risk or its addictive quality (p. 367). Thus many smokers have no clear sense of the short-term risks of smoking, and high-school seniors greatly overestimate the likelihood that they will not be smoking five years after starting to smoke (p. 369). Fewer than half of those who predicted that they would quite smoking after five years turned out, in fact, to quit. “Many young smokers perceive themselves to be at little or no risk from smoking because they expect to stop smoking before any damage to their health occurs. In reality, a high percentage of young smokers continue to smoke over a long period of time and are placed at risk by this behavior.” (p. 369). 
Slovic’s discussion of these points is brief but highly suggestive. There is a great deal to sort out here, on the vexing questions associated with ensuring adequate information, and with the proper place of paternalism, in the domain of risk-taking. With respect to smoking, people have been saturated with evidence about the adverse health effects. Viscusi’s claims should not be shocking in this light. But neither Viscusi nor Slovic offers a great deal of evidence about a key question: the statistical risk that each smoker believes he or she is running—as opposed to the statistical risk that smokers believe that smokers generally are running. If it is true that people generally suffer from excessive optimism with respect to the risks that they personally are facing, there is a problem of inadequate information even when people are well aware of the statistical risks. At the very least, this problem justifies a governmental effort to provide a corrective. (Consider the advertising campaign: “Drive defensively; watch out for the other guy.”) And this point raises the expert/lay person split from another angle. In some areas, the source of the split may be an emphasis on statistical realities on the part of experts—and an overinflated sense of personal invulnerability on the part of risk-takers. 
Slovic is a psychologist, not a policy analyst. While he gestures in the direction of legal reform, this is not his principal topic. Nonetheless, his work carries enormous importance for those concerned with policy and law. This is so partly because an understanding of human cognition (not excluding emotion) helps to explain people’s reactions to risks, and their demand for legal responses. It is so partly because such an understanding can help us see which approaches to risk regulation will work, and which will not. And it is so partly because of the continuing battle between technocratic and populist approaches to risk. If we know why people think what they do, and whether their views are based on mistakes or instead reasonable judgments of value, we will be able to make some progress in understanding the role of science, and experts, in the world of risk regulation. 
It is useful here to distinguish among three different enterprises: positive, prescriptive, and normative.82 Positive work is concerned with making predictions. When will law take certain forms? What will be the effect of a particular step on human behavior? Prescriptive work is concerned with identifying the means of achieving shared goals. If government seeks to increase people’s fear of certain risks, and to dampen their fear of others, it needs to know which strategies will work. Normative work is concerned to show what government’s ends should be. If people make systematic mistakes about the risks that they face, perhaps government should feel more free to override their views. 
Slovic’s emphasis on conflicts between experts and ordinary people casts considerable light on all of these enterprises. If the availability heuristic helps to drive certain judgments about risks, we will be able to make predictions about the likely effects of salient events, both on people’s judgments about when to insure and to take precautions, and on the demand for legal responses.83 In the aftermath of a highly publicized incident, considerable movement should be expected; if illustrations of harm do not come to mind, people might persist in failing to take care of themselves. If emotions play a large role in risk-related behavior, educational campaigns are far more likely to work if they involve memorable images, rather than statistical probabilities. If government wants to encourage people to protect themselves, it should use particular examples and try to make them as vivid as possible. And if people make a large number of mistakes about risks—partly because of emotions, partly because of cognition— there are reasons to be skeptical of populist conceptions of regulatory government, and also about the kind of reflexive antipaternalism of much work on risk regulation. We might, in short, see Slovic’s findings as supportive of the conception of administration associated with the New Deal, which placed a high 82 See Christine Jolls et al., A Behavioral Approach to Law and Economics, Stan. L. Rev. (1998). 83 See Krier and Noll. premium on ensuring regulatory choices by people immersed in the subject at hand.84 
In this light, Slovic’s findings can be brought in close contact with Justice Stephen Breyer’s discussion of risk regulation,85 probably the most influential treatment among students of law. Breyer urges, quite plausibly, that the regulatory state suffers from poor priority-setting. In his view, government is devoting a lot of resources to small problems, and spending too little time and effort on large problems.86 Breyer urges the creation of a new body of risk specialists, with expertise in many fields.87 The task of the specialists would be to reallocate resources from small problems to large ones. 
Breyer offers an impressive analysis of the problem of poor prioritysetting, and he provides a promising institutional correction. In many ways Breyer calls for a kind of newer New Deal—a system of administration run in substantial part by technocrats, subject of course to democratic override. But his book says almost nothing about how people think about risks.88 With reference to Slovic’s findings, Breyer’s analysis might well be criticized on the ground that ordinary people would distrust any body of risk specialists, on the ground that they were unlikely to be sufficiently responsive to their concerns. Perhaps more interestingly, Breyer’s approach might be criticized on the ground that the sheer numbers cannot tell us whether a problem is “large” or “small.” People are reasonably concerned about a range of other variables. This is a large thrust of Slovic’s work. 
In the end, however, Slovic’s findings seem to me mostly supportive of Breyer’s analysis; indeed Slovic provides strong cognitive ammunition for Breyer’s diagnosis and even his remedy. Slovic shows that ordinary people make many mistakes in thinking about the risks associated with various activities. He also shows that much of the time, people treat safety as a kind of “all or nothing” matter, are vulnerable to the “zero risk mentality,” overreact to small signals of dangers, and in some domains show excessive optimism. In these circumstances, a sensible system of risk regulation will not respond mechanically to what people think; it will impose large filters on the public’s own conception of appropriate priority-setting. 84 See James Landis, The Administrative Process (1925). 85 Breyer, supra note. 86 Id at 10-29. 87 Id. at 59-68. 88 Breyer does offer a brief treatment of cognitive factors. See id. at 16. 
But there are two grounds for objecting to a purely technocratic approach to risk regulation. The first emerges from analysis of some of the qualitative factors that Slovic highlights. Some deaths are particularly bad, and these deserve unusual attention. When it is especially easy to avoid certain risks, government should not spend a great deal of time and effort in response. It would indeed to obtuse to treat all risks as if they were the same, regardless of context and quality. But it remains true that a sensible society is greatly concerned to ensure that people have longer and healthier lives, and that if policies lead government to spend a lot on small problems, and little on large problems, something is amiss. Note in this regard that a more sensible allocation of resources could save tens of thousands of lives, tens of billions of dollars, or both.89 
The second criticism of a purely technocratic approach involves people’s likely reaction to it. To work well, a regulatory system needs public support and confidence, even if we do not believe that a lack of confidence would be fully rational. To the extent that government relies on statistical evidence alone, it is unlikely to promote its own goals. Partly this is because people will assess that evidence in light of their own motivations and their limited cognitive capacities. Regulators who are alert to the importance of both confidence and trust will do what they can to provide information in a way that is well-tailored to how people think about risk—and that tries to educate people when their usual ways of thinking lead them astray. In some circumstances, an understanding of how people think will lead government toward approaches that technocrats, insensitive to Slovic’s findings, will not have on their viewscreen. We might say that good technocrats need to know not only economics and science, but psychology as well. 
But the most important lesson of this book seems to me to lie elsewhere. Because of predictable features of human cognition, people’s intuitions are unreliable,90 and they are prone to blunder about the facts. As we have seen, these blunders have harmful consequences for regulatory policy. To be effective, regulators must be aware of perceived risk, not only actual risk. But for purposes of policy, what is most important, most of the time, is actual risk rather than perceived risk. The task for the future is to respond to people’s values, not to their errors. 89 See Tammy Tengs and John Graham, The Opportunity Costs of Haphazard Social Investments in Life-Saving, in Risks, Costs, and Lives Saved 167, 172-74 176 (Robert Hahn ed. 1996). 90 See the discussion of System 1 and System 2 in Kahneman and Frederick, supra note. Readers with comments should address them to: Cass R. Sunstein Karl N. Llewellyn Dist. Service Prof. of Jurisprudence University of Chicago Law School 1111 East 60th Street Chicago, IL 60637 USA Phone: 773-702-9498 E-mail: csunstei@midway.uchicago.edu William M. Landes, Copyright Protection of Letters, Diaries and Other Unpublished Works: An Economic Approach (July 1991). 
Richard A. Epstein, The Path to The T. J. Hooper: The Theory and History of Custom in the Law of Tort (August 1991). 
Cass R. Sunstein, On Property and Constitutionalism (September 1991). Richard A. Posner, Blackmail, Privacy, and Freedom of Contract (February 1992). Randal C. Picker, Security Interests, Misbehavior, and Common Pools (February 1992). 
Tomas J. Philipson & Richard A. Posner, Optimal Regulation of AIDS (April 1992). Douglas G. Baird, Revisiting Auctions in Chapter 11 (April 1992). 
William M. Landes, Sequential versus Unitary Trials: An Economic Analysis (July 1992). 
William M. Landes & Richard A. Posner, The Influence of Economics on Law: A Quantitative Study (August 1992). 
Alan O. Sykes, The Welfare Economics of Immigration Law: A Theoretical Survey With An Analysis of U.S. Policy (September 1992). 
Douglas G. Baird, 1992 Katz Lecture: Reconstructing Contracts (November 1992). Gary S. Becker, The Economic Way of Looking at Life (January 1993). 
J. Mark Ramseyer, Credibly Committing to Efficiency Wages: Cotton Spinning Cartels in Imperial Japan (March 1993). 
Cass R. Sunstein, Endogenous Preferences, Environmental Law (April 1993). Richard A. Posner, What Do Judges and Justices Maximize? (The Same Thing Everyone Else Does) (April 1993). 
Lucian Arye Bebchuk and Randal C. Picker, Bankruptcy Rules, Managerial Entrenchment, and Firm-Specific Human Capital (August 1993). 
J. Mark Ramseyer, Explicit Reasons for Implicit Contracts: The Legal Logic to the Japanese Main Bank System (August 1993). 
William M. Landes and Richard A. Posner, The Economics of Anticipatory Adjudication (September 1993). 
Kenneth W. Dam, The Economic Underpinnings of Patent Law (September 1993). Alan O. Sykes, An Introduction to Regression Analysis (October 1993). 
Richard A. Epstein, The Ubiquity of the Benefit Principle (March 1994). Randal C. Picker, An Introduction to Game Theory and the Law (June 1994). William M. Landes, Counterclaims: An Economic Analysis (June 1994). J. Mark Ramseyer, The Market for Children: Evidence from Early Modern Japan (August 1994). 
Robert H. Gertner and Geoffrey P. Miller, Settlement Escrows (August 1994). Kenneth W. Dam, Some Economic Considerations in the Intellectual Property Protection of Software (August 1994). 
Cass R. Sunstein, Rules and Rulelessness, (October 1994). 
David Friedman, More Justice for Less Money: A Step Beyond Cimino (December 1994). 
Daniel Shaviro, Budget Deficits and the Intergenerational Distribution of Lifetime Consumption (January 1995). 
Douglas G. Baird, The Law and Economics of Contract Damages (February 1995). Daniel Kessler, Thomas Meites, and Geoffrey P. Miller, Explaining Deviations from the Fifty Percent Rule: A Multimodal Approach to the Selection of Cases for Litigation (March 1995). 
Geoffrey P. Miller, Das Kapital: Solvency Regulation of the American Business Enterprise (April 1995). 
Richard Craswell, Freedom of Contract (August 1995). 
J. Mark Ramseyer, Public Choice (November 1995). 
Kenneth W. Dam, Intellectual Property in an Age of Software and Biotechnology (November 1995). 
Cass R. Sunstein, Social Norms and Social Roles (January 1996). 
J. Mark Ramseyer and Eric B. Rasmusen, Judicial Independence in Civil Law Regimes: Econometrics from Japan (January 1996). 
Richard A. Epstein, Transaction Costs and Property Rights: Or Do Good Fences Make Good Neighbors? (March 1996). 
Cass R. Sunstein, The Cost-Benefit State (May 1996). 
William M. Landes and Richard A. Posner, The Economics of Legal Disputes Over the Ownership of Works of Art and Other Collectibles (July 1996). John R. Lott, Jr. and David B. Mustard, Crime, Deterrence, and Right-to-Carry Concealed Handguns (August 1996). 
Cass R. Sunstein, Health-Health Tradeoffs (September 1996). 
G. Baird, The Hidden Virtues of Chapter 11: An Overview of the Law and Economics of Financially Distressed Firms (March 1997). 
Richard A. Posner, Community, Wealth, and Equality (March 1997). 
William M. Landes, The Art of Law and Economics: An Autobiographical Essay (March 1997). 
Cass R. Sunstein, Behavioral Analysis of Law (April 1997). 
John R. Lott, Jr. and Kermit Daniel, Term Limits and Electoral Competitiveness: Evidence from California’s State Legislative Races (May 1997). 
Randal C. Picker, Simple Games in a Complex World: A Generative Approach to the Adoption of Norms (June 1997). 
Richard A. Epstein, Contracts Small and Contracts Large: Contract Law through the Lens of Laissez-Faire (August 1997). 
Cass R. Sunstein, Daniel Kahneman, and David Schkade, Assessing Punitive Damages (with Notes on Cognition and Valuation in Law) (December 1997). William M. Landes, Lawrence Lessig, and Michael E. Solimine, Judicial Influence: A Citation Analysis of Federal Courts of Appeals Judges (January 1998). John R. Lott, Jr., A Simple Explanation for Why Campaign Expenditures are Increasing: The Government is Getting Bigger (February 1998). 
Lisa Bernstein, The Questionable Empirical Basis of Article 2’s Incorporation Strategy: A Preliminary Study (May 1999) Richard A. Epstein, Deconstructing Privacy: and Putting It Back Together Again (May 1999) William M. Landes, Winning the Art Lottery: The Economic Returns to the Ganz Collection (May 1999) Cass R. Sunstein, David Schkade, and Daniel Kahneman, Do People Want Optimal Deterrence? (June 1999) Tomas J. Philipson and Richard A. Posner, The Long-Run Growth in Obesity as a Function of Technological Change (June 1999) David A. Weisbach, Ironing Out the Flat Tax (August 1999) Eric A. Posner, A Theory of Contract Law under Conditions of Radical Judicial Error (August 1999) David Schkade, Cass R. Sunstein, and Daniel Kahneman, Are Juries Less Erratic than Individuals? Deliberation, Polarization, and Punitive Damages (September 1999) Cass R. Sunstein, Nondelegation Canons (September 1999) Richard A. Posner, The Theory and Practice of Citations Analysis, with Special Reference to Law and Economics (September 1999) Randal C. Picker, Regulating Network Industries: A Look at Intel (October 1999) Cass R. Sunstein, Cognition and Cost-Benefit Analysis (October 1999) Douglas G. Baird and Edward R. Morrison, Optimal Timing and Legal Decisionmaking: The Case of the Liquidation Decision in Bankruptcy (October 1999) Gertrud M. Fremling and Richard A. Posner, Market Signaling of Personal Characteristics (November 1999) Matthew D. Adler and Eric A. Posner, Implementing Cost-Benefit Analysis When Preferences Are Distorted (November 1999) Richard A. Posner, Orwell versus Huxley: Economics, Technology, Privacy, and Satire (November 1999) David A. Weisbach, Should the Tax Law Require Current Accrual of Interest on Derivative Financial Instruments? (December 1999) Cass R. Sunstein, The Law of Group Polarization (December 1999) Eric A. Posner, Agency Models in Law and Economics (January 2000) Karen Eggleston, Eric A. Posner, and Richard Zeckhauser, Simplicity and Complexity in Contracts (January 2000) Douglas G. Baird and Robert K. Rasmussen, Boyd’s Legacy and Blackstone’s Ghost (February 2000) David Schkade, Cass R. Sunstein, Daniel Kahneman, Deliberating about Dollars: The Severity Shift (February 2000) Richard A. Posner and Eric B. Rasmusen, Creating and Enforcing Norms, with Special Reference to Sanctions (March 2000) 2000) 2000) 2000) 
Position (August 2000) 
Internet (November 2000) 
System (November 2000) 
Relations: A Rational Choice Perspective (November 2000) 2000) 
Liability, Class Actions and the Patient’s Bill of Rights (December 2000) 
Economic Approach (December 2000) (January 2001) 
Finance (February 2001) (March 2001) 
Political Theory Perspective (April 2001) 
Age (April 2001) the Conceptual Foundations of Corporate Reorganization (April 2001) 
