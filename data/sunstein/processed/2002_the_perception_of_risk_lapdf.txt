Follow this and additional works at: https://chicagounbound.uchicago.edu/journal_articles Part of the Law Commons
Cass R. Sunstein, "The Perception of Risk," 115 Harvard Law Review 1119 (2002).
This Article is brought to you for free and open access by the Faculty Scholarship at Chicago Unbound. It has been accepted for inclusion in Journal Articles by an authorized administrator of Chicago Unbound. For more information, please contact unbound@law.uchicago.edu.
THE PERCEPTION OF RISK. By Paul SIovic. 2oo0. Publications Ltd. Pp. xxxvii, 473. £19.95.
In the late I98Os, the Environmental Protection Agency (EPA) embarked on an ambitious project, designed to compare the views of the public and EPA experts on the seriousness of environmental problems. The project revealed some striking anomalies, for the two groups sharply diverged on some crucial issues.5
With respect to health risks, the public's top ten concerns included radioactive waste, radiation from nuclear accidents, industrial pollution of waterways, and hazardous waste sites. But in the view of EPA experts, not one of these problems deserved a high level of concern.' Two of the public's top concerns (nuclear accident radiation and radioactive waste) were not even ranked by EPA experts. Of health risks considered by the public, among the lowest ranked were indoor air pollution and indoor radon - even though both were ranked among the worst environmental problems by expertsY The EPA concluded that there was a remarkable disparity between the views of the public and the views of its own experts.' It also noted, with evident concern, that EPA policies and appropriations seemed to reflect the public's preoccupations, not its own." If regulatory law and policy reflect what has been called a combination of "paranoia and neglect,"1 the public's own concerns may be largely responsible.
In the domain of risks, the persistent split between experts and ordinary people raises some of the most interesting problems in all of social science. For purposes of understanding these disputes, we might distinguish between two approaches: the technocratic and the populist. Good technocrats emphasize that ordinary people are frequently illinformed and urge that the task of regulators is to follow science and evidence, not popular opinion. From the technocratic standpoint, a crucial question is what the facts really show. When people are mistaken on the facts, they should be educated so that they do not persist in their errors. Of course, technocrats acknowledge that science will
often leave gaps. They know that science alone cannot determine the proper course of action. But they urge that facts are often the key issue and that whenever possible government should follow the evidence rather than public beliefs. Technocrats also insist that when people know the facts, they will often have a clear sense of what to do.
For their part, populists tend to distrust experts and to think that in a democracy, government should follow the will of the citizenry rather than that of a technocratic elite. In their view, law and policy should reflect what people actually fear, not what scientists, with their inevitably fallible judgments, urge society to do. Populists insist that the very characterization of risks involves no simple "fact," but a host of normative judgments made by so-called experts. To take just one example, while experts usually emphasize the number of fatalities involved in an activity, they could as easily select alternative measures, such as the number of life-years at risk, the percentage of people at risk, or the percentage of the exposed population at risk. According to many populists, risks do not exist "out there," and any judgment about risk is subjective rather than objective in character.1 If expert judgments inevitably involve values of one kind or another, it might seem only sensible to say that popular opinion should be a central ingredient in law and policy. For populists, ordinary intuitions and concerns have normative force and deserve to count in the democratic arena.
To resolve the disagreement, it would be valuable to have a clearer sense of what, exactly, accounts for the split between experts and ordinary people. Is one or another group mistaken? Are ordinary people systematically ill-informed, and if so, why? Are intuitions likely to reflect false judgments of fact, or worthy judgments of value? What
does it mean to say that expert judgments are based on values? Once we answer these questions, there will remain normative problems about what to do in the face of the relevant divisions. Perhaps what matters is not whether people are right on the facts, but whether they are frightened (an obvious possibility in a nation facing a risk of terrorism). Perhaps ordinary people have a special rationality, as worthy in its own way as that of experts. Certainly experts can have their own biases and agendas.1 Perhaps the real issue is how to increase the public's role in risk regulation so that government will respond to its concerns.
In this Review, I attempt to make some progress in answering these questions for purposes of law and policy, above all by attending to psychological evidence on people's perceptions of risk. I do so largely by focusing on the work of the psychologist Paul Slovic and his collaborators, as catalogued in Slovic's recent book, The Perception of Risk. Of all those who have contributed to an understanding of the division between experts and ordinary people, Slovic and his collaborators have been the most systematic and wide-ranging. They have produced a series of empirical studies designed to understand perception of risk - to see when people are frightened and when they are not, and exactly why. Slovic's own views defy easy categorization, but he has strong populist leanings. Slovic denies that risks can be assessed in objective terms (p. 369). In some of his most striking chapters, Slovic claims that ordinary people display a rival rationality that is worthy of consideration and respect (p. 23). Insisting that "risk" is not simply a matter of numbers, Slovic argues that a good system of risk regulation should be democratic as well as technocratic - and that it should pay a great deal of attention to what he sees as the structured and sometimes subtle thinking of ordinary people (p. 231).
The Perception of Risk is an illuminating and important book, written over a number of years and with many coauthors. It covers a great deal of ground. Among other things, Slovic deals with the "so(pp. 232-45) toxicology" (pp. 285315) trust and distrust (pp. 316-26) risk-taking by adolescents (pp. 364-7 i) smokers' (lack of) awareness of the risks of smoking (pp. 364-
71) differences across lines of race and gender (pp. 390-412) and the role of emotions in assessing, taking, and avoiding risks (pp. In this space it would be foolhardy to try to examine all of these issues in detail. Instead, I focus on Slovic's own unifying theme: the difference between the risk perceptions of experts and those of ordinary people. I also try to connect Slovic's claims to issues in law and policy, issues with which he deals only briefly.20 Psychological evidence has implications for many recent problems, emphatically including those raised by the threat of terrorism. It is noteworthy in this connection that Slovic's work is already having a significant impact on discussions within the federal government.
In my view, though not necessarily in Slovic's, the overriding lesson of these essays is that ordinary people often deal poorly with the topic of risk. This fact stems from predictable features of human cognition. For example, people often neglect probabilities, focus on worst case outcomes, and use heuristics that can lead to massive errors. They are also subject to social influences that can lead them astray. This lesson has major implications for private and public law. First, Slovic sheds new light on why the system of legal regulation has taken its current form, showing some of the cognitive mechanisms that produce both paranoia and neglect. Second, Slovic helps to show how government could produce more helpful and effective warnings about risks. Some of his principal lessons involve how to make law and policy work better including how to structure information campaigns, which are unlikely to succeed unless we have a sense of how people perceive risks. Third, I believe that Slovic establishes, perhaps unintentionally, why sensible policymakers should generally follow science and evidence, not the public. This point bears on the design of government institutions as well as the functioning of Congress, regulatory agencies, judges, and
even juries. More generally still, there are many problems with ordinary intuitions, which provide the starting point for the assessment of central issues in many fields. Intuitions about probabilities often go awry the same may well be true for intuitions in general, including moral intuitions. The respect accorded to intuitions - in law, philosophy, and elsewhere - has yet to take adequate account of what has been learned about cognitive errors and biases.23
I make two basic objections to this admirable book. The first and more general is that some of Slovic's own findings seem to raise questions about his claim to have found a "rival rationality." Whatever Slovic's intentions, much of the importance of his work lies in the strong empirical support that it provides for a more technocratic view of regulation, one that draws ordinary intuitions into grave doubt. My second and more specific objection is that Slovic says too little about the social mechanisms by which individuals come to decide whether a risk is serious or trivial. These mechanisms have multiple connections with the cognitive points that Slovic emphasizes. Discussion with others, for example, can make a risk both vivid and salient, and when individuals see a risk as both, they are likely to talk to others, thus increasing both vividness and salience. When like-minded people discuss issues together, they may move one another in extreme directions. As we shall see, both of these objections have implications for law and for government institutions.
This Review comes in several parts, separating Slovic's claims into various categories. I begin with the simpler and more established claims and follow with the more complex and contested ones. In each Part, I offer an outline of the relevant claims, evaluate them, and discuss some of their implications for law. Part I deals with the idea of cognitive heuristics, or mental shortcuts. My emphasis here is on the "availability heuristic," by which people think a risk is more serious if an example can be readily brought to mind. This Part also deals with what Slovic calls "intuitive toxicology," consisting of a set of simple intuitions by which people assess certain risks. Part II explores the role of emotions and, in particular, Slovic's more recent claim that the "affect heuristic" helps to explain people's reactions to risks. Part III discusses Slovic's "psychometric paradigm," the basis for his effort to claim that ordinary people have a "rival rationality" consisting of a value-laden approach to risk competing with that of experts. Part IV discusses some of Slovic's fascinating findings about demographic dif-
ferences, knowledge of risks, and trust. Finally, Part V offers a brief, general discussion of issues of law and policy.
I. MENTAL SHORTCUTS AND INTUITIVE TOXICOLOGY
In several chapters, Slovic emphasizes that people use heuristics, or mental shortcuts, to assess the presence and magnitude of risks.2 As Slovic makes clear, he owes a large debt here to Daniel Kahneman and Amos Tversky, who have uncovered several heuristics that people use to assess probabilities.2 Consider, for example, the "availability heuristic," in accordance with which people assess the probability of an event by seeing whether relevant examples are cognitively "available" (PP. 37-38). Thus, for example, people are likely to think that on a random page more words begin with the letter r or k than have either letter in the third position - even though those consonants are more often in the third position than in the first.2
In the relevant experiments, Kahneman and Tversky explored cognition in general. They did not deal with policy issues or with people's evaluations of social risks. Slovic's major contribution is to show the great importance of the availability heuristic in helping to generate ordinary judgments about risks to health, safety, and the environment. But Slovic also shows a more general way that ordinary people go wrong. They rely on "intuitive toxicology," which contains a range of scientifically implausible judgments, many of them apparently working as mental shortcuts (p. Moving beyond individual cognition, Slovic also calls attention to the "social amplification" of risk. In this Part, I outline Slovic's findings and offer one criticism, or perhaps a friendly amendment: Slovic pays too little attention to the social forces that drive people to fear, or not to fear, certain hazards. I provide a brief discussion of how this gap might be filled.
A. The Availability Heuristic
In Slovic's view, "[t]he notion of availability is potentially one of the most important ideas for helping us understand the distortions likely to occur in our perceptions of natural hazards" (p. 14). These distortions have concrete consequences for behavior. For example, the
another study, people were given two causes of death and asked to say which produces more fatalities (p. 38). People made large mistakes, and when they did so, the availability heuristic was partly responsible. "In keeping with availability considerations," Slovic writes, "overestimated items were dramatic and sensational whereas underestimated items tended to be unspectacular events which claim one victim at a time and are common in non-fatal form" (p. 107). Specifically, participants significantly overestimated highly publicized causes of death, including tornadoes, cancer, botulism, and homicide. By contrast, they underestimated the number of deaths from strokes, asthma, emphysema, and diabetes. At the same time, people tended to think that the number of deaths from accidents is higher than the number of deaths from disease, whereas the opposite is true. In the same vein, participants mistakenly believed that more people die from homicides than from suicides. Availability can also "lull people into complacency," as when certain risks, not easily accessible, seem invisible. What is out of sight is "effectively out of mind" (pp. lo8-O9) recall here the case of inadequate security measures at American airports.
These points suggest that highly publicized events make people fearful of statistically small risks.3 Both law and policy are likely to be adversely affected by people's use of mental shortcuts. Public officials, no less than ordinary people, are prone to use the availability heuristic. And in a democracy, officials, including lawmakers, will respond to public alarm. If citizens are worried about abandoned hazardous waste dumps, we might well expect that substantial resources will be devoted to cleaning them up, even if the risks are relatively small. Similar problems will appear in courts, with juries and judges taking "phantom risks" quite seriously.3 There is also a lesson for potential entrepreneurs about how to attract public attention to a risk: make a vivid example of its occurrence highly salient to the public.
HARVARD LAW REVIEW
This way of proceeding, far more than statistical analysis, is likely to activate public concern.
B. Intuitive Toxicology
Are ordinary people toxicologists? Slovic thinks so (p. 285). He uncovers the content of "intuitive toxicology" by comparing how experts (professional toxicologists) and ordinary people think about the risks associated with chemicals. The result is a fascinating picture. It is not clear what identifiable heuristics are at work in intuitive toxicology it would be valuable to make more progress here. But it is clear that people are using mental shortcuts and that these shortcuts lead to errors.
Slovic elicits the views of toxicologists and ordinary people on the following propositions, among others (pp. 290-98):
i. If you are exposed to a carcinogen, then you are likely to get cancer.
There is no safe level of exposure to a cancer-causing agent.
3. If a scientific study produces evidence that a chemical causes cancer in animals, then we can be reasonably sure that the chemical will cause cancer in humans.
4. The land, air, and water around us are, in general, more contaminated now than ever before.
5. All use of prescription drugs must be risk-free.
6. Natural chemicals, as a rule, are not as harmful as man-made chemicals.
7. Residents of a small community (3o,ooo people) observed that several malformed children had been born there during each of the past few years. The town is in a region where agricultural pesticides have been used during the last decade. It is very likely that these pesticides were the cause of the malformations.
When asked whether they agree, disagree, or have no opinion, ordinary people expressed agreement with many such statements by pluralities or even majorities (pp. 290-98). By contrast, toxicologists disagreed with such statements, usually by overwhelming majorities (pp. 290-98).
What are ordinary people thinking? Can we discern some structure to their perceptions? There does seem to be a "carcinogens are deadly" heuristic here, which is responsible for agreement with the first three propositions set forth above. But three more fundamental beliefs seem to play a large role. First, many people appear to believe that risk is an "all or nothing" matter something is either safe or dan-
have to be made. Nonspecialists may do well to rely on such principles, but policymakers should do a good deal better.
Slovic also finds that experts do not entirely agree among them(pp. 292-93, by industry are far more optimistic about chemical risks than are toxicologists employed by government or academic institutions there is a large "affiliation bias," so that people tend to believe what their institutions would want them to believe (p. 311). 1 shall say more about this particular bias below. But the differences among toxicologists are dwarfed by the differences between toxicologists and ordinary people. Experts are likely to be biased by their work for an organization with a stake in the outcome - but even acknowledging this point, experts are, on many fundamental issues, in basic accord with one another.
C. Social Amplification
As Slovic is aware, mental shortcuts do not operate in a social vacuum interpersonal influences play a large role. Most of the time, most of us lack independent knowledge of risks, and we must therefore rely on the beliefs of others in making our assessments.
Slovic's principal treatment of this point comes in a discussion of what he terms the "social amplification of risk" (pp. The primary purpose of this discussion is not to explain how social influences affect people's perception of risks, but instead to show what might be missed by conventional efforts to tabulate the costs and benefits associated with risks. For example, the 1979 accident at Three Mile Island "demonstrated dramatically that factors other than injury, death and property damage can impose serious costs and social repercussions" (p. 234). Although no one was killed or even harmed by the accident, it imposed "enormous costs" on the nuclear industry and society more generally, resulting in tighter regulations, decreased operation of reactors globally, heightened public opposition to nuclear power, and a less viable role for nuclear power as a major long-term energy source (pp. 234-35). Slovic is concerned that a conventional risk analysis, focusing on the likelihood and severity of injury, will overlook these kinds of consequences (p.
In some domains, however, Slovic finds that society systematically underestimates potentially serious harms, a phenomenon he terms "social attenuation of risk" (p. 235). As examples, Slovic offers indoor radon, smoking, and driving without a seat belt (p. 235). Certainly the terrorist attacks of September provide the most recent and salient illustration those attacks produced harms of multiple kinds that extended well beyond the day of the attacks. A comprehensive account will take a long time to develop, but consider, for illustrative purposes, the following facts and projections, which were developed within a short period following the attacks: New York City was expected to
publicly, "[y]ou get made to feel like a pedophile. 1 Or suppose that in October 200i a policy analyst objected that new security measures at airports would be unlikely to increase safety because, by reducing the convenience of air travel, they might lead more people to drive, thus causing a net loss of lives. Such an analyst might well remain silent out of a desire to avoid the opprobrium that would predictably follow.
Lawmakers, even more than ordinary citizens, are vulnerable to reputational pressures. In fact, that is part of their job. Lawmakers might well support legislation to control risks that they privately know to be insignificant. In the context of regulation of hazardous waste dumps, reputational factors helped to fuel a cascade effect that eventually led to the enactment of the Superfund statute,5 which was designed to ensure the remediation of abandoned hazardous waste sites. The apparent adverse health effects of abandoned waste at Love Canal made it extremely difficult for government officials to resist public concern, even though the evidence of serious risk was far from clear.5
But the problem of reputational pressure does not necessarily result in excessive legal controls. In a phenomenon similar to Slovic's social attenuation of risks, we can imagine "unavailability cascades," in which an individual's relative indifference to statistically significant risks leads other people to be indifferent as well. Return again to American neglect of airport security, a problem that greatly concerned experts. Undoubtedly, informational and reputational forces help to account for public indifference to many hazards that trouble experts.
There is an additional point. Some people are especially influential because they are known to be reliable, and here the informational "signals" of their statements and actions are especially loud. Some people are particularly influential because they can inflict high reputational costs on people who do not conform. For this reason, prestigious people are in a good position to fuel a cascade.
Note that these points about social influences complement Slovic's cognitive claims, including the availability heuristic and intuitive toxicology. What is "available" is a function of informational and reputational forces. If people are spreading the idea that arsenic "causes cancer" and should therefore be banned, then the commonly held assumptions described by Slovic as intuitive toxicology will spread as
if by contagion. If people are talking about the risks associated with pesticides, disposable diapers, air travel, or shark attacks, those risks will be available in the public mind. The case of shark attacks offers an especially vivid example. A LEXIS search found over iooo references to shark attacks between August 4, and September 4,
and references to "the summer of the shark, notwithstanding the absence of any reliable evidence of an increase in shark attacks in the summer of Predictably, there was discussion of new legislation to control the problem.
3. Group Polarization. - A related phenomenon, "group polarization,"67 helps to strengthen social pressure. It is well established that when a group deliberates, group members tend to move toward a more extreme position in line with their predeliberation inclinations. Thus, for example, people who tend to fear the effects of second-hand smoke, or who believe that pesticides carry significant risks, are likely
previously neglected problems. But sometimes it will lead to perverse results, in the form of massive expenditures on small or even nonexistent problems.7 I have argued here that some of the gaps in Slovic's presentation will be filled if we look more closely at social influences on behavior.
II. EMOTIONS AND THE AFFECT HEURISTIC
As Slovic emphasizes, most psychological work on risk has focused primarily on cognition (p. 413), asking whether mental heuristics produce errors and how people depart from what is generally considered rational behavior. But thinking about perceived risks only in these terms seems incomplete. With respect to risks, many of our ordinary ways of speaking suggest strong emotions: panic, hysteria, terror.7 A central point here is that when strong emotions are at work, people tend to focus on the "worst case," in a way that makes the outcome, but not its probability, highly salient.7 Slovic explores a key question: how do people's feelings affect their reactions to risks?
A. The Affect Heuristic
Slovic's interest in this topic appears to have been triggered by a remarkable finding: when asked to assess the risks and benefits associated with certain items, people tend to say that risky activities carry low benefits and that beneficial activities carry low risks (pp. 415It is rare that people will see an activity as both highly beneficial and quite dangerous, or as both benefit-free and danger-free. This phenomenon is extremely odd. Why don't people more frequently think that an activity is both highly beneficial and highly risky? Why do they seem to make a gestalt-type judgment, one that drives assessment of both risks and benefits?
Aware that risk and benefit are "distinct concepts," Slovic thinks that "affect" comes first and helps to "direct" judgments of both risk and benefit. Hence he suggests an "affect heuristic," by which people have an emotional, all-things-considered reaction to certain processes and products. The affect heuristic operates as a mental shortcut for a more careful evaluation. Slovic finds that the affect heuristic even
plays a role among toxicologists, whose judgments also show an inverse relationship between perceived risk and perceived benefit.
To test this hypothesis, Slovic offers several studies. One of the most interesting is designed to test whether new information about an item's risks alters people's judgments about its benefits - and inversely, whether new information about benefits alters people's judgments about risks (pp. 421-26). The motivation for this study is simple: if people's judgments are analytical and purely cognitive, then information about the great benefits of, say, food preservatives should not produce a judgment that the risks are low - just as information about the great risks of, say, fossil fuels should not make people think that the benefits are low.
Strikingly, these studies confirm Slovic's hypothesis that information about benefits alters judgments about risks and that information about risks alters judgments about benefits (pp. 423-25). When people learn of the low risks of an item, they are moved to think that the benefits are high when they learn of the high benefits of an item, they are moved to think that the risks are low. Slovic concludes that people assess products and activities through affect: information that improves people's affective responses improves their judgments of all dimensions of those products and activities. He connects judgments about risks with the "halo effect," which predicts that "the favorability of the overall impression of an attitude object is a good predictor of how strongly positive or negative qualities are ascribed to the object"
to increase their "liking," will improve their judgments of all aspects of those products and activities. The central idea is that when presented with a risk, people have a general emotional attitude to it - hence an "affect" - and that this general attitude operates as a heuristic, influencing people's judgments about both benefits and costs. As Slovic puts it, "the affect heuristic enables us to be rational actors in many important situations," but not in all, and it "fails miserably when the consequences turn out to be much different in character than we anticipated."7
B. What's an Emotion?
Slovic's basic claim is quite plausible, for emotions obviously play a large role in reactions to risk and they help to explain otherwise anomalous behavior.80 I raise some questions about the usefulness of this claim below. But as we shall see, one result of strong emotions is to drive out quantitative judgments, including judgments about probability, by making the best case or the worst case seem highly salient. This point has received empirical confirmation.8 In one study, people's willingness to pay to avoid an electric shock varied little, despite changes in probability from I% to The same phenomenon seems to extend to people's reactions to risks in general. Indeed, there seems to be a close and thus far neglected link between the availability heuristic, studied in purely cognitive terms, and emotions. When the availability heuristic is at work, a salient example is driving judgment, often undermining the mental operation of assessing probability.
An affect heuristic operates in many social domains. To take some issues far afield from Slovic's own concerns, consider the remarkable fact that stock prices increase significantly on sunny days, a phenomenon that is hard to explain in terms that do not rely on affect.8 Consider too the fact that the emotion of outrage plays a large role in pro-
that fear in human beings is precognitive or noncognitive, and even if it were in some cases, it is not clear that noncognitive fear would be triggered by most of the risks faced in everyday human lives. Perhaps more to the point, existing experiments suggest that when it comes to risk, a key question is whether people can imagine or visualize the "worst case" outcome they also suggest that surprisingly little role is played by the stated probability of that outcome actually occurring. In other words, people's reactions to risks are often based mostly on the harms caused by the potential outcome and the vividness of that outcome rather than on the probability of its occurrence. Consider these points:
i. When people discuss a low-probability risk, their concern rises even if the discussion consists mostly of apparently trustworthy assurances that the likelihood of harm is infinitesimal.
When asked how much they would pay for flight insurance against losses resulting from terrorism, people indicate a willingness to pay more than they would pay for flight insurance against losses resulting from all
3. People show "alarmist bias": when presented with competing accounts of danger, people tend to believe the more alarming ac-
4. Visualization or imagery greatly influences people's reactions to risks. When an image of a bad outcome is easily accessible, people will become greatly concerned about the risk associated with it, holding probability
5. If the potential outcome of a gamble has a great deal of associated affect (for example, an electric shock), its attractiveness or unat-
tractiveness is remarkably insensitive to changes in probability, even when the probability falls from 99% to
6. People's willingness to pay to eliminate a small probability of cancer varies a great deal, depending on whether the cancer is described simply as "cancer" or instead quite emotionally as "very gruesome and intensely painful, as the cancer eats away at internal organs of the body."97 The relevant experiment involved four conditions. People in the first group were asked to state their maximum willingness to pay to eliminate a cancer risk of i in ioo,ooo, without embellishment the second group was asked about the same statistical risk, but with the addition of the "very gruesome and intensely painful" language members of the third group were asked to state their willingness to pay to eliminate a cancer risk of i in i,ooo,ooo, without embellishment and the fourth group was asked about a risk of i in i,ooo,ooo, with the "very gruesome and intensely painful" language. The emotional description of the cancer doubled people's willingness to pay - the same effect as a tenfold increase in the probability of harm! And under the emotion-laden conditions, variations in probability had essentially no effect, producing the same median willingness to pay for a i in ioo,ooo risk as for a i in i,ooo,ooo risk.
A possible conclusion is that with respect to risks of injury or harm, vivid images and concrete pictures of disaster can "crowd out" other kinds of thoughts, including the crucial thought that the probability of disaster is tiny. John Weingart observes that "[i]f someone is predisposed to be worried, degrees of unlikeliness seem to provide no comfort, unless one can prove that harm is absolutely impossible, which itself is not possible." This point explains "why societal concerns about hazards such as nuclear power and exposure to extremely small amounts of toxic chemicals fail to recede in response to information about the very small probabilities of the feared consequences from such hazards." ' With respect to hope, those who operate gambling casinos and state lotteries are well aware of the underlying psychological mechanisms. They play on people's emotions by conjuring up palpable images of victory and easy living. With respect to risks, insurance companies, environmental groups, and terrorists do exactly the
An important lesson follows: if government seeks to encourage people to avoid large risks and to worry less about small risks, it might best accomplish this by appealing to their emotions. With respect to cigarette smoking, reckless driving, and alcohol or drug abuse, this is exactly what government occasionally attempts to do. It should be no surprise that some of the most effective efforts to reduce cigarette smoking appeal to people's emotions by convincing them that if they smoke, they will be dupes of the tobacco companies or will harm innocent third parties.1
What Produces Affect? - Is the affect heuristic really a heuristic? If so, for what? What is the relationship between affect and the judgment for which it serves as a heuristic? To answer these questions, we have to step back a bit.
On the conventional understanding, a heuristic is a mental shortcut, or a rule of thumb, that substitutes an easy question for a harder one.' When asked about the probability of getting caught in a major traffic jam on a particular route, you might refuse to engage in a statistical analysis and instead ask whether you, or people you know, have been stuck in traffic jams on that route. As we have seen, people often assess probabilities by substituting an inquiry into salient examples for an assessment of statistics. People use other rules of thumb to simplify complex issues. In politics, some people think that if the Republican Party platform is for something, they should assume that they are against it party identification operates as a heuristic. In law, some people think that if Justice Antonin Scalia makes a constitutional argument, they should assume that it is correct. As elsewhere, the relevant heuristic works as a rule of thumb though the heuristic will sometimes point the wrong way, it will usually lead people in directions that they prefer.
Does the affect heuristic work this way? Does it substitute an easy question for a harder one, operating as a rule of thumb? If so, for what? Slovic argues that "[u]sing an overall, readily available affective impression can be far easier - more efficient - than weighing the pros and cons or retrieving from memory many relevant examples, especially when the required judgment or decision is complex or mental resources are limited" (p. 415). More particularly, Slovic appears to think that in asking whether a product or activity is risky, people substitute an easy question ("How do I feel about it?") for a harder question ("What are the pros and cons?"). On this view, it is often sensible
to use the affect heuristic, although it can sometimes lead people to mistakes.
All of this seems plausible, but there is a difficulty here: "affect" itself remains to be explained, and it may be a product of, rather than a substitute for, some kind of assessment of pros and cons. If so, it is unhelpful to call affect a heuristic after all:1 we cannot say that an assessment of pros and cons is a heuristic for that very assessment. Perhaps it could be said, as Slovic urges, that affect is rapid and to some extent unconscious, very much a substitute for an explicit weighing of relevant variables Slovic's studies are highly suggestive here. But to learn more, we would need more information about why people have one or another affect toward nuclear power, pesticides, x-rays, and the like. In most cases, affect is probably a result, at least in part, of a very rough assessment of risks and benefits - an assessment itself produced, in many cases, by the availability heuristic (as Slovic suggests might be the case, p. 427), intuitive toxicology, other mental shortcuts, and certain value judgments. Much remains to be learned about the relationships among affect, statistical assessment, and other aspects of human cognition. As Slovic acknowledges, his work here is "merely a beginning" (p. 426).
THE PSYCHOMETRIC PARADIGM: ARE EXPERTS
IRRATIONAL?
Thus far, my discussion has emphasized the susceptibility of ordinary people to mistaken beliefs about risks. This problem is indeed one of Slovic's central concerns. But he also insists that, in many ways, ordinary people are not mistaken at all and that their perceptions about risks involve evaluative judgments that are worthy of respect. For Slovic, experts often seem obtuse. This claim is one of the most striking in Slovic's book, and it deserves careful attention, in part because the federal government is now taking some of Slovic's arguments directly into account.
A. Rival Rationality? The Basic Claim
The idea that ordinary people make subtle judgments of value is embodied in what Slovic calls the "psychometric paradigm" (p.
According to the psychometric paradigm, ordinary people, like experts, certainly care about the number of people at risk as a result of some product or activity. But there is a significant difference between the two camps. Experts tend to focus on only one variable: the number of lives at stake. But ordinary people, according to Slovic, have a less crude and more complex approach (p. 223). They care about a range of qualitative variables that can produce materially different evaluations of statistically identical risks. 1°"
Thus, Slovic uncovers a long list of factors that, holding expected fatalities constant, can make risks more or less acceptable. These factors include whether the risk is dreaded, potentially catastrophic, inequitably distributed, involuntary, uncontrollable, new, and faced by future generations (p. 140). Because these factors are so crucial, "riskiness means more to people than 'expected number of fatalities"' (p. 231). And because they focus on such factors, Slovic believes that in an important respect ordinary people think better, and more rationally, than experts do. According to Slovic, people's "basic conceptualization of risk is much richer than that of experts and reflects legitimate concerns that are typically omitted from expert risk assessments" (p. This distinction provides the basis for Slovic's claim that experts and ordinary people display "rival rationalities" and that "[e]ach side must respect the insights and intelligence of the other" (p. 23).
This is a striking and provocative claim. And in some ways, it is clearly correct. The risks associated with voluntary activities (for example, skiing, horseback riding, playing football) generate less public concern than statistically much smaller risks from involuntary activities (for example, ingesting food preservatives, pesticides, herbicides, certain forms of air pollution). People do seem willing to pay more to prevent a cancer-related death than to prevent a sudden, unanticipated death, in part because cancer is especially "dreaded. 1o In pointing to the importance of qualitative factors, Slovic makes a significant advance - one that deserves to be incorporated into regulatory policy.
We can go further. If Slovic is right, then the populist view has strong empirical support and the technocratic position has taken a real blow. The reason is that (some) technocrats miss important values to which ordinary people rightly call attention. Indeed, Slovic's conclusion suggests that regulatory policy should be fundamentally re-
thought, with the government incorporating the relevant values far more thoroughly than it has yet done.
1 do not believe, however, that Slovic's evidence establishes as much as he thinks it does. Some of the central factors he identifies dread, voluntariness, control - seem underanalyzed. I also suspect that Slovic misses an important part of the picture, supplied partly by his own work on the availability and affect heuristics an understanding of that part reinforces the view that when experts and ordinary people differ, experts are right and ordinary people are wrong.
B. Are Risks Real?
To explore these issues, it is important to investigate one of Slovic's most general claims. In Slovic's view, risk is "inherently subjective" (p.
It "does not exist 'out there', independent of our minds and cultures, waiting to be measured" (p. Slovic argues that "human beings have invented the concept risk to help them understand and cope with the dangers and uncertainties of life" (p. 392). Hence "there is no such thing as 'real risk' or 'objective risk"' (p. 392). To support this claim, Slovic emphasizes that mortality risks can be expressed in many different ways, including deaths per million in the population, deaths per facility, deaths per ton of chemical produced, deaths per million people within x miles of the source of exposure, and loss of life expectancy associated with exposure to the hazard (p. 393). Slovic also argues that "subjectivity permeates risk assessments" (p. 393), because multiple assumptions can be made, including the account of the doseresponse relationship and the means of measuring the number of deaths (p. 393).
I think that Slovic is actually making three different claims here. Two of them are correct and important, but the third, and most striking, seems to me largely semantic and unhelpful, even misleading. Slovic is correct, first, to identify both the different ways to describe mortality risks and the normative judgments that accompany the choice. To take a mundane example, we can express risks in terms of lives lost or in terms of life-years lost. Obviously the choice will matter a great deal if, for example, we are debating a legal intervention that would mostly extend the lives of elderly people by a short period.
Slovic is correct, second, to say that in the face of scientific uncertainty, assumptions must be made and science may not enable us to say which assumptions are correct. For many carcinogens, for exam-
fits and impose significant risks. On this view, the rankings on the qualitative dimensions do not have much explanatory power. On the contrary, they are explained, at least in part, by people's intuitive belief that large statistical risks are associated with certain activities. The point should not be overstated. Slovic is undoubtedly right to say that the qualitative factors matter. But his evidence seems too crude to disprove a competing hypothesis - that people's rankings of risks reflect, in significant part, the roles of affect and crude, rough-andready assessments of net benefits, and are not mostly driven by a richer rationality. 117
If we look closely at Slovic's actual list of hazards, it is not at all clear how to explain the results in terms of qualitative factors. Of ninety hazards, smoking ranks ninth, while marijuana ranks eightyfifth (pp. 143-45). Do the "qualitative factors" on Slovic's list really account for this dramatic difference? What is the richer rationality that produces these judgments? Are the risks of smoking less voluntary and more inequitably distributed? Or consider the fact that pesticides and herbicides rank seventh and eighth on the list, while x-rays rank thirtieth, food preservatives thirty-fifth, and food irradiation thirty-ninth (pp. 143-45). Can this pattern of judgments really be explained in terms of voluntariness, equity, potentially catastrophic quality, and risk to future generations? The answer is far from obvious. Might it not be more reasonable to speculate that people's rankings are based on some combination of the affect heuristic and a quick, intuitive, imperfectly informed assessment of the magnitude of the relevant hazards and accompanying benefits? To be sure, it is possible to suggest that people think that smoking is addictive and hence peculiarly
Margolis's basic account is exceedingly simple. He thinks that in some cases ordinary people are alert to the hazards of some activity, but not very alert to its benefits, which are cognitively "off-screen.' In such cases, people will tend to think "better safe than sorry,"' causing them to have a highly negative reaction to the risk. In such cases, they will demand aggressive and immediate regulation. In other cases, the benefits, but not the hazards, of the activity will be very much on people's minds, and people will tend to think "nothing ventured, nothing gained.' In these cases, they will think that regulators are overzealous, even fanatical. In still other cases - in Margolis's view, the cases in which observers behave most sensibly - both benefits and risks will be "on-screen," and people will assess risks by comparing the benefits with the costs.
It is reasonable to think that, for experts, benefits and costs are usually on-screen and that when ordinary people are much more alarmed than experts, it is sometimes because the risks are apparent but the benefits are not. How else are we to make sense of the fact that the very small risks associated with x-rays do not occasion much concern, while the very small risks associated with pesticides and herbicides frequently appear on the list of most feared risks? A sensible conjecture is that people know that, on balance, a world with x-rays is safer and better than a world without them. For pesticides and herbicides, by contrast, the benefits seem far less tangible, if visible at all. It is safe to predict that if a reliable source told people that eliminating pesticides would lead to serious health problems - for example, because pesticide-free fruits and vegetables carried special dangers the perceived risk of pesticides would decline dramatically. Indeed, I predict that if people were informed that eliminating pesticides would lead to a significant increase in the price of apples and oranges, the perceived risk would decrease as well.
Margolis offers a nice example to support this prediction. The removal of asbestos from schools in New York City was initially quite popular, indeed demanded by parents, even though experts believed that the risks were statistically small.' ' But when it emerged that the removal would cause schools to be closed for a period of weeks, greatly inconveniencing parents, parental attitudes turned right around and
ties. In the terms of Slovic's more recent work, experts rely on a form of statistical analysis, while ordinary people use more "experiential thinking," which leads to errors. But Slovic is right to insist that values matter. When people's reflective judgments drive their perceptions, those judgments deserve to play a role in policy. To the extent that experts focus only on the number of lives at stake, they are genuinely obtuse. It is reasonable to devote special attention to dangers that are hard to avoid, accompanied by unusual suffering, or faced principally by children. On the positive side, what is needed is more empirical work to determine the extent to which ordinary risk perceptions are based on errors or instead on values. On the normative side, what is needed is more thought about the nature of concepts like "dread," "voluntariness," and "controllability." With respect to policy, what is needed is incorporation of people's values to the extent that they can survive a process of reflection.
IV. DEMOGRAPHY, TRUST, AND PATERNALISM
I now turn to three issues on which Slovic offers a number of intriguing findings of special importance to law and policy. These involve demographic differences, the crucial issue of trust, and paternalism.
A. Demography
Do members of different social groups agree about risks? Slovic offers several interesting findings. The most general is what he calls "[t]he white-male effect" (p. 399). Apparently white men are less concerned about risks than are members of other groups. With respect to nearly every risk, white women, nonwhite women, and nonwhite men are far more troubled than are white men (pp. 397-99). But this is the punchline of the story, and it is helpful to provide the details.
Slovic asked large numbers of people to rank a large number of activities as posing little or no risk, slight risk, moderate risk, or high risk. Women ranked nearly every risk higher than men did (pp. 396402). These results obtained for ordinary Americans. They hold as well for British toxicologists: female British toxicologists ranked risks as more serious than did their male counterparts (p. 397). Among ordinary Americans, the differences between men and women are most pronounced for stress, suntanning, nuclear waste, nuclear power plants, ozone depletion, and AIDS, and least pronounced for medical x-rays, commercial air travel, and genetically engineered bacteria (p.
398). Among British toxicologists, the sex differences are especially large with respect to nuclear waste, nuclear power reactors, outdoor air pollution, alcoholic beverages, and suntanning (p. 397), and smallest for radon in homes, motor vehicle accidents, and mercury in fillings (p. 397).146 There are other demographic differences too: as education and income increase, fear of almost all risks decreases (p. 399). Notably, however, sex differences hold even when the results are controlled for education and income.
Once the data are disaggregated, a still more striking and somewhat different picture emerges. Although men as a group perceive risks as less serious than do women as a group, race affects risk perception as well. Nonwhites see risks as more serious than do whites, with especially large differences for bacteria in food, genetically engineered bacteria, and pesticides in food, and smaller differences for outdoor air quality, ozone depletion, and stress (p. 399). But there is more: white males turn out to be the real outliers, ranking risks as less serious than other demographic groups do. Across a large number of hazards, white men perceive risks as consistently lower than do white women, nonwhite women, and nonwhite men (pp. 398-4oo). In fact, there are no large-scale differences among the latter three groups. Even more interestingly, what drives the "white-male effect" is not the view of all white males, but the view of about 30% of them who believe almost all risks to be very low (pp. 398-99). The other 70% of white males are not greatly different from the other subgroups.
Slovic observes that the relevant 30% tend to have more education and more household income and also to be more conservative (pp. 399-4oo). They tend to disagree more with the view that technological development is destroying nature, to reject the idea that they have very little control over risks to their health, and to think that future generations can take care of themselves when facing risks imposed as a result of today's technologies. Slovic thinks that affect is part of what is responsible for this distribution of beliefs about risks (pp. 40309).
All of this is extremely interesting. There is, however, a question in the background: is it helpful or even meaningful, for purposes of analysis, to isolate the 30% of white men who believe almost all risks to be low? This group was selected not because of anything that unites them, but because the views of its members differed systematically from the entire remaining pool of nonwhites, white females, and
nologies involving radiation and chemicals (such as x-rays) as high benefit and low risk, whereas they tend to view industrial technologies involving radiation and chemicals (such as nuclear power and pesticides) as high risk and low benefit (p. 316). Far more important than technical risk assessments is the level of trust in those who try to manage risks and to give assurances.
Slovic also stresses the fragility of trust, which is far easier to destroy than to create.1 Slovic offers a study of forty-five hypothetical news events involving nuclear power some of these events were designed to increase trust in those managing its risks, whereas others
were judged to have had a much more significant effect than positive events. For example, a nuclear power plant accident in another state had a strong negative effect on trust, whereas careful selection and training of employees or an absence of any safety problems in the past year had little effect in increasing trust (pp. Slovic also finds that sources of bad, trust-destroying news are seen as more credible than are sources of good, trust-creating news (pp. This finding is in line with a related one mentioned above: when people are unconcerned about a risk, discussion that is designed to provide still more assurance tends to increase anxiety rather than to diminish it.
With respect to trust, Slovic therefore establishes the existence of an "asymmetry principle": events that weaken trust have a significant effect, whereas events that strengthen trust do very little (p. Does this mean that people are confused or irrational? Slovic doesn't think so: "Conflicts and controversies surrounding risk management are not due to public irrationality or ignorance but, instead, can be seen as expected side effects of these psychological tendencies, interacting with our remarkable form of participatory democratic government, and amplified by certain powerful technological and social changes in our society" (p. 323). Many of those changes involve the news media, which can instantaneously publicize risk-related events from anywhere in the world. (Consider the anthrax frenzy of October And the news media, no less than ordinary people, give special emphasis to bad, trust-destroying events. This asymmetry has important effects on public attitudes. Slovic argues that "[t]he young science of risk assessment is too fragile, too indirect, to prevail in such a hostile atmosphere" (p. 324). Slovic does not explain exactly why trust is more easily destroyed than created perhaps the asymmetry principle is partly a product of risk aversion, which makes people respond more when trust
is challenged than when it is confirmed. In any case, Slovic shows that the asymmetry is quite important for policymakers.
What can be done to increase trust? Slovic favors an increase in public participation in decisionmaking, going well beyond public relations to include much more in the way of actual power-sharing (p. 325).154 His fear is that governmental efforts to reassure people are unlikely, without broad participation, to breed anything but further distrust. On this count, Slovic might well be right, but his own data raise serious questions about the value of public participation in increasing trust.155 If bad news is more salient than good news and if people act as intuitive toxicologists whose perceptions of risk can be amplified by social interactions, there is special danger: high levels of public participation in technical domains could simply heighten public fear, with unfortunate consequences for policy. Recall here the epigraph to this Review: "At the Siting Board's open houses, people would invent scenarios and then dare Board members and staff to say they were impossible."'. Like the questioners in this passage, many Americans would find that more exposure to this type of information feeds their fears.
Slovic is right to emphasize the importance of trust and also to connect that issue to the split between experts and ordinary people. But efforts to increase public participation in the regulatory process could have many unforeseen consequences. Greater participation might make things worse rather than better, fueling both fear and distrust. If people come to the process with the intuitions that Slovic catalogues, then it is not clear that broad public involvement will be helpful for either sound regulation or trust itself. Group polarization could well increase the problem. Imagine, for example, that government attempted to increase public participation in a decision to license a nuclear power plant or to allow a certain level of arsenic in drinking water. It is easy to imagine a situation in which reassurances would fall on deaf ears and in which the very fact of participation, and broader citizen involvement, would increase fear and distrust.
V. RISK REGULATION, PSYCHOLOGICALLY INFORMED
findings, Breyer's analysis might well be criticized on the ground that ordinary people would distrust any body of risk specialists, viewing them as unlikely to be sufficiently responsive to their concerns. Perhaps more interestingly, Breyer's approach might be criticized on the ground that sheer numbers cannot tell us whether a problem is "large" or "small." People are reasonably concerned about a range of other variables. This is a large thrust of Slovic's work.
In the end, however, Slovic's findings seem to me mostly supportive of Breyer's analysis indeed, Slovic provides strong cognitive ammunition for Breyer's diagnosis and even his remedy. Slovic shows that ordinary people make many mistakes in thinking about the risks associated with various activities. He also shows that, much of the time, people treat safety as an "all or nothing" matter, are vulnerable to the "zero-risk mentality," overreact to small signals of dangers, and sometimes show excessive optimism. In these circumstances, a sensible system of risk regulation will not respond mechanically to what people think it will impose thick filters on the public's own conception of appropriate priorities.
But there are three grounds for objecting to a purely technocratic approach to risk regulation. The first emerges from analysis of some of the qualitative factors that Slovic highlights. Some deaths are particularly bad, and these deserve unusual attention. When it is especially easy to avoid certain risks, government should not spend a great deal of time and effort in response. Inequitably distributed risks deserve special attention, as do risks that impose special suffering. It would indeed be obtuse to treat all statistically equivalent risks as if they were the same, regardless of context or quality. But it remains true that a sensible society is greatly concerned with ensuring that people have longer and healthier lives - and that if policies lead government to spend a lot on small problems and little on large ones, something is amiss. Note in this regard that a more sensible allocation of public resources could save tens of thousands of lives, tens of billions of dollars, or both. 183 It makes sense both to acknowledge qualitative distinctions among statistically equivalent deaths and to insist that better priority-setting is extremely important.
The second criticism of a purely technocratic approach involves people's likely reaction to it. To work well, a regulatory system needs public support and trust, even if we do not believe that a lack of trust would be fully rational. To the extent that government relies on statistical evidence alone, it is unlikely to promote its own goals. This is
