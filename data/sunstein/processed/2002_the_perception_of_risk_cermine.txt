THE LAWS OF FEAR 
Publications Ltd. Pp. xxxvii, 473. Â£19.95. 
"The nation is quickly buying up stocks of gas masks, shelves are being stripped of antibiotics,and bottled water may not be far behind. Many travelers have canceled trips by airand taken trains or cars instead, even across the country. New Yorkers fearfid of an attack on the subways insist on riding in cars on traffic-choked streets. Doctors in Boston report that patients with minor ailments like colds and sore throats have been calling out of fear that they may have been sickened by a toxic chemical or lethal germ introduced by terrorists. Meanwhile, business at McDonald's and Haagen-Dazs is thriving. What does this say about how people respond to potential threats to their health and lives?"1 "With the deaths of two people from shark attacks over the Labor Day weekend, the summerlong fascination with these fear-inducing creatures of the deep has turned into a near obsession as Americans wonder whether the oceans are safe for recreation and sport. But to scientists who study sharks and their occasionallytragic interactionswith humans, the numbers of shark attacks and fatalities appear well within the range of normal. What is out of the ordinary is the amount of attention being paid to them.... A person is 30 times more likely to be struck by lightning than bitten by a shark .... 2 "At the Siting Board's open houses, people would invent scenariosand then dare Board members and staff to say they were impossible. A person would ask, 'What would happen if a plane crashed into a concrete bunker filled with radioactive waste and exploded?' We would explain that while the plane and its contents might explode, nothing in the disposalfacility could. And they would say, 'But what if explosives had mistakenly been disposed of and the monitoring devices at the facility had malfunctioned so they weren't noticed?' We would head down the road of saying that this was an"e3xtremely unlikely set of events. And they would say, 'Well, it could happen, couldn't it?" * Karl N. Llewellyn Distinguished Service Professor, Law School and Department of Political Science, University of Chicago. I am grateful to Daniel Kahneman, James Krier, Howard Margolis, Martha Nussbaum, Richard A. Posner, and Paul Slovic for very helpful comments on a previous draft. Participants in work-in-progress lunches at Columbia University and the University of Chicago Law School also provided a great deal of help. A discussion with Christopher Hsee was extremely helpful. Some of the arguments in this Review are developed at greater length in CASS R. SUNSTEIN, RISK AND REMsON: SAFETv, LAW, AND THE ENVIRONMENT (forthcoming 2002). 
1 Jane E. Brody,Don't Lose Sight of Real, Everyday Risks, N.Y. TIMES, Oct. 9, 2001, at F6. 2 Stevenson Swanson, High-Profile Attacks Feed Shark Fears,Experts Say, CHI. TRIB., Sept. 5, 2oo, at i. 
3 JOHN WEINGART, WASTE IS A TERRIBLE THING To MIND 362 (2001). 
In the late I98Os, the Environmental Protection Agency (EPA) embarked on an ambitious project, designed to compare the views of the public and EPA experts on the seriousness of environmental problems. 4 The project revealed some striking anomalies, for the two groups sharply diverged on some crucial issues.5 
With respect to health risks, the public's top ten concerns included radioactive waste, radiation from nuclear accidents, industrial pollution of waterways, and hazardous waste sites.6 But in the view of EPA experts, not one of these problems deserved a high level of concern.' Two of the public's top concerns (nuclear accident radiation and radioactive waste) were not even ranked by EPA experts. 8 Of health risks considered by the public, among the lowest ranked were indoor air pollution and indoor radon - even though both were ranked among the worst environmental problems by expertsY The EPA concluded that there was a remarkable disparity between the views of the public and the views of its own experts.' 0 It also noted, with evident concern, that EPA policies and appropriations seemed to reflect the public's preoccupations, not its own." If regulatory law and policy reflect what has been called a combination of "paranoia and neglect,"1 2 the public's own concerns may be largely responsible. 13 
In the domain of risks, the persistent split between experts and ordinary people raises some of the most interesting problems in all of social science. For purposes of understanding these disputes, we might distinguish between two approaches: the technocratic and the populist. Good technocrats emphasize that ordinary people are frequently illinformed and urge that the task of regulators is to follow science and evidence, not popular opinion. 1 4 From the technocratic standpoint, a crucial question is what the facts really show. When people are mistaken on the facts, they should be educated so that they do not persist in their errors. Of course, technocrats acknowledge that science will 4 Leslie Roberts, Countingon Science at EPA, 249 SCIENCE 616, 616 (i9go). 5 Id. at 66-r7. 6 Id. at 66. 7 Id. 8 Id. 9 Id. 10 Id. 11 See id. 
12 John D. Graham, Making Sense of Risk: An Agendafor Congress, in RISKS, COSTS, AND LIVES SAVED: GETTING BETTER RESULTS FROM REGULATION 183, 183 (Robert W. Hahn ed., I996). 
13 Of course, interest groups play a large role, both independently and in dampening and heightening public concern. 
14 See, e.g., STEPHEN BREYER, BREAKING THE VICIOUS CIRCLE: TOWARD EFFECTIVE RISK REGULATION 33-51 (1993); HOWARD MARGOLIS, DEALING WITH RISK: WHY THE PUBLIC AND THE EXPERTS DISAGREE ON ENVIRONMENTAL ISSUES 166-89 (1996). often leave gaps. They know that science alone cannot determine the proper course of action. But they urge that facts are often the key issue and that whenever possible government should follow the evidence rather than public beliefs. Technocrats also insist that when people know the facts, they will often have a clear sense of what to do. 
For their part, populists tend to distrust experts and to think that in a democracy, government should follow the will of the citizenry rather than that of a technocratic elite. 15 In their view, law and policy should reflect what people actually fear, not what scientists, with their inevitably fallible judgments, urge society to do. Populists insist that the very characterization of risks involves no simple "fact," but a host of normative judgments made by so-called experts. To take just one example, while experts usually emphasize the number of fatalities involved in an activity, they could as easily select alternative measures, such as the number of life-years at risk, the percentage of people at risk, or the percentage of the exposed population at risk. According to many populists, risks do not exist "out there," and any judgment about risk is subjective rather than objective in character.1 6 If expert judgments inevitably involve values of one kind or another, it might seem only sensible to say that popular opinion should be a central ingredient in law and policy. For populists, ordinary intuitions and concerns have normative force and deserve to count in the democratic arena. 
To resolve the disagreement, it would be valuable to have a clearer sense of what, exactly, accounts for the split between experts and ordinary people. Is one or another group mistaken? Are ordinary people systematically ill-informed, and if so, why? Are intuitions likely to reflect false judgments of fact, or worthy judgments of value? What 15 See, e.g., ELIZABETH ANDERSON, VALUE IN ETHICS AND ECONOMICS 195-203 (I993); Lisa Heinzerling, PoliticalScience, 62 U. CHI. L. REV. 449 (1995) (reviewing BREYER, supranote 14). In Justice Breyer's confirmation hearings, Senator Joseph Biden remarked: The American people have no doubt that more people die from coal dust than from nuclear reactors, but they fear the prospect of a nuclear reactor more than they do the empirical data that would suggest that more people die from coal dust, from having coalfired burners. 
They also know that more lives would be saved if we took that 25 percent we spend in the intensive care units in the last few months of the elderly's lives, that more children would be saved. But part of our culture is that we have concluded as a culture that we are going to, rightly or wrongly, we are going to spend the money, costing more lives, on the elderly... 
I think it is incredibly presumptuous and elitist for political scientists to conclude that the American people's cultural values in fact are not ones that lend themselves to a cost-benefit analysis and to presume that they would change their cultural values if in fact they were aware of the cost-benefit analysis. 
Nomination of Stephen G. Breyer To Be an Associate Justice of the Supreme Court of the United States: Hearings Before the Senate Comm. on the Judiciary,1o3d Cong. 310 (1994). I take up the relationship between Breyer's views of risk and Slovic's findings at pp. i166-67, below. 
16 This is Paul Slovic's view, which I discuss further below. PAUL SLOVIC, THE PERCEPTION OF RISK 392 (2000). [VoI. 115:11'9 does it mean to say that expert judgments are based on values? Once we answer these questions, there will remain normative problems about what to do in the face of the relevant divisions. Perhaps what matters is not whether people are right on the facts, but whether they are frightened (an obvious possibility in a nation facing a risk of terrorism). Perhaps ordinary people have a special rationality, as worthy in its own way as that of experts. 7 Certainly experts can have their own biases and agendas.1 8 Perhaps the real issue is how to increase the public's role in risk regulation so that government will respond to its concerns. 
In this Review, I attempt to make some progress in answering these questions for purposes of law and policy, above all by attending to psychological evidence on people's perceptions of risk. I do so largely by focusing on the work of the psychologist Paul Slovic and his collaborators, as catalogued in Slovic's recent book, The Perception of Risk. Of all those who have contributed to an understanding of the division between experts and ordinary people, Slovic and his collaborators have been the most systematic and wide-ranging. They have produced a series of empirical studies designed to understand perception of risk - to see when people are frightened and when they are not, and exactly why. Slovic's own views defy easy categorization, but he has strong populist leanings. Slovic denies that risks can be assessed in objective terms (p. 369). In some of his most striking chapters, Slovic claims that ordinary people display a rival rationality that is worthy of consideration and respect (p. 23). Insisting that "risk" is not simply a matter of numbers, Slovic argues that a good system of risk regulation should be democratic as well as technocratic - and that it should pay a great deal of attention to what he sees as the structured and sometimes subtle thinking of ordinary people (p. 231). 
The Perception of Risk is an illuminating and important book, written over a number of years and with many coauthors. 19 It covers a great deal of ground. Among other things, Slovic deals with the "social amplification" of risk (pp. 232-45); "intuitive toxicology" (pp. 285315); trust and distrust (pp. 316-26); risk-taking by adolescents (pp. 364-7 i); smokers' (lack of) awareness of the risks of smoking (pp. 36417 Clayton P. Gillette & James E. Krier, Risk, Courts, and Agencies, 138 U. PA. L. REV. 1027, 1071-85 (i99o). 
18 For a popular treatment, see SHELDON RAMPTON & JOHN STAUBER, TRUST US, WE'RE EXPERTS! How INDUSTRY MANIPULATES SCIENCE AND GAMBLES WITH YOUR FUTURE (2oo), and in particular the authors' alarming accounts of the link between corporate funding sources and purportedly objective research outcomes, id. at 2 15-2 1. 
19 I refer to the author of the chapters discussed in this Review as "Slovic" for ease of exposition, although Slovic collaborated with other authors on all but seven of the twenty-six chapters. For present purposes, it is worthwhile to single out Sarah Lichtenstein and Baruch Fischhoff, with whom Slovic has collaborated on many occasions. 71); differences across lines of race and gender (pp. 390-412); and the role of emotions in assessing, taking, and avoiding risks (pp. 413-29). In this space it would be foolhardy to try to examine all of these issues in detail. Instead, I focus on Slovic's own unifying theme: the difference between the risk perceptions of experts and those of ordinary people. I also try to connect Slovic's claims to issues in law and policy, issues with which he deals only briefly.20 Psychological evidence has implications for many recent problems, emphatically including those raised by the threat of terrorism. It is noteworthy in this connection that Slovic's work is already having a significant impact on discussions within the federal government. 2 1 
In my view, though not necessarily in Slovic's, the overriding lesson of these essays is that ordinary people often deal poorly with the topic of risk. This fact stems from predictable features of human cognition. For example, people often neglect probabilities, focus on worst case outcomes, and use heuristics that can lead to massive errors. They are also subject to social influences that can lead them astray. This lesson has major implications for private and public law. First, Slovic sheds new light on why the system of legal regulation has taken its current form, showing some of the cognitive mechanisms that produce both paranoia and neglect. Second, Slovic helps to show how government could produce more helpful and effective warnings about risks. Some of his principal lessons involve how to make law and policy work better including how to structure information campaigns, which are unlikely to succeed unless we have a sense of how people perceive risks. Third, I believe that Slovic establishes, perhaps unintentionally, why sensible policymakers should generally follow science and evidence, not the public. This point bears on the design of government institutions as well as the functioning of Congress, regulatory agencies, judges, and 20 Since the original publication of the essays in this book, Slovic has produced a considerable amount of new work, much of which deals with the affect heuristic, discussed in Part II, pp. 1137-44, below. See, e.g., Paul Slovic, CigaretteSmokers: Rational Actors or Rational Fools?, in SMOKING: RISK, PERCEPTION, & POLICY 97, 105-07 (Paul Slovic ed., 2001); Paul Slovic, Melissa Finucane, Ellen Peters & Donald G. MacGregor, The Affect Heuristic, in INTUITIVE JUDGMENT. HEURISTICS AND BIASES (T. Gilovich, D. Griffin & D. Kahneman eds., forthcoming 2002) (manuscript, on file with the Harvard Law School Library). 
21 See, e.g., National Primary Drinking Water Regulations, 66 Fed. Reg. 6976, 7013-15 (Jan. 22, 2001) (to be codified at 40 C.F.R. pts. 9, 141, 142) (providing a sensitivity analysis involving voluntariness and controllability in connection with valuing the benefits of arsenic regulation); U.S. ENVTL. PROT. AGENCY, GUIDELINES FOR PREPARING ECONOMIC ANALYSES 91 & n.36 (2ooo) [hereinafter EPA, ECONOMIC ANALYSES] (referring to Slovic's work), available at http://yosemitei.epa.gov/ee/epa/eed.nsf/guidelines#download; see also SCIENTIFIC ADVISORY BD., U.S. ENVTL. PROT. AGENCY, VALUING THE BENEFITS OF FATAL CANCER RISK REDUCTION: AN SAB-REPORT ON EPA'S WHITE PAPER 2 (2oo0) (noting a disagreement within the Scientific Advisory Board about whether the literature supports adjustments in valuation of risk based on risk characteristics other than timing). even juries. More generally still, there are many problems with ordinary intuitions, which provide the starting point for the assessment of central issues in many fields. Intuitions about probabilities often go awry; 2 the same may well be true for intuitions in general, including moral intuitions. The respect accorded to intuitions - in law, philosophy, and elsewhere - has yet to take adequate account of what has been learned about cognitive errors and biases.23 
I make two basic objections to this admirable book. The first and more general is that some of Slovic's own findings seem to raise questions about his claim to have found a "rival rationality." Whatever Slovic's intentions, much of the importance of his work lies in the strong empirical support that it provides for a more technocratic view of regulation, one that draws ordinary intuitions into grave doubt. My second and more specific objection is that Slovic says too little about the social mechanisms by which individuals come to decide whether a risk is serious or trivial. These mechanisms have multiple connections with the cognitive points that Slovic emphasizes. Discussion with others, for example, can make a risk both vivid and salient, and when individuals see a risk as both, they are likely to talk to others, thus increasing both vividness and salience. When like-minded people discuss issues together, they may move one another in extreme directions. As we shall see, both of these objections have implications for law and for government institutions. 
This Review comes in several parts, separating Slovic's claims into various categories. I begin with the simpler and more established claims and follow with the more complex and contested ones. In each Part, I offer an outline of the relevant claims, evaluate them, and discuss some of their implications for law. Part I deals with the idea of cognitive heuristics, or mental shortcuts. My emphasis here is on the "availability heuristic," by which people think a risk is more serious if an example can be readily brought to mind. This Part also deals with what Slovic calls "intuitive toxicology," consisting of a set of simple intuitions by which people assess certain risks. Part II explores the role of emotions and, in particular, Slovic's more recent claim that the "affect heuristic" helps to explain people's reactions to risks. Part III discusses Slovic's "psychometric paradigm," the basis for his effort to claim that ordinary people have a "rival rationality" consisting of a value-laden approach to risk competing with that of experts. Part IV discusses some of Slovic's fascinating findings about demographic dif22 See MARGOLIS, supra note 14, at 166-89. 
23 Valuable discussions of the role of intuitions can be found in MARGOLIS, supra note 14, at 49-69, and Jonathan Baron, Nonconsequentialist Decisions, i7 BEHAV. & BRAIN SC. I, I-io (1994). ferences, knowledge of risks, and trust. Finally, Part V offers a brief, general discussion of issues of law and policy. 
In several chapters, Slovic emphasizes that people use heuristics, or mental shortcuts, to assess the presence and magnitude of risks.2 4 As Slovic makes clear, he owes a large debt here to Daniel Kahneman and Amos Tversky, who have uncovered several heuristics that people use to assess probabilities.2 5 Consider, for example, the "availability heuristic," in accordance with which people assess the probability of an event by seeing whether relevant examples are cognitively "available" (PP. 37-38). Thus, for example, people are likely to think that on a random page more words begin with the letter r or k than have either letter in the third position - even though those consonants are more often in the third position than in the first.2 6 
In the relevant experiments, Kahneman and Tversky explored cognition in general. They did not deal with policy issues or with people's evaluations of social risks. Slovic's major contribution is to show the great importance of the availability heuristic in helping to generate ordinary judgments about risks to health, safety, and the environment. But Slovic also shows a more general way that ordinary people go wrong. They rely on "intuitive toxicology," which contains a range of scientifically implausible judgments, many of them apparently working as mental shortcuts (p. 291). Moving beyond individual cognition, Slovic also calls attention to the "social amplification" of risk. In this Part, I outline Slovic's findings and offer one criticism, or perhaps a friendly amendment: Slovic pays too little attention to the social forces that drive people to fear, or not to fear, certain hazards. I provide a brief discussion of how this gap might be filled. 
In Slovic's view, "[t]he notion of availability is potentially one of the most important ideas for helping us understand the distortions likely to occur in our perceptions of natural hazards" (p. 14). These distortions have concrete consequences for behavior. For example, the [VOL. 115'.1119 decision to buy insurance for natural disasters is greatly affected by recent experiences (p. 40). If floods have not occurred in the immediate past, people who live on flood plains are far less likely to purchase insurance. In the aftermath of an earthquake, insurance for earthquakes rises sharply - but it declines steadily from that point, as vivid memories recede. It is sensible to think that the lax security measures at airports before the terrorist attacks on September 1i, 2001, had a great deal to do with the lack of cognitively available incidents in recent years, which lulled people into a false sense of immunity. Many experts had insisted that the threat of terrorism on airplanes was significant.2 7 
Note that the use of the availability heuristic, in these contexts, is hardly irrational.2" Both insurance and precautionary measures can be expensive, and what has happened before often seems to be the best available guide to what will happen again. Imperfectly informed people might do well to rely on the availability heuristic. The problem is that the availability heuristic can lead to serious errors of fact, in terms of both excessive fear and neglect. 
Do people know which risks lead to many deaths and which risks lead to few? They do not. In fact, they make huge blunders. 29 In some especially striking studies, Slovic demonstrates that the availability heuristic helps to explain people's mistakes in assessing the frequency of various causes of death. In one study, participants were told the annual number of deaths from motor vehicle accidents in the United States (at the time about 5o,ooo) and then were asked to estimate the number of deaths from forty other causes (pp. lO6-O7). 30 In 27 See, e.g., U.S. COMM'N ON NAT'L SEC./21ST CENTURY, ROADMAP FOR NATIONAL SECURITY: IMPERATIVE FOR CHANGE (2001) (report of a bipartisan commission chaired by Gary Hart and Warren Rudman), available at http:llwww.nssg.govlphasell.pdf; see also GEN. ACCOUNTING OFFICE, SECURITY BREACHES AT FEDERAL AGENCIES AND AIRPORTS 2 (May 25, 2000) (finding two commercial airports vulnerable to weapons and chemical agents). 
28 Kahneman and Tversky emphasize that the heuristics they identify "are highly economical and usually effective" but also that they "lead to systematic and predictable errors." Tversky & Kahneman, supra note 25, at 2o. Gerd Gigerenzer, among others, has emphasized that some heuristics can work extremely well and has used this point as a rejoinder to those who stress the errors introduced by heuristics and biases. GERD GIGERENZER, ADAPTIVE THINKING: RATIONALITY IN THE REAL WORLD (2oo); GERD GIGERENZER, PETER M. TODD & THE ABC RESEARCH GROUP, SIMPLE HEURISTICS THAT MAKE US SMART (r999). For a helpful recent discussion, see Kahneman & Frederick, supra note 24. Even if many heuristics mostly work well in daily life, a sensible government can do much better than to rely on them. 
29 See, e.g., W. Kip Viscusi, Jurors,Judges, and the Mistreatment of Risk by the Courts, 30 J. LEGAL STUD. 107, 130-34 (2001). 
30 There is an important point here, not discussed by Slovic: people have little idea what number is in the ballpark, and they need some kind of "anchor," in the form of an annual death toll for some familiar activity, to ensure that the numbers are not completely random. See Cass R. Sunstein, Daniel Kahneman & David Schkade, Assessing Punitive Damages (with Notes on Cognition another study, people were given two causes of death and asked to say which produces more fatalities (p. 38). People made large mistakes, and when they did so, the availability heuristic was partly responsible. "In keeping with availability considerations," Slovic writes, "overestimated items were dramatic and sensational whereas underestimated items tended to be unspectacular events which claim one victim at a time and are common in non-fatal form" (p. 107). Specifically, participants significantly overestimated highly publicized causes of death, including tornadoes, cancer, botulism, and homicide. By contrast, they underestimated the number of deaths from strokes, asthma, emphysema, and diabetes. At the same time, people tended to think that the number of deaths from accidents is higher than the number of deaths from disease, whereas the opposite is true. In the same vein, participants mistakenly believed that more people die from homicides than from suicides. Availability can also "lull people into complacency," as when certain risks, not easily accessible, seem invisible. What is out of sight is "effectively out of mind" (pp. lo8-O9); recall here the case of inadequate security measures at American airports. 
These points suggest that highly publicized events make people fearful of statistically small risks.3 1 Both law and policy are likely to be adversely affected by people's use of mental shortcuts. Public officials, no less than ordinary people, are prone to use the availability heuristic. 32 And in a democracy, officials, including lawmakers, will respond to public alarm. If citizens are worried about abandoned hazardous waste dumps, we might well expect that substantial resources will be devoted to cleaning them up, even if the risks are relatively small. 33 Similar problems will appear in courts, with juries and judges taking "phantom risks" quite seriously.3 4 There is also a lesson for potential entrepreneurs about how to attract public attention to a risk: make a vivid example of its occurrence highly salient to the public. and Valuation in Law), 107 YALE L.J. 2071, 2109-10 (1998); see also Viscusi, supranote 29, at 131 (providing the number of annual auto deaths to anchor the analysis). 
31 For a vivid demonstration in the context of catastrophes, see Jacob E. Gersen, Strategy and Cognition: Regulatory Catastrophic Risk (Aug. 2000) (unpublished Ph.D. dissertation, University of Chicago) (on file with the Harvard Law School Library). 
32 See Timur Kuran & Cass R. Sunstein, Availability Cascadesand Risk Regulation, 51 STAN. L. REV. 683, 691-703 (I999); Roger G. Noll & James E. Krier, Some Implications of Cognitive Psychologyfor Risk Regulation, 19 J. LEGAL STUD. 747, 749-60 (1990). 
33 See JAMES T. HAMILTON & W. KIP ViscusI, CALCULATING RISKS? THE SPATIAL AND POLITICAL DIMENSIONS OF HAZARDOUS WASTE POLICY (1999); Kuran & Sunstein, supra note 32, at 693-98. 
34 See The Legal Context, in PHANTOM RISK: SCIENTIFIC INFERENCE AND THE LAW 425, 425-28 (Kenneth R. Foster, David E. Bernstein & Peter W. Huber eds., 1993) (discussing scientifically unsupportable outcomes involving "traumatic cancer" and harm to immune systems); The Legal Context, in PHANTOM RISK, supra, at 137, 137-46 (discussing lawsuits with unclear scientific bases). This way of proceeding, far more than statistical analysis, is likely to activate public concern. 35 
Are ordinary people toxicologists? Slovic thinks so (p. 285). He uncovers the content of "intuitive toxicology" by comparing how experts (professional toxicologists) and ordinary people think about the risks associated with chemicals. The result is a fascinating picture. It is not clear what identifiable heuristics are at work in intuitive toxicology; it would be valuable to make more progress here. But it is clear that people are using mental shortcuts and that these shortcuts lead to errors. 
Slovic elicits the views of toxicologists and ordinary people on the following propositions, among others (pp. 290-98): 
i. If you are exposed to a carcinogen, then you are likely to get cancer. 
cancer in animals, then we can be reasonably sure that the chemical will cause cancer in humans. 
contaminated now than ever before. 

chemicals. 
several malformed children had been born there during each of the past few years. The town is in a region where agricultural pesticides have been used during the last decade. It is very likely that these pesticides were the cause of the malformations. 
When asked whether they agree, disagree, or have no opinion, ordinary people expressed agreement with many such statements by pluralities or even majorities (pp. 290-98). By contrast, toxicologists disagreed with such statements, usually by overwhelming majorities (pp. 290-98). 
What are ordinary people thinking? Can we discern some structure to their perceptions? There does seem to be a "carcinogens are deadly" heuristic here, which is responsible for agreement with the first three propositions set forth above. But three more fundamental beliefs seem to play a large role. First, many people appear to believe that risk is an "all or nothing" matter; something is either safe or dan3S As we shall soon see, there is a relationship between the availability heuristic and the important role of emotions, or affect,-in the assessment of risks. See infra Part 11, pp. 1137-44. gerous, and there is no middle ground.3 6 Second, many people tend to subscribe to the "zero-risk" mentality, at least in some domains, apparently believing that it is both possible and appropriate to eliminate risk entirely.37 This belief appears closely connected to the notion that risk is a matter of "all or nothing." Third, many people seem committed to a belief in the benevolence of nature. They think that the products of human beings and human activities are more likely to be dangerous than are the products of natural processes. 38 Here we can even identify a possible heuristic: natural processes are unlikely to create unacceptable risks.3 9 
Experts in toxicology believe that all three beliefs are false. Moreover, it seems clear that expert judgments are far more accurate than are those of ordinary people. Why do people think as they do? At least some of their beliefs might work well for nonspecialists in most contexts. 40 People want to know, for example, whether an activity is "safe," not the statistical probability of harm; and in many contexts, the excessively simple category of "safe" can tell them what they need to know. 4 1 The problem is that ideas of this kind lead to mistakes in contexts in which regulatory choices, and even some daily decisions, 36 This point is connected with an aspect of prospect theory, termed the "certainty effect," which finds that "people overweight outcomes that are considered certain, relative to outcomes which are merely probable." Daniel Kahneman & Amos Tversky, Prospect Theory: An Analysis of Decision Under Risk, in CHOICES, VALUES, AND FRAMES 17, 20 (Daniel Kahneman & Amos Tversky eds., 2000). 
37 See Viscusi, supra note 29, at 133-34 (discussing the zero-risk mentality in the context of judges and jurors). 
38 For an interesting challenge to the general belief in nature's benevolence from the perspective of ecology, see Daniel B. Botkin, Adjusting Law to Nature's Discordant Harmonies, 7 DUKE ENVTL. L. & POL'Y F. 25, 26-31 (i996). 
39 This idea might also be based on a moral judgment that the risks associated with nature should not be a source of intense human concern. Although I cannot establish the point, I believe that such a moral judgment is a holdover from certain theological positions. See JANET RADCLIFFE RICHARDS, HUMAN NATURE AFTER DARWIN 242-52 (2000). For the moment, consider also the possibility that emotional reactions play a significant role in intuitive toxicology, particularly in people's intense reactions to small exposures to carcinogens. See Yuval Rottenstreich & Christopher K. Hsee, Money, Kisses, and Electric Shocks: On the Affective Psychology of Risk, 12 PSYCHOL. SCI. 185, i86-88 (2001). I turn to the affect heuristic in section ILA, pp. 1137-39, below. 
40 This is a standard justification for most heuristics. See, e.g., Gerd Gigerenzer & Peter M. Todd, Fast and FrugalHeuristics: The Adaptive Toolbox, in GIGERENZER, TODD & THE ABC RESEARCH GROUP, supra note 28, at 3, 3-8; Tversky & Kahneman, supra note 25, at 20 ("These heuristics are highly economical and usually effective."). 
41 It cannot, however, be said that a belief in the benevolence of nature is a sensible heuristic. In fact, this is a dangerous idea, because the unnatural may very well be safer than the natural. See, e.g., JAMES P. COLLMAN, NATURALLY DANGEROUS: SURPRISING FACTS ABOUT FOOD, HEALTH, AND THE ENVIRONMENT 1-2, 29-33, 61-85 (2001) (discussing dangers in natural foods); ALAN MCHUGHEN, PANDORA'S PICNIC BASKET 233-37 (2ooo) (discussing hazards in organic food). For a criticism of the proposition that nature, left alone, produces the best environment for human and other creatures, see Botkin, supranote 38, at 26-3 I. have to be made. Nonspecialists may do well to rely on such principles, but policymakers should do a good deal better. 
Slovic also finds that experts do not entirely agree among themselves (pp. 292-93, 311-12). Most interestingly, toxicologists employed by industry are far more optimistic about chemical risks than are toxicologists employed by government or academic institutions; there is a large "affiliation bias," so that people tend to believe what their institutions would want them to believe (p. 311). 1 shall say more about this particular bias below. But the differences among toxicologists are dwarfed by the differences between toxicologists and ordinary people. Experts are likely to be biased by their work for an organization with a stake in the outcome - but even acknowledging this point, experts are, on many fundamental issues, in basic accord with one another. 
As Slovic is aware, mental shortcuts do not operate in a social vacuum; interpersonal influences play a large role. Most of the time, most of us lack independent knowledge of risks, and we must therefore rely on the beliefs of others in making our assessments. 
Slovic's principal treatment of this point comes in a discussion of what he terms the "social amplification of risk" (pp. 232-45). The primary purpose of this discussion is not to explain how social influences affect people's perception of risks, but instead to show what might be missed by conventional efforts to tabulate the costs and benefits associated with risks. For example, the 1979 accident at Three Mile Island "demonstrated dramatically that factors other than injury, death and property damage can impose serious costs and social repercussions" (p. 234). Although no one was killed or even harmed by the accident, it imposed "enormous costs" on the nuclear industry and society more generally, resulting in tighter regulations, decreased operation of reactors globally, heightened public opposition to nuclear power, and a less viable role for nuclear power as a major long-term energy source (pp. 234-35). Slovic is concerned that a conventional risk analysis, focusing on the likelihood and severity of injury, will overlook these kinds of consequences (p. 235). 
In some domains, however, Slovic finds that society systematically underestimates potentially serious harms, a phenomenon he terms "social attenuation of risk" (p. 235). As examples, Slovic offers indoor radon, smoking, and driving without a seat belt (p. 235). Certainly the terrorist attacks of September 2001 provide the most recent and salient illustration; those attacks produced harms of multiple kinds that extended well beyond the day of the attacks. A comprehensive account will take a long time to develop, but consider, for illustrative purposes, the following facts and projections, which were developed within a short period following the attacks: New York City was expected to forfeit between $i billion and $3 billion in lost revenue; 4 global airline losses were projected to exceed $io billion, with international carriers potentially losing $7.5 billion and U.S. carriers anticipating $5 billion in losses; 43 total "big company" layoffs were about 20o,00o, 4 4 with British Airways laying off 12.5% of its employees and decreasing flights by Io%, 4 5 and with American Airlines and Boeing each laying off at least 20,000 people; 46 car rental companies were expected to lose 5o-6o% of their business within the months following the attacks; 4 7 and the national government developed a $I5 billion package to assist airlines. 4 Of course, all this is just the beginning: the ripple effects of the attacks, both financial and psychological, extend to countless domains. 
Slovic is especially concerned with the "secondary impacts" of dangers whose effects are generally viewed more narrowly. The broader impact of a well publicized problem might include a sharp decline in social trust, an increase in liability and insurance costs, public pressure to impose new regulations, and even a period of social disorder. Because dramatic events are an important determinant of individual risk judgments, a highly publicized incident might exacerbate unwarranted or irrational fears. The flow of information, especially via the media, can be extremely important not only in spreading facts but also in shaping perceptions. Slovic's social amplification model embodies his effort to shed light on the processes by which "seemingly minor risks or risk events often produce extraordinary public concern and social and economic impacts, with rippling effects across time, space and social institutions" (pp. 244-45). 4 9 
Slovic's emphasis on social amplification of risks is a valuable supplement to his work on individual cognition. But his discussion neglects to address the social origins of individual belief. As we shall see, individuals contribute to the intensity of the very forces by which they are influenced. My aim here is to fill a gap in Slovic's presentation by offering a sketch of the relevant influences on individual beliefs. 
i. Informational Influences. - A good way to begin is to draw on emerging work on social cascades5 0 and to sketch an understanding of "risk perception cascades" in particular. The starting point is a simple recognition that in many domains people possess little first-hand information. Certainly this is the case for many hazards. Because few people have independently researched the dangers of arsenic in water, global warming, Lyme disease, airplane travel, or asbestos in the workplace, they must rely on the signals given by other people, as revealed by their behavior or their public statements. 
To see how risk perception cascades operate, consider a stylized example: Ann is unsure whether global warming is a serious problem, but Bob, whom Ann trusts, believes that it is. Influenced by Bob's views, Ann concludes that global warming is indeed a serious problem. Carl is inclined, on his own, to discount the risk; but confronted with informational signals given by the shared views of Ann and Bob, Carl might well come to believe that global warming is indeed a serious problem. Deborah, a skeptic about global warming, would need a great deal of confidence in the correctness of her view to reject the shared belief of Ann, Bob, and Carl. The members of this little community will come to share the belief that global warming is a matter of considerable concern. 5 ' 
Stylized though it is, this description reveals the dynamics of many social movements concerned with the evaluation of risk, s2 captured in the notion of an informational cascade. Most of us think and fear 50 See, e.g., Lisa R. Anderson & Charles A. Holt, Information Cascades in the Laboratory,87 AM. ECON. REV. 847 (1997); Abhijit V. Banerjee, A Simple Model of Herd Behavior, 107 Q.J. ECON. 797 (I992) (examining, through a model termed "herd behavior," the tendency of individuals to follow what others are doing rather than to assess independently what action to take); Sushil Bikhchandani, David Hirshleifer & Ivo Welch, Learningfrom the Behavior of Others: Conformity, Fads, and Informational Cascades, J. ECON. PERSP., Summer 1998, at 151; Andrew F. Daughety & Jennifer F. Reinganum, Stampede to Judgment: Persuasive Influence and Herding Behavior by Courts, i AM. L. & ECON. REV. 158 (,999). 
51 See Bikhchandani, Hirshleifer & Welch, supra note 5o, at 15-63; David Hirshleifer, The Blind Leading the Blind: Social Influences, Fads, and Informational Cascades, in THE NEW ECONOMICS OF HUMAN BEHAVIOR i88, igo-98 (Marianno Tommasi & Kathryn lerulli eds., '995). 
52 For several examples of grassroots environmental protection movements, see PENINA MIGDAL GLAZER & MYRON PERETZ GLAZER, THE ENVIRONMENTAL CRUSADERS (1998). what we do because of what we think other people think and fear. The spreading fear of Lyme disease is a case in point, with many people believing they have the disease and many doctors confirming this diagnosis, simply because they have heard from others that Lyme disease is associated with certain symptoms. 53 Because of the availability heuristic, the informational cascade may be greatly fueled if a salient event easily comes to mind.54 
not all that is at work; reputation matters too. Because most people care about how others view them, it is easy to imagine reputational cascades with respect to actions or stated beliefs.5 5 Suppose, for example, that Alan and Betty would think ill of anyone who argued that global warming is not a problem. Charles, who is unsure what to think about global warming, might be unmoved privately by the views of Alan and Betty and might even consider them fanatical. But Charles might nonetheless be unwilling to incur the scorn of Alan and Betty, or to appear ignorant or indifferent to the welfare of future generations. If so, Charles might not express opposition to a proposal to take dramatic steps to halt global warming and might even agree publicly with Alan and Betty that such steps are necessary. If Dana is otherwise undecided, she might be extremely reluctant to oppose Alan, Betty, and Charles publicly. Mounting reputational pressure might well lead Ellen, Frank, George, Helen, and many more to join the bandwagon. Eventually the result would be to change law and policy, as citizens support massive social efforts and ask their representatives to respond. 
I have suggested that with respect to risk, informational influences seem to be the most important factors in altering behavior, as individual fear grows with a sense that other (reasonable) people are frightened as well. But reputational influences matter too. If the example just given seems artificial, consider the comment of a medical researcher who questions the accuracy of many Lyme disease diagnoses: "Doctors can't say what they think anymore. If you quote me as saying these things, I'm as good as dead." 56 Or consider the remarks of a sociologist who has publicly raised questions about the health threats posed by mad-cow disease; he suggests that if you raise those doubts 53 See David Grann, Stalking Dr.Steere, N.Y. TIMES, July 17, 2001 (Magazine), at 52. On informational cascades among voters, political protestors, doctors, firms, and even animals, see Hirshleifer, supra note 5i, at 2oi-o6. 
54 See Kuran & Sunstein, supra note 32, at 712-15. 
55 See TIMUR KURAN, PRIVATE TRUTHS, PUBLIC LIES 26-30, 63-64, 356 nn.34, 42 (1995); Timur Kuran, The Unthinkable and the Unthought, 5 RATIONALITY & SOC'y 473, 487-88 (199563).Grann, supra note 53, at 56. [VoI. 115:111 9 publicly, "[y]ou get made to feel like a pedophile. '5 1 Or suppose that in October 200i a policy analyst objected that new security measures at airports would be unlikely to increase safety because, by reducing the convenience of air travel, they might lead more people to drive, thus causing a net loss of lives. Such an analyst might well remain silent out of a desire to avoid the opprobrium that would predictably follow. 
Lawmakers, even more than ordinary citizens, are vulnerable to reputational pressures. In fact, that is part of their job. Lawmakers might well support legislation to control risks that they privately know to be insignificant. In the context of regulation of hazardous waste dumps, reputational factors helped to fuel a cascade effect that eventually led to the enactment of the Superfund statute,5 8 which was designed to ensure the remediation of abandoned hazardous waste sites. The apparent adverse health effects of abandoned waste at Love Canal made it extremely difficult for government officials to resist public concern, even though the evidence of serious risk was far from clear.5 9 
But the problem of reputational pressure does not necessarily result in excessive legal controls. In a phenomenon similar to Slovic's social attenuation of risks, we can imagine "unavailability cascades," 60 in which an individual's relative indifference to statistically significant risks leads other people to be indifferent as well. Return again to American neglect of airport security, a problem that greatly concerned experts. Undoubtedly, informational and reputational forces help to account for public indifference to many hazards that trouble experts. 
There is an additional point. Some people are especially influential because they are known to be reliable, and here the informational "signals" of their statements and actions are especially loud. Some people are particularly influential because they can inflict high reputational costs on people who do not conform. For this reason, prestigious people are in a good position to fuel a cascade. 
Note that these points about social influences complement Slovic's cognitive claims, including the availability heuristic and intuitive toxicology. What is "available" is a function of informational and reputational forces. If people are spreading the idea that arsenic "causes cancer" and should therefore be banned, then the commonly held assumptions described by Slovic as intuitive toxicology will spread as 57 Andrew Higgins, It's a Mad, Mad, Mad-Cow World, WALL ST. J., Mar. 12, 2001, at Ai 3 (internal quotation marks omitted). 
58 42 U.S.C Â§Â§ 96oi-9675 (I994 & Supp. V 1999). 59 See Kuran & Sunstein, supranote 32, at 692-98. 
60 See id. at 730-31. if by contagion. 6 1 If people are talking about the risks associated with pesticides, disposable diapers, air travel, or shark attacks, those risks will be available in the public mind. The case of shark attacks offers an especially vivid example. 62 A LEXIS search found over iooo references to shark attacks between August 4, 2001, and September 4, 2001,63 and 126 references to "the summer of the shark, ''64 notwithstanding the absence of any reliable evidence of an increase in shark attacks in the summer of 2001.61 Predictably, there was discussion of new legislation to control the problem. 66 
polarization,"67 helps to strengthen social pressure. It is well established that when a group deliberates, group members tend to move toward a more extreme position in line with their predeliberation inclinations. 68 Thus, for example, people who tend to fear the effects of second-hand smoke, or who believe that pesticides carry significant risks, are likely 61 For a discussion of the controversy over the EPA's proposed arsenic regulations, see Cass R. Sunstein, The Arithmetic of Arsenic, go GEO. L.J. (forthcoming 2002) (manuscript at 22-25, on file with the Harvard Law School Library). 
62 As one journalist observes: 
A maritime expert said on last night's "NBC Nightly News" that more people die from bees, wasps, snakes or alligators than from shark attacks. 
But there's no ratings in bees. Unpleasant little critters, but not scary-looking enough. 
With "Jaws" music practically playing in the background, the media have turned this into the Summer of the Shark. Never mind that the number of attacks has actually dropped since last year. They're here, they're nasty and they could be coming to a beach near you. 
Howard Kurtz, Shark Tragedies Spark Increased Coverage, WASH. POST ON-LINE (Sept. 5, 2001), at http://www.washingtonpost.comlwp-dyn/articles/A44 720-200I Sep5 .html. 
63 The search was conducted in the ALLNWS file, using the search term "shark attack!" and the noted date restrictions. An identical search of sources dated between August 4, 2ooo, and September 4, 2000, found only 174 references to shark attacks. 
64 The search was conducted in the ALLNWS file, using the noted date restrictions. 65 See The International Shark Attack File, at http://www.flmnh.ufl.edu/fish/Sharks/ISAF/ ISAF.htm (last visited Jan. 6, 2002) (offering comparative risk data showing, for example, that while there were 18 injuries and deaths from sharks in the United States in 1996, there were io,ooo injuries from buckets and pails; over 15oo injuries from toilet bowl products; and over i98,ooo injuries from nails, tacks, screws, and bolts). 
66 See, e.g., Maya Bell, Divers Defend Courting the Fish So Many Fear:A Wave of Recent Shark Attacks Has Brought South Florida Shark-Feeding Groups Under State Scrutiny, ORLANDO SENTINEL, Aug. 29, 2001, at Ai, 2001 WL 28413950. 
67 See ROGER BROWN, SOCIAL PSYCHOLOGY: THE SECOND EDITION 200-45 (1986); Cass R. Sunstein, Deliberative Trouble? Why Groups Go to Extremes, iio YALE L.J. 71, 85-9o (2000). 
68 See BROWN, supra note 67, at 222-29; Sunstein, supranote 67, at 85-86. There is a lesson here for lawyers, particularly those who work in teams. Anecdotal evidence suggests that lawyers, simply by virtue of their working on a team, tend to gain confidence in their clients' positions. This can occur for opposing sides in the same case. Cf. Linda Babcock & George Loewenstein, Explaining BargainingImpasse: The Role of Self-Serving Bias, in BEHAVIORAL LAW AND ECONOMICS 355 (Cass R. Sunstein ed., 2000) (discussing the effects of self-serving bias without reference to group polarization). to believe, after discussion, that the relevant health dangers are more serious than they had previously thought. So too, people who tend to think that the risks of global warming have been exaggerated will likely believe, after discussion, that global warming is even less cause for concern. 69 
Considerable evidence from many different countries supports this basic prediction. 70 If like-minded people are talking with one another much of the time, it is especially likely that some groups of people will show heightened concern about certain risks, regardless of the evidence. It is also likely that other groups of people will show no concern at all, even if the evidence gives cause for alarm. There can be no doubt that group polarization has contributed to disagreements over proper risk regulation, with some groups, for example, thinking that global warming spells global disaster and others thinking that it is a fabrication of overzealous environmentalists.7 1 Indeed, Slovic's demonstrated "affiliation bias" undoubtedly has a close relation to group polarization; affiliates of an organization are all too likely to move to an extreme. More generally, when society shows alarm or indifference, group polarization will likely be part of the reason. 
In sum, Slovic performs a valuable service in showing how the availability heuristic and intuitive toxicology help to produce inaccurate judgments about risk. As compared with ordinary people, experts are better able to reach accurate judgments, if only because they have access to more information, making the easily recalled incident a less important determinant of judgment and producing greater accuracy than the rules of thumb on which ordinary people rely.72 Slovic is also correct to emphasize that social forces can amplify the effect of the availability heuristic. The principal gap in his discussion lies in his failure to explore those forces. There is much more to be done here, both at the level of theory and at the level of empirical detail. 
Slovic's psychological claims offer many clues to the development of regulatory policy - showing, for instance, how the vivid example can play a significant role in motivating the enactment of new legislation or new administrative regulations. Sometimes the use of such examples will be valuable because it will activate social concern about 69 There is no reason to think that experts are immune to group polarization; they are certainly vulnerable to cascade effects. See Hirshleifer, supranote 5i, at 204-05. 
70To summarize a complex story, group polarization occurs because of informational and reputational influences. See BROWN, supra note 67, at 212-22; Sunstein, supra note 67, at 88-90. 
71 The implications of group polarization extend well beyond regulation. See, e.g., Sharon Groch, Free Spaces: Creating Oppositional Consciousness in the Disability Rights Movement, in OPPOSITIONAL CONSCIOUSNESS: THE SUBJECTIVE ROOTS OF SOCIAL PROTEST 65, 69-74 (Jane Mansbridge & Aldon Morris eds., 2001) (discussing the rise of "deaf consciousness" as deaf people associate largely with each other). 
72 For a detailed treatment of lay-expert conflicts, see MARGOLIS, supra note 14, at IOI-I9. previously neglected problems. But sometimes it will lead to perverse results, in the form of massive expenditures on small or even nonexistent problems.7 3 I have argued here that some of the gaps in Slovic's presentation will be filled if we look more closely at social influences on behavior. 
As Slovic emphasizes, most psychological work on risk has focused primarily on cognition (p. 413), asking whether mental heuristics produce errors and how people depart from what is generally considered rational behavior. But thinking about perceived risks only in these terms seems incomplete. With respect to risks, many of our ordinary ways of speaking suggest strong emotions: panic, hysteria, terror.7 4 A central point here is that when strong emotions are at work, people tend to focus on the "worst case," in a way that makes the outcome, but not its probability,highly salient.7 5 Slovic explores a key question: how do people's feelings affect their reactions to risks? 
Slovic's interest in this topic appears to have been triggered by a remarkable finding: when asked to assess the risks and benefits associated with certain items, people tend to say that risky activities carry low benefits and that beneficial activities carry low risks (pp. 41516).76 It is rare that people will see an activity as both highly beneficial and quite dangerous, or as both benefit-free and danger-free. This phenomenon is extremely odd. Why don't people more frequently think that an activity is both highly beneficial and highly risky? Why do they seem to make a gestalt-type judgment, one that drives assessment of both risks and benefits? 
Aware that risk and benefit are "distinct concepts," Slovic thinks that "affect" comes first and helps to "direct" judgments of both risk and benefit. Hence he suggests an "affect heuristic," by which people have an emotional, all-things-considered reaction to certain processes and products. The affect heuristic operates as a mental shortcut for a more careful evaluation. Slovic finds that the affect heuristic even 73 See Noll & Krier, supra note 32, at 774 (discussing the benefits of "striking when the iron is cold'). 
74 See George F. Loewenstein, Christopher K. Hsee, Elke U. Weber & Ned Welch, Risk as Feelings, 127 PSYCHOL. BULL. 267, 267-68 (2001). 
75 Rottenstreich & Hsee, supra note 39, at i86-88. 
76 Note that the study clearly separates benefits and risks, marking them for independent analysis by participants. 2002] ple, we do not know what the dose-response curve looks like at low levels. We do not always know whether there is a safe threshold below which adverse effects are absent, whether adverse effects fall rapidly but do not disappear for low exposures, or whether the curve is simply linear.1 1' The choice among dose-response curves often depends on educated guesses and normative commitments, not science. 112 
These points establish that any description of risk contains value judgments, and at several levels. But they do not support Slovic's third claim, that "there is no such thing as 'real risk' or 'objective risk"' (p. 392). I am sure that Slovic would agree that smoking three packs of cigarettes per day for a period of years produces a real risk, one that is both objective and high, whether or not it can be quantified with precision. Slovic would also agree that swimming at the beach in Marblehead, Massachusetts, produces a real, but small, risk of being killed by a shark even if we are unable to quantify it. Of course, we can describe these risks, consistent with the evidence, in many different ways. But this point does not establish that risks are not real or that they are not objective. Indeed, Slovic agrees that "the dangers and uncertainties of life . .. are real" (p. 392). If dangers are real, so are risks. 
Now let us turn to the question whether ordinary people display a rival rationality. How do we know that ordinary people think that Slovic's qualitative factors are so important? The answer is not that people spontaneously point to such factors in explaining their assessments of risk. We do not have data to demonstrate that ordinary people have an accurate sense of the number of lives at stake, or that their all-things-considered evaluations show that they agree with experts on the numbers but introduce qualitative factors to evaluate risks. 1 3 Instead, the answer is that Slovic and his fellow experimenters expressly identified these factors, set them before experimental subjects, and asked their subjects to rate certain risks along these specific dimensions (pp. 203-o6). People were asked to rate, on a seven-point scale, various risks in terms of their catastrophic potential, their dreadedness, their threat to future generations, their controllability, and so forth (pp. 201-05). Is it at all surprising, or even informative, that the risks that people most fear tended to be rated most severely along these dimensions? 
111 See generally Sunstein, supranote 61 (manuscript at 25-28) (providing a detailed discussion, with many examples, of different types of dose-response curves). 
112 Id. (manuscript at 26). 113 In fact, Slovic finds that people do not know the actual numbers. See supra p. Ii26. 
Slovic finds that the most feared risks include DDT, nuclear power, pesticides, herbicides, and smoking (p. 143), whereas x-rays, microwave ovens, nonnuclear electric power, marijuana, and sunbathing rank far lower (pp. 144-45). He also finds that the most feared risks are considered more serious, along several of the "qualitative" dimensions, than are the least feared risks. But these findings do not establish that the qualitative dimensions are the grounds for people's rankings of these risks. People's evaluations of risks might well be explained by their general affect toward the product or activity in question. In fact, there is a real oddity in Slovic's list of factors: the absence of "magnitude of danger" as a reason for special concern. Whatever the nature of lay rationality, surely perceived magnitude of danger should count as an important factor. Its absence suggests that affect may be doing a lot of work. 
If a halo effect is involved, so that risks are subject to an overall affect, is it terribly surprising that certain risks receive bad ratings on most dimensions and others are regarded more favorably on most dimensions? When people fear DDT and pesticides, but not microwave ovens or sunbathing, it is probably because they have a rapid, intuitive response to these sources of risk, producing fear in some cases but not in others. If so, people are not making some richly rational judgment that some risks are controllable and others not, or that some of the relevant risks are equitably distributed and others not. Indeed, I predict that if people were asked to offer an "overall evaluation" of the riskiness of products and activities on a scale of zero to eight, their rankings would be similar to those that emerge from Slovic's studies. 
The idea of richer rationality is drawn into further question by the fact that lay people's concerns vary greatly across nations. Consider, for example, the fact that nuclear power, a source of real fear in the United States, occasions little concern in France. 114 Similarly, genetic engineering of food is treated as a serious problem in Europe, but much less so in the United States." 5 If lay people really have a richer rationality, it seems surprising to find cultural differences of this magnitude. 
Nothing in Slovic's data excludes the possibility that people fear certain risks in part because they have a general impression that these risks are statistically large. If this is right, then people will express more concern with certain activities, on the qualitative dimensions, when they have an intuitive sense that those activities have few benefits and impose significant risks. 16 On this view, the rankings on the qualitative dimensions do not have much explanatory power. On the contrary, they are explained, at least in part, by people's intuitive belief that large statistical risks are associated with certain activities. The point should not be overstated. Slovic is undoubtedly right to say that the qualitative factors matter. But his evidence seems too crude to disprove a competing hypothesis - that people's rankings of risks reflect, in significant part, the roles of affect and crude, rough-andready assessments of net benefits, and are not mostly driven by a richer rationality. 171 
If we look closely at Slovic's actual list of hazards, it is not at all clear how to explain the results in terms of qualitative factors. Of ninety hazards, smoking ranks ninth, while marijuana ranks eightyfifth (pp. 143-45). Do the "qualitative factors" on Slovic's list really account for this dramatic difference? What is the richer rationality that produces these judgments? Are the risks of smoking less voluntary and more inequitably distributed? Or consider the fact that pesticides and herbicides rank seventh and eighth on the list, while x-rays rank thirtieth, food preservatives thirty-fifth, and food irradiation thirty-ninth (pp. 143-45). Can this pattern of judgments really be explained in terms of voluntariness, equity, potentially catastrophic quality, and risk to future generations? The answer is far from obvious. Might it not be more reasonable to speculate that people's rankings are based on some combination of the affect heuristic and a quick, intuitive, imperfectly informed assessment of the magnitude of the relevant hazards and accompanying benefits? To be sure, it is possible to suggest that people think that smoking is addictive and hence peculiarly 116 Recall that people's judgments about risks and benefits tend to be inversely related; hence when people think something is dangerous, they believe that it has few benefits. See supra p. 
117 Consider the possibility that these "richer" factors merely operate as ex post justifications for decisions reached on other grounds. See MARGOLIS, supra note 14, at 102-o6. It is well established that people are not especially good at offering the actual grounds for their judgments and will sometimes offer accounts that demonstrably diverge from reality. See id.; see also Richard E. Nisbett & Timothy Decamp Wilson, Telling More than We Can Know: Verbal Reports on Mental Processes, 84 PSYCHOL. REV. 231 (1977). In an especially interesting experiment, people were asked to choose one item of clothing from a foursome. Id. at 243-44. Though the four items were identical, the subjects generally chose the item furthest to the right, apparently for no other reason than that it was on the right side. Id. at 243. When asked to justify their decisions, the subjects failed to state the object's location as a reason and, when asked directly, even denied that location played a role. Id. at 243-44. In this light, there is reason to be cautious about the suggestion that these qualitative factors are the actual bases for people's judgments about risks. In fact, things are still worse for Slovic's causal hypothesis, because as I have noted, the qualitative factors said to support ordinary risk perceptions are not generated by subjects on their own, but are suggested by experimenters. The research discussed in this footnote - showing a disconnection between the actual basis and the articulated basis for decisions - has obvious implications for the question whether judges' articulated reasons actually reflect their motivations. involuntary, or that pesticides and herbicides are more likely to affect future generations. But this seems speculative, a heroic effort to make sense of ordinary evaluations."" In short, Slovic has not demonstrated that ordinary people show a richer rationality.1 19 
It is possible to explain people's perceptions in many ways. Slovic's own discussion of the availability heuristic offers the beginning of an account that competes with the idea of a rival rationality: when a relevant incident readily comes to mind, people are much more likely to be fearful than when one does not. The risks associated with nuclear power, smoking, and pesticides receive a great deal of publicity. In the media and in everyday life, less attention is given to the risks associated with x-rays, food preservatives, and food irradiation. If this point is right, people are not unconcerned with the number of lives at risk. They simply err on the factual question: how large is the risk as a statistical matter? The various qualitative considerations are not, on this view, irrelevant; but they play much less of a role than Slovic suggests in explaining the split between experts and ordinary people. 
I have noted the possibility that some of people's judgments, rooted in affect, are attributable to rough-and-ready assessments of whether the activities in question are on balance beneficial or harmful. 120 Consider in this light Howard Margolis's powerful and detailed criticisms of the psychometric paradigm,1 2 1 criticisms with which Slovic does not engage. Margolis thinks that the psychometric paradigm does not explain divisions between experts and lay persons, for which he suggests a quite different explanation. 122 Margolis's bottom line: experts know the facts and ordinary people do not.123 While Slovic attempts to establish that lay people think more richly, Margolis leans heavily in the technocratic direction, challenging the populist underpinnings of those who celebrate the rival rationality of ordinary people. 
118 For similar heroism, see Gillette & Krier, supranote 17, at 1075-82. 
119 One possibility, stressed by Slovic himself and taken up in detail above, is that the affect heuristic underlies people's judgments (pp. 413-29). 
120 Of course, this point is not meant to deny that an unlikely catastrophic risk is likely to attract more attention than its discounted value warrants. There is a connection here to prospect theory in general, which predicts that people will be especially sensitive to low-probability catastrophes. For a good application of prospect theory to the incentives facing potential litigants, see Chris Guthrie, Framing Frivolous Litigation: A Psychological Theory, 67 U. CHI. L. REV. 163 (2000). 
121 See MARGOLIS, supra note 14, at 71-143. 122 See id. at 94-19. 
123 See id. 
Margolis's basic account 124 is exceedingly simple. He thinks that in some cases ordinary people are alert to the hazards of some activity, but not very alert to its benefits, which are cognitively "off-screen.' 1' 5 In such cases, people will tend to think "better safe than sorry,"1' 26 causing them to have a highly negative reaction to the risk. 2 7 In such cases, they will demand aggressive and immediate regulation. In other cases, the benefits, but not the hazards, of the activity will be very much on people's minds, and people will tend to think "nothing ventured, nothing gained.' 28 In these cases, they will think that regulators are overzealous, even fanatical. In still other cases - in Margolis's view, the cases in which observers behave most sensibly - both cboenmepfiatrsinagn dtherisbkesnewfiitlsl wbeith"otnh-esccroesetns.,"129and people will assess risks by 
It is reasonable to think that, for experts, benefits and costs are usually on-screen and that when ordinary people are much more alarmed than experts, it is sometimes because the risks are apparent but the benefits are not. How else are we to make sense of the fact that the very small risks associated with x-rays do not occasion much concern, while the very small risks associated with pesticides and herbicides frequently appear on the list of most feared risks? A sensible conjecture is that people know that, on balance, a world with x-rays is safer and better than a world without them. For pesticides and herbicides, by contrast, the benefits seem far less tangible, if visible at all. It is safe to predict that if a reliable source told people that eliminating pesticides would lead to serious health problems - for example, because pesticide-free fruits and vegetables carried special dangers 30 the perceived risk of pesticides would decline dramatically. Indeed, I predict that if people were informed that eliminating pesticides would lead to a significant increase in the price of apples and oranges, the perceived risk would decrease as well. 
Margolis offers a nice example to support this prediction. The removal of asbestos from schools in New York City was initially quite popular, indeed demanded by parents, even though experts believed that the risks were statistically small.' 3 ' But when it emerged that the removal would cause schools to be closed for a period of weeks, greatly inconveniencing parents, parental attitudes turned right around and 124 Id. at 75-92. 125 Id. at 73-79. 126 See id. 127 See id. at 74, 91-92. 128 See id. 129 See id. at 77-78. 
130 See McHUGHEN, supra note 41, at 232-37 (discussing the potential risks of organic food production). 
131 MARGOLIS, supra note 14, at 124-28. asbestos removal seemed like a really bad idea. 13 When the costs of the removal came on-screen, parents thought much more like experts and the risks of asbestos seemed like the risks of x-rays: statistically small and, on balance, worth incurring. 
A reasonable conjecture, then, is that when ordinary people diverge from experts, it is sometimes because ordinary people see the risks but not the benefits, whereas experts see both. Some evidence for this conjecture comes from Slovic's finding that when people think that a product or an activity has a high risk, they tend to think that it has a low benefit, and that when they think that a product or an activity has a high benefit, they tend to think that it carries a low risk too (pp. 415-17). This finding strongly suggests that when ordinary people are more fearful than experts, it is often because ordinary people are not looking at the benefits that accompany the product or activity at issue. The same finding suggests that when ordinary people are less fearful than experts - as was the case, for many years, with cigarettes - it is because ordinary people are looking not at the risks, but instead only at the benefits. 
In sum, the difference between experts and ordinary people might be explained not only or mostly by reference to rival rationality, but also and more fundamentally by some combination of the availability and affect heuristics and a failure, on the part of ordinary people, to put all of the effects of risks on-screen. Of course, none of these points means that experts do not err. They too are subject to cognitive errors, 133 and we have seen that experts show affiliation bias. I have emphasized that annual fatalities are not all that matter. My major point is that in the domains in which they specialize, experts are far more likely to be right than are ordinary people. Compare the area of law: experienced litigators are hardly infallible, but they have a comparative advantage, most of the time, in predicting what judges and juries will do. Similarly, brain surgeons certainly err, but they know far more about brain surgery than most of us do, and it would be odd to say that ordinary people have a rival rationality about brain surgery. 
There is a further problem. Many of the qualitative factors that are said to lead people to a "rich" conception of risk need a good deal of unpacking. 134 The qualitative variables should not be seen as 132 Id. at 127-28. 
133 See, e.g., Amos Tversky & David Kahneman, Belief in the Law of Small Numbers, 76 PSYCHOL. BULL. 105 (1970. 
134 For a sophisticated treatment that seems to me far too celebratory and uncritical of lay rationality, see Gillette & Krier, supranote 17, at 1076-82. 2002) marking dichotomies, or even clear distinctions. They raise many questions. 
i. The Example of Dread. - Some risks are said to be "dread," whereas other risks are not. But what does this mean? In the abstract, to say that a risk is dread seems to be to say that people fear it. This suggests that the idea of dread is just a synonym for perception of risk, not an explanation for it. If so, it is no surprise that there is a correlation between risks perceived as serious and risks deemed to be dread. (Is it surprising that people are scared of things that they find frightening?) It is even possible that when people say that a risk is dread, what they mean, in part, is that the risk is large in magnitude. People do not dread being attacked by unicorns or Martians. But they do dread cancer, partly (not only' 35 ) because the risk of getting cancer is relatively high. 
Slovic uses the term "common" as an antonym to "dread" (p. 94), but that dichotomy raises further problems. Cancer is a common risk. But it is also the prototype of a dread risk. In trying to explain how the qualitative factor of dread explains the divergence between experts and ordinary people, we seem to be approaching (so to speak) a dead end. 135 See Revesz, supra note io8, at 972-73. 
136 I defend this view in Cass R. Sunstein, Bad Deaths, 14 J. RISK & UNCERTAINTY 259, 26869 (1997). 
137 See Revesz, supra note lo8, at 972-74; George Tolley, Donald Kenkel & Robert Fabian, State-of-the-Art Health Values, in VALUING HEALTH FOR POLICY: AN ECONOMIC APPROACH 323, 339-44 (George Tolley, Donald Kenkel & Robert Fabian eds., 1994). 138 See Tolley, Kenkel & Fabian, supra note 137, at 339-44. voluntariness and controllability. 139 At first glance, the risks associated with pesticides and herbicides might seem involuntary and uncontrollable, whereas the risks from smoking and driving might seem voluntary and controllable. People choose to smoke and to drive. At least this is what people seem to say, when asked to rank risks along the relevant dimensions on seven-point scales (pp. 203-06). People do seem especially fearful when danger is random and when it seems as if nothing can be done to prevent it;14o terrorists are well aware of this point. 
On reflection, however, these issues are complicated, and people's assessments of risks, along the scales of voluntariness and controllability, may not make much sense. It is not especially difficult to avoid pesticides. Many people can and do select pesticide-free food. The decision to smoke might seem voluntary, but smoking is addictive, and many people seek to quit but find, or say, that they cannot.1 4 1 The decision to drive might seem voluntary, but in many places it is extremely hard or expensive to get to work without a car. Perhaps the risks associated with driving are controllable, but in many accidents at least one of the drivers involved is not at fault. Are the risks from ambient air pollution involuntarily incurred? It might seem that they are, but people can choose where to live and some areas have much cleaner air than others. Are the risks from flying uncontrollable? Many people seem to think so. But no one is required to fly. Why are the risks of flying so much less controllable than the risks from bicycles or microwave ovens (p. 142)? 
In this light, it is probably best not to see a dichotomy between voluntariness and involuntariness, but to start by asking about two issues: first, whether those who are subject to a risk know about its existence, and second, whether it is costly, or burdensome, to avoid the risk in question. 142 When a risk seems "involuntary," it is usually because people who face the risk do not know about it or because it is especially difficult or costly to avoid it. When a risk seems "voluntary," it is usually because those who run the risk are fully aware of it and because risk avoidance seems easy or cheap. If this is correct, there is no sharp dichotomy between "voluntary" and "involuntary" 139 These are the two factors that the EPA'emphasized in its sensitivity analysis involving arsenic. National Primary Drinking Water Regulations, 66 Fed. Reg. 6976, 7013-I5 (Jan. 22, 2001) (to be codified at 40 C.F.R. pts. 9, 141, 142). 
140 See JOEL BEST, RANDOM VIOLENCE: How WE TALK ABOUT NEW CRIMES AND NEW VICTIMS 8-17, 22 (I999). 
141 See Neal L. Benowitz, The Nature of Nicotine Addiction, in SMOKING: RISK, PERCEPTION, & POLICY, supra note 20, at 159, 159. 
142 For more details on this complex issue, see Sunstein, supranote 136, at 266-70. 2002] risks, or even between "controllable" and "uncontrollable" risks. Indeed, these terms seem far too crude to capture what is really important. In this way, they have much in common with intuitive toxicology, which also portrays things in "all or nothing" terms. (Is it unfair to say that the use of such dichotomies reflects a form of intuitive psychometrics?) To the extent that ordinary people care about voluntariness and controllability, they are gesturing toward a sensible way of thinking about risks. But it has not been shown that they have a rival rationality. 
Slovic is right to say that qualitative factors matter to ordinary perceptions of risk. But I think that he claims more than the evidence establishes and that the same evidence said to support rival rationality might reflect affect and simple errors of fact. An interesting way to test my claims would be to see whether people are able to generate statistically accurate judgments about relevant risks. When specifically asked about the number of expected deaths from various sources, do people make roughly the same judgments that experts do? If so, then it might indeed be the case that when ordinary people diverge from experts, it is because of the qualitative factors to which Slovic points. But if ordinary people err in estimating the number of lives at risk, then their errors might well explain the divergences. It is also possible that people are simply not focusing on the magnitude of the risk and that if this factor were brought to their attention, their judgments would shift accordingly. 
Actually, Slovic does provide some evidence on this point (pp. 1o507), and I believe that it undermines his claim. On the purely factual issues, he finds systematic mistakes by ordinary people (pp. io5-o7). 143 Other evidence supports Slovic's findings here. 144 No one, however, has sorted out the extent to which these errors, or instead qualitative judgments, underlie the relevant disagreements. There remains a large empirical agenda here. 
Where does this leave us? It suggests that many of the disagreements between experts and ordinary people stem from the fact that experts have more information and are prepared to look at the benefits 143 Gillette and Krier refer to intriguing evidence that ordinary people's estimates of annual fatalities do not closely correlate with ordinary people's judgments about riskiness. Gillette & Krier, supra note 17, at 1074. From this evidence, they conclude that the qualitative factors catastrophic quality, involuntariness, unfamiliarity, and the like - do actually motivate nonexperts. Id. But the conclusion need not follow. People's riskiness judgments might be based on overall affect and on the availability heuristic; it is also possible (likely!) that when making riskiness judgments, people are not focusing on annual fatalities, even though they would on reflection agree that annual fatalities matter a great deal. 
144 See Viscusi, supra note 29, at 130-34. as well as the risks associated with controversial products and activities. In the terms of Slovic's more recent work, experts rely on a form of statistical analysis, while ordinary people use more "experiential thinking," which leads to errors. 45 But Slovic is right to insist that values matter. When people's reflective judgments drive their perceptions, those judgments deserve to play a role in policy. To the extent that experts focus only on the number of lives at stake, they are genuinely obtuse. It is reasonable to devote special attention to dangers that are hard to avoid, accompanied by unusual suffering, or faced principally by children. On the positive side, what is needed is more empirical work to determine the extent to which ordinary risk perceptions are based on errors or instead on values. On the normative side, what is needed is more thought about the nature of concepts like "dread," "voluntariness," and "controllability." With respect to policy, what is needed is incorporation of people's values to the extent that they can survive a process of reflection. 
I now turn to three issues on which Slovic offers a number of intriguing findings of special importance to law and policy. These involve demographic differences, the crucial issue of trust, and paternalism. 
Do members of different social groups agree about risks? Slovic offers several interesting findings. The most general is what he calls "[t]he white-male effect" (p. 399). Apparently white men are less concerned about risks than are members of other groups. With respect to nearly every risk, white women, nonwhite women, and nonwhite men are far more troubled than are white men (pp. 397-99). But this is the punchline of the story, and it is helpful to provide the details. 
Slovic asked large numbers of people to rank a large number of activities as posing little or no risk, slight risk, moderate risk, or high risk. Women ranked nearly every risk higher than men did (pp. 396402). These results obtained for ordinary Americans. They hold as well for British toxicologists: female British toxicologists ranked risks as more serious than did their male counterparts (p. 397). Among ordinary Americans, the differences between men and women are most pronounced for stress, suntanning, nuclear waste, nuclear power plants, ozone depletion, and AIDS, and least pronounced for medical x-rays, commercial air travel, and genetically engineered bacteria (p. 
145 See Slovic, supra note 20, at 107-10. 398). Among British toxicologists, the sex differences are especially large with respect to nuclear waste, nuclear power reactors, outdoor air pollution, alcoholic beverages, and suntanning (p. 397), and smallest for radon in homes, motor vehicle accidents, and mercury in fillings (p. 397).146 There are other demographic differences too: as education and income increase, fear of almost all risks decreases (p. 399). Notably, however, sex differences hold even when the results are controlled for education and income. 
Once the data are disaggregated, a still more striking and somewhat different picture emerges. Although men as a group perceive risks as less serious than do women as a group, race affects risk perception as well. Nonwhites see risks as more serious than do whites, with especially large differences for bacteria in food, genetically engineered bacteria, and pesticides in food, and smaller differences for outdoor air quality, ozone depletion, and stress (p. 399). But there is more: white males turn out to be the real outliers, ranking risks as less serious than other demographic groups do. Across a large number of hazards, white men perceive risks as consistently lower than do white women, nonwhite women, and nonwhite men (pp. 398-4oo). In fact, there are no large-scale differences among the latter three groups. Even more interestingly, what drives the "white-male effect" is not the view of all white males, but the view of about 30% of them who believe almost all risks to be very low (pp. 398-99). The other 70% of white males are not greatly different from the other subgroups. 
Slovic observes that the relevant 30% tend to have more education and more household income and also to be more conservative (pp. 399-4oo). They tend to disagree more with the view that technological development is destroying nature, to reject the idea that they have very little control over risks to their health, and to think that future generations can take care of themselves when facing risks imposed as a result of today's technologies. Slovic thinks that affect is part of what is responsible for this distribution of beliefs about risks (pp. 40309). 
All of this is extremely interesting. There is, however, a question in the background: is it helpful or even meaningful, for purposes of analysis, to isolate the 30% of white men who believe almost all risks to be low? This group was selected not because of anything that unites them, but because the views of its members differed systematically from the entire remaining pool of nonwhites, white females, and 146 Male British toxicologists rank only two sources of risk as more severe than do female British toxicologists: the risks from burning fossil fuels and from chemical pollution in the workplace (P. 397). the other 70% of white men. 14 7 Hence the low-risk-perception group of 30% of white men is essentially a constructed pool. From what Slovic says here and from the supporting study,148 it is not even demonstrated that political perceptions closely correlate with risk perception. Because the 30% of white males were compared with the entire pool of nonwhites, white females, and the 70% of white males, it is not impossible that white males as a group are more conservative but that there is no correlation between conservatism and risk assessment within the group of white males. I strongly suspect that Slovic is right in suggesting that there is a correlation, but the analysis here does not prove that it exists. 49 
Even if Slovic is right, there is an additional gap in these studies. We know that white men are less concerned about certain risks than are members of other demographic groups. But we do not know whether white men believe that the statistical risk of harm is lower or whether they simply take those risks less seriously. White men might believe that the risks of pesticides deserve a "2"on a seven-point scale, whereas others believe that those risks deserve a "5," but all groups might have the same basic sense of the statistical risks. Does a lifetime risk of i in iooooo count as a "I" or a "5"? How can we tell? Is it possible that groups do not disagree on the statistical risks, but merely on how to evaluate them on some scale? 
On the data that Slovic presents here, it is possible that white men have a more accurate perception of the numbers.150 It is more likely (indeed, it is my guess) that no group has an accurate sense of the facts but that white men, or the key 30%, are simply less worried than are other people.'15 Of course, the level of worry is a source of concern, even if unfounded. If people are frightened when they should not be or are relaxed when they should be troubled, something is wrong and government might well do something about the situation. But for 147 See James Flynn, Paul Slovic & C.K. Mertz, Gender, Race, and Perception of Environmental Health Risks, 14 RISK ANALYSIS iios, iio6 (1994). 
148 Id. 
149 Not included in The Perception of Risk is a later study in which Slovic finds the "white male effect" to be "more complex than previously indicated." Melissa L. Finucane, Paul Slovic, C.K. Mertz, James Flynn & Theresa A. Satterfield, Gender,Race, and PerceivedRisk: The 'White Male' Effect, 2 HEALTH, RISK & SOCY 159, 159 (2000). This subsequent study confirms that nonwhites generally view risks as higher than do whites and that females generally view risks as higher than do men, but it also finds significant heterogeneity among other groups. See id. at 163-65. While suggesting that low-risk attitudes are driven by "greater confidence in experts and less confidence in public-dominated social processes," id. at 170, the study also eschews the causal language linking ideology and risk perception found in The Perception of Risk, see id. at 17o-71. 
1SO Note that if they do, it is not because of greater education; even holding education constant, Slovic finds sex differences. 
151 It would be interesting in this regard to know whether demographic groups differ much in their judgments of the actual numbers. 2002] many purposes, the facts do matter. It is important to know whether people are concerned because they have a sense of the statistical truth or because they lack such a sense. 
A reasonable hypothesis is that the lesser concern of white men results from the fact that white men, simply by virtue of being white and male, have a sense of relative security and safety, at least in some domains. For this reason, they might believe, and report that they believe, that the risks they face are generally low. Women and nonwhites, by contrast, may have a sense of relative insecurity and so think of most risks as more serious, not necessarily on the numbers, but on the slight risk/high risk scale. Indeed this is my hunch, strongly supported by three of Slovic's other findings. First, if people are most concerned about risks that they consider involuntarily incurred, beyond their control, and unfairly distributed, it should be unsurprising that for most risks white men are simply less concerned. Second, the affect heuristic might well lead to distinctive reactions from white men. When white men hear the words "pesticides" or "nuclear power," they might have no negative reaction, and possibly even a positive reaction, as compared to the reactions of members of other demographic groups. When women hear the term "stress" or "suntanning," their affective reaction might be especially negative (p. 398), perhaps because of the availability heuristic; so too when nonwhites hear the term "bacteria in food" or "genetic engineering" (p. 399). Third, trust is relevant to people's assessment of the severity of risks. White men might well have comparatively greater trust in private and public institutions and hence rank risks as relatively low. I now turn to this point. 
The topic of trust has been a greatly neglected issue in political and especially legal treatments of risk regulation. Slovic thinks that the topic is extremely important (pp. 316-26).1l 2 A basic puzzle here is that in the last twenty years, our society has grown healthier and safer, in part because we have spent billions of dollars on risk reduction; but at the same time, the American public has become more, not less, concerned about risks (p. 316). Slovic emphasizes that a lack of trust has played an important role in controversies over managing hazardous technologies. He also shows that when people are concerned about a hazard, it is often because they do not trust those who manage it. Consider, for example, the fact that people tend to view medical tech152 See WEINGART, supra note 3, at 362 ("Throughout the voluntary siting process, people would tell us that their fears were fueled by their conviction that the government had lied to them, or misled them, in the past. Things they had thought would be harmless had turned out to cause harm."). 
116o nologies involving radiation and chemicals (such as x-rays) as high benefit and low risk, whereas they tend to view industrial technologies involving radiation and chemicals (such as nuclear power and pesticides) as high risk and low benefit (p. 316). Far more important than technical risk assessments is the level of trust in those who try to manage risks and to give assurances. 
Slovic also stresses the fragility of trust, which is far easier to destroy than to create.1 5 3 Slovic offers a study of forty-five hypothetical news events involving nuclear power; some of these events were designed to increase trust in those managing its risks, whereas others were designed to decrease that trust (pp. 320-22). Negative events were judged to have had a much more significant effect than positive events. For example, a nuclear power plant accident in another state had a strong negative effect on trust, whereas careful selection and training of employees or an absence of any safety problems in the past year had little effect in increasing trust (pp. 320-22). Slovic also finds that sources of bad, trust-destroying news are seen as more credible than are sources of good, trust-creating news (pp. 322-23). This finding is in line with a related one mentioned above: when people are unconcerned about a risk, discussion that is designed to provide still more assurance tends to increase anxiety rather than to diminish it. 
With respect to trust, Slovic therefore establishes the existence of an "asymmetry principle": events that weaken trust have a significant effect, whereas events that strengthen trust do very little (p. 320). Does this mean that people are confused or irrational? Slovic doesn't think so: "Conflicts and controversies surrounding risk management are not due to public irrationality or ignorance but, instead, can be seen as expected side effects of these psychological tendencies, interacting with our remarkable form of participatory democratic government, and amplified by certain powerful technological and social changes in our society" (p. 323). Many of those changes involve the news media, which can instantaneously publicize risk-related events from anywhere in the world. (Consider the anthrax frenzy of October 2001.) And the news media, no less than ordinary people, give special emphasis to bad, trust-destroying events. This asymmetry has important effects on public attitudes. Slovic argues that "[t]he young science of risk assessment is too fragile, too indirect, to prevail in such a hostile atmosphere" (p. 324). Slovic does not explain exactly why trust is more easily destroyed than created; perhaps the asymmetry principle is partly a product of risk aversion, which makes people respond more when trust 153 See id. (noting that "the fact that some widespread fears have proved to be unfounded seems to have had no impact on the evaluation of present and future risks"). 2002] is challenged than when it is confirmed. In any case, Slovic shows that the asymmetry is quite important for policymakers. 
What can be done to increase trust? Slovic favors an increase in public participation in decisionmaking, going well beyond public relations to include much more in the way of actual power-sharing (p. 325).154 His fear is that governmental efforts to reassure people are unlikely, without broad participation, to breed anything but further distrust. On this count, Slovic might well be right, but his own data raise serious questions about the value of public participation in increasing trust.155 If bad news is more salient than good news and if people act as intuitive toxicologists whose perceptions of risk can be amplified by social interactions, there is special danger: high levels of public participation in technical domains could simply heighten public fear, with unfortunate consequences for policy. 156 Recall here the epigraph to this Review: "At the Siting Board's open houses, people would invent scenarios and then dare Board members and staff to say they were impossible."'. 7 Like the questioners in this passage, many Americans would find that more exposure to this type of information feeds their fears. 
Slovic is right to emphasize the importance of trust and also to connect that issue to the split between experts and ordinary people. But efforts to increase public participation in the regulatory process could have many unforeseen consequences. Greater participation might make things worse rather than better, fueling both fear and distrust. If people come to the process with the intuitions that Slovic catalogues, then it is not clear that broad public involvement will be helpful for either sound regulation or trust itself. Group polarization could well increase the problem. 158 Imagine, for example, that government attempted to increase public participation in a decision to license a nuclear power plant or to allow a certain level of arsenic in drinking water. It is easy to imagine a situation in which reassurances would fall on deaf ears and in which the very fact of participation, and broader citizen involvement, would increase fear and distrust. 
154 Increasing public participation in decisionmaking is a major theme in NAT'L RESEARCH COUNCIL, UNDERSTANDING RISK: INFORMING DECISIONS IN A DEMOCRATIC SOCIETY (1996). 
155 WEINGART, supra note 3, at 357-62 (raising similar questions). 156 See id. at 362-63. 157 Id. at 362. 158 See supra section I.D.3, pp. 1135-37. 
Much of Slovic's analysis, including his treatment of trust, raises the vexing issue of paternalism. He discusses that issue explicitly in connection with tobacco. 
To come to terms with paternalism for smokers, it is important to ask whether smokers know the risks associated with smoking. In an important study, W. Kip Viscusi argues that they do.159 Viscusi asked both smokers and nonsmokers how many smokers are likely to die from various smoking-related causes. Viscusi's basic findings were that people do indeed know the risks and that in fact they overestimate them. 
With respect to adolescents, at least, Slovic is quite critical of Viscusi's findings. 160 He suggests that young people tend to be overly optimistic, thinking that they themselves are peculiarly immune from risks that others face (p. 366). Hence Slovic believes that people's ability to generate accurate statistical figures for smokers as a whole can be consistent with the claim that smokers typically underestimate the risk that they themselves face. 16 1 Slovic also contends that people's quantitative judgments depend on how questions are asked and hence that slight alterations in questions can generate large differences in results and far more inaccurate numbers (pp. 366-67). Consider a vivid example: In Slovic's own, small-scale study, a simple question like Viscusi's - of lifetime smokers, how many are likely to die of lung cancer? - produced no underestimate (p. 367). But when the same people are asked how many lifetime smokers are likely to die of each of fifteen causes of death (homicide, suicide, car accident, and so forth), the number of predicted lung cancer deaths dropped for nearly everyone, often precipitously (p. 367). For Slovic, the instability of people's answers raises serious questions about the robustness of responses to any particular question. 
Slovic contends as well that people who know the statistical risk do not have a sufficient sense of what it is actually like to experience the adverse effects of smoking. He claims, finally, that people are not likely to understand either the cumulative nature of the risk (p. 367) or its addictive quality (p. 369). Thus, many smokers have no clear sense of the long-term risks of smoking. In addition, high school seniors greatly overestimate the likelihood that they will not be smoking five years after starting to smoke (p. 369). Fewer than half of those who 159 See W. KiP visCusI, SMOKING: MAKING THE RISKY DECISION 61-86 (1992). 160 The findings in The Perception of Risk are extended in SMOKING: RISK, PERCEPTION, & POLICY, supranote 20. 
161 See John Z. Ayanian & Paul D. Cleary, PerceivedRisks of HeartDisease and CancerAmong CigaretteSmokers, 281 JAMA lO19, 1020-21 (1999) (finding that most smokers do not view themselves at increased risk of heart disease and cancer). 2002] predicted that they would quit smoking after five years turned out, in fact, to have quit. Many young smokers perceive themselves to be at little or no risk from smoking because they expect to stop smoking before any damage to their health occurs. In reality, a high percentage of young smokers continue to smoke over a long period of time and are placed at significant risk by this behavior (p. 370). 
Slovic's discussion of these points is brief but highly suggestive. There is a great deal to sort out here on the vexing questions associated with ensuring adequate information and the proper place of paternalism. People have been saturated with evidence about the adverse health effects of smoking. Viscusi's data should not be shocking in this light. But neither Viscusi nor Slovic offers a great deal of evidence about a key issue: the statistical risk that each smoker believes he or she is running - as opposed to the statistical risk that each smoker believes that smokers generally are running. If it is true that people generally suffer from excessive optimism with respect to the risks that they face personally, 162 there is a problem of inadequate information even when people are well aware of the statistical risks. Indeed, some evidence suggests that smokers do not believe that they themselves are at an increased risk of heart disease and cancer. 163 This problem could justify a governmental effort to provide a corrective. (Consider the advertising campaign from several years ago: "Drive defensively; watch out for the other guy.") And this point raises the expert/layperson split from another angle. In some areas, the source of the split may be an emphasis on statistical realities by experts - and an overinflated sense of personal invulnerability by risktakers. 
Slovic is a psychologist, not a policy analyst. Although he gestures in the direction of legal reform, this is not his principal topic. Nonetheless, his work carries considerable importance for those concerned with law and policy, partly because an understanding of human cognition (not excluding emotion) helps to explain people's reactions to risk and their demands for legal responses. Such an understanding will help us to see which approaches to risk regulation will work and which will not. It will also illuminate the continuing battle between technocratic and populist approaches to risk. If we know why people think what they do, and whether their views are based on mistakes or 162 See Christine Jolls, Behavioral Economic Analysis of Redistributive Legal Rules, in BEHAVIORAL LAW AND ECONOMICS 288, 291-92 (Cass R. Sunstein ed., 2000). Jolls offers evidence that people inderestimate both their relative risk and their statistical risk. See id. 163 See Ayanian & Cleary, supra note r6i, at 1020-21. instead on reasonable judgments of value, we will be able to make some progress in understanding the role of science, and experts, in the world of risk regulation. 
I have said nothing thus far about the political valence of technocratic and populist approaches to risk regulation. But there are interesting issues in the background here. For those who favor aggressive efforts to protect safety, health, and the environment, technocratic approaches might seem unhelpful, even destructive, perhaps a form of obstructionism. 164 It is tempting to think that an emphasis on science will be favored by those who are skeptical of risk regulation and that populism will seem appealing to those who believe that risk regulation is desirable and should be increased. To ask a crude question: shouldn't conservatives be technocrats and liberals populists? 
As a description of current debates, this view is not entirely inaccurate. 16 Certainly science can slow things down. 166 But the associations should not be overstated. In many contexts, an effort to bring science to bear on regulatory issues, and even to engage in cost-benefit analysis, has led to more rather than less aggressive regulation; 167 consider stringent controls on ozone-depleting chemicals and the phaseout of lead in gasoline, both of which were spurred by technocrats armed with careful analyses of the underlying science. 68 Consider too the "prompt letter," a new innovation from President Bush's Office of Information and Regulatory Affairs. The purpose of the prompt letter is to spur further regulation in contexts in which the evidence calls for it.169 In the disability context, science and data have been invoked by disability rights advocates, attempting to quiet or delegitimate public fears about the dangers caused by disabled people. 170 As Samuel Bagenstos has shown, the political valence of technocracy on the one 164 See Samuel R. Bagenstos, The Americans with Disabilities Act as Risk Regulation, ioi COLUM. L. REV. z479, 1484-87 (2001). 
165 Id. 
166 This is especially true if people with a stake in the outcome fund experts. See RAMPTON & STAUBER, supra note 18, at 195-221. 
167 See, e.g., RICHARD BENEDICT, OZONE DIPLOMACY 63 (igg) (noting that a cost-benefit study produced by the President's Council of Economic Advisors was influential in persuading some skeptical executive officials to support the administration's stance on ozone regulation); James K. Hammitt, Stratospheric-OzoneDepletion, in ECONOMIC ANALYSES AT EPA 131, 13164 (Richard D. Morgenstern ed., 1998); Albert L. Nichols, Lead in Gasoline, in ECONOMIC ANALYSES AT EPA, supra,at 49, 77-83. 
168 Hammitt, supra note 167, at 131-64; Nichols, supra note 167, at 49-83. 
169 White House Office of Mgmt. & Budget, Prompt Letters, at http://www.whitehouse.gov/ omb/inforeg/promptjletter.html (last visited Jan. 6, 2002). 
170 Bagenstos, supra note 164, at 148o-81. hand, and populism on the other, is a product of the context; it cannot be established in advance.' 
But there is a still deeper issue in the background. If it is sensible to favor aggressive efforts to protect people against the risks posed by, say, arsenic in drinking water or nuclear power, it must be because those risks are substantial rather than trivial. And if it is sensible to oppose these efforts, it must be because the risks are too small to be worth addressing. Whether the risks are small or substantial depends, in large part, on what science shows. Of course, the "number of lives at risk" is not all that matters, and science will often leave gaps and uncertainties. But it is worse than senseless to have a kind of preanalytic commitment to regulation, or against regulation, and to use technocratic and populist arguments as mere rhetorical tools or props on behalf of that commitment. When people are skeptical of risk regulation in some domain, it must be because they do not believe that it will do much good. 7 2 It is absurd to think that science can tell us all we need to know or that values are irrelevant. But there is no necessary political valence to science; and when we favor regulation, or oppose it, it should be because of its likely consequences, 17 3 rather than because of a political commitment that precedes, or outruns, an investigation of that topic. 
To see the importance of Slovic's work for law, it is useful to distinguish among three different enterprises: positive, prescriptive, and normative. 7 4 Positive work is concerned with making predictions: When will law take certain forms? What will be the effect of a particular step on human behavior? Prescriptive work is concerned with identifying the means of achieving shared goals: if government seeks to increase people's fear of certain risks and to dampen their fear of others, it needs to know which strategies will actually work. Normative work is concerned with determining what government's ends should be. If people make systematic mistakes about the risks that they face, perhaps governmental paternalism is justified. 
Slovic's emphasis on conflicts between experts and ordinary people sheds light on all of these enterprises. If the availability heuristic and 171 Id. at i5o9-11. 
172 Some people may have a commitment to liberty that argues against regulation even when it would do a great deal of good. In the particular contexts I discuss, I doubt that any such commitment could be defended; but I will not defend that doubt here. 
173 Of course, no evaluation or even account of consequences is value-free. But an understanding of the likely effects of regulation will often produce agreement from people with quite different values. 
174 See Christine Jolls, Cass R. Sunstein & Richard Thaler, A BehavioralApproach to Law and Economics, 50 STAN. L. REV. 147I, 1474-75 (i998). intuitive toxicology help to drive people to certain judgments about risks, we will be able to make predictions about the likely effects of salient events, both on people's judgments about when to insure and take precautions and on the demand for legal responses. 7 In the aftermath of a highly publicized incident, considerable movement should be expected; but if illustrations of harm do not come to mind, people might persist in failing to take adequate care of themselves. If emotions play a large role in risk-related behavior, educational campaigns are far more likely to work when they involve memorable images rather than statistical probabilities. 176 If government wants to encourage people to protect themselves, it should use particular examples and try to make them as vivid as possible. And if people make a large number of mistakes about risks - partly because of emotions, partly because of cognition - there are reasons to be skeptical about populist conceptions of regulatory government and also about the reflexive antipaternalism of much work on risk regulation. In short, Slovic's findings may support the conception of administration associated with the New Deal period, which placed a high premium on ensuring regulatory choices by people immersed in the subject at hand.1 77 
In this light, Slovic's findings can be brought in close contact with Justice Stephen Breyer's discussion of risk regulation, 178 probably the most influential treatment within the legal culture. Breyer urges, quite plausibly, that the regulatory state suffers from poor priority-setting. In his view, government devotes a lot of resources to trivial problems and spends too little time and effort on more serious problems that place many people at risk.179 Breyer urges the creation of a new body of risk specialists, with expertise in many fields. 80 The task of these specialists would be to reallocate resources from small problems to large ones.' 81 
Breyer offers an impressive analysis of the problem of poor priority-setting, and he proposes a promising institutional correction. In many ways, Breyer calls for a kind of newer New Deal - a system of administration run in substantial part by technocrats, subject of course to democratic oversight and override. But his book says almost nothing about how people think about risks. 82 With reference to Slovic's 175 See Noll & Krier, supranote 32, at 760-77. An understanding of intuitive toxicology might, for example, have alerted officials to the likely adverse reaction to any effort to suspend the proposed regulation of arsenic in drinking water. See Sunstein, supranote 6i (manuscript at 6-7). 176 See Sunstein, supra note 81 (manuscript at ix). 177 See JAMES M. LANDIS, THE ADMINISTRATIVE PROCESS 6-46 (1938). 178 BREYER, supra note 14. 179 Id. at 10-29, 180 Id. at 59-68. 181 See id. at 67. 
182 Breyer does offer a brief treatment of cognitive factors. See id. at 16. findings, Breyer's analysis might well be criticized on the ground that ordinary people would distrust any body of risk specialists, viewing them as unlikely to be sufficiently responsive to their concerns. Perhaps more interestingly, Breyer's approach might be criticized on the ground that sheer numbers cannot tell us whether a problem is "large" or "small." People are reasonably concerned about a range of other variables. This is a large thrust of Slovic's work. 
In the end, however, Slovic's findings seem to me mostly supportive of Breyer's analysis; indeed, Slovic provides strong cognitive ammunition for Breyer's diagnosis and even his remedy. Slovic shows that ordinary people make many mistakes in thinking about the risks associated with various activities. He also shows that, much of the time, people treat safety as an "all or nothing" matter, are vulnerable to the "zero-risk mentality," overreact to small signals of dangers, and sometimes show excessive optimism. In these circumstances, a sensible system of risk regulation will not respond mechanically to what people think; it will impose thick filters on the public's own conception of appropriate priorities. 
But there are three grounds for objecting to a purely technocratic approach to risk regulation. The first emerges from analysis of some of the qualitative factors that Slovic highlights. Some deaths are particularly bad, and these deserve unusual attention. When it is especially easy to avoid certain risks, government should not spend a great deal of time and effort in response. Inequitably distributed risks deserve special attention, as do risks that impose special suffering. It would indeed be obtuse to treat all statistically equivalent risks as if they were the same, regardless of context or quality. But it remains true that a sensible society is greatly concerned with ensuring that people have longer and healthier lives - and that if policies lead government to spend a lot on small problems and little on large ones, something is amiss. Note in this regard that a more sensible allocation of public resources could save tens of thousands of lives, tens of billions of dollars, or both. 183 It makes sense both to acknowledge qualitative distinctions among statistically equivalent deaths and to insist that better priority-setting is extremely important. 
The second criticism of a purely technocratic approach involves people's likely reaction to it. To work well, a regulatory system needs public support and trust, even if we do not believe that a lack of trust would be fully rational. To the extent that government relies on statistical evidence alone, it is unlikely to promote its own goals. This is 183 See Tammy 0. Tengs & John D. Graham, The Opportunity Costs of Haphazard Social Investments in Life-Saving, in RISKS, COSTS, AND LIVES SAVED x67, 172-74, 176 (Robert W. Hahn ed., 1996) (discussing the rationality of resource allocation for 185 interventions designed to avert premature deaths). partly because people will assess that evidence in light of their own motivations and their imperfect cognitive capacities; recall the availability and affect heuristics and intuitive toxicology. Regulators who are alert to the importance of both confidence and trust will do what they can to provide information in a way that is well tailored to how people think about risk - and that tries to educate people when their usual ways of thinking lead them astray. In some circumstances, an understanding of how people think will lead government toward approaches that technocrats, if insensitive to Slovic's findings, would not have on their viewscreen. We might say that good technocrats need to know not only economics and science, but psychology as well. 
The third ground for objecting to a purely technocratic approach involves the reality and effects of fear.184 The mere fact of fear is a social loss, in some cases a large one, and as we have seen, fear is likely to have a range of ripple effects. Ripple effects are likely even if the fear turns out to be baseless or to be rooted in beliefs that technocrats rightly reject. Of course, the most straightforward response to baseless fear is information and education. But sometimes these will be ineffectual; recall the discussions of availability, affect, and trust. If government cannot dissipate fear through information, it might be well advised to regulate, at least if regulation will eliminate fear in a relatively inexpensive manner. Suppose, for example, that by banning the feeding of sharks or erecting physical barriers in the ocean, government can assure people that they will not be attacked by sharks. If the relevant steps would be both inexpensive and effective in eliminating fear, they would make a great deal of sense, not to reduce the risk (which may be infinitesimal) but to quell the fear. This point has implications for many imaginable issues involving governmental response to panics and catastrophes. 
But the most important lesson of Slovic's book seems to lie elsewhere. Because of predictable features of human cognition, people's intuitions are unreliable,18 - and we are prone to blunder about the facts in predictable ways. These blunders have harmful consequences for regulatory policy. To be effective, regulators must be aware of not only actual but also perceived risk. For purposes of policy, however, what is most important is actual risk rather than perceived risk. I have offered some qualifications to this point. But most of the time, government and law should respond to people's values, not to their errors. 
