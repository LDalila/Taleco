Follow this and additional works at: https://chicagounbound.uchicago.edu/law_and_economics Recommended Citation 
RANKING LAW SCHOOLS: A MARKET TEST?  
Cass R. Sunstein                
T H E   L A W   S C H O O L   T H E   U N I V E R S I T Y   O F   C H I C A G O   
  
July 2005  This paper can be downloaded without charge at the John M. Olin Program in Law and Economics  Working Paper Series: http://www.law.uchicago.edu/Lawecon/index.html and  
The Social Science Research Network Electronic Paper Collection:  
http://ssrn.com/abstract_id=703282   Preliminary draft 11/18/04 All rights reserved 
Cass R. Sunstein* 
Instead of ranking law schools through statistical aggregations of expert judgments, or by combining a list of heterogeneous factors, it would be possible to rely on a market test, simply by examining student choices. This tournament-type approach would have the large advantage of relying on the widely dispersed information that students actually have; it would also reduce reliance on factors that can be manipulated (and whose manipulation does no good other than to increase rankings). On the other hand, a market test has several problems as a measure of law school quality, partly because cognitive biases and social influences may lead some or many students to make bad choices and thus to participate in the production of inaccurate rankings. 
I. 
Law school rankings are notoriously contentious. Many such rankings are obtained in the following way. Much of the analysis comes from the judgments of a group of experts, who are asked to assess law school quality, in general and along particular dimensions. For example, forty percent of the US News and World Report rankings are generated in this way. Academic reputation, measured by responses of law school deans and faculty, counts for 25% of the total score; reputation among lawyers and judges counts for 15% of the total score. In Brian Leiter’s rankings, faculty quality is measured solely through aggregating surveys, in a way that makes expert judgments crucial.1 
This method is controversial, but there is a great deal of logic to relying on statistical aggregations of expert opinions. When experts are available, it makes sense to obtain a statistical answer from them, rather than to select one or a few. If experts are likely to have relevant information, a statistical group of experts should have a significant advantage over individual experts, just as a statistical group of ordinary people has a * Karl N. Llewellyn Distinguished Service Professor, Law School and Department of Political Science, University of Chicago. 1 See http://www.utexas.edu/law/faculty/bleiter/rankings/rankings03.html significant advantage over ordinary individuals.2 If the goal is to rank baseball players, or football teams, a large set of informed people might well be asked, instead of relying on a small set. 
This claim is hard to test in the contest of rankings, simply because we lack objective evidence that would permit us to test the question whether the experts got it right or wrong. But where the claim can be tested, that is in the domain of predictive tests, a great deal of evidence supports the use of statistical aggregations of expert judgments.3 In a series of thirty comparisons, statistical groups of experts had significantly fewer errors than individual experts on forecasting tasks involving such diverse issues as company earnings, cattle and chicken prices, real and nominal GNP, survival of patients, and housing starts.4 Statistical groups of experts significantly outperformed individual experts in predicting the annual earnings of firms; changes in the American economy; and annual peek rainfall runoff in eight different countries.5 
Or consider political polling, where it has become standard practice to combine a set of poll results and to rely on the mean or median, rather than to select one or two.6 The most sophisticated treatment here involves “Polly,” a program designed to predict the 2004 election.7 Polly make her predictions after combining a large set of sources: polls, computer models, expert panels, and information markets. In eight months, she continued to predict that President Bush would receive at least 50% of the vote; her final forecast was that President Bush would receive 51.5% of the vote, a number that was stunningly close to precisely right.8 2 See James Suriewicki, The Wisdom of Crowds (2004). 3 See J. Scott Armstrong, Combining Forecasts, in Principles of Forecasting 416 (2001). 4 Id. at 428. 5 Id. at 430-31. 6 See, e.g., Sam Wang, Electoral College Meta-Analysis (2004), available at http://synapse.princeton.edu/~sam/pollcalc.html 7 See http://pollyvote.com 8 Id.: “POLLY WAS RIGHT! Who would win in November - George W. Bush or John F. Kerry? This question consumed Polly since March, when her page was launched. She heard from many sources, including 268 polls, 10 forecasting models, three surveys over as many months of a select panel of American politics experts, and the Iowa Electronic Markets (IEM). All of it she dutifully reported in her tables and graphs. As well as passing along what she found, Polly performed a simple computation, something she calls “pollynizing.” Averaging across methods, which Armstrong (2001) shows reduces forecasting error, Polly calculated the Pollyvote, the share of the two-party vote (that is, omitting third-parties) that Bush was predicted to win. Once or twice a week our parrot would post the latest value of the Pollyvote on this page and, invariably, even as Bush’s standing in the polls sank in July (see Pollygraph 2) or plummeted in the IEM in August (see Pollygraph 3), this variable showed Bush would win on election day. Not once during the past eight months did the value of the Pollyvote dip below 50 percent (see Pollygraph 4). On the morning of November 2, after pollynizing the latest information gleaned from her sources (the polls, the models, the experts’ panel, and the IEM), Polly 
If groups of experts do well for predictive judgments, they are likely to do well for judgments involving ranking as well. But no one doubts that errors and perhaps biases might well play a role in any set of assessments, including those by groups. If individual experts are likely to make mistakes, then statistical aggregations of expert judgments will be mistaken too.9 Suppose, for example, that a law school has been exceedingly good for many decades, but that its quality has sharply diminished in the last years. Some experts might fall to update their judgments; hearing the name of the school, they might rank it exceedingly high. Perhaps it would be said that such experts are not so expert. And if we define expertise to require people to have a great deal of information on the issue at hand, experts are unlikely to blunder by definition. But law school deans, law school faculty, and (even more) practicing lawyers might not know all that they need to know to rank schools correctly. 
Note in this connection that the US News and World Report also attends to other factors that do not depend at all on expert judgments. These include median LSAT (12.5% of overall score), employment rate nine months after graduation (12%), median GPA (10%), average per capita expenditures (11%, divided into 9.5% for instruction, library, and support services, and 1.5% for all else, such as utilities and financial aid), employment rate after graduation (6%), student-teacher ratio (3%), acceptance rate for students (2.5%), bar pass rate (2%), and number of volumes in the library (.75%). But this is a diverse list of factors, to say the least, and there is no assurance that the aggregation produces sense rather than nonsense. How can we be assured that a combination of expert judgments with factors of this kind lead to a sensible ranking of institutions? 
Consider another method. Perhaps law schools should be ranked through a market test, one that relies on the choices of the people directly involved: Students who apply to law schools. Such students have every incentive to make good decisions about where to enroll. They have every incentive, moreover, to identify and to care about what factors are relevant. The number of volumes in the library, and the employment rate after graduation, are likely to receive the attention they deserve, at least if numerous student choices are aggregated. For most students, the choice of law school counts among the most important decisions of their lives. They are most unlikely to decide cavalierly or foolishly. On the contrary, they are likely to acquire a great deal of information about the various options. And if students pick Yale over Harvard, or Stanford over Berkeley, or New York University over the University of Southern California, or the University of Illinois over the University of Utah, then we have strong presumptive reason to believe that their choices are correct. issued her final forecast: President Bush would take 51.5 percent of the two-party vote. According to the following Friday's New York Times, with 99.8 percent of precincts reporting, this is exactly what he received. Polly was right all along.” 9 See Cass R. Sunstein, Group Judgments, NYU L Rev (forthcoming 2005). 
Here, then, is a simple proposal: To rank law schools, information should be obtained by student choices, head-to-head, among relevant schools. The results of the resulting “tournaments” would produce an overall ranking. 
Does this idea seem exotic, or quixotic? In fact Christopher Avery and his colleagues have carried out exactly this task for colleges and universities.10 Avery et al. are particularly concerned that many currently available measures of quality, such as admissions rate and matriculation rate, can be manipulated by schools that want to act strategically to improve their rankings. A particular problem here is that many schools would prefer not to have to manipulate these factors, but the current system of ranking strongly pressures them to do so. If schools do not engage in manipulation, but their competitors do, then they will lose students – and eventually much else as well. Avery and his coauthors contend that a “revealed preference ranking,” based on actual student choices, would provide important information without also distorting admissions processes.11 Their own ranking of undergraduate programs is both plausible and interesting, with a top fifteen of Harvard, Yale, Stanford, California Institute of Technology, MIT, Princeton, Brown, Columbia, Amherst, and Dartmoth, Wellesley, the University of Pennsylvania, Notre Dame, and Swarthmore. 
For law school, a ranking of this sort would have many advantages. Most modestly, it could be taken simply as a measure of what it reflects: student preferences. So taken, it might be a complement to existing rankings, offering information that many people might want to have. Somewhat more ambitiously, a revealed preference ranking might be used as an input into aggregate rankings, taking its place alongside other factors. Most ambitiously of all, a revealed preference ranking could be seen as the most reliable one of all – as a figure that, all by itself, tells people all or almost all that they need to know to rank schools. 
Consider in this connection Friedrich Hayek’s greatest contribution to social thought, that is, his emphasis on the diffusion of information within society, and on the great difficulty of aggregating that information through any mechanism other than the price signal.12 In Hayek’s view, market choices, based on the preferences and judgments of countless consumers, reflect more information than could possibly be obtained by any central planner, even one who is both well-motivated and expert. In the same vein, student choices might be thought to produce a kind of “price,” at least in the form of rankings that reflect those choices. Perhaps revealed preferences ranking have greater accuracy than would emerge from the judgments of a group of experts. (I will raise questions about this proposition shortly.) 10 See Christopher Avery et al., A Revealed Preference Ranking of U.S. Colleges and Universities, available on ssrn.com 11 Id. at 2. 12 See F.A. Hayek, The Use of Knowledge in Society, 35 Am Econ Rev 519 (1945). 
Whether or not this is so, a revealed preference ranking would eliminate or reduce the need to consult experts, with their potential biases and their personal hobbyhorses. It would also eliminate strategic behavior on the part of law schools -- in the form, for example, of efforts to increase applications simply in order to have an apparently more selective admissions rate. Casual empiricism suggests that this form of strategic behavior is common; and no one seems to like it. Perhaps best of all, and in good Hayekian fashion, a revealed preference ranking would take advantage of the extraordinary amount of information held by the numerous students who choose where to go. It should also prove able to incorporate new information as it emerges over time. If one school has significant gains in terms of faculty, and another shows significant losses, student judgments are likely to respond to that fact. 
The simplest point is that all by itself, a revealed preference ranking would provide important information to law students. When choosing among schools, students would probably care what other students choose, and for two different reasons. That information is intrinsically valuable; it provides a clue to which schools have the most desirable student. In addition, that information provides a good heuristic for what school is best. If School A is regularly chosen over School B, there is reason to believe that School A is, in fact, better than School B. 
What might be wrong with this idea? Might errors be expected? Might a revealed preference ranking increase, rather than reduce, the distortions contained in, and produced by, current ranking systems? 
preferences simply to provide a ranking of student choices, without believing that the ranking is a reliable heuristic for quality or anything else. Perhaps applicants and others would overweight the revealed preference ranking, treating it as decisive when it should be merely informative. Students might easily think: “I like the Northwestern University Law School; all things considered, I like it much more than New York University Law School. But a strong majority of students choose the latter over the former, and so my inclination to go to Northwestern must be odd or foolish.” Thoughts of this kind may or may not lead students in good directions; some of those who follow the decisions of most people will undoubtedly err by their own lights. On the other hand, many applicants will treat a revealed preference ranking for what it is, neither more nor less, and the mere risk of overweighting is not usually a reason to fail to provide relevant information. The optimistic scenario is that a revealed preference ranking will inform choices without distorting them. 
not merely for what it shows, but as the best available evidence of real rankings. If so, oddities might emerge, and they might give us reason to doubt the rankings that emerge from student choices. From the rankings offered by Avery et al., several anomalies are immediately apparent. Notre Dame is certainly a superb college, but should it really be ranked above Swarthmore, Cornell, the University of Chicago, and the University of Michigan? Georgia Tech is excellent, but is it really better than Wesleyan, Carnegie Mellon, and Johns Hopkins? I am hardly an expert on these questions, but there is reason for doubt. It is sensible to speculate that Notre Dame has a particularly enthusiastic applicant pool, intensely drawn to the institution because of its distinctive characteristics. The same is plausibly true of Georgia Tech. If those who apply to Notre Dame and Georgia Tech are especially likely to go there if admitted, even if also admitted at (say) Swarthmore and Johns Hopkins, their choices may tell us little or nothing about relative quality. What choices may be reflecting instead is a kind of intense loyalty on the part of applicants to particular institutions. 
The same problem might materialize for law schools. Consider a few possibilities. It is conceivable that the University of Chicago Law School would be ranked below Stanford Law School, purely in terms of student choices. (I don’t have the data, but this result would not be a shocker.) Nonetheless I am firmly convinced that as wonderful as Stanford Law School is, the University of Chicago Law School is even more wonderful. I am undoubtedly biased on this count. But I predict, with some confidence. that the following law schools would be “underranked” by the market process I am describing: the University of Southern California Law School, the University of San Diego Law School, George Mason Law School, and Chicago-Kent Law School. The first of these has long had a first-rate faculty, and an excellent culture, but it probably does not draw students in a way that is quite commensurate with its overall quality. The University of San Diego Law School is known by insiders to have an extremely strong faculty, but outsiders may not have similar knowledge. George Mason and Chicago-Kent have become extremely good more recently, and I speculate that their overall quality would not show up in student choices. Why not? What is going on here? 
colleges, it bears on the particularly high rankings of Notre Dame and Georgia Tech as well). Law schools are not only giving students an education. They are also giving students at least two other things: an experience and a credential. Students might prefer a school that is slightly worse on educational grounds if it provides a much better experience. For example, schools in lovely locations might be preferred on that ground. But the credential point is even more important. For law schools as for colleges, the credential – the “signal” given to relevant others by the degree -- will matter a great deal. When choosing among institutions, students are selecting signals, not simply educations. For many choosers, the signal may loom largest of all. It follows that that students might prefer schools that are slightly worse, on educational grounds, if they provide a significantly better credential. If students choose Notre Dame over Swarthmore, it may be because Notre Dame admittees believe that relevant people, in their community, will be enthusiastic about a Notre Dame degree, and less so about a Swarthmore degree. If the University of Southern California Law School, the University of San Diego Law School, George Mason, and Chicago-Kent do worse than their actual educational quality suggests, the “credential factor” might be one reason. 
For those who emphasize revealed preferences, another complication involves tuition and scholarships (and cost-of-living as well). In deciding where to attend law school, most students engage in an informal cost-benefit analysis; and for this reason, education, experience, and credential are not all that matter. If the costs of attending one school are particularly low, they might attend that school even if the benefits are relatively low as well. For some students, a tuition-free degree from Boston College might be preferable to a fully-funded degree from Stanford. Or suppose that a student is admitted to the University of Utah but also to the University of Virginia; suppose too that the costs of the former are far lower than the costs of the latter. A choice in favor of Utah does not reflect a judgment about the quality of the two schools. 
It follows that we should expect some distortion, from the revealed preference ranking, in the direction of those schools that are most able to give their admittees an economic incentive to come. This point provides a reason to discount the resulting rankings if they are taken as a heuristic for overall quality. It even provides a reason to discount the rankings if they are taken as fully informative evidence, for individual applicants, about relevant student choices. They are choices, to be sure, but for an applicant who has a distinctive pattern of economic inducements, the choices of the average or median student may not be decisive or even terribly informative. 
on revealed preferences. In the law school context, student choices might well be an artifact, at least in part, of other ranking systems – for example, the ranking from the US News and World Report. If so, then the information provided by student choices is endogenous to ranking systems that are, by hypothesis, biased or not entirely reliable. Suppose, for example, that students make their choices partly on the basis of rankings that are produced partly as a result of manipulative, ranking-focused admissions decisions. If so, revealed preferences will not do what Avery et al. hope, which is to correct the effects of those decisions. On the contrary, they will reflect their influence. What – it might be asked – if the gain of relying on a market process that is itself infected by artificial rankings systems? Isn’t there a kind of vicious circle here? 
The objection is valid, but its importance should not be overstated. Students have access to the standard rankings, to be sure, but they have access to much other information as well. At least in principle, their decisions should incorporate relevant considerations that the official rankings omit. Many students do not go by the standard rankings. Their own knowledge about quality, and about “fit,” can lead them to choose schools that are ranked somewhat lower than others to which they have been admitted. Of course more empirical work would be helpful here; if student choices are tightly correlated with US News and World Report rankings, then those choices would seem to be less informative. But if there is some divergence, a revealed preference ranking would provide information that is, in an important respect, more valuable than the rankings that are now used. And if there is no divergence, we have reason to substitute revealed preference rankings for existing ones, with the hope that over time, student choices will not be distorted by ranking systems that we have reason to distrust. 
Of course it would be possible to object that revealed preference rankings would become a self-fulfilling prophecy, or a kind of closed circle, unable to incorporate new information. If students emphasize the choices of previous students, then how can law schools disrupt previous patterns? The answer is the same as in any market. In one year, a certain brand of sneakers is popular, and it is popular partly because it is popular; but if better sneakers appear on the market, they will eventually find their customers. Some law schools are more popular now than they were twenty years ago, and some are less so. Revealed preference rankings would not be frozen in time. 
interesting problem casts doubt on the Hayek-inspired claim that when taken in the aggregate , student choices properly pool information in a way that results in something like an efficient market. This claim is closely connected with current arguments over the efficiency of the stock market.13 For a long period, it has been argued that stock markets are “efficient,” in the sense that multiple investors produce prices that are more or less accurate. Of course there is mounting evidence that people depart from rational actor models of human behavior.14 But in the face of such evidence, many people, still convinced of the efficient capital markets hypothesis, contend that even if individual investors are prone to err, market prices have a high degree of reliability as a measure of value, at least when compared to other available measures. This contention seems implausible to many who suggest that cognitive biases and social influences can lead not merely individuals but market prices in the wrong direction.15 
Arguments of this kind might be enlisted by those who distrust revealed preferences as a measure of law school quality. The question is whether cognitive biases and social influences might lead both individual students and aggregate patterns in unfortunate directions. In this context, one bias is readily identifiable: There is reason to think that people use name recognition as a proxy, or heuristic, for quality.16 The “name recognition” heuristic surely works well most of the time, but like other heuristics, it can lead to severe and systematic errors. For example, Harvard, Notre Dame, and Duke University are undoubtedly helped to this heuristic, whereas the University of Southern California, Chicago-Kent, and George Mason are undoubtedly hurt by it. If the name recognition heuristic is widely shared, then biased results should be seen at the systemic level, and not just in some individual decisions. Of course this heuristic will not produce errors if students care about name recognition as such – if, for example, signaling is what most motivates them. But if educational quality is what most interests students, then name recognition will produce some real blunders. 
Social influences can aggravate the problem. Law school applicants do not make choices independently; they are much affected by the influences of their peers. One result 13 See Andrei Shleifer, Inefficient Markets (2002). 14 See, e.g., Richard Thaler, Quasi-Rational Economics (1993). 15 See id.; Robert Shiller, Irrational Exuberance (2001). 16 See the treatment of the “recognition heuristic” in Gerd Gigerenzer & Peter Todd, Simple heuristics that make us smart (1999). can be cascade effects, in which a number of students favor (say) New York University, and reject (say) Michigan, not because they are relying on their own independent information, but because they are responding to the signals given by the choices of other students.17 When cascades are at work, participants amplify the very signals by which they are influenced. One difficulty is that the cascade may seem far more informative than it actually is. If many students in one area are choosing NYU over Michigan, the reason may lie in the decisions of a few “early movers,” who started the cascade; other choosers may be adding no new information but simply responding to the early signal. Undoubtedly some law schools benefit from cascade effects, whereas others suffer from them. Sometimes a small “shock” can lead a large numbers of students in the direction of one choice rather than another, and not for especially good reasons. 
With respect to educational institutions, there can be fads and fashions as elsewhere. I would predict, for example, that one or another American law school can become extremely popular in a particular country (Israel, Australia, Taiwan) not because it is better than imaginable alternatives, but because it has benefited from a cascade. Within the United States, similar processes can occur, with students in some regions showing otherwise inexplicable enthusiasm for one or a few colleges and law schools. Because it is so easy to obtain relevant information, especially bad cascades are unlikely to persist. But when name recognition interacts with peer influences, revealed preferences can be “locked” into persistent patterns that do not perfectly correlate with law school quality.18 
A related problem involves the process of group polarization,19 by which likeminded people, engaged in deliberation with one another, typically end up thinking a more extreme version of what they thought before they began to talk. When group members are inclined in a certain direction, arguments that go in that direction are likely to be repeated most often, and hence to be most persuasive. It is easy to see how this process might affect student choices. Suppose that students talk with one another about their decisions; suppose too that there is a predisposition, among those able to make the choice, to rank Fordham Law School relatively low, and to rank Cardozo Law School relatively high. Arguments in favor of Cardozo will be heard more often; and peer influences will strongly favor Cardozo. Participants in a group discussion will be less likely to select Fordham if group members think that Cardozo is better, partly because of the arguments offered within the group, partly because of simple peer pressure. In sum, internal deliberations will aggravate the initial tendency not because of sensible aggregation of information, but instead through the dynamics of group interaction.20 Inferior judgments are a likely result.21 17 See Cass R. Sunstein, Why Societies Need Dissent (2003). 18 See the analogous processes discussed in Timur Kuran and Cass R. Sunstein, Availability Cascades and Risk Regulation, 51 Stan. L. Rev. 683 (1999). 19 See id. 20 See Sunstein, Group Judgments, supra note. 21 See the treatment of “hidden profiles” in Garold Stasser and William Titus, Hidden Profiles: A Brief History, 14 Psych Inquiry 304 (2003). 
If cognitive biases are interacting with social processes, large blunders should be expected.22 If, for example, name recognition is distorting judgments, group interactions should be expected to amplify, rather than to reduce, the distortion. For polarization as for cascade effects, the availability of relevant information should provide at least a partial corrective. Most students are unlikely to select a plainly inferior school, unless that school can offer inducements that make it a reasonable choice. But undoubtedly many students are affected by processes of this kind, in a way that raises legitimate doubts about any effort to rank law school by reference to market processes. 
These points should be sufficient to show that the revealed preferences of law school students are a highly imperfect proxy for law school quality. But those preferences are of considerable interest in themselves. If students generally choose the University of Illinois over the University of Iowa, that fact is relevant for other students to consider – partly because it is good to go to schools chosen by the strongest students, and partly because student choices convey relevant information. The choices of students also have some advantages over alternative rankings, such as those that stress faculty quality or a heterogeneous range of variables. Students are in a good position to give the various factors the weight that they deserve. A revealed preference ranking also reduces the incentive to manipulate relevant variables, not to improve quality, but to obtain a better ranking. 
I have not suggested, and do not believe, that revealed preference rankings would give a fully accurate account of law school quality, or even that they would be superior to real and imagined alternative ranking systems. But I do believe that they would provide valuable information and might well provide a helpful complement to other rankings systems. 22 See id.; William P. Bottom et al., Propagation of Individual Bias Through Group Judgment: Error in the Treatment of Asymmetrically Informative Signals, 25 J Risk and Uncertainty 147 (2002).                                                                 Readers with comments should address them to: Professor Cass Sunstein University of Chicago Law School 1111 East 60th Street Chicago, IL 60637 
csunstei@uchicago.edu    1.  38.  39.  40.  41.  
