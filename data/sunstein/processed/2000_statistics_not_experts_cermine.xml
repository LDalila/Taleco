<?xml version="1.0" encoding="UTF-8"?>
<article xmlns:xlink="http://www.w3.org/1999/xlink">
  <front>
    <journal-meta />
    <article-meta>
      <title-group>
        <article-title>Statistics, Not Experts</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <string-name>Coase-Sandor Working Paper Series in Law</string-name>
          <email>unbound@law.uchicago.edu</email>
          <xref ref-type="aff" rid="aff0">0</xref>
          <xref ref-type="aff" rid="aff1">1</xref>
          <xref ref-type="aff" rid="aff2">2</xref>
          <xref ref-type="aff" rid="aff3">3</xref>
          <xref ref-type="aff" rid="aff4">4</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Economics</string-name>
          <xref ref-type="aff" rid="aff0">0</xref>
          <xref ref-type="aff" rid="aff1">1</xref>
          <xref ref-type="aff" rid="aff2">2</xref>
          <xref ref-type="aff" rid="aff3">3</xref>
          <xref ref-type="aff" rid="aff4">4</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Cass R. Sunstein</string-name>
          <xref ref-type="aff" rid="aff0">0</xref>
          <xref ref-type="aff" rid="aff1">1</xref>
          <xref ref-type="aff" rid="aff2">2</xref>
          <xref ref-type="aff" rid="aff3">3</xref>
          <xref ref-type="aff" rid="aff4">4</xref>
        </contrib>
        <aff id="aff0">
          <label>0</label>
          <institution>Cass R. Sunstein &amp; William Meadow, "Statistics, Not Experts", John M. Olin Program in Law and Economics Working Paper No.</institution>
          <addr-line>109, 2000</addr-line>
        </aff>
        <aff id="aff1">
          <label>1</label>
          <institution>Cass R. Sunstein Karl N. Llewellyn Distinguished Service Professor of Jurisprudence Law School and Department of Political Science University of Chicago</institution>
          <addr-line>1111 East 60th Street Chicago, IL 60637</addr-line>
          ,
          <country country="US">USA</country>
        </aff>
        <aff id="aff2">
          <label>2</label>
          <institution>Coase-Sandor Institute for Law and Economics</institution>
        </aff>
        <aff id="aff3">
          <label>3</label>
          <institution>Dr. William L. Meadow Department of Pediatrics University of Chicago Children's Hospital C684, MC</institution>
          <addr-line>6060 5839 South Maryland Avenue Chicago, IL 60637</addr-line>
          ,
          <country country="US">USA</country>
        </aff>
        <aff id="aff4">
          <label>4</label>
          <institution>accepted for inclusion in Coase-Sandor Working Paper Series in Law and Economics by an authorized administrator of Chicago Unbound. For more information</institution>
          ,
          <addr-line>please contact</addr-line>
        </aff>
      </contrib-group>
      <abstract>
        <p>Follow this and additional works at: https://chicagounbound.uchicago.edu/law_and_economics Part of the Law Commons Recommended Citation</p>
      </abstract>
    </article-meta>
  </front>
  <body>
    <sec id="sec-1">
      <title>-</title>
      <p>JOHN M. OLIN LAW &amp; ECONOMICS WORKING PAPER NO. 109
(2D SERIES)
STATISTICS, NOT EXPERTS
William Meadow and Cass R. Sunstein
This paper can be downloaded without charge at:</p>
      <p>The Chicago Working Paper Series Index:
http://www.law.uchicago.edu/Publications/Working/index.html
The Social Science Research Network Electronic Paper Collection:
http://papers.ssrn.com/paper.taf?abstract_id=252824
Preliminary draft 11/20/00
all rights reserved</p>
    </sec>
    <sec id="sec-2">
      <title>William Meadow* and Cass R. Sunstein**</title>
      <p>The legal system should rely much more than it now does on statistical evidence. It
should be cautious about the judgments of experts, who make predictable cognitive errors. Like
everyone else, experts have a tendency to blunder about risk, a point that has been shown to hold
for doctors, whose predictions significantly err in the direction of optimism. We present new
evidence that individual doctors' judgments about the ordinary standard of care are incorrect
and excessively optimistic. We also show how this evidence bears on legal determinations of
negligence, by doctors and others.</p>
      <p>I.</p>
      <sec id="sec-2-1">
        <title>Introduction</title>
        <p>Trials frequently raise questions that call for expert intervention. Experts are asked to
answer a range of questions about what people, or professionals, ordinarily do. If a doctor is
accused of negligence, for example, it is necessary to know about the customary practice of
doctors. Of course negligence judgments depend, at least in part, on an assessment of ordinary
practice.1 But how is ordinary practice assessed?</p>
        <p>The basic answer is that the assessment comes via statements from expert witnesses,
describing the ordinary practice.2 There can be no doubt that experts know a great deal about
topics on which ordinary people lack information. But experts, no less than other people, are
subject to predictable biases.3 Their judgments about probability are affected by the same
heuristics and biases to which most people are subject, even if (and this is a disputed question)
expertise tends to reduce the most serious errors. Our basic proposal here is that the legal system
should rely, much more than it now does, on statistical data about doctors’ performance, rather
than the opinions of experts about doctors’ performance. For the first time, it is becoming
possible for law to rely on this evidence, precisely because such evidence is becoming
* Associate Professor, Pediatrics; Associate Director, Neonatology, Assistant Director, MacLean Center for Clinical
Medical Ethics, The University of Chicago
** Karl N. Llewellyn Distinguished Service Professor of Jurisprudence, Law School and Department of Political
Science, University of Chicago.
1 There is a dispute about the extent to which ordinary practice is determinative. For the classic case, see The T.J.
Hooper, 60 F.2d 737 (2d Cir. 1932).
2 We build on existing law here, without intending to enter into debates about the extent to which law should simply
incorporate, or instead sometimes improve on, the existing standard of care. On any view, the customary practice is
relevant, and that is sufficient for our purposes here.
3 See Jonathan Baron, Thinking and Deciding (2d ed 1994).
increasingly available. If our argument is convincing in the medical context, it should apply in
many other settings in which experts are asked to testify about negligence or deviations from
ordinary practices. In many settings, the fallible opinions of isolated experts should be
supplemented or replaced with statistical data. A step of this sort would dramatically increase
the sense and rationality of tort law.</p>
        <p>Our proposal rests partly on the claim that the use of statistical data will greatly simplify
litigation and reduce the role of strategic behavior in the litigation process.4 Battles of isolated
experts should be easier to mediate when there is a large pool of evidence on which to draw. But
our larger claim is that by using statistical data, the legal system will reach much more accurate
results. If the law is seeking the standard of care, it should not depend on fallible memories and
recollections of local experiences; it should draw instead on more global evidence of the kind
that is now increasingly available for use in court.</p>
        <p>II.</p>
      </sec>
      <sec id="sec-2-2">
        <title>Excessive Optimism About Risks: In General</title>
        <p>It is now well-known that most normal people tend to be risk optimists, in the sense that
they believe themselves to be relatively immune from risks that are faced by similarly situated
others.5 This is one of the most robust findings in social psychology. For example, 90% of
drivers believe that they are safer than most drivers and less likely to be involved in a serious
accident.6 Most people believe that they are distinctly unlikely to be subject to various risks,
such as cancer, heart disease, and divorce.7 Smokers appear to know the statistical risk of
smoking8; but they believe that they are less likely than most smokers to fall victim to the
various risks. In one study, less than half of smokers believed that they have a
higher-thanaverage risk of cancer or cardiovascular disease; indeed, most heavy smokers (over forty
cigarettes per day) believe that they are not at any increased risk.9</p>
        <p>Only one group of people does not show a tendency to excessive optimism: the clinically
depressed.10 It would be natural to infer that though unrealistic optimism makes for erroneous
predictions about outcomes, it is on balance adaptive, especially insofar as an optimistic attitude
increases the probability of a good outcome and otherwise creates hedonic benefits.11
4 Some of the data here is discussed, from a different angle, in William Meadow et al., Ought 'standard care' be the
'standard of care'? A study of the time to administration of antibiotics in children with meningitis, 147 Am J Dis
Child 40 (1993).
5 Neil D. Weinstein, Unrealistic Optimism About Future Life Events, 39 J. Personality &amp; Soc. Psych. 806 (1980).</p>
      </sec>
    </sec>
    <sec id="sec-3">
      <title>6 See Shelley Taylor, Positive Illusions 10 (1989).</title>
      <p>7 See Weinstein, supra note.
8 See W. Kip Viscusi, Smoking (1991).
9 John Ayanian and Paul Cleary, Perceived Risks of Cancer and Heart Disease Among Cigarette Smokers, 281
JAMA 1019 (1999).
10 See Taylor, supra note.
11 This is the thesis of id.; see also the discussion of the self-fulfilling prophecy in Nicholas Christakis, Death
Foretold: Prophecy and Prognosis in Medical Care 135-62 (2000).
Alternatively, it might be thought that some social settings would work against optimistic bias. In
markets, for example, entrepreneurs might have an incentive toward realism, especially because
market pressures, it might be expected, would punish, and drive out, those who are
unrealistically optimistic. Perhaps markets do move people toward realism; but entrepreneurs, no
less than anyone else, have been shown to suffer from unrealistic optimism.12 One might intuit
that those with specialized knowledge are less prone to this effect. This appears not to be the
case. It has almost universally been found that physicians, no less than others, have a substantial
tendency to err in predicting outcomes. Most often the errors are in the direction of optimism.</p>
      <p>"Virtually all" of the existing studies of physicians "have documented frequent and large
errors in predictions."13 No study finds a high level of accuracy. And the errors tend in a
particular direction: "physicians are prone to an optimistic bias."14 As an example, a study in
1972 showed that in making predictions about length of survival for cancer patients, only 47% of
the physicians provided prognoses that were even roughly accurate, and 80-90% of the mistakes
were excessively optimistic.15 A subsequent study, conducted in 1987, found overestimates of
survival time in 88% of the cases, by roughly a factor of three.16 A recent, large-scale study
found inaccurate predictions in 80% of cases, with 63% of these showing overestimates.17
Interestingly, patient characteristics (age, race, sex, illness duration) were not correlated with
inaccurate, optimistic estimates, but one factor was: "the better the doctor knew the patient - as
measured, for example, by the length and intensity of their contact - the more likely the doctor
was to err in the prognosis, most frequently by overestimating survival."18 The overall picture is
that in cases involving cancer patients, physicians accurately predict survival only 10% to 30%
of the time, and the rest of the time they overestimate survival by a factor of two to five.19</p>
      <p>All of this is highly suggestive, but not decisive, with respect to the particular point that
we mean to investigate here: whether experts make erroneous judgments about the ordinary
standard of medical care, and whether the errors go in a predictable direction. It would be
plausible to think that the general tendency toward optimism would affect those judgments as
well; but it would also be possible to imagine that (a) doctors’ predictions about their patients’
prospects are systematically overoptimistic, but (b) doctors have an accurate sense of what is
ordinarily done by themselves and other doctors. If there is no systematic error with respect to
(b), there would be little need to substitute data for experts. We now offer the first real evidence
on that question.
12 See K. MacCrimmon and D. Wehrung, Taking Risks (1986); Daniel Kahneman and Dan Lovallo, Timid
Choicers and Bold Forecasts: A Cognitive Perspective on Risk Taking, 39 Management Science 17 (1993).
13 See Christakis, supra note, at 66.
14 Id.
15 Id. at 66.
16 Id. at 66-67.
17 Id. at 67.
18 Id. at 67-68.
19 Id. at 68.</p>
      <sec id="sec-3-1">
        <title>III. Standard of Care: Experts vs. Data</title>
        <p>We now summarize some of our own evidence that doctors err with respect to ordinary
conduct in their own fields. As expected, the apparent basis for the magnitude and direction of
these errors is optimistic bias.</p>
      </sec>
      <sec id="sec-3-2">
        <title>A. Bacterial Meningitis</title>
        <p>Consider the problem of bacterial meningitis in children. For the purposes of the current
discussion, it is necessary to know only three things about that problem. Bacterial meningitis is
an infection of the brain; it can (usually) be treated with antibiotics; as a general rule, the sooner
the antibiotics are started, the better the outcome will be.</p>
        <p>In medical malpractice cases, the question often arises whether treatment of a particular
child was unduly "delayed." Of course a key issue is how a legal decision-maker would know
what kind of delay counts as "undue." For obvious reasons, juries are assumed to be ignorant of
the medical facts in such cases, and consequently are instructed to rely on the testimony of expert
witnesses. The instruction that jurors should rely on expert opinions for their knowledge about
the standard of medical care answers one question, but it raises another -- on what should the
experts themselves rely?</p>
        <p>In the traditional formulation, experts themselves are instructed to rely on their personal
"knowledge and training." But for reasons given above, such admonitions raise many problems.
In the context at hand, it would be reasonable to predict that experts would tend to be excessively
optimistic – that their recollections of the time that antibiotics are initiated in children with
meningitis would be tilted toward shorter values than actually occurred. And if so, experts,
relying on their "knowledge and training," will provide inaccurate accounts of behavior.</p>
        <p>We tested this hypothesis directly, by surveying two groups of pediatric experts about
their estimates of timing of antibiotic administration in children who present with bacterial
meningitis (ABTIME), and subsequently comparing these estimates to actual data obtained from
real cases of meningitis. We began by identifying 54 experts in pediatric emergency medicine
who were attending a national meeting of their subspecialty (most children with bacterial
meningitis present to the emergency room, where they receive their initial medical care). We
asked each of these experts to give us their estimate of the average time that would elapse from
arrival of a child with meningitis in their emergency room to the start of antibiotic therapy for
that child. The average estimate of these experts was 56 minutes -- 95% of these experts'
opinions fell within the range between 20 - 120 minutes.</p>
        <p>We then posed precisely the same question to another group, consisting of 23 experts in
the field of pediatric infectious diseases (the subspecialty with most experience in the care and
treatment of meningitis for children after admission to the hospital). The average estimate of
ABTIME offered by pediatric infectious disease experts was 87 minutes, slightly more than 50%
longer than the intuitions of the experts in emergency medicine.</p>
        <p>Moreover, and most importantly, the opinions of both of these groups of potential expert
witnesses were wrong, and wrong by a large margin. We compared both of these distributions of
expert opinion to actual times, determined on the basis of a review of 93 cases of children with
meningitis seen at two university medical centers in Chicago. The average value of ABTIME for
these 93 children was 120 minutes, with 95% of cases falling in the interval between 30 to 240
minutes. When the medical literature was reviewed in an attempt to extend these observations of
actual ABTIME, 200 reported cases of children with meningitis were found from hospitals in
South Carolina and California, with an average time comparable to those observed at our
Chicago hospitals (median 114-126 minutes). The following table captures the differences:</p>
      </sec>
      <sec id="sec-3-3">
        <title>Variations between Expert Opinions and Data-based Observations for ABTIME</title>
      </sec>
      <sec id="sec-3-4">
        <title>ID Experts</title>
        <p>56 minutes</p>
      </sec>
      <sec id="sec-3-5">
        <title>ER Experts</title>
      </sec>
      <sec id="sec-3-6">
        <title>Chicago Hospitals SC and CA Hospitals 87 minutes 120 Minutes 120 minutes</title>
        <p>Whatever "expert" opinion means in this context, it does not mean an accurate opinion.
Both groups of potential pediatric experts were simply wrong in their opinions about the time to
begin antibiotics in children with meningitis. The 77 expert estimates of ABTIME were
significantly shorter than the 293 actual determinations of ABTIME to a probability of less than
1 part in 1000. In addition, the inaccuracy noted in the estimates was consistent with our
hypothesis: responses were biased toward the outcome perceived to be 'desired', that is, shorter
waiting periods.</p>
        <p>Others, in varying contexts, have demonstrated a similar disconnect between physicians'
description of their own practices and "objective" determinations of the same behavior. As
examples, consider patient education by physicians in general practice. Physicians are
committed to the value of patient education,20 and spend up to 25% of their office time
counseling patients.21 Clearly, the desired practice is to spend this time explicitly informing
patients about health-related behaviors, such as the uncontested virtues of smoking cessation and
preventive oncology. Nevertheless, when physicians' reports of their office practices are
compared with taped interviews of these same encounters, there is only a weak, insignificant
correlation between the self-reported patient education activities and the actual performance .22
Not surprisingly, physician's recollection err in the predicted direction; that is more explicit
counseling is recalled than actually occurred.
20 H Wechsler et al., The physician's role in health promotion -- a survey of primary-care practitioners., 308 N Engl
J Med. 97 (1983); T.E. Kottke et al., Attributes of successful smoking cessation interventions in medical practice. A
meta-analysis of 39 controlled trials, 259 JAMA 2883 (1988).
21 C.T. Orleans et al., Health promotion in primary care: a survey of US family practitioners, 14 Prev Med 636
(1985).
22 See id; see also J.E. Davis et al, Cancer prevention and screening activities in primary care practice, 16 Prev
Med. 277 (1987).</p>
        <p>Once again, our fears are confirmed --- we expect anecdotal recall to be inaccurate,
biased, and inevitably, systematically imperfect, and we find it to be so. If, as the psychological
evidence suggests, our findings here are the rule and not the exception, it is hardly clear that the
law should continue to base a system of medical-legal jurisprudence on such a shaky foundation.</p>
      </sec>
      <sec id="sec-3-7">
        <title>B. The Proposal</title>
        <p>Of course the evidence that we have offered here is suggestive rather than exhaustive.
Perhaps there are contexts in which doctors, or other experts, have an accurate sense of the
ordinary standard of care. But we believe that general evidence of error, together with the
particular evidence introduced here, is sufficient to show that both error and optimism are highly
likely to infect a wide range of expert testimony.23 This point operates independently of the
ordinary incentives, in an adversary system, to assist one’s own side, whatever it may be.24 It
therefore makes sense to move toward greater reliance on data, and less reliance on the
recollections of isolated experts, certainly in the context of malpractice suits, and probably more
generally. Until recently, the legal system has been unable to rely on statistical data for the
simple reason that it has not existed. But it is increasingly common to develop data sets about
physician choices and behavior, and the legal system will have an increasingly large amount of
information on which to draw.</p>
        <p>Our proposal would have two large advantages. First, it would simply the task of
discerning the truth. It is now exceedingly difficult for juries to decide whether one or another
expert has adequately captured general experience. This issue may well turn on unreliable
judgments about credibility or on sympathy, not relevant to the standard of care issue, for one or
another side. Second, and more important, use of statistical evidence would inevitably increase
accuracy, reflecting the comparative advantage of pooled data over individual recollections.
There is no good reason for courts to continue to rely on the latter when they have access to the
former.</p>
        <p>III.</p>
      </sec>
      <sec id="sec-3-8">
        <title>Problems and Counter-arguments</title>
      </sec>
      <sec id="sec-3-9">
        <title>A. Statistical Error?</title>
        <p>Our proposal might be criticized on the ground that statistical analysis is itself subject to
error. Of course there can be no assurance that any particular account of relevant data is correct.
If this is so, it might be objected, the use of statistical evidence will merely be the beginning of a
continuing battle among imperfectly reliable experts. How much would be gained by that?</p>
        <p>This objection has a degree of truth, but rather than impeaching our proposal, it suggests
the need to develop good methods for evaluating any particular claim about what the data
establishes. There is no escaping tests of the data in the ordinary way, through the presentation of
conflicting views, including that of experts. Data might be met with data; it might also be met
with a professional critique. And of course it is possible that an individual expert will be able to
23 See Kahneman and Lovallo, supra note.
24 See Linda Babcock, Self-Serving Bias, in Behavioral Law and Economics (Cass R. Sunstein ed. 2000).
show, persuasively, that data does not establish what it claims. These sorts of disputes can be
handled in the standard fashion. What we are suggesting here is that because individual experts
are distinctly prone to error, it would be far better to begin the process with reliable evidence
rather than particular recollection. If individual experts can show that the statistical data are
wrong, the legal system will be better off for the demonstration. But in the long run, we predict,
these demonstrations will be the exception rather than the rule.</p>
      </sec>
      <sec id="sec-3-10">
        <title>B. Admissibility: Relevance and Hearsay</title>
        <p>Statistical data are not typically used in negligence cases. In fact the legal system is
uncomfortable with the use of such data. They might be excluded, as inadmissible, on several
grounds.</p>
        <p>It is possible to object that statistical data are not relevant, because they cannot determine
the proper standard of care. In many states, there is a continuing dispute about the proper role of
common practice in negligence cases. Where common practice is not determinative of the
standard of care,25 the data cannot resolve the legal issue. Some courts have suggested that in
such circumstances, the data should be inadmissible on grounds of irrelevance. But this is a
mistake. Even when common practice is not determinative, it is pertinent, as reflected by the
very fact that experts frequently are allowed to testify about what most doctors do. Statistical
data should be found admissible for the same reason that expert testimony is admissible.</p>
        <p>It might also be thought that statistical data should be seen as hearsay. Perhaps the
witness seeking to introduce hearsay evidence is attempting to say what others have said, and
perhaps these violate the prohibition on admitting materials of that sort.26 But this is a
misreading of the hearsay rules. Under federal law, experts are allowed to rely on “statements
contained in published treatises, periodicals, or pamphlets on a subject of history, medicine, or
other science or art, established by the testimony or admission of the witness or by other
testimony or by judicial notice.”27 Federal rules also allow experts to base opinions on facts and
data of the kind reasonably used by experts in the field when they form opinions or inferences.28
Statistical data, if published, should be permitted under the first rule just quoted; if unpublished,
they should be admissible to the extent that they are of the sort reasonably relied on by experts.
The conclusion is that data should not be treated as inadmissible hearsay.</p>
      </sec>
      <sec id="sec-3-11">
        <title>C. Textbooks?</title>
        <p>Perhaps there are alternatives, other than reliance on statistical data, to the problem of
idiosyncratic recollections of individual experts. One possible view is that the opinions of
25 See, e.g., Advincula v. United Blood Services, 678 Need 1009 (1996); Darling v. Charleston Community
Memorial Hosp., 211 NE2d 253 (1965); Roach v. Springfield Clinic, 1992 Ill. LEXIS 204 (1992).
26 See Schrag v. Chicago City, 265 Ill. 338 *1914); People v., Anderson, 113 Il 2d 1 (1986).
27 Fed. R. Evid. 803(18).
28 Fed. R. Evid. 703, 705. But see the puzzling ruling in Roach v. Springfield Clinic, 1992 Ill. LEXIS 204 (1992),
refusing to allow references to treatises on the ground that their authors are not available for cross-examination. This
seems to us an anachronistic and odd reading of the prohibition on hearsay.
experts, as codified in textbooks or journal publications, should define the standard of care . By
this view, when actual practices differ from the theoretical or recommended standards, it is the
practitioners who are at fault. They may not know the standard, may disagree with it, or have
misinterpreted it, but the burden of proof falls on the practitioners.</p>
        <p>We believe that this approach is oversimplified and misguided. Under the law, the
ordinary practice is important for its own sake, whether or not it is decisive. Unless a textbook is
actually based on ordinary practice, it will not report it reliably. If courts are going to consider
the ordinary practice, they should have an accurate understanding of what it is.</p>
        <p>Perhaps the textbook or journal can state a recommendation, one that is worth
considering even if it does not capture ordinary practice. This is not implausible. But
interpretation of the recommended standards (as expressed, for example, for AB-TIME in
authoritative texts) is not always straightforward, and one can rarely find authoritative
interpretations of authoritative texts. In the particular case of bacterial meningitis, what, in the
minds of a lay jury, is a reasonable interpretation of textbook recommendations that antibiotics
should be administered “promptly,”29 “immediately,”30 or “at the earliest possible time”31 to
children with suspected meningitis. In such a context is 30 minutes too long, - - is one hour, six
hours? The recommendation in one text that antibiotics be administered “within 30 minutes after
the diagnosis of meningitis is established”32 does little to resolve the question, as the issue
promptly arises - - when exactly is the diagnosis of meningitis established? If the question is
what do doctors do, it would be far better to rely on actual data about physician performance.</p>
      </sec>
      <sec id="sec-3-12">
        <title>Conclusion</title>
        <p>We have suggested here that there is general reason to believe that expert judgments
about the standard of medical care will be erroneous, and that the errors will run in a predictable
direction. The usual reason is optimistic bias, as people tend to believe that things can be done
more easily, more rapidly, and more successfully than the evidence suggests. To establish this
claim, we have drawn on existing evidence, highly suggestive on this point, and more particular
evidence, presented here, of mistaken reports with respect to standard of care.</p>
        <p>Our principal innovation has been to suggest that in light of the evident mismatch
between expert recollections and empirical reality, the legal system should rely, wherever it can,
not on the former but on statistical evidence of the latter. The best reason for reliance on
individual recollections has been an absence of statistical evidence; but this is a gap that is
rapidly being filled, and that is likely, in the next generation, to be replaced with a great deal of
reliable information. Our emphasis has been on the question of medical malpractice, but the
general implication is far broader: In any case in which a disputed question calls for expert
testimony about ordinary practice, it is hazardous to rely on what particular experts recall, and
far more sensible to make the outcome turn on statistical evidence, at least if the goal is accuracy
in adjudication.
29 See R.D. Feign et al., Textbook on Pediatric Infectious Disease (Saunders 1981).
30 See W.E. Nelson et al., Textbook of Pediatrics (13d ed. 1987).
31 A.M. Rudolph, Pediatrics (1977).
32 See G.L. Mandell et al., Principles and Practice of Infectious Diseases (1979).
Readers with comments should address them to:
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.</p>
        <p>Chicago Working Papers in Law and Economics</p>
        <p>(Second Series)
91.
92.
93.
97.
98.
99.
100.
101.
102.
103.
104.
105.
106.
107.
108.</p>
        <p>Richard A. Posner, Orwell versus Huxley: Economics, Technology, Privacy, and Satire
(November 1999)
David A. Weisbach, Should the Tax Law Require Current Accrual of Interest on Derivative
Financial Instruments? (December 1999)
Cass R. Sunstein, The Law of Group Polarization (December 1999)
Eric A. Posner, Agency Models in Law and Economics (January 2000)
Karen Eggleston, Eric A. Posner, and Richard Zeckhauser, Simplicity and Complexity in
Contracts (January 2000)
Douglas G. Baird and Robert K. Rasmussen, Boyd’s Legacy and Blackstone’s Ghost (February
2000)
David Schkade, Cass R. Sunstein, Daniel Kahneman, Deliberating about Dollars: The Severity
Shift (February 2000)
Richard A. Posner and Eric B. Rasmusen, Creating and Enforcing Norms, with Special Reference
to Sanctions (March 2000)
Douglas Lichtman, Property Rights in Emerging Platform Technologies (April 2000)
Cass R. Sunstein and Edna Ullmann-Margalit, Solidarity in Consumption (May 2000)
David A. Weisbach, An Economic Analysis of Anti-Tax Avoidance Laws (May 2000)
Cass R. Sunstein, Human Behavior and the Law of Work (June 2000)
William M. Landes and Richard A. Posner, Harmless Error (June 2000)
Robert H. Frank and Cass R. Sunstein, Cost-Benefit Analysis and Relative Position (August 2000)
Eric A. Posner, Law and the Emotions (September 2000)
Cass R. Sunstein, Cost-Benefit Default Principles (October 2000)
Jack Goldsmith and Alan Sykes, The Dormant Commerce Clause and the Internet (November
2000)
Richard A. Posner, Antitrust in the New Economy (November 2000)
Douglas Lichtman, Scott Baker, and Kate Kraus, Strategic Disclosure in the Patent System
(November 2000)
Jack L. Goldsmith and Eric A. Posner, Moral and Legal Rhetoric in International Relations: A
Rational Choice Perspective (November 2000)
William Meadow and Cass R. Sunstein, Statistics, Not Experts (December 2000)</p>
      </sec>
    </sec>
  </body>
  <back>
    <ref-list />
  </back>
</article>

