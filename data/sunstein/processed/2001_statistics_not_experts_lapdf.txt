University of Chicago Law School ersity
Cass R. Sunstein William Meadow
CASS R. SUNSTEINtt
INTRODUCTION Legal proceedings, both judicial and administrative, frequently raise questions about what people, including experts, ordinarily do.' If
DUKE LAW JOURNAL
a doctor is accused of negligence, for example, it is necessary to know about the customary practice of doctors.2 Of course, negligence judgments depend, at least in part, on an assessment of ordinary practice.' But how is ordinary practice by professionals or others to be assessed?
In trials and on appeal, the basic answer is that the assessment comes via statements from expert witnesses who describe the ordinary practice.4 There can be no doubt that experts know a great deal about topics on which ordinary people lack information. But experts, no less than other people, are subject to predictable biases.' Their judgments about risk are affected by the same heuristics and biases to
STATISTICS, NOT EXPERTS
which most people are subject, even if (and this is a disputed question) expertise tends to reduce the most serious errors.6
To correct the resulting problems, we offer a simple proposal: The legal system should rely, whenever it can and far more than it now does, on statistical data about doctors' performance rather than on the opinions of experts about doctors' performance. For the first time, it is becoming possible for law to rely on this evidence, for the simple reason that it is becoming increasingly available. If our argument is convincing in the medical context, it should apply in many other settings in which experts are asked to testify about negligence or deviations from ordinary practices. In many settings, the fallible opinions of isolated experts should be supplemented or replaced by statistical data. Those opinions should be seen as a kind of crude second-best, far inferior to the data that it approximates. A step toward reliance on such data would dramatically increase the sense and rationality of tort law, and perhaps other areas of law as well.
We are hopeful that the use of statistical data will simplify some of the central issues in litigation and reduce the role of strategic behavior in the litigation process. Battles between isolated experts will be easier to mediate when there is a large pool of evidence on which to draw. But .our larger claim is that by using statistical data, the legal system will reach far more accurate results. If the law is seeking to determine the standard of care, it should not depend on fallible memories and recollections of local experiences. It should draw instead on more global evidence of the kind that is increasingly available for use in court and elsewhere.
DUKE LAW JOURNAL
A. In General It is now well known that most normal people tend to be "risk optimists," in the particular sense that they believe themselves to be relatively insulated from risks that are faced by others who are similarly situated.! This is one of the most robust findings in cognitive psychology. For example, ninety percent of drivers believe that they are safer than most drivers and less likely to be involved in a serious accident.9 Most people believe that they are distinctly unlikely to be subject to various risks, such as cancer, heart disease, and divorce.'0 In some important domains, people appear not only to believe themselves less likely than others to fall prey to certain risks, but also to underestimate the actual risk to which they are subject." With respect to automobile accidents, people believe that the danger they face is less than it is as a matter of statistical fact. Smokers appear to know
STATISTICS, NOT EXPERTS
the statistical risks of smoking, but they believe that they are less likely than most smokers to fall victim to the various risks." In one study, less than half of smokers surveyed believed that they had a higher-than-average risk of cancer or cardiovascular disease indeed, most heavy smokers (at least forty cigarettes per day) believed that they were not at any increased risky Only one group of people does not show a tendency to excessive optimism: the clinically depressed.6
It would be natural to infer that unrealistic optimism, in many cases, is adaptive. Though such optimism may cause false predictions of outcomes, an optimistic attitude may increase the probability of a good outcome and otherwise create hedonic benefits." This may be particularly true in the medical context perhaps an excessively optimistic doctor is more likely to get good results. If so, optimism is well adapted to the social role of a doctor. It also might be thought that some social settings work against optimistic bias. In markets, for example, entrepreneurs might have an incentive toward realism, especially because market pressures-it might be expected-would punish and drive out those who are unrealistically optimistic. But even if markets move people toward realism, entrepreneurs, no less than anyone else, have been shown to suffer from unrealistic optimism." It is noteworthy that expert witnesses are not disciplined by ordinary market pressures.' If experts err, they might even help their clients,
DUKE LAW JOURNAL and hence excessive optimism might well be favored, rather than disfavored, by whatever market pressures exist in this domain. B. Optimism and Pessimism in Clinical Practice
It seems reasonable to speculate that those with specialized knowledge are far less prone to optimistic bias and to other motivational and cognitive errors. Unfortunately, this appears not to be the case. It has been found almost universally that physicians, no less than other people, are prone to make cognitive mistakes and indeed have a substantial tendency to err in predicting outcomes. "Virtually all" of the existing studies of physicians "have documented frequent and large errors in predictions., No study finds a high level of accuracy. The errors tend in a particular direction: "physicians are prone to an optimistic bias.'' 2
As an example, consider a study published in 1972 that showed that in making predictions about the length of survival for cancer patients, only 47% of the physicians provided prognoses that were even roughly accurate and that 80-90% of the mistakes were excessively optimistic. A subsequent study published in 1987 found overestimates of survival time in 88% of the cases, by roughly a factor of three.24 A recent, large-scale study found inaccurate predictions in 80% of cases, with 63% of these showing overestimates." Interestingly, patient characteristics (age, race, sex, illness duration) were not correlated with inaccurate, optimistic estimates, but one factor was:
[Vol. 51:629
STATISTICS, NOT EXPERTS
"the better the doctor knew the patient-as measured, for example, by the length and intensity of their contact-the more likely the doctor was to err in the prognosis, most frequently by overestimating survival.' 6 The overall picture is that in cases involving cancer patients, physicians accurately predict survival only 10-30% of the time, and the rest of the time they overestimate survival by a factor of two to five.2
All of this is highly suggestive but not decisive with respect to the particular point that we mean to urge here: that experts make erroneous judgments about the ordinary standard of medical care, and that they tend to err in a particular direction. Because expert witnesses in medical malpractice cases typically are doctors, it would be plausible to think that their general tendency toward optimism with their patients would affect their testimony as experts, causing them to overstate systematically the efficiency and effectiveness of ordinary practice. At the same time, it also would be possible to imagine that (a) doctors' predictions about their patients' prospects are systematically overoptimistic, but (b) doctors have an accurate sense of what is ordinarily done by themselves and other doctors. If there is no systematic error with respect to (b), there would be little need to substitute statistical data for the expert testimony of doctors. We now offer the first real evidence in the legal literature on the central question, showing that medical experts systematically err.2
DUKE LAW JOURNAL
A. Bacterial Meningitis Consider the problem of bacterial meningitis in children. For the purposes of the current discussion, it is necessary to know only three things about this disease and its treatment. Bacterial meningitis is an infection of the brain it (usually) can be treated with antibiotics and, as a general rule, the sooner the antibiotics are administered, the better the outcome will be.'
In medical malpractice cases, the question often arises whether treatment of a particular child was unduly "delayed,"' because delay can cause serious harm.' Of course a key issue is how a legal decisionmaker would know what kind of delay counts as "undue." For
STATISTICS, NOT EXPERTS obvious reasons, juries are assumed to be ignorant of the general medical facts in such cases, and consequently are instructed to rely on the testimony of expert witnesses. The instruction that jurors should rely on expert opinions for their knowledge about the standard of medical care answers one question, but it raises another-on what should the experts themselves rely?
In the traditional formulation, experts are instructed to rely on their personal "knowledge and training." For the reasons stated, however, such admonitions raise many problems. In the context of bacterial meningitis, for example, it would be reasonable to predict that experts would tend to be excessively optimistic-that their recollections of the amount of time that elapsed before antibiotics were initiated in children with meningitis would be tilted toward shorter durations than actually occurred. And if this is the case, these experts, relying on their "knowledge and training," will provide inaccurate accounts of behavior.
We attempted to test this hypothesis by surveying doctors about their experiences in cases of childhood meningitis and comparing these survey responses to detailed descriptions of physicians' behaviors in actual cases of meningitis.3 We began by identifying doctors with expertise in the two most relevant subspecialties of pediatrics, pediatric emergency room medicine (ER) and pediatric infectious diseases (ID). These are the fields from which potential experts most likely would be drawn to testify in lawsuits about deviations from the standard of care in meningitis cases. We presented these doctors with a questionnaire about a child who was brought to the emergency room with symptoms suggestive of bacterial meningitis. We asked how long, in their opinion, it would take to administer antibiotics in 2001]
DUKE LAW JOURNAL such cases in their own hospitals. We then compared the responses of these doctors to the actual time to antibiotic administration (ABTIME) observed in ninety-three children treated at two large university-associated pediatric centers in Chicago.
We found, as we had hypothesized, that the recollections of the potential expert witnesses were wrong. Moreover, they were wrong in the anticipated direction-that is, biased in favor of the response that, in retrospect, would be desired (in this case, toward a shorter ABTIME). The median estimate of the fifty-four ER specialists was 46 minutes the median estimate of the twenty-three ID specialists was 80 minutes. By contrast, the median ABTIME of ninety-three actual cases at the two university centers was 120 minutes, significantly longer than the estimates of either of the sub-specialty groups." When we reviewed the medical literature in an attempt to extend these observations beyond Chicago, we found two hundred reported cases of children with meningitis from hospitals in South Carolina" and California. These two hundred cases had a median ABTIME (114-126 minutes) comparable to the ninety-three Chicago cases, and longer than the estimates of either of the two sub-specialties.
[Vol. 51:629
STATISTICS, NOT EXPERTS
Table 1 captures the differences between the opinions of potential experts and the actual data-based observations:
ABTIME CHILDREN WITH SUSPECTED BACTERIAL MENINGITIS
Chicago Hospitals
120 Minutes
Whatever "expert" opinion means in this context, it does not mean an accurate opinion. Both groups of potential pediatric experts were wrong, and substantially so, in their opinions about the amount of time that elapses before doctors administer antibiotics to children with meningitis. The statistical probability that the potential experts were correct was less than one part in 100 for the ID doctors, and less than one part in 1000 for the ER doctors. As we have emphasized, the inaccuracy found in the estimates was consistent with our hypothesis: responses were biased toward the outcome perceived to be desired, that is, toward shorter waiting periods.
Note that this bias operates independently of the pressures imposed by the adversary system, which would naturally lead litigants to hire experts whose accounts will favor their side. The ID and ER respondents were not asked to consider their responses in the context of a review for either a plaintiff or defendant in this survey. Moreover, the optimistic bias presented by the potential experts here is operating outside of the bounds of the usual doctor-patient relationship, where it has been suggested that optimism, even if inaccurate, may serve a therapeutic purpose.'
ID Experts 80 minutes*
ER Experts
46 minutes**
SC and CA Hospitals
114-126 minutes
DUKE LAW JOURNAL
Others, in varying contexts, have demonstrated a similar disconnect between physicians' descriptions of their own practices and objective determinations of the same behavior. As examples, consider patient education by physicians in general practice. Physicians are committed to the value of patient education, and they devote considerable time and resources to health promotion." Clearly, the desired practice is to spend this time explicitly informing patients about health-related behaviors, such as the uncontested virtues of smoking cessation and preventive oncology. Nevertheless, when physicians' reports of their office practices are compared with taped interviews of these same encounters, there is only a weak, insignificant correlation between the self-reported patient education activities and the actual performance." Not surprisingly, physicians' recollections err in the predicted direction that is, more explicit counseling is recalled than actually occurred.
Here too the cognitive expectation is confirmed-anecdotal recall proves to be inaccurate, biased, and, inevitably, systematically imperfect. If our findings here are the rule and not the exception, as the psychological evidence suggests, it is hardly clear that the law should continue to base a system of medical-legal jurisprudence on such a shaky foundation.
STATISTICS, NOT EXPERTS
B. The Proposal We acknowledge that the evidence we have offered is suggestive rather than demonstrative. Perhaps there are contexts in which doctors, or other experts, have an accurate sense of the ordinary standard of care. But the general evidence of mistakes, together with the empirical evidence introduced here, is sufficient to show that both error and optimism are highly likely to infect a wide range of expert testimony. This point operates independently of the ordinary incentive to assist one's side in an adversary system."
It therefore makes sense, in the context of malpractice suits and probably more generally as well, to move toward greater reliance on actual data and less reliance on the recollections of isolated experts. Until recently, the legal system was unable to rely on statistical data for the simple reason that it did not exist. But it is now increasingly possible to develop data sets about physician choices and behavior, and the legal system will have an increasing amount of information on which to draw. Perhaps government should generate accounts of ordinary practice, based on the information that is available or perhaps people in the private sector should do so. In either case, the goal would be to ensure the compilation of actual data about practice, substituting accurate empirical evidence for.the fallible judgments of individuals.
Our proposal would have two notable advantages. First, it could simplify the task of discerning the truth. It is now exceedingly difficult for juries to decide which expert has best described the prevailing standard of medical care in the community. Under current practice, this issue may well turn on unreliable judgments about credibility or on sympathy that is unrelated to the standard of care. Second, and more important, use of statistical evidence would increase the accuracy of assessments of the standard of care, reflecting the comparative advantage of pooled data over individual recollections. There is no good reason for courts to continue to rely on the latter when they have access to the former.
DUKE LAW JOURNAL
A. Statistical Error? Our proposal might be criticized on the ground that statistical analysis is itself subject to error. Of course there can be no assurance that any particular compilation or interpretation of data is correct. Perhaps there will be several accounts of data that will compete with one another. If this is so, it might be objected, the use of statistical evidence will merely usher in a new phase in the battle between imperfect experts. How much would be gained by that?
This objection has a degree of plausibility, but rather than impeaching our proposal, it suggests the need to develop good methods to evaluate particular claims about the meaning of statistical data. In the event of disagreement, there is no escaping the need to test the data in the ordinary way through the presentation of conflicting views, including those of experts. Data might be met with contrary data it also might be met with a professional critique. And of course it is possible that an individual expert will be able to show persuasively that some purported evidence does not establish what it claims to establish. These sorts of disputes can be handled in the standard fashion. We suggest that because individual experts are prone to systematic error, it would be far better to begin the process with reliable evidence rather than with individual recollection. If individual experts can show that the statistical data are wrong, the legal system will be better off for the demonstration. In the long run, however, we predict that these demonstrations will be the exception rather than the rule. In any case, an inquiry into the evidence will anchor the legal process in something that is likely to be more reliable. B. Admissibility: Relevance and Hearsay
Statistical data are not typically used in negligence cases. The data might be excluded as inadmissible on several grounds. Most important, questions might be raised about whether the data are relevant and about whether they are hearsay.
It is possible to object that statistical data are not relevant, because they cannot determine the proper standard of care. In some states, there is a continuing dispute about the proper role of common
STATISTICS, NOT EXPERTS practice in negligence cases.' Where common practice is not determinative of the standard of care' statistical data cannot resolve the legal issue. It would be possible to argue that in such circumstances, the data should be inadmissible on grounds of irrelevance. But this argument should be rejected. Even when common practice is not determinative, it is pertinent, and its relevance is reflected by the very fact that experts frequently are allowed to testify about what most doctors do. Statistical data should be found admissible for the same reason that expert testimony is admissible.
It also might be thought that statistical data should be seen as hearsay. Perhaps the witness seeking to introduce the data is attempting to repeat what others have said in their reports of the data. If so, his testimony might be thought to violate the prohibition on hearsay." But this analysis would misread the hearsay rules. Under federal law, for example, experts are allowed to rely on "statements contained in published treatises, periodicals, or pamphlets on a subject of history, medicine, or other science or art, established as a reliable authority by the testimony or admission of the witness or by other expert testimony or by judicial notice."' Federal rules also allow experts to base opinions on facts and data of the kind reasonably used by experts in the field when they form opinions or inferences." Statistical data, if published, should be permitted under the first rule just quoted if unpublished, they should be admissible to the extent that they are of the sort reasonably relied on by experts. Under either scenario, however, data should not be treated as inadmissible hearsay. 20011
DUKE LAW JOURNAL
C. Textbooks and Treatises Perhaps there are other solutions, besides reliance on statistical data, to the problem of idiosyncratic recollections of individual experts. It might be urged that the opinions of experts, as reported in textbooks or journal publications, should define the standard of care. By this view, when actual practices differ from the theoretical or recommended standards, the practitioners are at fault. They may not know the standard, may disagree with it, or may have misinterpreted it, but the burden of proof falls on them.
We believe that this approach would be oversimplified and misguided. Under the law, ordinary practice is important for its own sake, regardless of whether it is decisive of the legally acceptable standard of care. If courts are going to consider the ordinary practice, they should have an accurate understanding of what it is here, textbook and treatise authors, like all other experts, are prone to error. Unless a textbook or treatise actually draws its conclusions from ordinary practice, it will not report the ordinary practice reliably.
Perhaps textbooks and journals merely state recommendations that are worth consideration even if they do not capture ordinary practice. This is not implausible. But interpreting these recommended standards is not always straightforward. In the particular context of bacterial meningitis, textbooks recommend that antibiotics should be administered "early,"49 "promptly,"' or "immediately"" to children with suspected meningitis. But what, in the minds of a lay jury, is a reasonable interpretation of this recommendation? In such a context, is thirty minutes too long? Is one hour, or six hours? The recommendation in one text that antibiotics be administered "within 30 minutes after the diagnosis of meningitis is established"52 helps a good deal, but it does not resolve the question, as it merely presents another troubling question-when exactly is the "diagnosis of meningitis established?"
But there is a more general point. At most, the author of a textbook can offer an opinion about the standard practice or about the
STATISTICS, NOT EXPERTS
practice that is desirable. If the opinion involves what is desirable, it should be evaluated as such. But if the opinion involves what is standard, the question should be whether the opinion is correct. To the extent that the question inquires into ordinary practice-into what doctors do-it would be far better to rely on actual data about physician performance.
Our plea here has been for attention to data, rather than to opinions. Ultimately we might hope for authoritative texts on what is established by the data, and such texts might come from the government or from the private sector. To the extent that there is competition among sources of data, all the better. And to the extent that data about actual practice are available, the data should inform, and improve, the debate about the extent to which the legal system should be bound by the ordinary standard of care when determining the appropriate standard of care.'
CONCLUSION It is well known that ordinary people make systematic errors in assessing probabilities and risks.' It is less well known that experts are subject to the same biases and likely to make the same mistakes." We have suggested here that there is general reason to believe that expert judgments about the standard of medical care will be erroneous, and that the errors will run in a predictable direction because of optimistic bias. People tend to believe that things can be done, and are in fact done, more easily, more rapidly, and more successfully than the evidence suggests. To establish this claim, we have drawn on existing evidence, highly suggestive on this point, and on more particular evidence, presented here, that experts do not accurately report the ordinary standard of care.
Our principal innovation has been to suggest that in light of the evident divergence between expert beliefs and empirical reality, the legal system should rely, whenever possible, not on the former but on
