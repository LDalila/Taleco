University of Chicago Law School
Chicago Unbound

Coase-Sandor Working Paper Series in Law and
Economics

2002

Hazardous Heuristics
Cass R. Sunstein

Coase-Sandor Institute for Law and Economics

Follow this and additional works at: https://chicagounbound.uchicago.edu/law_and_economics

Part of the Law Commons

Recommended Citation
Cass R. Sunstein, "Hazardous Heuristics" ( John M. Olin Program in Law and Economics Working Paper No. 165, 2002).

This Working Paper is brought to you for free and open access by the Coase-Sandor Institute for Law and Economics at Chicago Unbound. It has been
accepted for inclusion in Coase-Sandor Working Paper Series in Law and Economics by an authorized administrator of Chicago Unbound. For more
information, please contact unbound@law.uchicago.edu.

 

C H I C A G O 

JOHN M. OLIN LAW & ECONOMICS WORKING PAPER NO. 165 
(2D SERIES) 
 

 
 
Hazardous Heuristics 
 
Cass R. Sunstein 
 
THE LAW SCHOOL 
THE UNIVERSITY OF CHICAGO 
 
 
This paper can be downloaded without charge at: 
The Chicago Working Paper Series Index: 
http://www.law.uchicago.edu/Lawecon/index.html 
 
The Social Science Research Network Electronic Paper Collection: 
http://ssrn.com/abstract_id=344620

 

Hazardous Heuristics 
 
Cass R. Sunstein* 

Abstract 

 
New  work  on  heuristics  and  biases  has  explored  the  role  of  emotions  and  affect; 
 
the idea of “dual processing”; the place of heuristics and biases outside of the laboratory; 
and the implications of heuristics and biases for policy and law. This review-essay focuses 
on certain aspects of Heuristics and Biases: The Psychology of Intuitive Judgment, edited 
by  Thomas  Gilovich,  Dale  Griffin,  and  Daniel  Kahneman.  An  understanding  of 
heuristics  and  biases  casts  light  on  many  issues  in  law,  involving  jury  awards,  risk 
regulation, and political economy in general. Some attention is given to the possibility of 
“moral  heuristics”—rules  of  thumb,  for  purposes  of  morality,  that  generally  work  well 
but that also systematically misfire. 
 
 
 
In the early 1970s, Daniel Kahneman and Amos Tversky produced a series 
of pathbreaking papers about decisions under uncertainty.1 Their basic claim was 
that  in  assessing  probabilities,  “people  rely  on  a  limited  number  of  heuristic 
principles  which  reduce  the  complex  tasks  of  assessing  probabilities  and 
predicting  values  to  simpler  judgmental  operations.”2  Kahneman  and  Tversky 
did not argue that it is irrational for people to use the relevant heuristics. On the 
contrary,  they  claimed  that  as  a  general  rule,  the  heuristics  are  quite  valuable. 
The  problem  is  that  in  some  cases,  their  use  leads  “to  severe  and  systematic 
errors.”3 It is worth emphasizing the word “systematic.” One of the most striking 
features of their argument was that the errors were not random -- that they could 
be described and even predicted.  
 

                                                 

*Karl N. Llewellyn Distinguished Service Professor of Jurisprudence, Law School and Depart-
ment of Political Science, University of Chicago. I am grateful to Reid Hastie, Daniel Kahneman, 
Eric Posner, Richard Posner, and Adrian Vermeule for valuable comments on a previous draft. 

1The key papers can be found in Daniel Kahneman, Paul Slovic, and Amos Tversky Judgment 
Under  Uncertainty:  Heuristics  and  Biases  (1982).  The  heuristics-and-biases  literature  should  be 
distinguished from the literature on prospect theory, which involves the nature of people’s utility 
functions  under  conditions  of  risk,  not  mental  shortcuts  under  conditions  of  uncertainty.  See 
Daniel Kahneman and Amos Tversky, Choices, Values, and Frames (2001). 

2See  Amos  Tversky  and  Daniel  Kahneman,  Judgment  under  Uncertainty:  Heuristics  and 

Biases, in id. at 3. 

3Id. 

 

 

 

The  resulting  arguments  have  proved  highly  influential  in  many  fields, 
including law,4 where the influence has stemmed from the effort to connect legal 
analysis  to  a  realistic,  rather  than  hypothetical,  understanding  of  how  human 
beings think and behave. If human beings use identifiable heuristics, and if they 
are prone to systematic errors, we might be able to have a better understanding 
of  why  law  is  as  it  is,  and  we  might  be  able  to  generate  better  strategies  for 
ensuring  that  law  actually  promotes  social  goals.  Most  provocatively,  an 
understanding of heuristics and biases should improve our understanding of the 
legitimate role of paternalism in law. If people make systematic errors, perhaps 
government has, more often than antipaternalists think, good reason to override 
their choices.  
 
The influence of the heuristics-and-biases literature also stemmed from its 
obvious  connection  with  particular  problems  with  which 
lawyers  and 
policymakers  are  concerned,  including  risk  regulation,  litigation  behavior,  and 
discrimination. For example, the system of risk regulation has been said to show 
a combination of “paranoia and neglect.”5 An understanding of systematic errors 
might help show how and why this is so, and give a sense of what might be done 
by way of response. In fact the heuristics-and-biases literature cuts across many 
contemporary  issues,  including  global  warming,  tobacco  regulation,  punitive 
damage  reform,  racial  profiling,  and  responses  to  terrorism.  Many  other  issues 
might  also  be  illuminated.  Do  heuristics  and  biases  account  for  the  decision 
whether to bring suit at all? Might jury behavior, or even legislative and judicial 
behavior, be illuminated by a better understanding of intuitive judgment? 
 

Kahneman  and  Tversky  emphasized  three  general-purpose  heuristics: 
representativeness,  availability,  and  anchoring.  The  availability  heuristic  has 
probably  become the most  well-known in law.6 When people use this heuristic, 
they answer a question of probability by asking whether examples come readily 
to mind.7 How likely is a flood, an earthquake, an airplane crash, a traffic jam, a 

                                                 

4See,  e.g.,  Roger Noll and James Krier, Some Implications of Cognitive Psychology for Risk 
Regulation, 19 J. Legal Stud 747 (1991); Christine Jolls et al., A Behavioral Approach to Law and 
Economics, 50 Stan L Rev 1471, 1518-19 (2000); Behavioral Law and Economics (Cass R. Sunstein 
ed. 2000). 

5John  D.  Graham,  Making  Sense  of  Risk:  An  Agenda  for  Congress,  in  Risks,  Benefits,  and 

Lives Saved 183, 183 (Robert Hahn ed. 1996). 

6See  Noll  and  Krier.  A  LEXIS  search  of  law  reviews  found  well  over  200  references  to  the 

availability heuristic.  

7See  Amos  Tversky  and  Daniel  Kahneman,  Judgment  Under  Uncertainty:  Heuristics  and 

Biases, in Judgment Under Uncertainty: Heuristics and Biases, supra note 7, at 3, 11-14. 

2 

 

terrorist  attack,  or  a  disaster  at  a  nuclear  power  plant?  Lacking  statistical 
knowledge, people try to think of illustrations. Thus, “a class whose instances are 
easily  retrieved  will  appear  more  numerous  than  a  class  of  equal  frequency 
whose instances are less retrievable.”8 This is a point about how familiarity can 
affect the availability of instances. But salience is important as well. “The impact 
of  seeing  a  house  burning  on  the  subjective  probability  of  such  accidents  is 
probably greater than the impact of reading about a fire in the local paper.”9 For 
people  without  statistical  knowledge,  it  is  far  from  irrational  to  use  the 
availability heuristic; the problem is that this heuristic can lead to serious errors 
of fact, in the form of excessive fear of small risks and neglect of large ones.10 And 
undoubtedly  the  availability  heuristic  underlies  much  ethnic  and  racial 
discrimination.  While  such  discrimination  is  frequently  rational  as  a  statistical 
matter,  it  is  also  undoubtedly  true  that  some  of  the  time,  people  overlook 
statistical reality, and rely on easily accessible incidents, in thinking that people 
of a certain racial or ethnic group are prone to certain behavior. 

 
Kahneman  and  Tversky  also  suggested  that  in  the  face  of  uncertainty, 
estimates  are  often  made  from  an  initial  value,  or  “anchor,”  which  is  then 
adjusted  to  produce  a  final  answer.  The  initial  value  seems  to  have  undue 
influence.  What  percentage  of  African  countries  are  in  the  United  Nations?  In 
one study, Kahneman and Tversky spun a wheel of fortune to obtain a number 
between 0 and 100, and asked subjects to say whether the number that emerged 
from the wheel was higher or lower than the relevant percentage. It turned out 
that the starting point, though clearly random, greatly affected people’s answers. 
If  the  starting  point  was  65,  the  median  estimate  was  45%;  if  the  starting  point 
was 10, the median estimate was 25%. The process of anchoring-and-adjustment 
has an obvious application to many legal issues, including the setting of damage 
awards.11 

 
When  the  representativeness  heuristic  is  involved,  people  answer  a 
question of probability or causation—for example, how likely is it that object A 
belongs  to  class  B?—by  asking  about  the  extent  to  which  A  resembles  B. 

                                                 

8Id. at 11. 
9Id. 
10See  Roger  Noll  and  James  Krier,  Some  Implications  of  Cognitive  Psychology  for  Risk 
Regulation, 19 J. Legal Stud 747 (1991); Timur Kuran and Cass R. Sunstein, Availability Cascades 
and Risk Regulation, 51 Stan L Rev 683, 703-05 (1999). 

11Chris Guthrie, Jeffrey Rachlinski, and Andrew Wistrich, Inside the Judicial Mind, 86 Corn. 

L. Rev. 778 (2001).  

 

3 

 

Suppose,  for  example,  that  the  question  is  whether  some  person,  Nick,  is  a 
librarian or a farmer. If Nick is described as shy and withdrawn, and as having a 
passion for detail, most people will think that he is likely to be a librarian—and 
to ignore the “base-rate,” that is, the fact that the population has far more farmers 
than  librarians.  It  should  be  readily  apparent  that  the  representativeness 
heuristic will produce problems whenever people are ignoring base-rates, as they 
are prone to do. In one study, a majority of judges, in assessing probabilities, fell 
prey  to  the  representativeness  heuristic.12  The  representativeness  heuristic  also 
appears  to  underlie  as  serious  misunderstanding  of  probability  theory  in  the 
doctrine of res ipsa loquitur.13  

 
Since  the  early  discussions  of  heuristics  and  biases,  there  has  been  an 
explosion  of  further  work,  sometimes  contesting  the  basic  claims  of  Kahneman 
and  Tversky,14  but  usually  offering  more  applications,  an 
improved 
understanding  of  how  the  heuristics  work,  and  a  discovery  of  many  other 
heuristics  and  biases.  Heuristics  and  Biases:  The  Psychology  of  Intuitive  Judgment 
offers  a  massive,  state-of-the-art  treatment  of  the  literature,  supplementing  a 
similar  book  published  two  decades  ago.15  The  book,  consisting  of  forty-two 
chapters,  is  divided  into  three  parts.  The  first,  called  Theoretical  and  Empirical 
Extensions,  elaborates  on  the  three  main  heuristics  and  on  several  related 
heuristics  and  biases,  including  optimistic  bias.  The  second  part,  called  New 
Theoretical Directions, discusses the role of emotions and affect, support theory, 
and  alternative  perspectives  on  heuristics,  including  the  view,  set  forth  most 
prominently  by  Gerd  Gigerenzer,  that  outside  the  laboratory,  our  “fast  and 
frugal”  heuristics  work  very  well  (p.  559).  Part  III,  called  Real-World 
Applications,  offers  a  range  of  cases  in  which  intuitive  judgments  goes  wrong, 
including  those  of  ordinary  people  (falsely  believing,  for  example,  in  the  “hot 
hand”  phenomenon  in  basketball;  p.  601)  and  those  of  experts  (whose  clinical 
judgments  of  dangerousness,  for  example,  are  far  less  accurate  than  actuarial 
judgments—a point with many legal applications16). 

                                                 

12Id. 
13See id. Under that doctrine, a jury is permitted to infer that the defendant is negligent from 
the  occurrence  of  an  event  that  is  "of  a  kind  which  ordinarily  does  not  occur  in  the  absence  of 
negligence."  As  Guthrie  et  al.  explain,  the  appeal  of  this  inference  comes  from  the 
representativeness  heuristic.  But  the  inference  is  false:  An  event  that  does  not  ordinarily  occur 
when  negligence  is  absent  may  nonetheless  be  more  likely  to be the product of non-negligence 
than negligence. 

14See Gerd Gigerenzer et al., Simple Heuristics That Make Us Smart (1999). 
15See Kahneman, Slovic, and Tversky, supra note. 
16See William Meadow and Cass R. Sunstein, Statistics, Not Experts, Duke LJ (2002). 

4 

 

 
This  is  an  extremely  impressive  and  important  book,  and  it  is  full  of 
implications  for  law  and  policy.  The  collection  also  covers  an  extraordinary 
range  of  problems.  I  will  not  be  able  to  come  close  to  doing  justice  to  it  here. 
Instead I have a much narrower purpose: to connect some of the recent research 
with  a  set  of  legal  problems,  and  in  particular  the  law  relating  to  risk  and 
litigation behavior. In that connection, two aspects of the book deserve emphasis. 
The first involves a shift from the strictly cognitive focus of the early work to an 
effort  to  see  how  emotions  affect  decision  and  judgment.  The  second  is  the 
emphasis,  in  several  of  the  papers,  on  “dual  process”  approaches  to  human 
thinking.  According  to  these  approaches,  people  have  two  systems  for  making 
decisions. One of these is rapid, intuitive, but sometimes error-prone; the other is 
slower,  reflective,  and  more  statistical.  One  of  the  pervasive  themes  in  this 
collection  is  that  heuristics  and  biases  can  be  connected  with  the  intuitive 
system—and  that  the  slower,  more  reflective  system  might  be  able  to  make 
corrections.17 This emphasis on correction raises the possibility of “debiasing,” on 
which several of the papers also focus.  

                                                 

17The  papers  do  not  discuss  the  nature  of  the  brain,  but  suggestive  research  tends  to  be 
supportive of the dual-process idea. Some research suggests that the brain has special sectors for 
emotions, and that some types of emotions, including some fear-type reactions, can be triggered 
before  the  more  cognitive  sectors  become  involved  at  all.  See  Joseph  LeDoux,  The  Emotional 
Brain  157–69,  172–73,  283–96  (1996).  A  small,  almond-shaped  region  of  the  forebrain,  the 
amygdala,  appears  to  play  a  distinctive  role  in  registering  fear,  with  more  reflective  checks 
coming  from  the  cerebral  cortex.  See  id.  at  172-73,  suggesting  that  stimulation  of  the  amygdala 
produces  “a  sense  of  foreboding  danger,  or  fear,”  and  that  “studies  of  humans  with  amygdala 
damage also suggest that it plays a special role in fear .” Those who hear sudden, unexplained 
noises  are  fearful  before  they  are  able  to  identify  the  source  of  the  noise.  R.B.  Zajonc,  On  the 
Primacy of Affect, 39 Am Psych 117 (1984); R.B. Zajonc, Feeling and Thinking: Preferences Need 
No  Inferences,  35  Am  Psych  151  (1980).  People  who  have  been  given  intravenous  injections  of 
procaine,  which  stimulates  the  amygdala,  report  panic  sensations.  Servan-Schreiber  and 
Perlstein, Selective Limbic Activation and its Relevance to Emotional Disorders, 12 Cognition & 
Emotion 331 (1998). In research with human beings, electrical stimulation of the amygdala leads 
to  reported  feelings  of  fear  and  foreboding,  even  without  any  reason  for  these  things,  leading 
people  to  say,  for example, that they feel as if someone were chasing them. J. Panksepp, Mood 
Changes, in Handbook of Clinical Neurology (P.J. Vinken et al. eds. 1985).  

Indeed,  some  “emotional  responses  can  occur  without  the  involvement  of  the  higher 
processing  systems  of  the  brain,  systems  believed  to  be  involved  in  thinking,  reasoning,  and 
consciousness.”  Ledoux,  supra,  at  161.  The  sectors  of  the  brain  that  “cannot  make  fine 
distinctions” also have a strong advantage in speed. Id. at 163. The thalamic pathway, involving 
the amygdala, “can provide a fast signal that warns that something dangerous may be there. It is 
a quick and dirty processing system.” Id. at 163. An especially interesting finding: A patient with 
amygdala  damage  was  asked  to  detect  emotional  expression  on  faces,  and  she  succeeded  in 
identifying “most classes of expressions, except when the faces showed fear.” Id. at 173.  

5 

 

The  essay  comes  in  six  parts.  Part  I  discusses  some  real-world  effects  of 
availability  and  anchoring  and  brings  that  discussion  to  bear  on  a  common 
criticism of the heuristics-and-biases literature: that heuristics and errors do not 
play a significant role outside of the laboratory. Part II examines one of the most 
important  and  interesting  papers  in  the  book,  in  which  Daniel  Kahneman  and 
Shane  Frederick  offer  a  rethinking  and  generalization  of  the  whole  idea  of 
heuristics.  Part  III  explores  the  role  of  emotions  and  affect.  Part  IV  investigates 
optimistic  bias  and  its  relationship  to  legal  regulation.  Part  V  goes  beyond  the 
book  under  review  to  offer  some  speculative  remarks  about  the  possibility  of 
“normative  heuristics”  mental  shortcuts  that  generally  work  well,  but  that  lead 
to systematic errors in thinking about morality and law.  

 

I. Parlor Games? The Real World of Availability and Anchoring 

 
The  early  work  on  heuristics  and  biases  raised  a  natural  set  of  doubts18: 
Are  these  phenomena  important  in  the  real  world?  Are  people  really  likely  to 
make  systematic  errors?  On  one  view,  the  mistakes,  often  made  by  under-
graduate subjects, are a product of clever manipulations by psychologists, and in 
daily  life,  or  in  markets,  people  do  much  better.19  These  issues  receive  some 
helpful  attention  in  the  introduction  (pp.  7–15)  and  elsewhere,  but  they  are  not 
the book’s explicit focus. To answer them, it is important to emphasize that the 
goal  of  the  heuristics-and-biases  literature  is  emphatically  not  to  show  that 
people  are  fools,  or  that  they  are  systematically  irrational.  On  the  contrary, 
Kahneman and Tversky emphasized that the relevant heuristics are efficient and 
generally  work  well.20  But  in  the  laboratory,  at  least,  people  who  use  the 
heuristics sometimes blunder, and it is the blundering that has attracted the most 
academic attention. Consider,  for example, the fact that when asked how many 
words, on four pages of a novel, end in “ing,” people will give a larger number 
than when asked how many words have “n” as their second-to-last letter (p. 21) -
- a clear laboratory illustration of the availability heuristic. Several of the papers 
                                                 

18Richard  A.  Posner,  Rational  Choice,  Behavioral  Economics,  and  the  Law,  50  Stan.  L.  Rev. 

1551 (1998); Gerd Gigerenzer et al., Simple Heuristics That Make Us Smart (2000). 

19For  evidence  that  heuristics  and  biases  operate  in  the  real  world,  even  when  dollars  are 
involved,  see  Werner  Debondt  and  Richard  Thaler,  Do  Analysts  Overreact?,  at  pp.  666;  Robert 
Shiller, Irrational Exuberance 136-147 (2000) (discussing anchoring and overconfidence in market 
behavior); Colin Camerer and Robin Hogarth, The Effects of Financial Incentives in Experiments, 
19 J. Risk and Uncertainty 7 (1999), which finds that financial incentives have never eliminated 
anomalies  or  persistent  irrationalities.  Notably,  however,  one  study  finds  that  the  effects  of 
anchoring are decreased as a result of monetary payments. See id. 

20For this reason, Gigerenzer’s emphasis on the efficiency of certain heuristics, see p. 559, does 

not seem to me in any way compatible with the heuristics-and-biases program.  

6 

 

 

go well beyond the laboratory and demonstrate that heuristics lead to significant 
errors in the real world.  

A. Availability and Risk 

 
1.  Availability,  health,  and  safety.  It  is  reasonable  to  expect  that  people’s 
judgments  about  health  and  safety  risks  would  be  affected  by  the  availability 
heuristic, and Baruch Fischhoff offers some striking illustrations (p. 730). Should 
women  offer  physical  resistance  in  cases  of  assault?  In  popular  publications, 
experts  offer  contradictory  advice  (p.  733).  Those  who  claim  that  it  is  a  serious 
mistake consist disproportionately of people from law enforcement sources, who 
mostly  see  bad  outcomes  from  those  who  resisted  physically.  Hence  police 
officers  may  well  be  victims  of  the  availability  heuristic,  at  least  “  if  they 
remembered what they had seen and heard, but lacked an appreciation of what 
they were not seeing” (p. 733). More generally, Fischhoff discusses lay estimates 
of the frequency of forty-one causes of death in the United States. He finds that 
the  errors  in  these  estimates  are  consistent  with  the  availability  heuristic  (and 
hence  the  errors  were  predicted  before  the  data  were  seen;  p.  737).  Highly 
publicized  causes  of  death,  such  as  floods  and  tornadoes,  are  overestimated, 
whereas quieter killers, such as strokes and diabetes, are underestimated (p. 738). 
Other studies show a similar pattern.21  

 
Apart  from  surveys,  is  actual  behavior  affected  by  the  availability 
heuristic?  There  is  evidence  to  believe  that  it  is.  Whether  people  will  buy 
insurance for natural disasters is greatly affected by recent experiences.22 If floods 
have not occurred in the immediate past, people who live on flood plains are far 
less likely to purchase insurance.23 In the aftermath of an earthquake, insurance 
for  earthquakes  rises  sharply—but  it  declines  steadily  from  that  point,  as  vivid 

                                                 

21See W. Kip Viscusi, Jurors, Judges, and the Mistreatment of Risk by the Courts, 30 J Legal 
Stud  107  (2001).  A  possible  criticism  of  these  findings  is  that  they  might  show  the  effect  of 
anchoring.  In  the  relevant  surveys,  people  are  typically  given  a  starting  number,  such  as  the 
number of deaths from motor vehicle accidents each year (around 40,000). That starting number 
is  necessary  to  ensure  that  numbers,  for  imperfectly  informed  respondents,  will  not  be  all  over 
the lot. But the starting number, as an anchor, might also compress the range of answers, making 
high numbers lower and low numbers higher than they would otherwise be. It is possible that a 
general  finding—that  people  overestimate  low  risks  and  underestimate  high  ones—is  partly  a 
product of this anchoring effect. 

22For  a  vivid  demonstration  in  the  context  of  catastrophes,  see  Jacob  Gersen,  Strategy  and 
Cognition:  Regulatory  Catastrophic  Risk  (unpublished  manuscript  2001).  See  also  Paul  Slovic, 
The Perception of Risk 40 (2000). 

23Id. 

7 

 

memories  recede.24  Notice  that  the  use  of  the  availability  heuristic,  in  these 
contexts,  strongly  suggests  that  the  heuristics  operate  even  when  the  stakes  are 
large. Insurance decisions involve significant amounts of money. And it is even 
possible that the use of the availability heuristic, in such contexts, is fully rational 
for  people  who  lack  statistical  knowledge.  Perhaps  use  of  that  heuristic  is  the 
best way of minimizing the sum of decision costs and error costs.25 But it seems 
less  useful  to  debate  the  rationality  of  the  availability  heuristic  than  simply  to 
observe that it has a significant effect on behavior even when significant sums of 
money are involved. 

 
2.  The  sources  of  availability.  What,  in  particular,  produces  availability? 
An interesting essay attempts to test the effects of ease of imagery on perceived 
judgments  of  risk  (p.  98).  The  study  asked  subjects  to  read  about  an  illness 
(Hyposcenia-B)  that  “was  becoming  increasingly  prevalent”  (p.  99)  on  the  local 
campus.  In  one  condition,  the  symptoms  were  concrete  and  easy  to  imagine  -- 
involving  muscle  aches,  low  energy,  and  frequent severe headaches.  In  another 
condition, the symptoms were vague and hard to imagine, involving an inflamed 
liver,  a  malfunctioning  nervous  system,  and  a  vague  sense  of  disorientation. 
Subjects  in  both  conditions  were  asked  both  to  imagine  a  three-week  period  in 
which  they  had  the  disease  and  to  write  a  detailed  description  of  what  they 
imagined.  After  doing  so,  subjects  were  asked  to  assess,  on  a  ten-point  scale, 
their likelihood of contracting the disease. The basic finding was that likelihood 
judgments  were  very  different  in  the  two  conditions,  with  easily-imagined 
symptoms making people far more inclined to believe that they were likely to get 
the disease.  
 
The  implications  for  law  and  policy  should  not  be  obscure.  Consider 
positive issues first: Why is law as it is? The public demand for law will be much 
higher if people can easily imagine the harm in question; in such cases, the law 
might  well  reflect  a  kind  of  hysteria.  But  if  the  harm  is  difficult  to  imagine,  we 
might  well  see  a  pattern  of  neglect.26  We  would  therefore  predict  that  easily 

                                                 

24Id. 
25It is reasonable, however, to read Kahneman and Tversky as suggesting that the heuristics 
cannot  entirely  be  defended  in  this  way—that  some  of  the  time,  at  least,  the  heuristics  operate 
even  though  a  little  thought  would  improve  judgments.  Consider  the  discussion  of  “ing”  as 
possible to “n” as the next-to-last letter, above, and consider also the Linda problem, discussed 
below. 

26Compare  the  finding  that  teens’  rates  of  risk  behaviors—smoking,  driving  after  drinking, 
unsafe  sex—can  be  reduced  by  addressing  heuristics  and  biases,  in  part  by  explaining  that  the 

8 

 

imaginable  harms  would  lead  to  relatively  greater  private  precautions  and 
relatively greater governmental concern.27 Well-organized private groups should, 
and  do,  take  advantage  of  this  fact,  attempting  to  publicize  visible  examples  of 
harms  to  which  they  seek  to  draw  attention.28  The  point  has  prescriptive 
implications  for  law  as  well,  offering  implications,  for  example,  about  the 
appropriate  design  of  public  informational  campaigns.  If  government  wants  to 
encourage  people  to  take  protective  steps,  it  should  provide  information  about 
symptoms in a vivid rather than statistical way (p. 102), relying on examples that 
can  later  be  brought  to  mind.  (Terrorists  appear  to  show  a  good  intuitive 
understanding of the availability heuristic.) And there is a normative problem as 
well:  If  people  use  the  availability  heuristic,  and  if  officials  are  subject  to  the 
public  demand  for  law,  it  is  to  be  expected  that  the  law  will  impose  stringent 
controls on some small risks, and weak controls on some serious ones.  

 
 
But  there  is  an  interesting  puzzle  for  those  interested  in  the  real-world 
uses of this heuristic: In many contexts, multiple images are literally “available.” 
Consider  the  problem  of  gun  violence.  It  is  not  hard  to  find  cases  in  which  the 
presence  of  guns  led  to  many  deaths,  and  also  cases  in  which  the  presence  of 
guns  allowed  law-abiding  citizens  to  protect  themselves  against  criminals.29  In 
the  face  of  conflicting  instances,  which  cases  are  especially  available,  and  to 
whom?  The  same  question  can  be  raised  in  the  environmental  setting.  We  can 
find cases in which serious harm resulted from a failure to heed early warnings, 
suggesting the need for aggressive regulatory protection against risks that cannot 
yet be shown to be serious30; but we can also find cases in which the government 
expended a great deal to reduce risks that turned out, on reflection, to be small or 
illusory.31 The former cases are available to some people and the latter to others. 
Why should one or another kind of case be available?  
 

The behavior of the media, and of relevant interest groups, is undoubtedly 
important  here.  Many  perceived  “epidemics”  are  in  reality  no  such  thing,  but 

                                                                                                                                                 
availability  heuristic  leads  teens  to  overestimate  the  risk  behavior  of  their  peers.  See  Baruch 
Fischhoff, Heuristics and Biases in Application, in id. at 730, 747, 

27 See Noll and Krier, supra note; Kuran and Sunstein, supra note. 
28 See Kuran and Sunstein, supra note (discussing availability campaigns). 
29 Se Donald Braman and Dan M. Kahan, More Statistics, Less Persuasion: A Cultural Theory 

of Gun-Risk Perceptions (unpublished manuscript 2002). 

30 The Precautionary Principle in the 20th Century: Late Lessons from Early Warnings (Poul 

Harremoes et al. eds. 2002). 

31 For a catalogue, see Adam Wildavsky, But Is It True? (1996). 

9 

 

instead  a  product  of  media  coverage  of  gripping,  unrepresentative  incidents.32 
But this does not provide all of the picture. Beliefs and orientations are a product 
of  availability,  to  be  sure;  but  what  is  available  is  also  a  product  of  beliefs  and 
orientations.  In  other  words,  availability  may  be  endogenous  to  individual 
predispositions.  Social  processes  are  quite  important  here,  for  apparently 
representative  anecdotes  and  gripping  examples  can  move  rapidly  from  one 
person  to  another.33  Once  several  people  start  to  take  an  example  as  probative, 
many people may come to be influenced by their opinion, giving rise of cascade 
effects.34  

 
In  the  domain  of  risks,  “availability  cascades”  help  to  account  for  many 
social  beliefs,  and  here  local  variations  are  likely,  with  different  examples 
becoming salient in different communities. With respect to risks, religious, racial, 
and ethnic variations can be explained  partly in this way, as different instances 
become available to diverse groups of like-minded people. Indeed, processes of 

                                                 

32 See Howard Kurtz, The 'Crime Wave' Against Girls, available at washingtonpost.com: “If 
you  were  watching  cable  yesterday,  you  know  that  two  teenage  girls  were  kidnapped  at 
gunpoint  in  Lancaster,  Calif.  A  frightening  story,  to  be  sure.  Especially  after  all  the  hours  of 
coverage, with police news conferences, grieving relatives, ex-detectives and FBI profilers. Most 
other  news  was  obliterated  (except  for  a  brief  interlude  with  John  Ashcroft  announcing  the 
WorldCom  arrests).  There  was  even,  like  a  recycled  script,  a  white  Bronco.  By  mid-afternoon, 
police rescued the teenagers and shot the suspect dead—just in time for the evening wrap-ups.  

“Is  it  getting  more  dangerous  out  there  for  young  girls?  Ever  since  Chandra  Levy  and 
Elizabeth Smart, it seems that television is obsessing on some crime story involving girls. Could 
the  saturation  coverage  be  painting  a  distorted  picture,  like  the  great  shark  scare  last  summer? 
Northeastern University criminologist James Fox told us on CNN last weekend that ‘in a typical 
year,  we  have  50  to  100  kids  who  are  abducted  by  strangers  and  murdered.  This  year's  no 
different. . . .There's no epidemic. . . . Your child's chance of being killed by an abductor and by a 
stranger  is  significantly  less  than  the  chance  that  they'll,  for  example,  die  by  falling  off  their 
bicycle and hitting their head.’ 

“But  that's  not  the  impression  left  by  the  media  machine  these  days. . . [T]ake these much-
hyped abductions, add in the half dozen other cases mentioned by the national media since the 
first of the year . . . [it] still doesn't qualify as a new crime wave.” 

33  Chip  Heath  et  al.,  Emotional Selection in Memes: The Case of Urban Legends, Journal of 
Personality and Social Psychology (2001); Chip Heath, Do People Prefer to Pass Along Good or 
Bad News? Valence and Relevance as Predictors of Transmission Propensity," 68 Organizational 
Behavior and Human Decision Processes (1996). 

34 See Shiller, supra note, at 148-68; Sushil Biikhchandani et al., Learning from the Behavior of 
Others,  J.  Econ.  Persp.,  Summer  1998,  at  151;  Lisa  Anderson  and  Charles  Holt,  Information 
Cascades in the Laboratory, 87 Am. Econ. Rev. 847 (1997); David Hirshleifer, The Blind Leading 
the  Blind,  in  The  New  Economics  of  Human  Behavior  188,  190–95  (Marianno  Tommasi  and 
Kathryn  Ierulli  eds.,1995);  Timur  Kuran  and  Cass  R.  Sunstein,  Availability  Cascades  and  Risk 
Regulation, 51 Stan L Rev 683, 691–703 (1999). 

10 

 

deliberation typically lead like-minded people to accept a more extreme version 
of  their  original  views,35  making  it  likely  that  the  effects  of  certain  available 
examples  will  become  greatly  amplified  through  group  discussion.  And 
undoubtedly different cultural orientations play a large role in determining what 
turns  out  to  be  available.36  People  are  often  predisposed  to  take  one  or  another 
case  as  an  illustration  of  a  general  phenomenon,  and  predispositions  matter  a 
great deal in determining what is available. A great deal of work, normative and 
empirical, remains to be done on this topic. 
 

B. Anchors and Damages 

 
 
The original studies of anchoring-and-adjustment were memorable in part 
because  they  were  so  amusing.  They  suggested  that  when  people  lack 
information  about  an  appropriate  value,  they  are  highly  suggestible,  even  by 
apparently  irrelevant  numbers.37  But  the  original  studies  left  open  many 
questions  about  the  necessary  conditions for anchoring, and also about  the role 
of anchoring outside of the laboratory.38  
 

Gretchen  Chapman  and  Eric  Johnson  offer  a  great  deal  of  help  in 
answering  these  questions  (p.  120).  Their  most  general  lesson  is  that  anchors, 
even  irrelevant  and  extreme  ones,  have  large  effects.  Chapman  and  Johnson 
show  that  for  an  anchor  to  have  an  effect,  people  need  not  be  aware  of  its 
influence (p. 125); that an anchor is often operating even when people think that 
it  is  not  (id);  that  anchors  have  effects  even  when  people  believe,  and  say  they 
believe,  that  the  anchor  is  uninformative;  and  that  making  people  aware  of  an 
anchor’s effect does not reduce anchoring (id.). It follows that “debiasing” is very 
difficult  in  this  context.  Very  extreme  or  ludicrously  implausible  anchors  also 
seem to have an effect: Estimates of the year that Albert Einstein first visited the 
United  States  are  greatly  affected  by  asking  people  to  begin  by  considering 
anchors  of  1215  or  1992  (p.  124).  Chapman  and  Johnson  also  show  that  that 
economic  incentives  do  not  eliminate  the  effects  of  anchors  (p.  125);  hence 
anchoring is not a result of casualness about the underlying task.  

 
Anchors  have  major  effects  on  legal  outcomes.  The  plaintiff’s  demand 
influences  jury  verdicts,  in  terms  of  both  liability  judgments  and  amounts 

                                                 

35See Cass R. Sunstein, Deliberative Trouble? Why Groups Go To Extremes, Yale LJ (2000). 
36See Braman and Kahan, supra note. 
37See Tversky and Kahneman, supra note, at 14–16. 
38For discussion of market behavior and anchoring, see Shiller, supra note, at 135–42. 

11 

 

awarded (p. 137). In one study, a request for $500,000 produced a median mock 
jury  award  of  $300,000,  whereas  a  request  of  $100,000,  in  the  identical  case, 
produced  a  median  award  of  $90,000.39  Even  implausibly  low  and  implausibly 
high  demands  operate  as  anchors  (id.)  Opening  offers  in  negotiation  have  a 
significant influence on settlements. An ingenious study finds that anchors affect 
judges too. Judges were asked to come up with appropriate awards in a personal 
injury  case.40  The  study  involved  two  conditions.  The  “no  anchor”  condition 
involved a simple statement of the facts. The “anchor” condition was the same as 
the  first,  but  with  one  critical  difference:  The  defendant  filed  an  obviously 
meritless motion to dismiss the case on the ground that the $75,000 jurisdictional 
minimum had not been met. Almost all of the judges denied the motion, which 
nonetheless served as an anchor, with large effects on ultimate judgments. In the 
no anchor condition, the average award was $1.24 million, while it was $882,000 
in the anchor condition.41 

 
Anchors  also  play  a  role  in  “contingent valuation” studies, an influential 
method of valuing regulatory goods, such as increased safety and environmental 
protection.42  Initially  stated  values,  in  studies  of  people’s  willingness  to  pay  to 
save offshore birds, have a large effect on people’s answers (p. 137). Perhaps the 
most striking, and in a way hilarious, evidence to this effect comes from a study 
of  willingness  to  pay  to  reduce  annual  risks  of  death  and  injury  in  motor 
vehicles.43  The  authors  attempted  to  elicit  both  maximum  and  minimum 
willingness  to  pay  for  safety  improvements.  People  were  presented  with  a  risk 
and  an  initial  amount,  and  asked  whether  they  were  definitely  willing  or 
definitely  unwilling  to  pay  that  amount  to  eliminate  the  risk,  or  “not  sure.”  If 
they were definitely willing, the amount displayed was increased until they said 
that they were definitely unwilling; if they were unsure, the number was moved 
up and down until people could identify the minimum and maximum.  

                                                 

39See XX. 
40See Guthrie et al., supra note. 
41Id.  at.  There  is  a  possible  response  to  the  authors’  claim  to  have  shown  the  effects  of 
anchoring:  Perhaps  the  motion  to  dismiss  suggested  that  the  injury  was  less  serious  than  was 
apparent.  Why  would  a  lawyer  file  a  totally  frivolous  motion  to  dismiss?  But  the  abundant 
evidence of effects from anchors suggests that this is unlikely to explain all or even much of the 
authors’ finding. See also W. Kip Viscusi, Corporate Risk Analysis: A Reckless Act?, Stan L Rev 
(finding  an  anchoring  effect  from  monetary  value  of  life  on  jury  awards,  so  much  so  that 
companies that placed a high value on human life ended up paying higher punitive awards!). 

42See,  e.g.,  George  Tolley  et  al.,  Valuing  Health  For  Policy  (1995);  Valuing  Environmental 

Preferences (Ian Bateman & K. G. Willis eds., 1999). 

43See  Michael  Jones-Lee  and  Graham  Loomes,  Private  Values  and  Public  Policy,  in  Conflict 

and Tradeoffs in Decision Making 205, 210–12 (Elke Weber et al. eds. 2000). 

12 

 

 
The  authors  were  not  attempting  to  test  the  effects  of  anchors;  on  the 
contrary, they were alert to anchoring only because they “had been warned” of a 
possible  problem  with  their  procedure,  in  which  people  “might  be  unduly 
influenced by the first amount of money that they saw displayed.”44 To solve that 
problem, the authors allocated people randomly to two subsamples, one with an 
initial display of 25 pounds, the other with an initial display of 75 pounds. The 
authors  hoped  that  the  anchoring  effect  would  be  small,  with  no  significant 
consequences  for  minimum  and  maximum  values.  But  their  hope  was  dashed. 
For every level of risk, the minimum willingness to pay was higher, with the 75 
pound  starting  point,  than  the  maximum willingness to pay with the  25  pound 
starting  point!45  For  example,  a  reduction  in  the  annual  risk  of  death  by  4  in 
100,000  produced  a  maximum  willingness  to  pay  of  149  pounds,  with  the  25 
pound starting value, but a minimum willingness to pay of 232 pounds, with the 
75 pound starting value (and a maximum, in that case, of 350 pounds).46 

 
The most sensible conclusion is that whenever people are uncertain about 
appropriate values, anchors have a significant effect, and sometimes a startlingly 
large  one.  Clever  negotiators,  lawyers,  and  policymakers  should  be  able  to 
exploit  those  effects,  sometimes  even  by  providing  an  outlandish  or  apparently 
irrelevant  anchor.  There  is  a  real  opportunity  for  legal  reform  here,  in  part 
because  anchors  might  well  produce  results  that  are  not  easy  to  defend,  and  in 
part because different anchors will ensure that similarly situated people are not 
treated  similarly.  Perhaps  lawyers  should  not  be  permitted  to  inform  jurors  of 
potentially  effective  anchors,  such  as  the  annual  profits  of  the  firm  or  even  the 
plaintiff’s  demand,  at  least  in  cases  involving  punitive  awards  or  hard-to-
monetize  compensatory  awards.  Or  perhaps  judges  should  be  asked  to  review 
jury awards carefully and by reference to comparison cases, so as to weaken the 
effect of arbitrary anchors. In any case we now know that the effects of anchoring 
are hardly limited to the laboratory. 

 
This  point  raises  a  related  one:  Are  groups  able  to  avoid  the  judgment 
errors made by individuals? The evidence is mixed.47 In general, groups tend to 
polarize:  They  tend  up  in  a  more  extreme  position  in  line  with  their 

                                                 

44Id. at 210. 
45Id. at 211. 
46Id. 
47See Norbert Kerr et al., Bias in Judgment: Comparing Individuals and Groups, 103 Psych. 

Rev. 687 (1996). 

13 

 

 

 

 

predeliberation tendencies.48 At the same time, groups have been found to make 
better  decisions  than  individuals  with  respect  to  certain  statistical  problems.49 
There is some evidence that groups are slightly better at avoiding the problems 
created  by  use  of  the  availability  heuristic.50  On  the  other  hand,  some  evidence 
suggests  that  the  use  of  the  representativeness heuristic  is actually amplified  in 
groups.51  It  seems  clear  that  group  processes  do  not  eliminate  the  use  of 
heuristics, and it remains to be found whether and when they reduce or increase 
the resulting errors. 

II. 

Two Systems 

What,  exactly,  is  a  heuristic?  When  will  a  heuristic  be  overridden  by 
cognitive processes that produce a more accurate understanding of the problem 
in  question?  In  a  highlight  of  this  collection,  Daniel  Kahneman  and  Shane 
Frederick make real progress on these questions (p. 49). Their narrow goal is to 
revisit the representativeness heuristic, but they also have a broader ambition—
to  rethink  and  to  generalize  the  whole  idea  of  heuristics  and  biases.  Their 
discussion  is  packed  with  new  material,  and  I  touch  here  only  on  the  points  of 
particular relevance for policy and law. 

A. Dual Processing and Attribute Substitution 

 
Much of their argument turns on drawing a connection between heuristics 
and  dual-process  theories.52  Recall  that  those  theories  distinguish  between  two 
families  of  cognitive  operations,  sometimes  labeled  System  1  and  System  II. 
System I is intuitive; it is rapid, automatic, and effortless. System II, by contrast, 
is reflective; it is slower, self-aware, and deductive. Kahneman and Frederick are 
careful  to  disclaim  the  views  that  the  two  systems  operate  as  “autonomous 
homunculi,”  but  represent  “collections  of  processes  that  are  distinguished  by 
their speed, controllability, and the contents on which they operate” (p. 51). They 
suggest that System 1 proposes quick answers to problems of judgment, and that 

                                                 

48See Cass R. Sunstein, Deliberative Trouble? Why Groups Go To Extremes, Yale LJ (2000). 
49Alan  Blinder  and  John  Morgan,  Are  Two  Heads  Better  Than  One?  An  Experimental 

Analysis of Group Vs. Individual Decisionmaking, NBER Working Paper 7909 (2000). 

50Kerr et al., supra note, at 692. 
51Id. 
52For  an  overview,  see  Shelly  Chaiken  and  Yaacov  Trope,  Dual-Process  Theories  in  Social 

Psychology (1999). 

14 

 

System  2  operates  as  a  monitor,  confirming  or  overriding  those  judgments.53 
Consider, for example, someone who is flying from Chicago to New York in the 
month after an airplane crash. This person might make a rapid, barely conscious 
judgment, rooted in System I, that the flight is quite risky, but there might well 
be a System II override, bringing a more realistic assessment to bear. Or consider 
someone, bitten by a German Shepherd dog as a child, who encounters a German 
Shepherd at a neighbor’s house. The immediate intuitive reaction might be fear, 
but  System  II  might  well  provide  a  corrective.  In  making  a  distinction  between 
System 1 and System II, Kahneman and Frederick announce a theme that plays a 
significant role in this book.54  

 
Kahneman  and  Frederick  also  offer  a  general  claim  about  the  nature  of 
heuristics: That they operate through a process of attribute substitution (p. 53). In 
this process, people are interested in assessing a “target attribute,” and they do 
so by substituting a “heuristic attribute” of the object, which is easier to handle. 
Consider  the  question  whether  more  people  die  from  suicides  or  homicides. 
Lacking  statistical  information,  people  might  respond  by  asking  whether  it  is 
easier to recall cases in either class (the availability heuristic). As it happens, this 
is  how  most  people  proceed,  and  as  a  result  they  tend  to  give  the  incorrect 
answer that more people die from homicides.55 But it is easy to see that much of 
the time, the process of attribute substitution will lead in the right directions, or 
at  least  toward  the  best  possible  answer  for  people  who  lack  specialized 
knowledge.  
 

B. Amending the Theory (with a simple lesson for law) 

 
With  an  understanding  of  heuristics  as  attribute  substitution,  Kahneman 
and Frederick offer some significant amendments to the original presentation by 
Kahneman  and  Tversky.  They  suggest  that  anchoring  should  not  be  seen  as  a 
heuristic  at  all;  anchoring  operates  not  by  substituting  an  attribute,  but  by 
making a particular value seem more plausible (p. 56). They also argue that the 
third general-purpose heuristic, to replace anchoring, is the affect heuristic (id). I 
discuss  the  affect  heuristic  in  more  detail  below.  For  the  moment,  note  that 
Kahneman and Frederick urge that punitive damage awards are mediated by an 
outrage heuristic (p. 63), which we might see as an example of the affect heuristic 
                                                 

53Do  Systems  I  and  II  have  physical  locations  in  the  human  brain?  There  is  some  evidence 

that they do. See note supra. 

54See,  e.g,  Steven  Sloman,  Two  Systems  of  Reasoning,  p.  379;  Paul  Slovic  et  al.,  The  Affect 

Heuristic, . 397; Robyn Dawes et al., Clinical Versus Actuarial Judgment, p. 716. 

55See Paul Slovic, supra note X. 

15 

 

in action. Jurors do not have a good sense of how to set punitive damage awards 
(a hard question), and they begin the process by asking about the outrageousness 
of  the  defendant’s  conduct  (an  easier  question).56  Something  like  an  outrage 
heuristic  undoubtedly  plays  a  role  in  punishment  judgments  of  many  different 
kinds; there is a large research agenda here.  

 
Now  turn  to  the  authors’  focus,  the  representativeness  heuristic,  which 
has  lead  to  some  large  controversies.57  The  most  famous  of  these  involves 
questions about the likely career of a hypothetical woman named Linda (p. 62), 
described as follows: “Linda is 31 years old, single, outspoken, and very bright. 
She majored in philosophy. As a student, she was deeply concerned with issues 
of  discrimination  and  social 
justice  and  also  participated  in  antinuclear 
demonstrations.”  Subjects  were  asked  to  rank,  in  order  of  probability,  eight 
possible  futures  for  Linda.  Six  of  these  were  fillers  (psychiatric  social  worker, 
elementary  school  teacher);  the  two  crucial  ones  were  “bank  teller”  and  “bank 
teller and active in the feminist movement.” Most people said that Linda was less 
likely  to  be  a  bank  teller  than  to  be  a  bank  teller  and  active  in  the  feminist 
movement.  This  is  an  obvious  logical  mistake,  called  a  conjunction  error,  in 
which characteristics A and B are thought to be more likely than characteristic A 
alone. The error stems from the representativeness heuristic: Linda’s description 
seems to match “bank teller and active in the feminist movement” far better than 
“bank  teller.”  To  the  great  surprise  of  Kahneman  and  Tversky,  80%  of 
undergraduates  made  a  conjunction  error  when  asked  directly,  without  fillers, 
whether  Linda  is  more  likely  to  be  a  “bank  teller”  or  “bank  teller  and  active  in 
the feminist movement” (p. 66). 

 
As Kahneman and Frederick note, people’s answers to the Linda problem 
have been explained on numerous grounds (p. 67), with critics arguing that the 
structure  of  the  problem  increased  or  perhaps  even  generated  logical  mistakes. 
Indeed, the Linda problem can be redescribed in ways that will prevent people 
from erring. Kahneman and Frederick urge that this point should be taken not as 
a  challenge  to  the  claim  that  people  use the  representativeness  heuristic,  but  as 
evidence  that  under  certain  circumstances,  people  will  overcome  the  errors 
produced by that heuristic (including the conjunction fallacy and neglect of base-
                                                 

56Here  Kahneman  and  Frederick  draw  on  work  on  which  I  have  been  involved,  see,  e.g., 
Daniel  Kahneman  et  al.,  Shared  Outrage  and  Erratic  Awards:  The  Psychology  of  Punitive 
Damages, J Risk & Uncertainty; Cass R. Sunstein et al., Predictably Incoherent Judgments, Stan L 
Rev (2002).  

57For  a  treatment  of  the  representativeness  heuristic  and  investment  behavior,  see  Schiller, 

supra note, at 144. 

16 

 

rates). Kahneman and Frederick suggest that when these problems are overcome, 
it  is  often  because  of  the  operations  of  System  II,  which  works  as  a  kind  of 
supervisor  or  monitor.  Hence  intelligent  people,  and  those  with  statistical 
sophistication, are less likely to err (p. 68); for such people, System II is especially 
active.58  

 
For  law  and  policy,  the  general  lesson  is  simple:  Whenever  possible, 
institutionalize System II, at least when questions of fact are involved. Frequently 
the legal system disregards this advice, relying on juries and hence on ordinary 
intuitions  about  probability  and  causation.  The  twentieth  century  movement 
toward  greater  reliance  on  technical  expertise,  and  actual  data,  might  well  be 
seen as an implicit recognition of the unreliability of ordinary intuitions. Indeed, 
there is reason to think that experts themselves are vulnerable to heuristics, and 
that  reliance  on  actuarial  data  could  lead  to  substantial  improvements  in 
accuracy.59  In  the  domain  of  regulation,  quantitative  risk  analysis  is  the  most 
straightforward  way  of  overcoming  the  errors  that  sometimes  accompany 
heuristics.  Consider  the  controversy  over  regulation  of  arsenic  in  drinking 
water.60  The  availability  and  representativeness  heuristics  ensure  that  many 
people  will  be  quite  frightened  of  arsenic,  even  in  extremely  low  doses. 
Quantitative  risk  analysis  can  work  as  a  kind  of  System  II  check  on  potential 
errors. 
 

C. Generalizing Representativeness 

 
Kahneman and Tversky also suggest that judgment heuristics, understood 
as  attribute  substitution,  operate  not  only  to  answer  questions  about  uncertain 
events, but also in a diverse class of judgments. In making judgments of fact and 
value, we often substitute a simple question for a hard one. (In asking about the 
meaning  of  the  Constitution  in  a  difficult  case,  we  might  think  that  the  best 
approach  is  simply  to  investigate  the  view  of  people  with  whom  we  generally 
agree;  we  ask  not,  “what  does  the  Constitution  mean  here?”  but  instead  “what 
does  Judge  X  or  Professor  Y  think  the  Constitution  means  here?”)  Indeed 
availability,  representativeness,  and  affect  might  be  seen  as  “general  purpose” 
heuristics  in  the  sense  that  they  are  not  limited  to  issues  of  probability.  People 
seem  to  be  more  convinced,  for  example,  by  messages  with  many  arguments 
                                                 

58With respect to intelligence, there is a nice qualification: When the problem is very hard for 
everyone, intelligent respondents are most likely to err, because they “are more likely to agree on 
a plausible error than to respond randomly” (p. 68). 

59See Robyn Dawes et al., Clinical versus Actuarial Judgment, p. 716. 
60See Cass R. Sunstein, The Arithmetic of Arsenic, Geo. L.J. (2002). 

17 

 

(rather  than  few),  by  arguments  that  are  relatively  long,  and  by  messages  with 
statistics  or  supported  by  credible  experts;  these  patterns  seem  to  show  the 
representativeness heuristic in action.61 And it is easy to see how affect might be 
a kind of all-purpose heuristic for assessments of objects and their attributes—a 
point  suggesting  the  likely  importance  of  initial  reactions,  by  judges  and  juries 
alike, to  cases  presented  to  them. Kahneman and Frederick go so far as to urge 
that a modest generalization of the representativeness heuristic helps to explain 
strikingly  similar  biases 
in  economic  valuations  of  public  goods  and 
retrospective  evaluations  of  past  events.  In  particular,  Kahneman  and  Tversky 
emphasize the crucial role of prototypes, or representative exemplars, in making 
complex judgments.  

 
How  much  are  people  willing  to  pay  to  save  animals?  It  turns  out  that 
people are highly sensitive to the prototypes involved and highly insensitive to 
the numbers of animals at stake. A program that involves members of a popular 
species  will  produce  a  much  higher  willingness  to  pay  than  a  program  that 
involves  members  of  an  unpopular  species  (p.  71).  At  the  same  time,  people’s 
willingness  to  pay  does  not  differ  greatly  with  large  variations  in  the  numbers 
involved;  their  willingness  to  pay  is  about  the  same  to  save  2,000,  20,000,  and 
200,000 birds (p. 75). There is a clear parallel here to people’s neglect of base-rates 
in  using  the  representativeness  heuristic  to  make  probability 
judgments. 
Kahneman and Frederick also show that in evaluating past experiences, such as 
exposure to unpleasant noises, painful medical procedures, or horrific film clips, 
people  show  duration  neglect  (p.  77).  In  one  experiment,  for  example,  people’s 
evaluations  of  horrific  movies  were  largely  unaffected  by  substantial  variations 
in 
In  another  experiments,  people’s  evaluations  of 
colonoscopies were greatly influenced by the highest level of pain involved and 
also by the level of pain at the end, but not much by significant variations in the 
duration  of  the  procedure  (from  4  to  69  minutes).  Here  too  Kahneman  and 
Frederick  urge  that  the  prototype,  captured  in  the  Peak  Affect  and  the  End 
Affect,  dominates  evaluation.  The  finding  seems  in  light  with  more  casual 
empiricism:  People’s retrospective judgments, of very good times and very bad 
times, seem to have little to do with the duration of those times, and a great deal 
to do with Peak and End. 

length 

their 

(id.). 

With  respect  to  law  and  policy,  an  intriguing  implication  here  is  that 
people’s use of prototypes will crowd out variables that, on reflection, have clear 
importance.  There  is  a  serious  problem  with  contingent  valuation  studies  if 

                                                 

61See  Shelly  Chaiken,  The  Heuristic  Model  of  Persuasion,  in  Social  Influence:  The  Ontario 

Symposium, vol. 5 10-11 (Mark Zanna et al. eds. 1987). 

18 

 

 

people’s judgments do not attend to the number of animals at stake. And indeed, 
some  of  the  pathologies  in  regulatory  policy  do  seem  connected  with  this 
problem.  Evidence  suggests,  for  example,  that  people  “worry  more  about  the 
proportion of risk reduced than about the number of people helped.”62 A striking 
study of this effect finds that people pervasively neglect absolute numbers, and 
that  this  neglect  maps  onto  regulatory  policy.63  In  a  similar  vein,  Christopher 
Hsee  and  Yuval  Rottenstreich  have  shown  that  when  emotions  are  involved, 
people  neglect  two  numbers  that  should  plainly  be  relevant:  the  probability  of 
harm  and  the  extent  of  harm.64  This  finding  is  closely  connected  with  several 
papers in this volume, to which I now turn. 
 

III. Emotions, Contagion, and Affect 

 
How  are  judgments,  especially  judgments  about  the  likelihood  of  risk  or 
benefit,  influenced  by  emotions  and  affect?  Let  us  begin  with  two  papers  that 
draw on the representativeness heuristic. 

 

A.  False Contagions and Phony Cures 

Paul  Rozin  and  Carol  Nemeroff  explore “sympathetic  magical thinking,” 
including  the  beliefs  that  some  objects  have  special  contagious  properties  and 
that  causes  resemble  their  effects.65  Consider  some  findings  about  disgust  and 
fear.  Many  educated  Americans  will  not  eat  food  touched  by  a  sterilized 
cockroach  (p.  202).  They  refuse  chocolates  that  have  been  shaped  into  realistic-
looking  dog  feces  (id.).  They  are  reluctant  to  use  sugar  from  a  bottle  labeled 
“Sodium Cyanide, Poison,” even if they are assured, and believe, that the bottle 
really contains sugar and never contained cyanide (id.)—and indeed even if they 
themselves placed the label, arbitrarily, on that particular bottle (p. 205)! In fact 
people  are  reluctant  to  eat  sugar  labeled,  “Not  Sodium  Cyanide,”  apparently 
because  the  very  words  “Sodium  Cyanide”  automatically  bring  up  negative 

                                                 

Press, 3d ed. 2000). 

Risk Analysis 593 (1988). 

62See  Jonathan  Baron,  Thinking  and  Deciding  500–502  (Cambridge:  Cambridge  University 

63See T.L. McDaniels, Comparing Expressed and Revealed Preferences for Risk Reduction, 8 

64See Yuval Rottenstreich and Christopher Hsee, Money, Kisses, and Electric Shocks: On the 
Affective Psychology of Risk, 12 Psych. Science 185, 186-188 (2001); Christopher Hsee and Yuval 
Rottenstreich, Music, Pandas, and Muggers: On the Affective Psychology of Value (University of 
Chicago, June 26, 2002). 

65Paul  Rozin  and  Carol  Nemeroff,  Sympathetic  Magical  Thinking:  The  Contagion  and 

Similarity “Heuristics,” id at 201. 

19 

 

associations.66 People are reluctant to wear a sweater than has been worn for five 
minutes by a person with AIDS (p. 207). In this case, as in other cases involving 
contagion,  people  are  relatively  insensitive  to  dose.  A  sweater  worn  for  five 
minutes by someone with AIDS, and then washed, is not much more undesirable 
than  a  sweater  used  by  someone  with  AIDS  for  a  full  year.  According  to  most 
respondents, a single live AIDS virus that enters the human body is as likely to 
infect someone with the virus as 10,000 or even 1,000,000 viruses (p. 207). Note in 
this regard that disgust and fear tend to “travel”; in both experimental and real-
world  settings,  people  are  especially  likely  to  spread  “urban  legends”  that 
involve risks of contamination.67 
 

In  some  of  these  cases,  the  intuitive  fear  or  revulsion  can  be  easily 
overridden,  as  reflection  reveals  that  there  is  no  real  hazard.  Here  we  seem  to 
have good evidence of the relationship between System I and System II. System I 
gives rise to an immediate sense of alarm or revulsion, but System II will usually 
provide a corrective (even if System I continues to squawk). But not always. Paul 
Slovic  has  found  that  most  people  accept  a  kind  of  “intuitive  toxicology,” 
showing agreement with the suggestion that “there is no safe level of exposure to 
a  cancer-causing  agent”  and  that  “if  you  are  exposed  to  a  carcinogen,  then  you 
are  likely  to  get  cancer.”  68  Apparently  some  intuitions  about  fear  are  part  of 
everyday thinking, even reflective thinking, about social risks.  

 
 
Thomas Gilovich and Kenneth  Savitsky use the idea that “like goes with 
like”  to  unpack  the  structure  of  a  wide  range  of  false  beliefs,  both  ancient  and 
modern.69  Many  primitive  beliefs  about  medicine  reflect  the  belief  that  the 
symptoms  of  a  disease  are  likely  to  resemble  both  its  cause  and  its  cure. 
According to ancient Chinese medicine, for example, those with vision problems 
should  eat  ground  bats,  on  the  theory  that  bats  have  especially  good  vision, 
which might be transferred to people (p. 619). Homeopathy, which remains quite 
popular,  depends  in  part  on  the  idea  that  if  a  substance  creates  disease 
symptoms  in  a  healthy  person,  it  will  have  a  healthy  effect  on  someone  who 

                                                 

66There  is  a  lesson  here  about  rhetoric  in  law  and  politics:  Disclaimers,  or  statements 
distinguishing one’s position for an unpopular position that might be confused with it (“I’m not 
angry,  but”  or  “To  say  this  is  not  to  say  that  I  am  favoring  a  tax  increase”),  might  well  be 
ineffective. 

67See Heath et al., supra note, at 1032-1039. 
68Paul Slovic, supra note, at 291. 
69Thomas  Gilovich  and  Kenneth  Savitsky,  Like  Goes  With  Like:  The  Role  of 
Representativeness in Erroneous and Pseudo-Scientific Beliefs, in id. at 617. Some of these themes 
are illuminatingly addressed in Thomas Gilovich, How We Know What Isn’t So (1995). 

20 

 

therapies,  reflect  many  absurd  examples  of 

currently  suffers  from  those  symptoms  (p.  620).  The  idea  has  some  valid 
applications, but often the symptoms of a disease do not resemble it cause or its 
cure;  consider  sanitation  and  antibiotics  (p.  620).  Alternative  medicines, 
the 
including  New  Age 
representativeness  heuristic  (p.  620).  I  believe  that  the  immense  popularity  of 
organic  foods  owes  a  great  deal  to  heuristic-driven  thinking,  above  all  with  the 
view  that  there  is  an  association  between  the  natural  and  the  healthy,  and 
between  chemical  and  danger.70  To  the  extent  that  people  are  suspicious  of 
professionalized medicine, and trust scientifically dubious substitutes, it is often 
because  they  are  neglecting  base-rates,  making  selective  use  of  the  availability 
heuristic,  and  misperceiving  the  effects  of  randomness,  which  inevitably 
produces apparent patterns.71 

 

B. The Affect Heuristic 

 
Beliefs  about  contagion  are  emotionally  laden;  disgust  and  revulsion, 
whether  or  not  grounded  in  fact,  play  a  strong  role.  In  emphasizing  the  affect 
heuristic, Kahneman and Tversky refer to the chapter of that title by Paul Slovic 
and  several  coauthors.72  This  chapter  is  one  of  the  most  interesting  and 
suggestive  in  the  volume.  It  also  creates  numerous  puzzles,  many  of  them 
involving law and policy.73 

 
People  often  have  a  rapid,  largely  affective  response  to  objects  and 
situations, including job applicants,  consumer products, athletes, animals, risks, 
cars, plaintiffs, defendants, and causes of action. A jury might have an immediate 
negative  reaction  to  a  plaintiff  in  a  personal  injury  case;  a  judge  might  have  a 
positive  intuitive  reaction  to  an  equal  protection  claim;  an  employer  might 
instantly  like,  or  dislike,  someone  who  has  applied  for  a  job.74  But  what  does  it 
mean  to  say  that  affect  is  a  “heuristic”?  Slovic  et  al.  urge  that  our  affective 
responses occur rapidly and automatically, and that people use their feelings as a 
kind of substitute for a more systematic, all-things-considered judgment. It is in 
this  sense  that  attribute  substitution,  in  the  sense  meant  by  Kahneman  and 
Frederick,  may  be  at  work;  affect  toward  an  object  substitutes  for  a  more 

                                                 

70For criticism of that association, see Pandora’s Picnic Basket; Naturally Dangerous. 
71See Gilovich, supra note; Nassim Taleb, Fooled by Randomness (2001). 
72Paul Slovic et al., The Affect Heuristic, p. 397. 
73I have elsewhere discussed an earlier and less elaborate version of Slovic’s work on affect, 

and I draw on that discussion here. See Cass R. Sunstein, The Laws of Fear, Harv L Rev (2002). 

74See Timothy Wilson et al., Mental Contamination and the Debiasing Problem, at 185, 198–

99. 

21 

 

reflective  assessment  of  the  object.  But  there  is  an  obvious  sense  in  which  it  is 
unhelpful  to  treat  “affect”  as  an  explanation  for  someone’s  attitude  toward 
objects. In some settings, affect represents, or is, that very attitude, and therefore 
cannot explain or account for it. (Would it be helpful to explain Tom’s romantic 
attraction  to  Anne  by  saying  that  Anne  produces  a  favorable  affect  in  Tom?) 
Slovic  et  al.  must  be  urging  that  sometimes  affect  works  in  the  same  way  as 
availability  and  representativeness:  In  many  contexts,  people’s  emotional 
reactions  are  substituting  for  a  more  careful inquiry  into the (factual?) issues  at 
stake.  
 
The simplest way to establish this would be to proceed as Kahneman and 
Tversky originally did, by showing, for example, that people assess questions of 
probability  by  reference  to  affect,  and  that  this  method  leads  to  predictable 
errors. What is the probability of death from smoking, driving, flying, or eating 
pesticides?  If  people’s  affect  toward  these  activities  matched  their  probability 
judgments, producing systematic error, it would certainly be plausible to speak 
of an affect heuristic. Slovic et al. do not have data of this sort. But they do have 
some  closely  related  evidence,  highly  suggestive  of  an  affect  heuristic  in  the 
domain  of  risk  (pp.  410-413).  When  asked  to  assess  the  risks  and  benefits 
associated with certain items, people tend to say that risky activities contain low 
benefits,  and  that  beneficial  activities  contain  low  risks.  It  is  rare  that  they  will 
see an activity as both highly beneficial and quite dangerous, or as both benefit-
free and danger-free. Because risk and benefit are distinct concepts, this finding 
seems  to  suggest  that  “affect”  comes  first,  and  helps  to  “direct”  judgments  of 
both risk and benefit.  

 
Two  studies  fortify  this  hypothesis  (pp.  411–12).  The  first  of  these  tests 
whether  new  information  about  the  risks  associated  with  some  item  alters 
people’s  judgments  about  the  benefits  associated  with  the  item—and  whether 
new  information  about  benefits  alters  people’s  judgments  about  risks.  The 
motivation  for  this  study  is  simple.  If  people’s  judgments  were  analytical  and 
calculative,  information  about  the  great  benefits  of  (say)  food  preservatives 
should not produce a judgment that the risks are low—just as information about 
the great risks of (say) natural gas should not make people think that the benefits 
are low. Strikingly, however, information about benefits alters judgments about 
risks,  and  that  information  about  risks  alters  judgments  about  benefits.  When 
people  learn  about  the  low  risks  of  an  item,  they  are  moved  to  think  that  the 
benefits are high—and when they learn about the high benefits of an item, they 
are  moved  to  think  that  the  risks  are  low.  The  conclusion  is  that  people  assess 
products  and  activities  through  affect—and  that  information  that  improves 

22 

 

people’s  affective  response  will  improve  their  judgments  of  all  dimensions  of 
those  products  and  activities.  A  closely  related  experiment  shows  that  when 
people are inadequately informed, they tend to think that stocks that are “good” 
have both high return and low risk, whereas stocks that are “bad” are judges to 
have  low  return  and  high  risk  (p.  413).  In  the  presence  of  a  high  level  of 
information, analysts distinguish perceived risk and return, and their judgments 
are not produced by a global attitude (id.). 

 
The second study asked people to make decisions under time pressure (p. 
412).  The  motivating  claim  is  that  the  affect  heuristic  is  more  efficient  than 
analytic  processing  in  the  sense  that  it  permits  especially  rapid  assessments. 
Under time pressure, Slovic et al. hypothesize that there would be an unusually 
strong inverse correlation between judged risk and judged benefit, because affect 
will  be  the  determinant  of  assessment,  and  people  will  have  less  time  to 
undertake the kind of analysis that could begin to pull the two apart (p. 412). In 
other  words,  System  I  is  most  important  when  time  is  scarce,  and  in  such 
circumstances,  System  II  will  be  a  less  effective  monitor.  The  hypothesis  is 
confirmed:  Under  time  pressure,  the  inverse  correlation  is  even  stronger  than 
without time pressure.  

 
The  affect  heuristic  casts  a  number  of  facts  in  a  new  light.  Background 
mood,  for  example,  influences  decisions  and  reactions  in  many  domains.75 
Consider  the  remarkable  fact  that  stock  prices  increase  significantly  on  sunny 
days, a fact that is hard to explain in terms that do not rely on affect.76 One study 
urges  that  people  are  more  vulnerable  to  the  use  of  simple  cues  when  their 
emotions are aroused and particularly when they are under conditions of stress; 
in  such  circumstances,  intense  indoctrination,  in  which  people  do  not  process 
systematically,  is  especially  likely.77  A  related  study  suggests  that  when  people 
are  anxious  and  fearful,  they  are  less  likely  to  engage  in  systematic  processing, 

                                                 

75See Alice Isen, Positive Affect and Decision Making, in Research on Judgment and Decision 
Making 509 (William Goldstein and Robin Hogarth eds. 1997). Isen notes that “a growing body of 
research  indicates  that  even  mild  and  even  positive  affective  states  can  markedly  influence 
everyday  thought  processes,  and  do  so  regularly.”  Id.  But  the  evidence  is  inconclusive  on 
whether  positive  affect  increases  or  decrease  the  use  of  heuristics  or  instead  more  systematic 
forms of reasoning. Id. at 526—27. 

76See  David  Hirschleifer  and  Tyler  Shumway,  Good  Day  Sunshine:  Stock  Returns  and  the 

Weather , available at http://papers.ssrn.com/sol3/results.cfm. 

77See  Robert  Baron,  Arousal,  Capacity,  and  Intense  Indoctrination,  4  Personality  and  Social 

Psych. Review 238, 243–44 (2000). 

23 

 

and hence System II is especially unreliable.78 Note here that there is an evident 
relationship  between  social  influences  and  the  emotions:  if  emotions  weaken 
systematic  processing,  they  simultaneously 
increase  susceptibility  to  the 
apparent views of others.79 Fear itself is likely to make people susceptible to the 
acceptance  of  faulty  logic  and  to  pressure  to  conform.80  As  a  vivid  example, 
consider the widespread panic, involving millions of people, caused by the radio 
broadcast of “Invaders From Mars” in 1938; a study suggests that people do have 
a  critical  ability  that  protects  against  panic,  but  that  the  ability  can  be 
“overpowered  either  by  an  individual’s  own  susceptible  personality  or  by 
emotions  generated  in  him  by  an  unusual  listening  situation.”81  Social 
interactions, spreading the news of the invasion, played a crucial role here.82 Fear 
of an invasion by Martians is an exotic case from which general lessons perhaps 
cannot be drawn, but the example might well be seen as an extreme illustration 
of a common process by which affect, alongside and fueled by social interactions, 
can induce widespread fear. 

 
If the affect heuristic seriously influences people’s judgments, it should be 
possible  to  manipulate  those  judgments  simply  by  altering  affect,  and  by 
associating  the  altered  affect  with  a  commodity,  person,  or  experience.83 
Background  music  in  movies,  smiling  faces  in  mail  order  catalogues,  name 
changes  by  entertainers,  and  advertisements  that  link  smoking  to  rugged 
cowboys and lush waterfalls make a good deal of sense in this light (pp. 417-18). 
In  the  case  of  smoking,  the  authors  think  that  the  possibility  of  manipulation 
through “affective tags” creates serious problems. In their view, people who start 
smoking,  especially  when  young,  may  have  some  understanding  of  the 
associated risks,84 but their behavior is governed by “the affective impulses of the 

                                                 

78See  Chaiken,  supra  note,  at  19–20.  Chaiken  notes  that  “people  who  are  anxious  about  or 
vulnerable to a health threat, or otherwise experiencing stress may engage in less careful or less 
extensive processing of health-relevant information.” Id. at 20. 

79Baron, supra note, at 244–46. 
80Id. at 244–46. 
81See Joseph Bulgatz, Ponzi Schemes, Invaders from Mars, and More Extraordinary Popular 

Delusions and the Madness of Crowds 134 (1992). 

82Id. at 128-36. 
83Note  that  the  alteration  of  affect  might  come  through  information  alone.  In  the  risk 
experiments  mentioned  above,  information  about  risks  altered  affect,  as  did  information  about 
benefits. I do not discuss the relationship between emotions and cognition here, and simply note 
that on any view of emotions, cognition plays a large role, at least most of the time. See Ledoux, 
supra note; Martha Nussbaum, Upheavals of Thought (2002); Jon Elster, Alchemies of the Mind 
(2001). 

84See W. Kip Viscusi, Smoke-Filled Rooms (2002). 

24 

 

moment,  enjoying  smoking  as  something  new  and  exciting,  a  way  to  have  fun 
with  their  friends”  (p.  418).  The  authors  think  that  smoking  decisions  reflect  a 
failure  of  rationality,  producing  by  a  failure of the  experiential  system; this is a 
case in which the affect heuristic misfires. The point very much bears on public 
education  campaigns  decided  to  reduce  smoking,85  and  also  on  the  question 
whether government has sufficient reason, in this context, for paternalism. 
 
      The  authors  emphasize  another  point  with  important  implications  for  risk 
regulation:  When an outcome is accompanied by strong emotions, variations in 
probability have surprisingly little weight on people’s decisions.86 What matters 
are  the  images  associated  with  the  result.  The  point  has  received  empirical 
confirmation in a study of people’s willingness to pay to avoid electric shocks, or 
to be able to kiss favorite movie stars.87 In one study, people’s willingness to pay 
to avoid an electric shock varied little, depending on whether its probability was 
1% or 99%!88 This point helps explain “why societal concerns about hazards such 
as  nuclear  power  and  exposure  to  extremely  small  amounts  of  toxic  chemicals 
fail to recede in response to information about the very small probabilities of the 
feared  consequences  from  such  hazards”  (p.  xxx).  With  respect  to  hope,  those 
who  operate  gambling  casinos  and  state  lotteries  are  well-aware  of  the 
underlying mechanisms. They play on people’s emotions in the particular sense 
that they conjure up palpable pictures of victory and easy living. With respect to 
risks, insurance companies and environmental groups do exactly the same.89  

                                                 

85Of special interest is a project, run by high-school students in Illinois, designed to prevent 
young people from smoking. Their extremely successful advertising campaign was designed not 
to  focus  on  the  health  risks  associated  with  smoking,  but  to  portray,  in  very  vivid  terms,  the 
stupidity  and  self-destructiveness  of  smokers,  not  least  in  romantic  settings.  An  unanticipated 
effect  of  the  campaign  was  to  lead  smokers  to  quit,  not  merely  to  prevent  them  from  starting. 
Christi  Parsons,  State  dashes  teens'  edgy  anti-smoking  ad  campaign,  Chicago  Tribune,  June 25, 
2002,  p.1.  Their  motto:  “Smoking  Makes  You  Look  Dumb.”  For  general  information,  see 
http://www.idecide4me.com/  

86I discuss some implications of this point in Cass R. Sunstein, Probability Neglect: Emotions, 

Worst Cases, and the Law, Yale LJ (forthcoming 2002). 

87Yuval  Rottenstreich  and  Christopher  Hsee,  Money,  Kisses,  and  Electric  Shocks:  On  the 

Affective Psychology of Probability Weighting, supra, at 176–88. 

88See id. at 188, showing a willingness to pay $7 to avoid a 1% risk and $10 to avoid a 99% 
risk. Note that there was a large spread, on the basis of probability, for the less “affect-rich” loss 
of $20, where the median willingness to pay was $1 for a 1% chance of loss and $18 for a 99% of 
loss. Id. at 188 

89  Slovc  notes  that  with  respect  to  products  of  all  kinds,  advertisers  try  to  produce  a  good 
affect to steer consumers into a certain direction, often through the use of appealing celebrities, 
through cheerful scenes, or through the creation of an association between the product and the 
consumer’s preferred self-image. See Slovic, The Affect Heuristic, supra. 

25 

 

It follows that if government is seeking to encourage people to avoid large 
risks, and to worry less over small risks, it might well attempt to appeal to their 
emotions,  perhaps  by  emphasizing  the  worst-case  scenario.  With  respect  to  a 
cigarette smoking, abuse of alcohol, reckless driving, and abuse of drugs, this is 
exactly  what  government  occasionally  attempts  to  do.  It  should  be  no  surprise 
that  some  of  the  most  effective  efforts  to  control  cigarette  smoking  appeal  to 
people’s emotions, by making them feel that if they smoke, they will be dupes of 
the  tobacco  companies  or  imposing  harms  on  innocent  third  parties.90  There  is 
also  an  opportunity  here  to  try  to  activate  System  II,  by  promoting  critical 
scrutiny  of  reactions  that  are  based  on  “affective  ties”  in  cases  in  which  people 
are  neglecting  serious  risks  or  exaggerating  them.  Note  here  that  “the  latent 
anxieties  conducive  to  panic  may  nevertheless  be  minimized  if  the  critical 
abilities  of  people  can  be  increased,”  and  that  education  is  “one  of  the  greatest 
preventives of panic behavior.”91 

 

IV.  Are People Unrealistically Optimistic? 

 
Thus  far  I  have  emphasized  heuristics,  which  can  lead  to  predictable 
errors,  and  which  have  accompanying  biases.  “Availability  bias”  exists,  for 
example, when recall of examples makes people toward unrealistically high and 
unrealistically  low  assessments  of  risks.  But  the  heuristics-and-biases  literature 
also explores “pure” biases, in the form of tendencies to err that do not involve 
attribute substitution. An example is self-serving bias: People care about fairness, 
but  their  judgments  about  fairness  are  systematically  biased  in  their  own 
direction—a  finding  that  helps  to  explain  litigation  behavior,  including  failures 
to  settle.92  One  of  the  most  intriguing  and  complex  biases  involves  optimism.93 
With  respect  to  most  of  the  hazards  of  life,  people  appear  to  be  unrealistically 
optimistic.  This  claim  is  closely  related  to  the  suggestive,  with  prominent 
advocates in economics, that people may attempt to reduce cognitive dissonance 
by  thinking  that  the  risks  they  face  are  lower  than  they  are  in  fact.94  If  people 
systematically  understate  risks,  there  is  a  serious  problem  for  law  and  policy, 

                                                 

90See note supra; Lisa Goldman and Stanton Glantz, Evaluation of Antismoking Advertising 

Campaigns, 279 Journal of the American Medical Association 772 (1998). 

91Hadley  Cantril,  The  Invasion  From  Mars:  A  Study  in  the  Psychology  of  Panic  204  (2d  ed. 

1966). 

92See  Linda  Babcock  and  George  Loewenstein,  Explaining  Bargaining  Impasse:  The  Role  of 

Self-Serving Biases, 11 J Econ Persp 109 (1997). 

93See Shelley Taylor, Positive Illusions (1989). 
94See George Akerlof, The Economic Consequences of Cognitive Dissonance, in An Economic 

Theorist’s Book of Tales (19XX). 

26 

 

 

and  a  serious  problem  too  for  those  who  accept  the  rational  actor  model  in  the 
social sciences. At a minimum, efforts should be able to increase information, so 
that when people run risks, they do so with knowledge of what they are doing. 
And  if  such  efforts  are  unsuccessful,  and  if  optimistic  bias  is  intransigent, 
perhaps people should be blocked from running certain risks entirely.  

A.  Evidence 

 
The  most  well-documented  findings  of  optimism  involve  relative  (as 
opposed to absolute) risk. About 90% of drivers think that they are safer than the 
average  driver  and  less  likely  to  be  involved  in  a  serious  accident.95  People 
generally think that they are less likely than other people to be divorced, to have 
heart  disease,  to  be  fired  from  a  job,  to  be  divorced,  and  much  more.96  At  first 
glance, a belief in relative immunity from risk seems disturbing, but by itself this 
finding does not establish that people underestimate the risks that they actually 
face. Perhaps people have an accurate understanding of their own statistical risks 
even  if  they  believe,  wrongly,  that  other  people  are  more  vulnerable  than  they 
are.  The  “above  average”  effect  might  well  coexist  with  largely  accurate 
assessments  of  abilities  and  susceptibilities.97  With  respect  to  absolute  risk,  the 
evidence  for  unrealistic  optimism  is  less  clear,  as  Daniel  Armor  and  Shelley 
Taylor show in their contribution to this collection. For significant and personally 
relevant  events,  including  unwanted  pregnancy,  people  show  an  accurate 
understanding  of  their  susceptibility  (p.  335).  With  respect  to  some  low-
probability events, including life-threatening risks such as AIDS, people actually 
tend  to  overestimate  their  own  susceptibility,  and  in  that  sense  seem  to  show 
pessimistic  bias  (id.).98  One  survey  finds  general  overestimates  of  personal  risk 
levels  for  such  hazards  as  breast  cancer  (where  women  rate  their  actual  risk  as 
40%,  with  the  actual  risk  being  roughly  10%); prostate cancer (where men rank 
their  actual  risk  as  40%,  with  the  actual  risk  again  being  roughly  10%);  lung 

                                                 

95See Shelley Taylor, Positive Illusions 10 (1989). 
96See Neil D. Weinstein, Unrealistic Optimism About Future Life Events, 39 J Personality & 
Soc. Psychol. 806 (1980); Neil D. Weinstein, Unrealistic Optimism About Susceptibility to Health 
Problems,  10  J.  Behav.  Med  481  (1987);  Christine  Jolls,  Behavioral  Economic  Analysis  of 
Redistributive  Legal  Rules,  in  Behavioral  Law  and  Economics  288,  291  (Cass  R.  Sunstein  ed. 
2000). 

97See W. Kip Viscusi, Smoke-Filled Rooms 162–66 (2002). 
98Armor  and  Taylor  doubt  this  conclusion,  suggesting  that  “these  estimates  may  simply 

reflect difficulties interpreting and reporting extreme possibilities” (p, 335). 

27 

 

cancer (estimated at 35%, compared to an actual risk of under 20%); and stroke 
(estimated at 45%, compared to an actual risk of roughly 20%).99 

 
But  in  many  domains,  people  do  underestimate  their  statistical  risk.  For 
example, professional financial experts consistently overestimate likely earnings, 
and  business  school  students  overestimate  their  likely  starting  salary  and  the 
number  of  offers  that  they  will  receive  (pp.  334-35).  People  also  underestimate 
their  own  likelihood  of  being  involved  in  a  serious  automobile  accident,100  and 
their  own  failure  to  buy  insurance  for  floods  and  earthquakes  is  at  least 
consistent with the view that people are excessively optimistic.101 The evidence of 
optimistic  bias,  both  relative  and  absolute,  is  sufficient  to  raise  questions  about 
informational and regulatory interventions. 
 

B. Debiasing? 

 
Neil  Weinstein  and  William  Klein  explore  a  variety  of  apparently 
 
promising strategies to reduce optimistic bias with respect to relative risk.102 The 
punch  line?  None  of  these  strategies  worked,  not  even  a  little.  One  study,  for 
example, asked people to generate their own list of personal “factors” that might 
either  increase  or  decrease  their  risk  of  developing  a  weight  problem  or  a 
drinking  problem.  The  authors  hypothesized  that  an  identification  of  factors 
would decrease optimistic bias in many cases; but no such effect was observed. 
Nor was optimistic bias reduced by asking participants to read about major risk 
factors for certain hazards, to report their standing with respect to these factors, 
and  to  offer  an  overall  risk  estimate  after  responding  to  the  list  of  factors.  The 
authors conclude that “health campaigns emphasizing high-risk targets (such as 
smoking 
interventions  that  show  unattractive  pictures  of  smokers)  and 
campaigns conveying information about undesirable actions (as with pamphlets 
listing factors that raise the risk for a particular health problem) may unwittingly 
worsen the very biases they are designed to reduce” (p. 323).  
 

As the authors note, one intervention has been found to reduce optimistic 
bias:  Giving  people  information  about  their  own  standing  on  risk  factors  or 
about  their  peers’  standing  on  risk  factors.  But  they  observe,  sensibly  enough, 
                                                 

99See Humphrey Taylor, Perceptions of Risks, available at  
http://www.harrisinteractive.com/harris_poll/index.asp?PID=44. 

100Jolls, supra note, at 291. 
101Id. 
102Neil D. Weinstein and William Klein, Resistance of Personal Risk Perceptions to Debiasing 

Intervention, p. 313. 

28 

 

that  it  is  not  easy  to  adapt  this  information  to  media  campaigns  designed  to 
improve human health. This is a valuable paper, in part because it adds to still-
emerging literature on the possibility of debiasing (or the activation of System II). 
But because the focus is on the “above average” effect, the findings do not offer 
clear  guidance  about  campaigns  designed  to  give  people  a  better  sense  of  the 
statistical reality. It would be valuable to learn much more about that topic. 
 

C. Optimistic Fools? 

 
In  their  extremely  illuminating  paper, David Armour and  Shelley  Taylor 
 
are  concerned  with  some  obvious  puzzles:  If  people  are  excessively  optimistic, 
why don’t they pursue ambitious goals recklessly, and blunder? Why don’t alert 
people—psychologists?  entrepreneurs?—take  systematic  advantage  of  human 
optimism?  This  is  what  Armor  and  Taylor  see  as  the  “dilemma  of  unrealistic 
optimism”—the likelihood that if real, this bias would produce extremely serious 
harmful  effects.  Perhaps  unrealistic  optimism  does  lead  to  real-world  harms.103 
But  if  optimism  were  as  widespread  as  some  research,  we  should  probably  see 
far more recklessness and failure than we generally observe. 
 
 
The  authors  resolve  the  dilemma  by  giving  a  more  refined  sense  of  the 
nature of optimistic bias. In their view, people are not indiscriminately or blindly 
optimistic.  Their  predictions  are  usually  within  reasonable  bounds  (p.  346). 
People are less likely to be optimistic when the consequences of error are severe 
(p.  339)—suggesting  that  people  may  not,  because  of  optimistic  bias,  risk  their 
lives  and  their  health.  In  addition,  optimism  decreases  if  the  outcome  will  be 
known  in  the  near  future;  when  performance  will  occur  and  be  evaluated 
quickly,  people’s  predictions  become  more  accurate  (id.).  Optimism  also 
decreases when people are in a predecisional state of deliberation. When people 
are  choosing  among  goals,  or  among  possible  courses  of  action,  the  bias  is 
attenuated, and it increases again only after people have selected goals and begin 
to  implement  their  plans  (p.  340).  There  is  also  evidence  that  optimistic  bias, 
when  it  exists,  can  be  adaptive,104  leading  to  (almost)  self-fulfilling  policies, 
increasing the likelihood of success (p. 341).  
 

                                                 

103For a suggestion to this effect in an interesting context, see Robert Frank and Philip Cook, 
The  Winner-Take-All  Society  (1995).  On  excessive  optimism  among  entrepreneurs,  see 
Avishalom Tor, The Fable of the Bees, Michigan Law Review (forthcoming 2002). 

104A general treatment is Shelley Taylor, Positive Illusions (1995). 

29 

 

These claims very much bear on the role of law, and provide good reason 
 
to question the view that optimistic bias provides a good reason for paternalistic 
interventions. To be sure, we know enough about optimistic bias to give serious 
consideration to informational campaigns to ensure that people will not have an 
inflated  belief  in  their  own  immunity.  In  the  context  of  smoking,  statistical 
knowledge of risks105 might be inadequate if people believe themselves relatively 
immune.106  But  in  view  of  the  arguments  by  Armor  and  Taylor,  the  idea  that 
paternalism  is  generally  justified  by  optimistic  bias  must  be  regarded  as  an 
unproven  speculation.  If  people  are  not  excessively  optimistic  when  the 
consequences  of  error  are  severe,  if  the  bias  is  small  or  nonexistent  when 
decisions are actually being made, and if people overstate low-probability risks, 
there may be no problem for the law to correct. 
 

V. Moral Heuristics? 

 
The chapters in this book are almost entirely concerned with how people 
resolve questions of fact. This should not be surprising. The heuristics-and-biases 
literature was originally focused on issues of probability, and while many of the 
chapters go beyond that topic, they do not much deal with normative question—
with the role of heuristics in informing judgments about morality and politics.107 
It  is  natural  to  wonder  whether  the  rules  of  morality  also  have  heuristics  (isn’t 
that  inevitable?),  and  whether  the  normative  judgments  involved  in  law  and 
politics  are  also  prone  to  heuristics,  or  to  rapid  System  I  assessments  and  to 
possible System II override. 

 
We can imagine some very ambitious claims here. On one view, much of 
everyday  morality,  nominally  concerned  with  fairness,  should  be  seen  a  set  of 
heuristics  for  the  real  issue,  which  is  how  to  promote  utility.108  Armed  with 
psychological  findings,  utilitarians  might  be  tempted  to  claim  that  ordinary 
moral  commitments  are  a  set  of  mental  shortcuts  that  generally  work  well,  but 
                                                 
105See Viscusi, supra note. 
106See  J.Z.  Ayanian  and  P.D.  Cleary,  Perceived  Risks  of  Heart  Disease  and  Cancer  Among 
Cigarette  Smokers,  281  JAMA  1019,  1020–21  (1999)  (finding  that  most  smokers  think  that  their 
risks  are  average  or  below  average).  Optimistic  bias  is  raised  in  this  context  in  Paul  Slovic, 
Smoking; but see Viscusi, supra note. 

107An exception is the suggestive discussion by Philip Tetlock, who urges that many people 
believe  in  “taboo  tradeoffs,”  and  that  we  might  see  such  people  not  as  defective  intuitive 
economists,  but  as  defenders  of  sacred  values.  See  Philip  Tetlock,  Intuitive  Politicians, 
Theologians, and Prosecutors, 582, 596–98. 

108See Jonathan Baron, Judgment Misguided: Intuition and Error in Public Decision Making 

(1998). 

30 

 

that also produce severe and systematic errors. (Is retribution a cognitive error? 
Is  Kantianism?)  For  their  part,  deontologists  could  easily  turn  the  tables. 
Deontologists  might  well  claim  that  the  rules  recommended  by  utilitarians  are 
consistent,  much  of  the  time,  with  what  morality  requires  --  but  also  that 
utilitarianism,  taken  seriously,  produces  bad  mistakes  in  some  cases.  And 
indeed, many debates about deontologists and utilitarians involve claims, by one 
or another side, that the opposing view leads to results that are inconsistent with 
widespread  intuitions  and  should  be  rejected  for  that  reason.  Unfortunately, 
these  large  debates  are  unlikely  to  be  tractable,  simply  because  utilitarians  and 
deontologists  are  most  likely  to  be  unconvinced  by  the  suggestion  that  their 
defining commitments are biases or mere heuristics. But in some particular cases, 
we  might  be  able  to  make  some  progress  by  entertaining  the  hypothesis  that 
some widely accepted rules of morality are heuristics. Consider, for example, the 
idea that one should “never lie” or “never steal”—good rules of thumb, certainly, 
but injunctions that badly misfire when the lie, or the theft, is needed to protect 
the deaths of innocent people.109 I deal here with several possibilities that relate 
directly to law. 
 
1. Pointless punishment. People’s intuitions about punishment seem quite 
disconnected  with  the  consequences  of  punishment,  in  a  way  that  suggests  a 
moral heuristic may well be at work. Consider, for example, an intriguing study 
of  people’s  judgments  about  penalties  in  cases  involving  harms  from  vaccines 
and birth control pills.110 In one case, subjects were told that the result of a higher 
penalty  would  be  to  make  companies  try  harder  to  make  safer  products.  In  an 
adjacent case, subjects were told that the consequence of a higher penalty would 
be to make the company more likely to stop making the product, with the result 
that less safe products would be on the market. Most subjects, including a group 
of judges, gave the same penalties in both cases. Can this outcome be defended 
in principle? Perhaps it can, but I think it is far more sensible to think that people 
are  operating  under  a  heuristic,  to  the  effect  that  penalties  should  be  a 

                                                 

109Note  rule-utilitarian  defense  of  these  ideas:  They  might  misfire  in  particular  cases,  but  it 
might be best for people to treat them as firm rules, because a case-by-case inquiry would prove 
even  more  errors.  If  people  ask  whether  the  circumstances  warrant  an  exception  to  the 
prohibition on lying or stealing, there might well be excessive or self-serving lying and stealing. 
The strong voice of conscience—calling for adherence to what I am calling moral heuristics even 
in cases in which they badly misfire—probably serves some valuable social functions. For human 
beings,  a  decision  to  go  right  to  the  issue  of  consequences,  without  firm  moral  rules  of  thumb, 
would likely produce serious problems.  

110Jonathan  Baron  and  Ilana  Ritov,  Intuitions  About  Penalties  and  Compensation  in  the 

Context of Tort Law, 7 J Risk and Uncertainty 17 (1993) 

31 

 

proportional response to the outrageousness of the act, and should not be based 
on consequential considerations. 

  
If this claim seems too adventurous, consider a similar test of punishment 
judgments,  which  asked  subjects,  including  judges  and  legislators,  to  choose 
penalties for dumping hazardous waste.111 In one case, the penalty would make 
companies  try  harder  to  avoid  waste.  In  another,  the  penalty  would  lead 
companies  to  cease  making  a  beneficial  product.  Most  people  did  not  penalize 
companies differently in the two cases. Perhaps most strikingly, people preferred 
to  require  companies  to  clean  up  their  own  waste,  even  if  the  waste  did  not 
threaten  anyone,  instead  of  spending  the  same  amount  to  clean  up  far  more 
dangerous  waste  produced  by  another,  now-defunct  company.  It  is  at  least 
plausible  to  think  that  in  thinking  about  punishment,  people  use  a  simple 
heuristic, the now-familiar outrage heuristic. This heuristic produces reasonable 
results  in  most  circumstances,  but  in  some  cases,  it  seems  to  me  to  lead  to 
systematic errors. 

 
2.  Aversion  to  cost-benefit  analysis.  An  automobile  company  is  deciding 
whether to take certain safety precautions for its cars. In deciding whether to do 
so,  it  conducts  a  cost-benefit  analysis,  in  which  it  concludes  that  certain 
precautions  are  not  justified—because,  say,  they  would  cost  $100  million  and 
save only four lives, and because the company has a “ceiling” of $10 million per 
lives saved (a ceiling that is, by the way, significantly higher than the amount the 
Environmental  Protection  Agency  uses  for  a  statistical  life).  How  will  ordinary 
people react to this decision? The answer is that they will not react favorably.112 
In  fact  they  tend  to  punish  companies  that  base  their  decisions  on  cost-benefit 
analysis,  even  if  a  high  valuation  is  placed  on  human  life.  By  contrast,  they  do 
not  much  punish  companies  that  are  willing  to  impose  a  “risk”  on  people.113 
What underlies these moral judgments?  

 
A  careful  look  raises  the  possibility  that  when  people  disapprove  of 
trading money for risks, they are generalizing from a set of moral principles that 
are  generally  sound,  and  even  useful,  but  that  work  poorly  in  some  cases. 
Consider the following moral principle: Do not knowingly cause a human death. 
                                                 

111Jonathan Baron et al., Attitudes Toward Managing Hazardous Waste, 13 Risk Analysis 183 

(1993). 

112See W. Kip Viscusi, Corporate Risk Analysis: A Reckless Act?, Stan L Rev (2000). 
113See  id.  See  also  See  Philip  Tetlock,  Coping  With  Tradeoffs,  in  Elements  of  Reason: 
Cognition,  Choice,  and  the  Bounds  of  Rationality  239,  Arthur  Lupia  et  al.  eds.  (Cambridge: 
Cambridge University Press, 2000) 

32 

 

People disapprove of companies that fail to improve safety when they are fully 
aware  that  deaths  will  result—whereas  people  do  not  disapprove  of  those  who 
fail to improve safety while appearing not to know, for certain, that deaths will 
ensue.  When  people  object  to  risky  action  taken  after  cost-benefit  analysis,  it 
seems to be partly because that very analysis puts the number of expected deaths 
squarely  “on  screen.”114  Companies  that  fail  to  do  such  analysis,  but  that  are 
aware that a “risk” exists, do not make clear, to themselves or to jurors, that they 
caused  deaths  with  full  knowledge  that  this  was  what  they  were  going  to  do. 
People disapprove, above all, of companies that cause death knowingly. 

 
I suggest, then, that a genuine heuristic is at work, one that imposes moral 
condemnation on those who knowingly engage in acts that will result in human 
deaths.  The  problem  is  that  it  is  not  always  unacceptable  to  cause  death 
knowingly, at least if the deaths are relatively few and an unintended byproduct 
of generally desirable activity. If government allows new highways to be built, it 
will  know  that  people  will  die  on  those  highways;  if  government  allows  new 
power  plants  to  be  built,  it  will  know  that  some  people  will  die  from  the 
resulting  pollution;  if  companies  produce  tobacco  products,  and  if  government 
does not ban those products, hundreds of thousands of people will die; the same 
is  true  for  alcohol.  Much  of  what  is  done, by  both  industry  and  government, is 
likely to result in one or more deaths. Of course it would make sense, in most or 
all of these domains, to take extra steps to reduce risks. But that proposition does 
not  support  the  implausible  claim  that  we  should  disapprove,  from  the  moral 
point of view, of any action taken when deaths are foreseeable.  

 
I do believe that it is impossible to vindicate, in principle, the widespread 
social  antipathy  to  cost-benefit  balancing.  But  to  adapt  a  claim  about  the 
representativeness heuristic by Stephen Jay Gould (p. 68), “a little homunculus in 
my  head  continues  to  jump  up  and  down,  shouting  at me” that corporate cost-
benefit  analysis,  trading  dollars  for  a  known  number  of  deaths,  is  morally 
unacceptable. The voice of the homunculus, I am suggesting, is not the result of 
conscience, but instead of a crude but quite tenacious moral heuristic. 

 
3.  Acts  and  omissions.  There  has  been  much  discussion  of  whether  and 
why the distinction between acts and omissions might matter for law and policy. 
In  one  case,  for  example,  a  patient  might  ask  a  doctor  not  to  provide  life-

                                                 

114It is also the case that explicit trading of money for lives is strongly disfavored, see Tetlock, 
supra note. I am hypothesizing that some of this effect, and possibly a great deal of it, comes from 
the fact that someone has knowingly engaged in action that will result in deaths. 

33 

 

sustaining  equipment,  thus  ensuring  the  patient’s  death.  In  another  case,  a 
patient  might  ask  a  doctor  to  inject  a  substance  that  will  immediately  end  the 
patient’s  life.  People  seem  to  have  a  strong  moral  intuition  that  the  failure  to 
provide life-sustaining equipment, and even the withdrawal of such equipment, 
is  acceptable  and  legitimate  --  but  that  the  injection  is  morally  abhorrent.  And 
indeed  constitutional  law  reflects  judgments  to  this  effect.115  But  what  is  the 
morally relevant difference?  

 
It is worth considering the possibility that the action-omission distinction 
operates  as  a  heuristic  for  the  more  complex  and  difficult  assessment  of  the 
moral  issues  at  stake.  From  the  moral  point of view,  harmful acts are generally 
worse  than  harmful  omissions,  in  terms  of  both  the  state  of  mind  of  the 
wrongdoer  and  the  likely  consequences  of  the  wrong.  But  harmful  acts  are  not 
always worse than harmful omissions, and the moral puzzles arise when life, or 
a clever interlocutor, comes up with a case in which there is no morally relevant 
distinction  between  acts  and  omissions,  but  when  moral  intuitions  strongly 
suggest that there must be such a difference. In such cases, we might hypothesize 
that moral intuitions reflect an overgeneralization of principles that usually make 
sense—but that fail to make sense in the particular case.116 In other words, moral 
intuitions  reflect  System  I  and  they  need  to  be  corrected  by  System  II.  I  believe 
that the persistent acceptance of withdrawal of life-saving equipment, alongside 
persistent doubts about euthanasia, is a demonstration of the point. 

 
Consider  in  this  regard  the  dispute  over  two  well-known  problems  in 
moral philosophy.117 The first, called the trolley problem, asks people to suppose 
that a runaway trolley is headed for five people, who will be killed if the trolley 
continues  on  its  current  course.  The  question  is  whether  you  would  throw  a 
switch that would move the trolley onto another set of tracks, killing one person 
rather  than  five.  Most  people  would  throw  the  switch.  The  second,  called  the 
footbridge  problem,  is  the  same  as  that  just  given,  but  with  one  difference:  the 
only way to save the five is to throw a stranger, now on a footbridge that spans 
the  tracks,  into  the  path  of  the  trolley,  killing  that  stranger  but  preventing  the 
trolley from reaching the others. Most people will not kill the stranger. But what 
is  the  difference  between  the  two  cases,  if  any?  A  great  deal  of  philosophical 
work has been done on this question, much of it trying to suggest that our firm 
                                                 

115See Washington v. Glucksberg. 
116See Jonathan Baron, Nonconsequentialist Decisions, in 17 Behavioral and Brain Sciences 1 

(1994). 

Judgment, 293 Science 2105 (2001). 

117See  Joshua  Greene  et  al.,  An  fMRI  Investigation  of  Emotional  Engagement  in  Moral 

34 

 

intuitions can indeed be defended in principle. Let me suggest a simpler answer. 
As  a  matter  of  principle,  there  is  no  difference  between  the  two  cases.  People’s 
different reactions are based on moral heuristics which condemn the throwing of 
the  stranger  but  support  the  throwing  of  the  switch.  These  heuristics  generally 
point in the right direction. But they misfire in drawing a distinction between the 
two cases. In this sense, the action-omission distinction leads to systematic errors. 

 
Is  there  anything  to  be  said  to  those  who  believe  that  their  moral 
judgments,  distinguishing  the  trolley  and  footbridge  problems,  are  entirely 
reflective,  and  reflect  no  heuristic  at  all?  Consider  an  intriguing  experiment, 
designed to see how the human brain responds to the two problems. The authors 
do  not  attempt  to  answer  the  moral  questions  in  principle,  but  they  find  “that 
there  are  systematic  variations  in  the  engagement  of  emotions  in  moral 
judgment,”118 and that brain areas associated with emotion are far more active in 
contemplating  the  footbridge  problem  than  in  contemplating  the  trolley 
problem.119 As in the case of fear, where an identifiable region of the brain makes 
helpfully immediate but not entirely reliable judgments,120 and where other, also 
identifiable  regions  can  supply  correctives,  so  too,  perhaps,  in  the  context  of 
morality and law.  

 
4. Betrayals. To say the least, people do not like to be betrayed. A betrayal 
of trust is likely to produce a great deal of outrage. If a babysitter neglects a child, 
or if a security guard steals from his employer, people will be angrier than if the 
identical acts were performed by someone in whom trust has not been reposed. 
So far, perhaps, so good. And it should not be surprising that people will favor 
greater  punishment  for  betrayals  than for otherwise identical crimes.121 Perhaps 
the  disparity  could  be  justified  on  the  ground  that  the  betrayal  of  trust  is  an 
independent harm, one that warrants greater deterrence and retribution—a point 
that draws strength from the fact that trust, once lost, is not easily regained. But 
consider a finding that is harder to explain: People are especially averse to risks 
of  death  that  come  from  products  designed  to  promote  safety,  so  much  so  that 
people  have  been  found  to  prefer  a  greater  chance  of  dying,  as  a  result  of 
accidents  from  a  crash,  to  a  significantly  lower  chance  of  dying  in  a  crash  as  a 
result of a malfunctioning air bag.122 Indeed, “most people are willing to double 

                                                 

118Id. at 2106, 
119Id. 
120See Ledoux, supra note. 
121See Jonathan Koehler and Andrew Gershoff, Betrayal Aversion (2000). 
122Id. 

35 

 

their  chance  of  dying  to  avoid  incurring  a  very  small  chance  of  dying  via 
betrayal.”123 
 
What  explains  this  seemingly  bizarre  and  self-destructive  preference?  I 
suggest that a heuristic is at work: Punish, and never reward, betrayals of trust. 
The heuristic generally works well. But it misfires in some cases, as when those 
who deploy it end up increasing the risks they themselves face. An air bag is not 
a security guard or a babysitter, endangering those whom they have been hired 
to protect. It is a product, to be chosen if it decreases aggregate risks. If an air bag 
makes people safer on balance, it should be used, even if in a tiny percentage of 
cases  it  will  create  a  risk  that  would  not  otherwise  exist.  To  reject  air  bags  on 
grounds  of  betrayal  is  irrational—irrational  but  understandable,  the  sort  of 
mistake  to  which  heuristics  often  lead  human  beings.  The  distinctive  feature  of 
the  anti-betrayal  heuristic  is  that  it  involves  moral  and  legal  judgments  rather 
than judgments of fact. 

 
These are speculative remarks on some complex subjects. But if heuristics 
play a role in factual judgments, and sometimes lead people to make systematic 
errors,  there  is  every  reason  to  believe  that  heuristics  also  help  produce 
normative judgments, both moral and legal, and sometimes produce errors there 
as well. If this is harder to demonstrate, it is largely because we are able to agree, 
in  the  relevant  cases,  about  what  constitutes  error  in  the  domain  of  facts,  and 
often  less  able  to  agree  about  what  constitutes  error  in  the  domain  of  values.  I 
have  suggested  here  that  an  understanding  of  heuristics  and  biases  has  many 
implications  for  legal  problems,  and  I  believe  that  Heuristics  and  Biases:  The 
Psychology of Intuitive Judgment will illuminate problems of law and policy for 
many  years  to  come.  But  we  should  not  be  surprised  if  the  ideas  of  attribute 
substitution,  and  of  the  correction  of  rapid,  intuitive  assessments  by  more 
reflective processes, have analogues in moral and legal intuitions as well.  
 
 
 
 
 
 
 
 
 
 
                                                 

123 Id. 

36 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
Readers with comments should address them to: 
 
Cass R. Sunstein 
University of Chicago Law School 
1111 East 60th Street 
Chicago, IL 60637 
 
 

csunstei@midway.uchicago.edu 

37 

 

 
1. 

2. 

3. 
4. 
5. 

6. 
7. 
8. 

9. 

10. 

11. 
12. 
13. 

14. 
15. 

16. 

17. 

Chicago Working Papers in Law and Economics 
(Second Series) 

William M. Landes, Copyright Protection of Letters, Diaries and Other 
Unpublished Works: An Economic Approach (July 1991) 
Richard A. Epstein, The Path to The T. J. Hooper: The Theory and History of 
Custom in the Law of Tort (August 1991) 
Cass R. Sunstein, On Property and Constitutionalism (September 1991) 
Richard A. Posner, Blackmail, Privacy, and Freedom of Contract (February 1992) 
Randal C. Picker, Security Interests, Misbehavior, and Common Pools (February 
1992) 
Tomas J. Philipson & Richard A. Posner, Optimal Regulation of AIDS (April 1992) 
Douglas G. Baird, Revisiting Auctions in Chapter 11 (April 1992) 
William M. Landes, Sequential versus Unitary Trials: An Economic Analysis (July 
1992) 
William M. Landes & Richard A. Posner, The Influence of Economics on Law: A 
Quantitative Study (August 1992) 
Alan O. Sykes, The Welfare Economics of Immigration Law: A Theoretical 
Survey With An Analysis of U.S. Policy (September 1992) 
Douglas G. Baird, 1992 Katz Lecture: Reconstructing Contracts (November 1992) 
Gary S. Becker, The Economic Way of Looking at Life (January 1993) 
J. Mark Ramseyer, Credibly Committing to Efficiency Wages: Cotton Spinning 
Cartels in Imperial Japan (March 1993) 
Cass R. Sunstein, Endogenous Preferences, Environmental Law (April 1993) 
Richard A. Posner, What Do Judges and Justices Maximize? (The Same Thing 
Everyone Else Does) (April 1993) 
Lucian Arye Bebchuk and Randal C. Picker, Bankruptcy Rules, Managerial 
Entrenchment, and Firm-Specific Human Capital (August 1993) 
J. Mark Ramseyer, Explicit Reasons for Implicit Contracts: The Legal Logic to the 
Japanese Main Bank System (August 1993) 

18.  William M. Landes and Richard A. Posner, The Economics of Anticipatory 

Adjudication (September 1993) 
Kenneth W. Dam, The Economic Underpinnings of Patent Law (September 1993) 
Alan O. Sykes, An Introduction to Regression Analysis (October 1993) 
Richard A. Epstein, The Ubiquity of the Benefit Principle (March 1994) 
Randal C. Picker, An Introduction to Game Theory and the Law (June 1994) 

19. 
20. 
21. 
22. 
23.  William M. Landes, Counterclaims: An Economic Analysis (June 1994) 
24. 

J. Mark Ramseyer, The Market for Children: Evidence from Early Modern Japan 
(August 1994) 
Robert H. Gertner and Geoffrey P. Miller, Settlement Escrows (August 1994) 
Kenneth W. Dam, Some Economic Considerations in the Intellectual Property 
Protection of Software (August 1994) 
Cass R. Sunstein, Rules and Rulelessness, (October 1994) 

25. 
26. 

27. 

38 

 

28. 

29. 

30. 
31. 

32. 

33. 
34. 
35. 

36. 
37. 

38. 

46. 
47. 

48. 

49. 

50. 

David Friedman, More Justice for Less Money: A Step Beyond Cimino (December 
1994) 
Daniel Shaviro, Budget Deficits and the Intergenerational Distribution of Lifetime 
Consumption (January 1995) 
Douglas G. Baird, The Law and Economics of Contract Damages (February 1995) 
Daniel Kessler, Thomas Meites, and Geoffrey P. Miller, Explaining Deviations 
from the Fifty Percent Rule: A Multimodal Approach to the Selection of Cases for 
Litigation (March 1995) 
Geoffrey P. Miller, Das Kapital: Solvency Regulation of the American Business 
Enterprise (April 1995) 
Richard Craswell, Freedom of Contract (August 1995) 
J. Mark Ramseyer, Public Choice (November 1995) 
Kenneth W. Dam, Intellectual Property in an Age of Software and Biotechnology 
(November 1995) 
Cass R. Sunstein, Social Norms and Social Roles (January 1996) 
J. Mark Ramseyer and Eric B. Rasmusen, Judicial Independence in Civil Law 
Regimes: Econometrics from Japan (January 1996) 
Richard A. Epstein, Transaction Costs and Property Rights: Or Do Good Fences 
Make Good Neighbors? (March 1996) 
Cass R. Sunstein, The Cost-Benefit State (May 1996) 

41. 

39. 
40.  William M. Landes and Richard A. Posner, The Economics of Legal Disputes 
Over the Ownership of Works of Art and Other Collectibles (July 1996) 
John R. Lott, Jr. and David B. Mustard, Crime, Deterrence, and Right-to-Carry 
Concealed Handguns (August 1996) 
Cass R. Sunstein, Health-Health Tradeoffs (September 1996) 
G. Baird, The Hidden Virtues of Chapter 11: An Overview of the Law and 
Economics of Financially Distressed Firms (March 1997) 
Richard A. Posner, Community, Wealth, and Equality (March 1997) 

44. 
45.  William M. Landes, The Art of Law and Economics: An Autobiographical Essay 

42. 
43. 

(March 1997) 
Cass R. Sunstein, Behavioral Analysis of Law (April 1997) 
John R. Lott, Jr. and Kermit Daniel, Term Limits and Electoral Competitiveness: 
Evidence from California=s State Legislative Races (May 1997) 
Randal C. Picker, Simple Games in a Complex World: A Generative Approach to 
the Adoption of Norms (June 1997) 
Richard A. Epstein, Contracts Small and Contracts Large: Contract Law through 
the Lens of Laissez-Faire (August 1997)  
Cass R. Sunstein, Daniel Kahneman, and David Schkade, Assessing Punitive 
Damages (with Notes on Cognition and Valuation in Law) (December 1997)  

51.  William M. Landes, Lawrence Lessig, and Michael E. Solimine, Judicial Influence: 
A Citation Analysis of Federal Courts of Appeals Judges (January 1998)  
John R. Lott, Jr., A Simple Explanation for Why Campaign Expenditures are 
Increasing: The Government is Getting Bigger (February 1998)  

52. 

39 

 

53.  

54. 

55. 

56. 

57. 

58. 

59. 
60. 

61. 

62. 

63. 

64. 

65. 

66. 

67. 

68. 

69. 

70. 
71. 

73. 

Richard A. Posner, Values and Consequences: An Introduction to Economic 
Analysis of Law (March 1998)  
Denise DiPasquale and Edward L. Glaeser, Incentives and Social Capital: Are 
Homeowners Better Citizens? (April 1998)  
Christine Jolls, Cass R. Sunstein, and Richard Thaler, A Behavioral Approach to 
Law and Economics (May 1998) 
John R. Lott, Jr., Does a Helping Hand Put Others At Risk?: Affirmative Action, 
Police Departments, and Crime (May 1998) 
Cass R. Sunstein and Edna Ullmann-Margalit, Second-Order Decisions (June 
1998) 
Jonathan M. Karpoff and John R. Lott, Jr., Punitive Damages: Their Determinants, 
Effects on Firm Value, and the Impact of Supreme Court and Congressional 
Attempts to Limit Awards (July 1998) 
Kenneth W. Dam, Self-Help in the Digital Jungle (August 1998) 
John R. Lott, Jr., How Dramatically Did Women=s Suffrage Change the Size and 
Scope of Government? (September 1998) 
Kevin A. Kordana and Eric A. Posner, A Positive Theory of Chapter 11 (October 
1998) 
David A. Weisbach, Line Drawing, Doctrine, and Efficiency in the Tax Law 
(November 1998) 
Jack L. Goldsmith and Eric A. Posner, A Theory of Customary International Law 
(November 1998) 
John R. Lott, Jr., Public Schooling, Indoctrination, and Totalitarianism (December 
1998) 
Cass R. Sunstein, Private Broadcasters and the Public Interest: Notes Toward A 
AThird Way@ (January 1999) 
Richard A. Posner, An Economic Approach to the Law of Evidence (February 
1999) 
Yannis Bakos, Erik Brynjolfsson, Douglas Lichtman, Shared Information Goods 
(February 1999) 
Kenneth W. Dam, Intellectual Property and the Academic Enterprise (February 
1999) 
Gertrud M. Fremling and Richard A. Posner, Status Signaling and the Law, with 
Particular Application to Sexual Harassment (March 1999) 
Cass R. Sunstein, Must Formalism Be Defended Empirically? (March 1999) 
Jonathan M. Karpoff, John R. Lott, Jr., and Graeme Rankine, Environmental 
Violations, Legal Penalties, and Reputation Costs (March 1999) 

72.  Matthew D. Adler and Eric A. Posner, Rethinking Cost-Benefit Analysis (April 

1999) 
John R. Lott, Jr. and William M. Landes, Multiple Victim Public Shooting, 
Bombings, and Right-to-Carry Concealed Handgun Laws: Contrasting Private 
and Public Law Enforcement (April 1999)  

40 

 

74. 

75. 

77. 

78. 

79. 
80. 

81. 

82. 
83. 

84. 
85. 
86. 

87. 

89. 

90. 

91. 
92. 
93. 

94. 

95. 

96. 

Lisa Bernstein, The Questionable Empirical Basis of Article 2=s Incorporation 
Strategy: A Preliminary Study (May 1999) 
Richard A. Epstein, Deconstructing Privacy: and Putting It Back Together Again 
(May 1999) 

76.  William M. Landes, Winning the Art Lottery: The Economic Returns to the Ganz 

Collection (May 1999) 
Cass R. Sunstein, David Schkade, and Daniel Kahneman, Do People Want 
Optimal Deterrence? (June 1999) 
Tomas J. Philipson and Richard A. Posner, The Long-Run Growth in Obesity as a 
Function of Technological Change (June 1999) 
David A. Weisbach, Ironing Out the Flat Tax (August 1999) 
Eric A. Posner, A Theory of Contract Law under Conditions of Radical Judicial 
Error (August 1999) 
David Schkade, Cass R. Sunstein, and Daniel Kahneman, Are Juries Less Erratic 
than Individuals? Deliberation, Polarization, and Punitive Damages (September 
1999) 
Cass R. Sunstein, Nondelegation Canons (September 1999) 
Richard A. Posner, The Theory and Practice of Citations Analysis, with Special 
Reference to Law and Economics (September 1999) 
Randal C. Picker, Regulating Network Industries: A Look at Intel (October 1999) 
Cass R. Sunstein, Cognition and Cost-Benefit Analysis (October 1999) 
Douglas G. Baird and Edward R. Morrison, Optimal Timing and Legal 
Decisionmaking: The Case of the Liquidation Decision in Bankruptcy (October 
1999) 
Gertrud M. Fremling and Richard A. Posner, Market Signaling of Personal 
Characteristics (November 1999) 

Preferences Are Distorted (November 1999) 
Richard A. Posner, Orwell versus Huxley: Economics, Technology, Privacy, and 
Satire (November 1999) 
David A. Weisbach, Should the Tax Law Require Current Accrual of Interest on 
Derivative Financial Instruments? (December 1999) 
Cass R. Sunstein, The Law of Group Polarization (December 1999) 
Eric A. Posner, Agency Models in Law and Economics (January 2000) 
Karen Eggleston, Eric A. Posner, and Richard Zeckhauser, Simplicity and 
Complexity in Contracts (January 2000)  
Douglas G. Baird and Robert K. Rasmussen, Boyd=s Legacy and Blackstone=s 
Ghost (February 2000)  
David Schkade, Cass R. Sunstein, Daniel Kahneman, Deliberating about Dollars: 
The Severity Shift (February 2000) 
Richard A. Posner and Eric B. Rasmusen, Creating and Enforcing Norms, with 
Special Reference to Sanctions (March 2000) 

88.  Matthew D. Adler and Eric A. Posner, Implementing Cost-Benefit Analysis When 

41 

 

97. 

98. 

99. 

Douglas Lichtman, Property Rights in Emerging Platform Technologies (April 
2000)  
Cass R. Sunstein and Edna Ullmann-Margalit, Solidarity in Consumption (May 
2000) 
David A. Weisbach, An Economic Analysis of Anti-Tax Avoidance Laws (May 
2000, revised May 2002)  

100.  Cass R. Sunstein, Human Behavior and the Law of Work (June 2000)  
101.  William M. Landes and Richard A. Posner, Harmless Error (June 2000) 
102.  Robert H. Frank and Cass R. Sunstein, Cost-Benefit Analysis and Relative 

Position (August 2000)  
Eric A. Posner, Law and the Emotions (September 2000)  
103. 
104.  Cass R. Sunstein, Cost-Benefit Default Principles (October 2000)  
105. 

Jack Goldsmith and Alan Sykes, The Dormant Commerce Clause and the Internet 
(November 2000) 

106.  Richard A. Posner, Antitrust in the New Economy (November 2000) 
107.  Douglas Lichtman, Scott Baker, and Kate Kraus, Strategic Disclosure in the Patent 

108. 

System (November 2000) 
Jack L. Goldsmith and Eric A. Posner, Moral and Legal Rhetoric in International 
Relations: A Rational Choice Perspective (November 2000) 

109.  William Meadow and Cass R. Sunstein, Statistics, Not Experts (December 2000) 
110. 
111. 

Saul Levmore, Conjunction and Aggregation (December 2000) 
Saul Levmore, Puzzling Stock Options and Compensation Norms (December 
2000) 

112.  Richard A. Epstein and Alan O. Sykes, The Assault on Managed Care: Vicarious 
Liability, Class Actions and the Patient=s Bill of Rights (December 2000) 
113.  William M. Landes, Copyright, Borrowed Images and Appropriation Art: An 

Economic Approach (December 2000) 

114.  Cass R. Sunstein, Switching the Default Rule (January 2001) 
115.  George G. Triantis, Financial Contract Design in the World of Venture Capital 

(January 2001) 
Jack Goldsmith, Statutory Foreign Affairs Preemption (February 2001) 

116. 
117.  Richard Hynes and Eric A. Posner, The Law and Economics of Consumer 

Finance (February 2001) 

118.  Cass R. Sunstein, Academic Fads and Fashions (with Special Reference to Law) 

119. 

(March 2001) 
Eric A. Posner, Controlling Agencies with Cost-Benefit Analysis: A Positive 
Political Theory Perspective (April 2001) 

120.  Douglas G. Baird, Does Bogart Still Get Scale? Rights of Publicity in the Digital 

Age (April 2001) 

121.  Douglas G. Baird and Robert K. Rasmussen, Control Rights, Priority Rights and 
the Conceptual Foundations of Corporate Reorganization (April 2001) 

122.  David A. Weisbach, Ten Truths about Tax Shelters (May 2001) 

42 

 

123.  William M. Landes, What Has the Visual Arts Rights Act of 1990 Accomplished? 

124.  Cass R. Sunstein, Social and Economic Rights? Lessons from South Africa (May 

125.  Christopher Avery, Christine Jolls, Richard A. Posner, and Alvin E. Roth, The 

Market for Federal Judicial Law Clerks (June 2001)   

126.  Douglas G. Baird and Edward R. Morrison, Bankruptcy Decision Making (June 

(May 2001) 

2001) 

2001) 

127.  Cass R. Sunstein, Regulating Risks after ATA (June 2001) 
128.   Cass R. Sunstein, The Laws of Fear (June 2001) 
129.  Richard A. Epstein, In and Out of Public Solution: The Hidden Perils of Property 

Transfer (July 2001) 

130.  Randal C. Picker, Pursuing a Remedy in Microsoft: The Declining Need for 

Centralized Coordination in a Networked World (July 2001) 

131.  Cass R. Sunstein, Daniel Kahneman, David Schkade, and Ilana Ritov, Predictably 

132. 
133. 

Incoherent Judgments (July 2001) 
Eric A. Posner, Courts Should Not Enforce Government Contracts (August 2001) 
Lisa  Bernstein,  Private  Commercial  Law  in  the  Cotton  Industry:  Creating 
Cooperation through Rules, Norms, and Institutions (August 2001) 

134.  Richard A. Epstein, The Allocation of the Commons: Parking and Stopping on 

the Commons (August 2001) 

135.  Cass R. Sunstein, The Arithmetic of Arsenic (September 2001) 
136. 

Eric A. Posner, Richard Hynes, and Anup Malani, The Political Economy of 
Property Exemption Laws (September 2001) 
Eric A. Posner and George G. Triantis, Covenants Not to Compete from an 
Incomplete Contracts Perspective (September 2001) 

137. 

138.  Cass R. Sunstein, Probability Neglect: Emotions, Worst Cases, and Law 

(November 2001) 

139.  Randall S. Kroszner and Philip E. Strahan, Throwing Good Money after Bad? 
Board Connections and Conflicts in Bank Lending (December 2001) 
140.  Alan O. Sykes, TRIPs, Pharmaceuticals, Developing Countries, and the Doha 

141. 

ASolution@ (February 2002) 
Edna Ullmann-Margalit and Cass R. Sunstein, Inequality and Indignation 
(February 2002) 

142.  Daniel N. Shaviro and David A. Weisbach, The Fifth Circuit Gets It Wrong in 

Compaq v. Commissioner (February 2002) (Published in Tax Notes, January 28, 
2002) 

143.  Warren F. Schwartz and Alan O. Sykes, The Economic Structure of Renegotiation 

and Dispute Resolution in the WTO/GATT System (March 2002, forthcoming 
Journal of Legal Studies 2002) 

144.  Richard A. Epstein, HIPAA on Privacy: Its Unintended and Intended 
Consequences (March 2002, forthcoming Cato Journal, summer 2002) 

43 

 

145.  David A. Weisbach, Thinking Outside the Little Boxes (March 2002, forthcoming 

146. 

Texas Law Review) 
Eric A. Posner, Economic Analysis of Contract Law after Three Decades: Success 
or Failure (March 2002) 

147.  Randal C. Picker, Copyright as Entry Policy: The Case of Digital Distribution 

(April 2002, forthcoming The Antitrust Bulletin) 

148.  David A. Weisbach, Taxes and Torts in the Redistribution of Income (April 2002, 

Coase Lecture February 2002) 

149.  Cass R. Sunstein, Beyond the Precautionary Principle (April 2002) 
150.  Robert W. Hahn and Cass R. Sunstein, A New Executive Order for Improving 

Federal Regulation? Deeper and Wider Cost-Benefit Analysis (April 2002) 

151.  Douglas Lichtman, Copyright as a Rule of Evidence (May 2002) 
152.  Richard A. Epstein, Steady the Course: Property Rights in Genetic Material (May 

153. 

2002) 
Jack Goldsmith and Cass R. Sunstein, Military Tribunals and Legal Culture: What 
a Difference Sixty Years Makes (June 2002) 

154.  William M. Landes and Richard A. Posner, Indefinitely Renewable Copyright 

(July 2002) 

155.  Anne Gron and Alan O. Sykes, Terrorism and Insurance Markets: A Role for the 

Government as Insurer? (July 2002) 

156.  Cass R. Sunstein and Adrian Vermeule, Interpretation and Institutions (July 2002) 
157.   Cass R. Sunstein, The Rights of Animals: A Very Short Primer (August 2002) 
158.  Cass R. Sunstein, Avoiding Absurdity? A New Canon in Regulatory Law (with 

Notes on Interpretive Theory) (August 2002) 

159.  Randal C. Picker, From Edison to the Broadcast Flag: Mechanisms of Consent 

162. 

160. 
161 

and Refusal and the Propertization of Copyright (September 2002) 
Eric A. Posner, A Theory of the Laws of War (September 2002) 
Eric A. Posner, Probability Errors: Some Positive and Normative Implications for 
Tort and Contract Law (September 2002) 
Lior Jacob Strahilevitz, Charismatic Code, Social Norms, and the Emergence of 
Cooperation on the File-Swapping Networks (September 2002) 
163.  David A. Weisbach, Does the X-Tax Mark the Spot? (September 2002) 
164.  Cass R. Sunstein, Conformity and Dissent (September 2002) 
165.  Cass R. Sunstein, Hazardous Heuristics (October 2002) 

 

44 

