University of Chicago Law School ersity
Cass R. Sunstein
The legal culture contains countless discussions of the vices and virtues of cost-benefit analysis.' But it has hardly any explorations of the real world of cost-benefit analysis-of what agencies do, concretely, when they do costbenefit analysis. We know very little about the interactions among law, science, and economics in the actual, rather than theoretical, world of cost-benefit balancing.
The Arithmetic of Arsenic2 represents a modest effort to begin to fill this gap. My goal was to explore not whether cost-benefit is a good idea in the abstract, but what it means in practice. In fact my major purposes were descriptive and conceptual, not normative. I wanted to highlight a topic about which lawyers and judges know too little: the specific questions whose answers can drive "benefits numbers" in one or another direction. For low levels of arsenic, we lack clear knowledge of the likely health benefits, and any extrapolation from the data will depend on some disputable judgments, involving above all: (1) the dose-response curve, (2) the use of data from other nations, and (3) the decision whether to quantify certain risks on the basis of highly speculative evidence. Monetization of benefits raises further problems involving the valuation of both life and health and the appropriate discount rate for cancers to be averted in the future. With an understanding of these problems, we can see exactly why there are reasonable disputes about both nonmonetized and monetized benefits-why some estimates are high and others are lower. We can also isolate the issues on which more progress, both conceptual and empirical, might be made in the future.
In identifying those issues, I did not seek to resolve them, much less say how the arsenic controversy should be handled (though in a few places I offered some tentative judgments). I meant instead to provide a kind of lawyer's primer on the real world of cost-benefit analysis, in the process showing where legal objections might be raised, explaining why a deferential judicial role might make sense, and indicating why a "benefits range," and a wide one at that, is sometimes the most that can be offered.
I am extremely grateful to Lisa Heinzerling and Thomas McGarity for their valuable comments and criticisms. Their objections seem to me to fall into two
categories-the first involving the assessment of benefits (my major theme, and a topic emphasized by Heinzerling), the second involving the assessment of cost-benefit analysis itself (which both of them dislike). In this brief reply, I take up these two kinds of criticisms in sequence.
Much of Heinzerling's response urges that the EPA should have placed a higher monetary value on life. She emphasizes, for example, that the value-oflife data is quite old (from the 1970s). In her view, per capita growth in economic activity requires a significant upward adjustment.4 She also contends that the EPA should have used a cancer premium on the ground that cancer risks receive a higher willingness to pay than other deaths. These are entirely plausible claims and well worth further investigation.
But some of Heinzerling's other suggestions seem to me more doubtful. She contends, for example, that because cancer is involved, the EPA was wrong to have concluded that the benefits of the arsenic rule should be discounted because cancer arises decades after exposure to its cause.6 To be sure, it is hard to know how the discounting problem should be handled. But isn't it much worse to get cancer tomorrow than to get cancer twenty years from tomorrow? Isn't it much worse to die in a plane crash tomorrow than to die from cancer twenty years from tomorrow? Heinzerling is right to emphasize the existence of a cancer premium. But even with that premium, it seems reasonable to discount risks that will not turn into harms for many decades.
I also wonder about Heinzerling's suggestion that the EPA should have
assessed the benefits of reducing arsenic in drinking water by examining the market for bottled water.8 As Heinzerling notes, many people buy bottled water for the taste, not for the increase in safety. In any case, most people do not buy bottled water. It would be extremely odd for government to use the Safe Drinking Water Act to do the equivalent of forcing people to buy bottled water when this option is rejected by most people. (Should the National Highway Safety Traffic Administration require all car manufacturers to spend the amount spent on safety by Volvo owners?)
Heinzerling also suggests that instead of using a number for statistical lives, the EPA might have done better to survey consumers, asking them how much they would be willing to pay to reduce arsenic levels in drinking She thinks that such a survey would reveal that people would be willing to pay more than enough for significant reductions. As it happens, I conducted just such a survey with University of Chicago law students. The survey produced a median willingness to pay $100 to eliminate a 1/100,000 risk and a median willingness to pay $50 to eliminate a 1/1,000,000 risk.' These numbers, among a fairly affluent group, do not suggest that the EPA's choice was as easy as Heinzerling indicates, because the mean cost per household, for the 10 ppb standard, ranged from under $1 to over $300.11 Of course there are serious doubts about the meaningfulness of people's answers to survey questions of this kind, especially when low probabilities are involved. Hence it was reasonable, it seems to me, for the EPA (assuming that it was interested in willingness to pay) to use its standard dollar amount per life saved, rather than to rely on surveys.
McGarity emphasizes the EPA's legitimate difficulties in handling adverse effects that it considered nonquantifiable, including certain cancers, hypertension, diabetes, and reproductive effects. He is concerned that these effects "fade in and out of' my analysis "like an aircraft warning beacon"-and also that
once all of them are included, my low-end estimates "are too low" and my high-end estimates "may only be slightly too high or perhaps not high enough."13 All of these points are reasonable. My basic suggestion is that benefits should be quantified if this is at all possible.' In general, the EPA should assign numbers to adverse effects whenever it can, if only to provide a range, and it should have done more in this vein here. In this case, as in many others, efforts at quantification illuminate the analysis, even when there is uncertainty. '
Heinzerling dislikes the willingness-to-pay criterion. Her most interesting claim is that market behavior is likely to be inadequately informed, not least because people are "intuitive toxicologists," acting in the market domain as they do in politics. For this reason she doubts that market evidence reveals people's informed tradeoffs between dollars and statistical risks. Heinzerling is right to say that cognitive problems and simple ignorance play a role in the market domain her Perrier example might well be a case in point. The real question, an empirical one, is whether some correctives might not be provided by the sheer number of people involved in market decisions. Many of us do not know whether products we buy will do what they are supposed to do yet the knowledge of some is enough to ensure, most of the time at least, that there is a correlation between quality and cost. In the domain of risks, it is possible that market processes will work in the same way, ensuring, much of the time, that risky products will cost more than less risky ones, and that other things being equal, employers will have to pay people a wage premium for higher risks. The evidence here is suggestive if not conclusive, indicating that in markets, at least, people will not pay infinite or even huge amounts to eliminate low-level cancer risks. Indeed the amounts they are willing to pay fall within a certain range, one that is wide but far from unbounded.' Of course, Heinzerling is right to question whether an examination of market behavior can uncover informed judgments about the price of a statistical life.
II.
To say the least, Heinzerling and McGarity are skeptics about cost-benefit analysis. I do not share their skepticism. In my view, cost-benefit analysis should be seen as a valuable tool, partly as a way to counteract the errors that we all make in thinking about risk, partly as a mechanism for ensuring that government addresses serious problems rather than trivial ones, partly as a way
of getting a sense of the consequences in front of decisionmakers before they act. Is this so terribly controversial? Is it not revealing that President Clinton, no less than Presidents Reagan and Bush, required agencies to produce costbenefit analyses of major rules?
To be sure, cost-benefit analysis may be unhelpful or misused in practice, simply because of the indeterminacy in the data. A lurking question, pressed by both Heinzerling and McGarity, is whether the arsenic controversy supports or undermines the argument for cost-benefit analysis. I think that the analysis was helpful, simply because it showed the range of potential effects from low levels of arsenic exposure. The analysis greatly weakened the efforts of interest groups and others to treat the old 50 ppb standard as unexceptionable. After the EPA did its work, no one could responsibly contend that it would be silly to rethink the old standard. Indeed, the analysis--especially taken in light of the 2001 report from the National Research Council '-helped show that the 10 ppb standard would not be exceedingly costly and likely would be a real improvement, in terms of public health, over the 50 ppb standard. At the same time, the analysis showed why reasonable people might be uncertain whether there would be very large public health gains from a standard below (say) 20 ppb. (Note in this regard that Canada, a nation not unconcerned with the health
Heinzerling's attack on cost-benefit analysis seems to be based not on a belief that costs and benefits are irrelevant, but on the willingness-to-pay criterion, which she identifies with cost-benefit analysis. The identification is understandable: Those who do cost-benefit analysis generally use that criterion. But to engage in cost-benefit analysis, we can value benefits however we like. In the 1960s and 1970s, it was popular to use the "human capital" approach, which values life by looking at lost earnings.23 This method, still popular within the courts, produced amounts that were a mere fraction of those elicited by willingness-to-pay methods. Or suppose that a contemporary analyst rejected willingness to pay and instead chose, for her own reasons, $15 million as the presumptive amount to spend per statistical life-with the presumptive amount subject to increase or decrease if the circumstances warranted either.24 Such an analyst could still compare costs and benefits. We might even start with the amounts that emerge from labor market studies, not because we have a deep
commitment to willingness to pay, but because of a democratic judgment that those amounts provide a reasonable place to begin. What seems to me most important is not to be dogmatic about willingness to pay, but to ensure against uninformed stabs in the dark and to promote coherence and sense in regulation-so that we are not spending small sums on large problems and large sums on small problems.
McGarity thinks that the largest lesson of the arsenic controversy involves the "daunting scientific uncertainties" that plague cost-benefit analysis.25 In view of these uncertainties, he thinks that cost-benefit analysis is a matter of "frequently preposterous and always manipulable number spinning.' He has a point. But it need not be a matter of "spinning." We might try instead to identify the likely range of effects, and when the range is large, we might want to know why the benefits might be small and why they might be big. If we are choosing among several levels of protection, what should we do instead? Guess? Flip a coin? Does it really make sense to conclude (as both Heinzerling and McGarity seem to do) that whenever people are being exposed to a carcinogen (any carcinogen?), the government should regulate to the point that is "feasible" for industry28-- even if the cost is (say) $900 billion and the health benefits are likely.to be trivial?
I suspect that both Heinzerling and McGarity think that cost-benefit analysis fails cost-benefit analysis. They believe that all things considered, we would be better off with some other standard-for example, basing standards on feasibility. They might be right! If cost-benefit analysis simply makes it harder for agencies to protect the public, and mostly increases the power of regulated groups to block desirable regulation, it is hard to celebrate cost-benefit analysis. But there are reasons to believe that cost-benefit analysis is not simply an antiregulatory tool. Indeed cost-benefit analysis helped spur the removal of lead from gasoline and dramatic steps, pushed by the United States, to eliminate
